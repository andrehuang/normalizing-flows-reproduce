{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1152,
     "status": "ok",
     "timestamp": 1616434247721,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "Hs9ruTPs4eON",
    "outputId": "1091285c-80db-48cb-c1e1-7c9f978bc108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3208,
     "status": "ok",
     "timestamp": 1616434249783,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "cY4kKbnm4-A6",
    "outputId": "e13aaeb4-d7f6-4ef7-a518-ef4e54582026"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3206,
     "status": "ok",
     "timestamp": 1616434249785,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "dPHv5ij84_Kk",
    "outputId": "d0b2baa1-3e2a-4453-de31-fddf275c904f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "cd \"/content/drive/MyDrive/Colab Notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792594,
     "status": "ok",
     "timestamp": 1616441039177,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "TTq6LxY65AxF",
    "outputId": "1546c589-e9f3-4b4b-bafe-a7c06feb9a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3346\n",
      "Epoch: 378 [  100/50000 ( 0%)]  \tLoss:   89.817009\trec:   64.313507\tkl:   25.503496\n",
      "Epoch: 378 [10100/50000 (20%)]  \tLoss:   91.310097\trec:   64.763420\tkl:   26.546684\n",
      "Epoch: 378 [20100/50000 (40%)]  \tLoss:   90.967476\trec:   64.438637\tkl:   26.528843\n",
      "Epoch: 378 [30100/50000 (60%)]  \tLoss:   94.770439\trec:   68.099121\tkl:   26.671312\n",
      "Epoch: 378 [40100/50000 (80%)]  \tLoss:   91.685913\trec:   66.029854\tkl:   25.656063\n",
      "====> Epoch: 378 Average train loss: 92.4645\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2872\n",
      "Epoch: 379 [  100/50000 ( 0%)]  \tLoss:   88.674568\trec:   63.819164\tkl:   24.855408\n",
      "Epoch: 379 [10100/50000 (20%)]  \tLoss:   91.840034\trec:   66.092285\tkl:   25.747755\n",
      "Epoch: 379 [20100/50000 (40%)]  \tLoss:   92.453232\trec:   66.733360\tkl:   25.719872\n",
      "Epoch: 379 [30100/50000 (60%)]  \tLoss:   91.625565\trec:   66.689743\tkl:   24.935822\n",
      "Epoch: 379 [40100/50000 (80%)]  \tLoss:   93.019806\trec:   67.271904\tkl:   25.747898\n",
      "====> Epoch: 379 Average train loss: 92.4323\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2560\n",
      "Epoch: 380 [  100/50000 ( 0%)]  \tLoss:   92.555779\trec:   66.847832\tkl:   25.707951\n",
      "Epoch: 380 [10100/50000 (20%)]  \tLoss:   93.133789\trec:   67.574043\tkl:   25.559738\n",
      "Epoch: 380 [20100/50000 (40%)]  \tLoss:   93.037773\trec:   66.804199\tkl:   26.233564\n",
      "Epoch: 380 [30100/50000 (60%)]  \tLoss:   95.656425\trec:   69.543594\tkl:   26.112827\n",
      "Epoch: 380 [40100/50000 (80%)]  \tLoss:   93.498398\trec:   68.236053\tkl:   25.262339\n",
      "====> Epoch: 380 Average train loss: 92.4556\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2731\n",
      "Epoch: 381 [  100/50000 ( 0%)]  \tLoss:   93.056152\trec:   66.722115\tkl:   26.334036\n",
      "Epoch: 381 [10100/50000 (20%)]  \tLoss:   91.696678\trec:   65.729736\tkl:   25.966936\n",
      "Epoch: 381 [20100/50000 (40%)]  \tLoss:   92.791359\trec:   66.723183\tkl:   26.068171\n",
      "Epoch: 381 [30100/50000 (60%)]  \tLoss:   90.964401\trec:   65.869255\tkl:   25.095144\n",
      "Epoch: 381 [40100/50000 (80%)]  \tLoss:   96.447365\trec:   69.970367\tkl:   26.476994\n",
      "====> Epoch: 381 Average train loss: 92.4497\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3480\n",
      "Epoch: 382 [  100/50000 ( 0%)]  \tLoss:   86.643730\trec:   61.263611\tkl:   25.380121\n",
      "Epoch: 382 [10100/50000 (20%)]  \tLoss:   91.590126\trec:   65.583565\tkl:   26.006561\n",
      "Epoch: 382 [20100/50000 (40%)]  \tLoss:   93.852158\trec:   67.548592\tkl:   26.303564\n",
      "Epoch: 382 [30100/50000 (60%)]  \tLoss:   91.756210\trec:   66.407578\tkl:   25.348629\n",
      "Epoch: 382 [40100/50000 (80%)]  \tLoss:   93.503357\trec:   67.923386\tkl:   25.579975\n",
      "====> Epoch: 382 Average train loss: 92.4371\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3871\n",
      "Epoch: 383 [  100/50000 ( 0%)]  \tLoss:   90.270317\trec:   64.519485\tkl:   25.750837\n",
      "Epoch: 383 [10100/50000 (20%)]  \tLoss:   97.830460\trec:   70.560280\tkl:   27.270176\n",
      "Epoch: 383 [20100/50000 (40%)]  \tLoss:   95.624809\trec:   68.330208\tkl:   27.294607\n",
      "Epoch: 383 [30100/50000 (60%)]  \tLoss:   90.557236\trec:   65.416573\tkl:   25.140661\n",
      "Epoch: 383 [40100/50000 (80%)]  \tLoss:   96.474625\trec:   69.376144\tkl:   27.098480\n",
      "====> Epoch: 383 Average train loss: 92.4157\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3819\n",
      "Epoch: 384 [  100/50000 ( 0%)]  \tLoss:   96.653908\trec:   70.626343\tkl:   26.027561\n",
      "Epoch: 384 [10100/50000 (20%)]  \tLoss:   89.503502\trec:   63.609291\tkl:   25.894211\n",
      "Epoch: 384 [20100/50000 (40%)]  \tLoss:   94.917198\trec:   68.819382\tkl:   26.097815\n",
      "Epoch: 384 [30100/50000 (60%)]  \tLoss:   92.064209\trec:   66.500908\tkl:   25.563303\n",
      "Epoch: 384 [40100/50000 (80%)]  \tLoss:   89.908188\trec:   63.554764\tkl:   26.353428\n",
      "====> Epoch: 384 Average train loss: 92.4252\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2731\n",
      "Epoch: 385 [  100/50000 ( 0%)]  \tLoss:   92.102203\trec:   66.061226\tkl:   26.040976\n",
      "Epoch: 385 [10100/50000 (20%)]  \tLoss:   96.112091\trec:   69.720680\tkl:   26.391411\n",
      "Epoch: 385 [20100/50000 (40%)]  \tLoss:   89.325424\trec:   63.515263\tkl:   25.810165\n",
      "Epoch: 385 [30100/50000 (60%)]  \tLoss:   92.593788\trec:   66.908302\tkl:   25.685478\n",
      "Epoch: 385 [40100/50000 (80%)]  \tLoss:   90.851089\trec:   64.661400\tkl:   26.189688\n",
      "====> Epoch: 385 Average train loss: 92.4092\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3687\n",
      "Epoch: 386 [  100/50000 ( 0%)]  \tLoss:   92.954002\trec:   66.224411\tkl:   26.729593\n",
      "Epoch: 386 [10100/50000 (20%)]  \tLoss:   91.232887\trec:   65.950615\tkl:   25.282267\n",
      "Epoch: 386 [20100/50000 (40%)]  \tLoss:   92.333580\trec:   66.298195\tkl:   26.035393\n",
      "Epoch: 386 [30100/50000 (60%)]  \tLoss:   94.387047\trec:   68.480026\tkl:   25.907021\n",
      "Epoch: 386 [40100/50000 (80%)]  \tLoss:   92.738419\trec:   66.324913\tkl:   26.413498\n",
      "====> Epoch: 386 Average train loss: 92.4031\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2334\n",
      "Epoch: 387 [  100/50000 ( 0%)]  \tLoss:   93.112793\trec:   66.963455\tkl:   26.149338\n",
      "Epoch: 387 [10100/50000 (20%)]  \tLoss:   92.295021\trec:   66.050293\tkl:   26.244720\n",
      "Epoch: 387 [20100/50000 (40%)]  \tLoss:   98.046753\trec:   71.786201\tkl:   26.260555\n",
      "Epoch: 387 [30100/50000 (60%)]  \tLoss:   92.928627\trec:   67.056931\tkl:   25.871700\n",
      "Epoch: 387 [40100/50000 (80%)]  \tLoss:   95.974709\trec:   68.637131\tkl:   27.337580\n",
      "====> Epoch: 387 Average train loss: 92.4030\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2695\n",
      "Epoch: 388 [  100/50000 ( 0%)]  \tLoss:   94.873650\trec:   68.690544\tkl:   26.183107\n",
      "Epoch: 388 [10100/50000 (20%)]  \tLoss:   92.206001\trec:   66.510315\tkl:   25.695688\n",
      "Epoch: 388 [20100/50000 (40%)]  \tLoss:   91.422798\trec:   65.882820\tkl:   25.539978\n",
      "Epoch: 388 [30100/50000 (60%)]  \tLoss:   92.718224\trec:   66.685349\tkl:   26.032875\n",
      "Epoch: 388 [40100/50000 (80%)]  \tLoss:   89.811951\trec:   64.363075\tkl:   25.448879\n",
      "====> Epoch: 388 Average train loss: 92.4020\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.4083\n",
      "Epoch: 389 [  100/50000 ( 0%)]  \tLoss:   93.074379\trec:   67.589607\tkl:   25.484772\n",
      "Epoch: 389 [10100/50000 (20%)]  \tLoss:   92.017982\trec:   66.102974\tkl:   25.915007\n",
      "Epoch: 389 [20100/50000 (40%)]  \tLoss:   91.986755\trec:   65.406807\tkl:   26.579948\n",
      "Epoch: 389 [30100/50000 (60%)]  \tLoss:   95.682579\trec:   68.618385\tkl:   27.064186\n",
      "Epoch: 389 [40100/50000 (80%)]  \tLoss:   91.067703\trec:   65.383575\tkl:   25.684122\n",
      "====> Epoch: 389 Average train loss: 92.4079\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3581\n",
      "Epoch: 390 [  100/50000 ( 0%)]  \tLoss:   95.231033\trec:   68.710396\tkl:   26.520638\n",
      "Epoch: 390 [10100/50000 (20%)]  \tLoss:   90.767029\trec:   64.640358\tkl:   26.126669\n",
      "Epoch: 390 [20100/50000 (40%)]  \tLoss:   91.706581\trec:   65.548592\tkl:   26.157993\n",
      "Epoch: 390 [30100/50000 (60%)]  \tLoss:   91.395981\trec:   65.212486\tkl:   26.183495\n",
      "Epoch: 390 [40100/50000 (80%)]  \tLoss:   94.156502\trec:   67.728943\tkl:   26.427561\n",
      "====> Epoch: 390 Average train loss: 92.3893\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3179\n",
      "Epoch: 391 [  100/50000 ( 0%)]  \tLoss:   96.399200\trec:   69.288994\tkl:   27.110203\n",
      "Epoch: 391 [10100/50000 (20%)]  \tLoss:   85.463799\trec:   59.884983\tkl:   25.578815\n",
      "Epoch: 391 [20100/50000 (40%)]  \tLoss:   90.317696\trec:   65.160690\tkl:   25.157007\n",
      "Epoch: 391 [30100/50000 (60%)]  \tLoss:   90.251259\trec:   65.109039\tkl:   25.142214\n",
      "Epoch: 391 [40100/50000 (80%)]  \tLoss:   89.965767\trec:   63.500233\tkl:   26.465536\n",
      "====> Epoch: 391 Average train loss: 92.3674\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3285\n",
      "Epoch: 392 [  100/50000 ( 0%)]  \tLoss:   94.596153\trec:   67.874336\tkl:   26.721819\n",
      "Epoch: 392 [10100/50000 (20%)]  \tLoss:   93.149002\trec:   67.215485\tkl:   25.933517\n",
      "Epoch: 392 [20100/50000 (40%)]  \tLoss:   94.440636\trec:   67.461609\tkl:   26.979023\n",
      "Epoch: 392 [30100/50000 (60%)]  \tLoss:   93.328194\trec:   67.612350\tkl:   25.715839\n",
      "Epoch: 392 [40100/50000 (80%)]  \tLoss:   92.004883\trec:   66.466843\tkl:   25.538036\n",
      "====> Epoch: 392 Average train loss: 92.3997\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2774\n",
      "Epoch: 393 [  100/50000 ( 0%)]  \tLoss:   92.030678\trec:   66.326515\tkl:   25.704174\n",
      "Epoch: 393 [10100/50000 (20%)]  \tLoss:   89.171196\trec:   64.218170\tkl:   24.953030\n",
      "Epoch: 393 [20100/50000 (40%)]  \tLoss:   95.744987\trec:   69.232803\tkl:   26.512182\n",
      "Epoch: 393 [30100/50000 (60%)]  \tLoss:   91.943146\trec:   65.956779\tkl:   25.986366\n",
      "Epoch: 393 [40100/50000 (80%)]  \tLoss:   92.711456\trec:   66.989914\tkl:   25.721535\n",
      "====> Epoch: 393 Average train loss: 92.3790\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2448\n",
      "Epoch: 394 [  100/50000 ( 0%)]  \tLoss:   88.996971\trec:   64.355812\tkl:   24.641159\n",
      "Epoch: 394 [10100/50000 (20%)]  \tLoss:   92.344757\trec:   66.583061\tkl:   25.761686\n",
      "Epoch: 394 [20100/50000 (40%)]  \tLoss:   96.174683\trec:   68.905281\tkl:   27.269402\n",
      "Epoch: 394 [30100/50000 (60%)]  \tLoss:   94.297478\trec:   68.133545\tkl:   26.163927\n",
      "Epoch: 394 [40100/50000 (80%)]  \tLoss:   93.959053\trec:   68.144554\tkl:   25.814499\n",
      "====> Epoch: 394 Average train loss: 92.3608\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1635\n",
      "Epoch: 395 [  100/50000 ( 0%)]  \tLoss:   90.935730\trec:   64.859558\tkl:   26.076174\n",
      "Epoch: 395 [10100/50000 (20%)]  \tLoss:   89.613884\trec:   64.522308\tkl:   25.091581\n",
      "Epoch: 395 [20100/50000 (40%)]  \tLoss:   89.921524\trec:   64.665085\tkl:   25.256439\n",
      "Epoch: 395 [30100/50000 (60%)]  \tLoss:   93.189919\trec:   66.206688\tkl:   26.983234\n",
      "Epoch: 395 [40100/50000 (80%)]  \tLoss:   88.559639\trec:   62.811699\tkl:   25.747936\n",
      "====> Epoch: 395 Average train loss: 92.3496\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1530\n",
      "Epoch: 396 [  100/50000 ( 0%)]  \tLoss:   92.896873\trec:   68.180595\tkl:   24.716276\n",
      "Epoch: 396 [10100/50000 (20%)]  \tLoss:   94.323143\trec:   68.482086\tkl:   25.841059\n",
      "Epoch: 396 [20100/50000 (40%)]  \tLoss:   88.027618\trec:   63.674500\tkl:   24.353119\n",
      "Epoch: 396 [30100/50000 (60%)]  \tLoss:   92.433647\trec:   66.464561\tkl:   25.969091\n",
      "Epoch: 396 [40100/50000 (80%)]  \tLoss:   93.587479\trec:   67.004364\tkl:   26.583113\n",
      "====> Epoch: 396 Average train loss: 92.3354\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1534\n",
      "Epoch: 397 [  100/50000 ( 0%)]  \tLoss:   93.115097\trec:   66.385994\tkl:   26.729095\n",
      "Epoch: 397 [10100/50000 (20%)]  \tLoss:   89.378540\trec:   64.006935\tkl:   25.371611\n",
      "Epoch: 397 [20100/50000 (40%)]  \tLoss:   91.880577\trec:   64.759758\tkl:   27.120817\n",
      "Epoch: 397 [30100/50000 (60%)]  \tLoss:   94.061020\trec:   68.482994\tkl:   25.578028\n",
      "Epoch: 397 [40100/50000 (80%)]  \tLoss:   92.009438\trec:   66.346298\tkl:   25.663141\n",
      "====> Epoch: 397 Average train loss: 92.3299\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2852\n",
      "Epoch: 398 [  100/50000 ( 0%)]  \tLoss:   90.573814\trec:   64.820328\tkl:   25.753487\n",
      "Epoch: 398 [10100/50000 (20%)]  \tLoss:   90.543823\trec:   65.307289\tkl:   25.236540\n",
      "Epoch: 398 [20100/50000 (40%)]  \tLoss:   92.243599\trec:   66.948853\tkl:   25.294748\n",
      "Epoch: 398 [30100/50000 (60%)]  \tLoss:   92.368355\trec:   66.205002\tkl:   26.163359\n",
      "Epoch: 398 [40100/50000 (80%)]  \tLoss:   89.045448\trec:   64.424309\tkl:   24.621145\n",
      "====> Epoch: 398 Average train loss: 92.3146\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3674\n",
      "Epoch: 399 [  100/50000 ( 0%)]  \tLoss:   90.992958\trec:   65.503067\tkl:   25.489887\n",
      "Epoch: 399 [10100/50000 (20%)]  \tLoss:   96.730118\trec:   70.371620\tkl:   26.358498\n",
      "Epoch: 399 [20100/50000 (40%)]  \tLoss:   93.825859\trec:   66.468605\tkl:   27.357256\n",
      "Epoch: 399 [30100/50000 (60%)]  \tLoss:   95.441833\trec:   69.176720\tkl:   26.265114\n",
      "Epoch: 399 [40100/50000 (80%)]  \tLoss:   92.820366\trec:   66.265358\tkl:   26.555012\n",
      "====> Epoch: 399 Average train loss: 92.3197\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2936\n",
      "Epoch: 400 [  100/50000 ( 0%)]  \tLoss:   91.231384\trec:   65.302399\tkl:   25.928989\n",
      "Epoch: 400 [10100/50000 (20%)]  \tLoss:   91.670021\trec:   65.248352\tkl:   26.421659\n",
      "Epoch: 400 [20100/50000 (40%)]  \tLoss:   94.132217\trec:   68.218155\tkl:   25.914062\n",
      "Epoch: 400 [30100/50000 (60%)]  \tLoss:   87.272026\trec:   62.127163\tkl:   25.144863\n",
      "Epoch: 400 [40100/50000 (80%)]  \tLoss:   90.694283\trec:   64.932281\tkl:   25.762001\n",
      "====> Epoch: 400 Average train loss: 92.2934\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2114\n",
      "Epoch: 401 [  100/50000 ( 0%)]  \tLoss:   93.217598\trec:   67.928230\tkl:   25.289362\n",
      "Epoch: 401 [10100/50000 (20%)]  \tLoss:   93.217567\trec:   66.929832\tkl:   26.287731\n",
      "Epoch: 401 [20100/50000 (40%)]  \tLoss:   94.020798\trec:   67.449028\tkl:   26.571762\n",
      "Epoch: 401 [30100/50000 (60%)]  \tLoss:   92.660240\trec:   65.741646\tkl:   26.918591\n",
      "Epoch: 401 [40100/50000 (80%)]  \tLoss:   93.691422\trec:   67.973717\tkl:   25.717705\n",
      "====> Epoch: 401 Average train loss: 92.3213\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2648\n",
      "Epoch: 402 [  100/50000 ( 0%)]  \tLoss:   93.739159\trec:   67.231133\tkl:   26.508030\n",
      "Epoch: 402 [10100/50000 (20%)]  \tLoss:   90.350662\trec:   65.231735\tkl:   25.118925\n",
      "Epoch: 402 [20100/50000 (40%)]  \tLoss:   94.319351\trec:   67.947693\tkl:   26.371662\n",
      "Epoch: 402 [30100/50000 (60%)]  \tLoss:   95.107033\trec:   67.964211\tkl:   27.142822\n",
      "Epoch: 402 [40100/50000 (80%)]  \tLoss:   94.812683\trec:   69.295250\tkl:   25.517429\n",
      "====> Epoch: 402 Average train loss: 92.2975\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2989\n",
      "Epoch: 403 [  100/50000 ( 0%)]  \tLoss:   96.455475\trec:   69.309875\tkl:   27.145603\n",
      "Epoch: 403 [10100/50000 (20%)]  \tLoss:   91.425537\trec:   65.334930\tkl:   26.090603\n",
      "Epoch: 403 [20100/50000 (40%)]  \tLoss:   90.837677\trec:   65.104446\tkl:   25.733225\n",
      "Epoch: 403 [30100/50000 (60%)]  \tLoss:   94.156921\trec:   67.319168\tkl:   26.837751\n",
      "Epoch: 403 [40100/50000 (80%)]  \tLoss:   92.469688\trec:   66.015289\tkl:   26.454390\n",
      "====> Epoch: 403 Average train loss: 92.2950\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2354\n",
      "Epoch: 404 [  100/50000 ( 0%)]  \tLoss:   93.685875\trec:   67.844658\tkl:   25.841215\n",
      "Epoch: 404 [10100/50000 (20%)]  \tLoss:   91.559021\trec:   65.621437\tkl:   25.937588\n",
      "Epoch: 404 [20100/50000 (40%)]  \tLoss:   88.943748\trec:   62.911755\tkl:   26.031990\n",
      "Epoch: 404 [30100/50000 (60%)]  \tLoss:   91.089760\trec:   65.242065\tkl:   25.847694\n",
      "Epoch: 404 [40100/50000 (80%)]  \tLoss:   91.684601\trec:   64.957306\tkl:   26.727295\n",
      "====> Epoch: 404 Average train loss: 92.2826\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2011\n",
      "Epoch: 405 [  100/50000 ( 0%)]  \tLoss:   89.565620\trec:   65.035904\tkl:   24.529711\n",
      "Epoch: 405 [10100/50000 (20%)]  \tLoss:   94.818085\trec:   68.504166\tkl:   26.313923\n",
      "Epoch: 405 [20100/50000 (40%)]  \tLoss:   93.194275\trec:   66.846230\tkl:   26.348049\n",
      "Epoch: 405 [30100/50000 (60%)]  \tLoss:   96.537628\trec:   69.972862\tkl:   26.564768\n",
      "Epoch: 405 [40100/50000 (80%)]  \tLoss:   88.373077\trec:   63.008759\tkl:   25.364313\n",
      "====> Epoch: 405 Average train loss: 92.2821\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1738\n",
      "Epoch: 406 [  100/50000 ( 0%)]  \tLoss:   94.595314\trec:   67.796356\tkl:   26.798960\n",
      "Epoch: 406 [10100/50000 (20%)]  \tLoss:   94.642807\trec:   68.350861\tkl:   26.291950\n",
      "Epoch: 406 [20100/50000 (40%)]  \tLoss:   88.503471\trec:   63.057732\tkl:   25.445736\n",
      "Epoch: 406 [30100/50000 (60%)]  \tLoss:   88.459084\trec:   63.185555\tkl:   25.273529\n",
      "Epoch: 406 [40100/50000 (80%)]  \tLoss:   94.955009\trec:   68.742508\tkl:   26.212502\n",
      "====> Epoch: 406 Average train loss: 92.2687\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1522\n",
      "Epoch: 407 [  100/50000 ( 0%)]  \tLoss:   89.236504\trec:   64.728065\tkl:   24.508432\n",
      "Epoch: 407 [10100/50000 (20%)]  \tLoss:   94.974709\trec:   68.759766\tkl:   26.214941\n",
      "Epoch: 407 [20100/50000 (40%)]  \tLoss:   87.964371\trec:   62.394344\tkl:   25.570026\n",
      "Epoch: 407 [30100/50000 (60%)]  \tLoss:   99.344780\trec:   71.761711\tkl:   27.583078\n",
      "Epoch: 407 [40100/50000 (80%)]  \tLoss:   90.091408\trec:   63.506084\tkl:   26.585321\n",
      "====> Epoch: 407 Average train loss: 92.2689\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1089\n",
      "Epoch: 408 [  100/50000 ( 0%)]  \tLoss:   90.459618\trec:   64.784088\tkl:   25.675524\n",
      "Epoch: 408 [10100/50000 (20%)]  \tLoss:   90.240623\trec:   64.172722\tkl:   26.067898\n",
      "Epoch: 408 [20100/50000 (40%)]  \tLoss:   90.554420\trec:   64.517654\tkl:   26.036770\n",
      "Epoch: 408 [30100/50000 (60%)]  \tLoss:   91.304131\trec:   65.330048\tkl:   25.974081\n",
      "Epoch: 408 [40100/50000 (80%)]  \tLoss:   94.478806\trec:   68.243530\tkl:   26.235275\n",
      "====> Epoch: 408 Average train loss: 92.2363\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1330\n",
      "Epoch: 409 [  100/50000 ( 0%)]  \tLoss:   94.224159\trec:   67.564522\tkl:   26.659634\n",
      "Epoch: 409 [10100/50000 (20%)]  \tLoss:   93.327774\trec:   67.424034\tkl:   25.903740\n",
      "Epoch: 409 [20100/50000 (40%)]  \tLoss:   90.329361\trec:   64.428467\tkl:   25.900896\n",
      "Epoch: 409 [30100/50000 (60%)]  \tLoss:   89.491844\trec:   64.741280\tkl:   24.750559\n",
      "Epoch: 409 [40100/50000 (80%)]  \tLoss:   93.766212\trec:   67.133385\tkl:   26.632824\n",
      "====> Epoch: 409 Average train loss: 92.2560\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2500\n",
      "Epoch: 410 [  100/50000 ( 0%)]  \tLoss:   92.278809\trec:   66.858650\tkl:   25.420160\n",
      "Epoch: 410 [10100/50000 (20%)]  \tLoss:   85.465996\trec:   60.856682\tkl:   24.609316\n",
      "Epoch: 410 [20100/50000 (40%)]  \tLoss:   91.152061\trec:   64.421249\tkl:   26.730810\n",
      "Epoch: 410 [30100/50000 (60%)]  \tLoss:   94.881866\trec:   68.547585\tkl:   26.334274\n",
      "Epoch: 410 [40100/50000 (80%)]  \tLoss:   90.174744\trec:   64.708282\tkl:   25.466459\n",
      "====> Epoch: 410 Average train loss: 92.2506\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1747\n",
      "Epoch: 411 [  100/50000 ( 0%)]  \tLoss:   89.553398\trec:   64.785881\tkl:   24.767513\n",
      "Epoch: 411 [10100/50000 (20%)]  \tLoss:   92.466011\trec:   66.892860\tkl:   25.573158\n",
      "Epoch: 411 [20100/50000 (40%)]  \tLoss:   93.817909\trec:   68.189819\tkl:   25.628086\n",
      "Epoch: 411 [30100/50000 (60%)]  \tLoss:   85.524445\trec:   61.002834\tkl:   24.521603\n",
      "Epoch: 411 [40100/50000 (80%)]  \tLoss:   89.911453\trec:   63.856831\tkl:   26.054621\n",
      "====> Epoch: 411 Average train loss: 92.2359\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2386\n",
      "Epoch: 412 [  100/50000 ( 0%)]  \tLoss:   88.761246\trec:   63.804062\tkl:   24.957188\n",
      "Epoch: 412 [10100/50000 (20%)]  \tLoss:   93.628914\trec:   67.412964\tkl:   26.215944\n",
      "Epoch: 412 [20100/50000 (40%)]  \tLoss:   89.856148\trec:   64.374283\tkl:   25.481865\n",
      "Epoch: 412 [30100/50000 (60%)]  \tLoss:   93.739601\trec:   67.001091\tkl:   26.738510\n",
      "Epoch: 412 [40100/50000 (80%)]  \tLoss:   90.026802\trec:   64.135674\tkl:   25.891134\n",
      "====> Epoch: 412 Average train loss: 92.2431\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2648\n",
      "Epoch: 413 [  100/50000 ( 0%)]  \tLoss:   89.518852\trec:   64.395210\tkl:   25.123642\n",
      "Epoch: 413 [10100/50000 (20%)]  \tLoss:   93.276619\trec:   67.775352\tkl:   25.501272\n",
      "Epoch: 413 [20100/50000 (40%)]  \tLoss:   92.630234\trec:   66.253258\tkl:   26.376980\n",
      "Epoch: 413 [30100/50000 (60%)]  \tLoss:   91.171738\trec:   65.278976\tkl:   25.892763\n",
      "Epoch: 413 [40100/50000 (80%)]  \tLoss:   92.712204\trec:   66.478073\tkl:   26.234127\n",
      "====> Epoch: 413 Average train loss: 92.2201\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.3485\n",
      "Epoch: 414 [  100/50000 ( 0%)]  \tLoss:   91.414871\trec:   65.101303\tkl:   26.313572\n",
      "Epoch: 414 [10100/50000 (20%)]  \tLoss:   92.893379\trec:   66.633415\tkl:   26.259970\n",
      "Epoch: 414 [20100/50000 (40%)]  \tLoss:   90.392662\trec:   65.173508\tkl:   25.219162\n",
      "Epoch: 414 [30100/50000 (60%)]  \tLoss:   92.520760\trec:   66.504448\tkl:   26.016310\n",
      "Epoch: 414 [40100/50000 (80%)]  \tLoss:   89.595055\trec:   63.579720\tkl:   26.015333\n",
      "====> Epoch: 414 Average train loss: 92.2307\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1487\n",
      "Epoch: 415 [  100/50000 ( 0%)]  \tLoss:   91.916924\trec:   66.184570\tkl:   25.732351\n",
      "Epoch: 415 [10100/50000 (20%)]  \tLoss:   89.803154\trec:   63.873573\tkl:   25.929583\n",
      "Epoch: 415 [20100/50000 (40%)]  \tLoss:   90.586510\trec:   64.836090\tkl:   25.750422\n",
      "Epoch: 415 [30100/50000 (60%)]  \tLoss:   88.615211\trec:   63.653698\tkl:   24.961514\n",
      "Epoch: 415 [40100/50000 (80%)]  \tLoss:   94.929245\trec:   68.091881\tkl:   26.837360\n",
      "====> Epoch: 415 Average train loss: 92.2037\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2238\n",
      "Epoch: 416 [  100/50000 ( 0%)]  \tLoss:   91.131210\trec:   64.632896\tkl:   26.498312\n",
      "Epoch: 416 [10100/50000 (20%)]  \tLoss:   93.861198\trec:   67.584572\tkl:   26.276628\n",
      "Epoch: 416 [20100/50000 (40%)]  \tLoss:   90.962875\trec:   64.299835\tkl:   26.663038\n",
      "Epoch: 416 [30100/50000 (60%)]  \tLoss:   88.426613\trec:   62.964504\tkl:   25.462101\n",
      "Epoch: 416 [40100/50000 (80%)]  \tLoss:   93.850288\trec:   68.103210\tkl:   25.747074\n",
      "====> Epoch: 416 Average train loss: 92.2044\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0820\n",
      "Epoch: 417 [  100/50000 ( 0%)]  \tLoss:   87.026642\trec:   62.043720\tkl:   24.982918\n",
      "Epoch: 417 [10100/50000 (20%)]  \tLoss:   92.628204\trec:   66.829208\tkl:   25.798990\n",
      "Epoch: 417 [20100/50000 (40%)]  \tLoss:   93.959488\trec:   67.811256\tkl:   26.148230\n",
      "Epoch: 417 [30100/50000 (60%)]  \tLoss:   91.966988\trec:   65.970894\tkl:   25.996094\n",
      "Epoch: 417 [40100/50000 (80%)]  \tLoss:   87.506210\trec:   62.690556\tkl:   24.815649\n",
      "====> Epoch: 417 Average train loss: 92.1837\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1304\n",
      "Epoch: 418 [  100/50000 ( 0%)]  \tLoss:   87.889503\trec:   63.027676\tkl:   24.861828\n",
      "Epoch: 418 [10100/50000 (20%)]  \tLoss:   93.716118\trec:   67.717003\tkl:   25.999123\n",
      "Epoch: 418 [20100/50000 (40%)]  \tLoss:   92.165382\trec:   66.119270\tkl:   26.046110\n",
      "Epoch: 418 [30100/50000 (60%)]  \tLoss:   87.014275\trec:   61.354641\tkl:   25.659630\n",
      "Epoch: 418 [40100/50000 (80%)]  \tLoss:   91.648476\trec:   65.422066\tkl:   26.226410\n",
      "====> Epoch: 418 Average train loss: 92.1733\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2563\n",
      "Epoch: 419 [  100/50000 ( 0%)]  \tLoss:   94.147583\trec:   67.687019\tkl:   26.460567\n",
      "Epoch: 419 [10100/50000 (20%)]  \tLoss:   95.823006\trec:   69.059937\tkl:   26.763062\n",
      "Epoch: 419 [20100/50000 (40%)]  \tLoss:   92.339157\trec:   66.287941\tkl:   26.051218\n",
      "Epoch: 419 [30100/50000 (60%)]  \tLoss:   90.235825\trec:   63.931698\tkl:   26.304131\n",
      "Epoch: 419 [40100/50000 (80%)]  \tLoss:   91.609512\trec:   65.260933\tkl:   26.348572\n",
      "====> Epoch: 419 Average train loss: 92.1947\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2081\n",
      "Epoch: 420 [  100/50000 ( 0%)]  \tLoss:   90.635818\trec:   64.511131\tkl:   26.124681\n",
      "Epoch: 420 [10100/50000 (20%)]  \tLoss:   91.178848\trec:   65.352440\tkl:   25.826403\n",
      "Epoch: 420 [20100/50000 (40%)]  \tLoss:   91.379234\trec:   65.287811\tkl:   26.091427\n",
      "Epoch: 420 [30100/50000 (60%)]  \tLoss:   94.871658\trec:   68.819580\tkl:   26.052076\n",
      "Epoch: 420 [40100/50000 (80%)]  \tLoss:   92.084061\trec:   65.880875\tkl:   26.203186\n",
      "====> Epoch: 420 Average train loss: 92.1762\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1680\n",
      "Epoch: 421 [  100/50000 ( 0%)]  \tLoss:   88.202690\trec:   62.420120\tkl:   25.782568\n",
      "Epoch: 421 [10100/50000 (20%)]  \tLoss:   90.210800\trec:   64.972641\tkl:   25.238153\n",
      "Epoch: 421 [20100/50000 (40%)]  \tLoss:   95.193192\trec:   67.837761\tkl:   27.355431\n",
      "Epoch: 421 [30100/50000 (60%)]  \tLoss:   94.451111\trec:   67.382057\tkl:   27.069054\n",
      "Epoch: 421 [40100/50000 (80%)]  \tLoss:   93.001915\trec:   66.769226\tkl:   26.232687\n",
      "====> Epoch: 421 Average train loss: 92.1583\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2799\n",
      "Epoch: 422 [  100/50000 ( 0%)]  \tLoss:   92.788094\trec:   66.370255\tkl:   26.417833\n",
      "Epoch: 422 [10100/50000 (20%)]  \tLoss:   89.720406\trec:   64.060318\tkl:   25.660086\n",
      "Epoch: 422 [20100/50000 (40%)]  \tLoss:   93.364334\trec:   67.622345\tkl:   25.741997\n",
      "Epoch: 422 [30100/50000 (60%)]  \tLoss:   94.228378\trec:   67.995232\tkl:   26.233139\n",
      "Epoch: 422 [40100/50000 (80%)]  \tLoss:   92.682137\trec:   67.186035\tkl:   25.496103\n",
      "====> Epoch: 422 Average train loss: 92.1780\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2360\n",
      "Epoch: 423 [  100/50000 ( 0%)]  \tLoss:   97.024704\trec:   70.070793\tkl:   26.953913\n",
      "Epoch: 423 [10100/50000 (20%)]  \tLoss:   94.353104\trec:   68.204811\tkl:   26.148285\n",
      "Epoch: 423 [20100/50000 (40%)]  \tLoss:   92.035164\trec:   66.174034\tkl:   25.861128\n",
      "Epoch: 423 [30100/50000 (60%)]  \tLoss:   92.729469\trec:   66.136139\tkl:   26.593330\n",
      "Epoch: 423 [40100/50000 (80%)]  \tLoss:   92.191872\trec:   66.060738\tkl:   26.131132\n",
      "====> Epoch: 423 Average train loss: 92.1533\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2202\n",
      "Epoch: 424 [  100/50000 ( 0%)]  \tLoss:   91.249786\trec:   65.939323\tkl:   25.310463\n",
      "Epoch: 424 [10100/50000 (20%)]  \tLoss:   95.414070\trec:   68.748924\tkl:   26.665144\n",
      "Epoch: 424 [20100/50000 (40%)]  \tLoss:   92.025978\trec:   66.346481\tkl:   25.679495\n",
      "Epoch: 424 [30100/50000 (60%)]  \tLoss:   95.215584\trec:   68.486656\tkl:   26.728922\n",
      "Epoch: 424 [40100/50000 (80%)]  \tLoss:   90.999199\trec:   65.258965\tkl:   25.740234\n",
      "====> Epoch: 424 Average train loss: 92.1597\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2495\n",
      "Epoch: 425 [  100/50000 ( 0%)]  \tLoss:   92.611229\trec:   66.749435\tkl:   25.861799\n",
      "Epoch: 425 [10100/50000 (20%)]  \tLoss:   90.116089\trec:   64.371063\tkl:   25.745024\n",
      "Epoch: 425 [20100/50000 (40%)]  \tLoss:   94.248337\trec:   67.424858\tkl:   26.823483\n",
      "Epoch: 425 [30100/50000 (60%)]  \tLoss:   91.427055\trec:   65.371445\tkl:   26.055614\n",
      "Epoch: 425 [40100/50000 (80%)]  \tLoss:   89.331818\trec:   63.435772\tkl:   25.896049\n",
      "====> Epoch: 425 Average train loss: 92.1555\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1158\n",
      "Epoch: 426 [  100/50000 ( 0%)]  \tLoss:   88.761925\trec:   63.678120\tkl:   25.083805\n",
      "Epoch: 426 [10100/50000 (20%)]  \tLoss:   89.544235\trec:   64.520729\tkl:   25.023510\n",
      "Epoch: 426 [20100/50000 (40%)]  \tLoss:   91.822647\trec:   65.911720\tkl:   25.910929\n",
      "Epoch: 426 [30100/50000 (60%)]  \tLoss:   91.616348\trec:   66.451698\tkl:   25.164650\n",
      "Epoch: 426 [40100/50000 (80%)]  \tLoss:   93.048203\trec:   66.982880\tkl:   26.065325\n",
      "====> Epoch: 426 Average train loss: 92.1312\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1802\n",
      "Epoch: 427 [  100/50000 ( 0%)]  \tLoss:   91.380875\trec:   65.693039\tkl:   25.687841\n",
      "Epoch: 427 [10100/50000 (20%)]  \tLoss:   93.374802\trec:   67.446404\tkl:   25.928402\n",
      "Epoch: 427 [20100/50000 (40%)]  \tLoss:   92.153008\trec:   65.591972\tkl:   26.561029\n",
      "Epoch: 427 [30100/50000 (60%)]  \tLoss:   92.197052\trec:   65.541237\tkl:   26.655815\n",
      "Epoch: 427 [40100/50000 (80%)]  \tLoss:   93.128220\trec:   66.753006\tkl:   26.375212\n",
      "====> Epoch: 427 Average train loss: 92.1250\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1425\n",
      "Epoch: 428 [  100/50000 ( 0%)]  \tLoss:   90.262985\trec:   65.404732\tkl:   24.858255\n",
      "Epoch: 428 [10100/50000 (20%)]  \tLoss:   89.541130\trec:   63.963779\tkl:   25.577353\n",
      "Epoch: 428 [20100/50000 (40%)]  \tLoss:   94.788338\trec:   68.080193\tkl:   26.708141\n",
      "Epoch: 428 [30100/50000 (60%)]  \tLoss:   95.018112\trec:   68.768394\tkl:   26.249716\n",
      "Epoch: 428 [40100/50000 (80%)]  \tLoss:   93.608963\trec:   67.354973\tkl:   26.253986\n",
      "====> Epoch: 428 Average train loss: 92.1389\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1495\n",
      "Epoch: 429 [  100/50000 ( 0%)]  \tLoss:   92.789215\trec:   66.894829\tkl:   25.894379\n",
      "Epoch: 429 [10100/50000 (20%)]  \tLoss:   92.944588\trec:   66.538437\tkl:   26.406147\n",
      "Epoch: 429 [20100/50000 (40%)]  \tLoss:   92.732574\trec:   67.063522\tkl:   25.669060\n",
      "Epoch: 429 [30100/50000 (60%)]  \tLoss:   96.040092\trec:   69.802856\tkl:   26.237236\n",
      "Epoch: 429 [40100/50000 (80%)]  \tLoss:   96.074059\trec:   69.838776\tkl:   26.235291\n",
      "====> Epoch: 429 Average train loss: 92.1230\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0330\n",
      "Epoch: 430 [  100/50000 ( 0%)]  \tLoss:   94.422813\trec:   68.454346\tkl:   25.968466\n",
      "Epoch: 430 [10100/50000 (20%)]  \tLoss:   90.598953\trec:   65.101776\tkl:   25.497175\n",
      "Epoch: 430 [20100/50000 (40%)]  \tLoss:   93.904129\trec:   67.485207\tkl:   26.418919\n",
      "Epoch: 430 [30100/50000 (60%)]  \tLoss:   98.358788\trec:   71.480698\tkl:   26.878086\n",
      "Epoch: 430 [40100/50000 (80%)]  \tLoss:   91.714737\trec:   66.239838\tkl:   25.474894\n",
      "====> Epoch: 430 Average train loss: 92.1061\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0932\n",
      "Epoch: 431 [  100/50000 ( 0%)]  \tLoss:   92.951149\trec:   67.360550\tkl:   25.590603\n",
      "Epoch: 431 [10100/50000 (20%)]  \tLoss:   91.102722\trec:   65.740334\tkl:   25.362392\n",
      "Epoch: 431 [20100/50000 (40%)]  \tLoss:   92.925964\trec:   66.238487\tkl:   26.687475\n",
      "Epoch: 431 [30100/50000 (60%)]  \tLoss:   87.576759\trec:   61.641491\tkl:   25.935265\n",
      "Epoch: 431 [40100/50000 (80%)]  \tLoss:   90.313950\trec:   64.942825\tkl:   25.371130\n",
      "====> Epoch: 431 Average train loss: 92.1148\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1853\n",
      "Epoch: 432 [  100/50000 ( 0%)]  \tLoss:   93.311180\trec:   67.831596\tkl:   25.479584\n",
      "Epoch: 432 [10100/50000 (20%)]  \tLoss:   91.149864\trec:   65.056938\tkl:   26.092922\n",
      "Epoch: 432 [20100/50000 (40%)]  \tLoss:   90.859161\trec:   65.634666\tkl:   25.224487\n",
      "Epoch: 432 [30100/50000 (60%)]  \tLoss:   95.500870\trec:   69.039948\tkl:   26.460917\n",
      "Epoch: 432 [40100/50000 (80%)]  \tLoss:   87.696266\trec:   62.628017\tkl:   25.068254\n",
      "====> Epoch: 432 Average train loss: 92.1037\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2061\n",
      "Epoch: 433 [  100/50000 ( 0%)]  \tLoss:   91.649765\trec:   66.123795\tkl:   25.525976\n",
      "Epoch: 433 [10100/50000 (20%)]  \tLoss:   95.454880\trec:   68.712280\tkl:   26.742598\n",
      "Epoch: 433 [20100/50000 (40%)]  \tLoss:   91.434677\trec:   64.731651\tkl:   26.703024\n",
      "Epoch: 433 [30100/50000 (60%)]  \tLoss:   95.932533\trec:   69.170738\tkl:   26.761793\n",
      "Epoch: 433 [40100/50000 (80%)]  \tLoss:   91.642143\trec:   65.740570\tkl:   25.901577\n",
      "====> Epoch: 433 Average train loss: 92.0979\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1398\n",
      "Epoch: 434 [  100/50000 ( 0%)]  \tLoss:   90.508278\trec:   65.168541\tkl:   25.339731\n",
      "Epoch: 434 [10100/50000 (20%)]  \tLoss:   91.969429\trec:   65.285378\tkl:   26.684050\n",
      "Epoch: 434 [20100/50000 (40%)]  \tLoss:   94.022003\trec:   68.207008\tkl:   25.814989\n",
      "Epoch: 434 [30100/50000 (60%)]  \tLoss:   88.633041\trec:   63.147881\tkl:   25.485170\n",
      "Epoch: 434 [40100/50000 (80%)]  \tLoss:   91.794022\trec:   66.032852\tkl:   25.761166\n",
      "====> Epoch: 434 Average train loss: 92.0664\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0522\n",
      "Epoch: 435 [  100/50000 ( 0%)]  \tLoss:   93.807777\trec:   67.901657\tkl:   25.906130\n",
      "Epoch: 435 [10100/50000 (20%)]  \tLoss:   92.056610\trec:   66.233940\tkl:   25.822672\n",
      "Epoch: 435 [20100/50000 (40%)]  \tLoss:   93.314507\trec:   67.170929\tkl:   26.143578\n",
      "Epoch: 435 [30100/50000 (60%)]  \tLoss:   92.311836\trec:   65.866783\tkl:   26.445051\n",
      "Epoch: 435 [40100/50000 (80%)]  \tLoss:   94.811813\trec:   68.504074\tkl:   26.307739\n",
      "====> Epoch: 435 Average train loss: 92.0886\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0982\n",
      "Epoch: 436 [  100/50000 ( 0%)]  \tLoss:   91.463203\trec:   65.391815\tkl:   26.071381\n",
      "Epoch: 436 [10100/50000 (20%)]  \tLoss:   90.118896\trec:   64.576859\tkl:   25.542032\n",
      "Epoch: 436 [20100/50000 (40%)]  \tLoss:   90.368492\trec:   64.219589\tkl:   26.148903\n",
      "Epoch: 436 [30100/50000 (60%)]  \tLoss:   92.624390\trec:   66.391411\tkl:   26.232983\n",
      "Epoch: 436 [40100/50000 (80%)]  \tLoss:   90.687599\trec:   64.963860\tkl:   25.723745\n",
      "====> Epoch: 436 Average train loss: 92.0789\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0378\n",
      "Epoch: 437 [  100/50000 ( 0%)]  \tLoss:   90.529732\trec:   65.320450\tkl:   25.209286\n",
      "Epoch: 437 [10100/50000 (20%)]  \tLoss:   92.159218\trec:   65.343781\tkl:   26.815441\n",
      "Epoch: 437 [20100/50000 (40%)]  \tLoss:   94.396034\trec:   68.105804\tkl:   26.290226\n",
      "Epoch: 437 [30100/50000 (60%)]  \tLoss:   92.846001\trec:   67.084503\tkl:   25.761505\n",
      "Epoch: 437 [40100/50000 (80%)]  \tLoss:   89.506950\trec:   63.781498\tkl:   25.725449\n",
      "====> Epoch: 437 Average train loss: 92.0646\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1112\n",
      "Epoch: 438 [  100/50000 ( 0%)]  \tLoss:   95.288300\trec:   69.298019\tkl:   25.990278\n",
      "Epoch: 438 [10100/50000 (20%)]  \tLoss:   90.978592\trec:   65.570747\tkl:   25.407845\n",
      "Epoch: 438 [20100/50000 (40%)]  \tLoss:   94.129128\trec:   66.998817\tkl:   27.130314\n",
      "Epoch: 438 [30100/50000 (60%)]  \tLoss:   90.470528\trec:   64.627487\tkl:   25.843031\n",
      "Epoch: 438 [40100/50000 (80%)]  \tLoss:   96.280060\trec:   70.010201\tkl:   26.269855\n",
      "====> Epoch: 438 Average train loss: 92.0799\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1492\n",
      "Epoch: 439 [  100/50000 ( 0%)]  \tLoss:   91.805626\trec:   66.025139\tkl:   25.780483\n",
      "Epoch: 439 [10100/50000 (20%)]  \tLoss:   89.001503\trec:   63.626446\tkl:   25.375061\n",
      "Epoch: 439 [20100/50000 (40%)]  \tLoss:   90.362648\trec:   64.815300\tkl:   25.547346\n",
      "Epoch: 439 [30100/50000 (60%)]  \tLoss:   91.057426\trec:   65.676933\tkl:   25.380493\n",
      "Epoch: 439 [40100/50000 (80%)]  \tLoss:   88.128296\trec:   62.994724\tkl:   25.133574\n",
      "====> Epoch: 439 Average train loss: 92.0600\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1931\n",
      "Epoch: 440 [  100/50000 ( 0%)]  \tLoss:   91.606903\trec:   65.279259\tkl:   26.327648\n",
      "Epoch: 440 [10100/50000 (20%)]  \tLoss:   93.735115\trec:   67.257332\tkl:   26.477785\n",
      "Epoch: 440 [20100/50000 (40%)]  \tLoss:   91.068466\trec:   65.335533\tkl:   25.732927\n",
      "Epoch: 440 [30100/50000 (60%)]  \tLoss:   88.696587\trec:   62.624294\tkl:   26.072294\n",
      "Epoch: 440 [40100/50000 (80%)]  \tLoss:   91.013496\trec:   64.717300\tkl:   26.296188\n",
      "====> Epoch: 440 Average train loss: 92.0493\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1394\n",
      "Epoch: 441 [  100/50000 ( 0%)]  \tLoss:   90.487045\trec:   65.140274\tkl:   25.346781\n",
      "Epoch: 441 [10100/50000 (20%)]  \tLoss:   92.326157\trec:   66.296913\tkl:   26.029245\n",
      "Epoch: 441 [20100/50000 (40%)]  \tLoss:   89.082703\trec:   64.277161\tkl:   24.805538\n",
      "Epoch: 441 [30100/50000 (60%)]  \tLoss:   91.231133\trec:   65.052803\tkl:   26.178324\n",
      "Epoch: 441 [40100/50000 (80%)]  \tLoss:   96.120461\trec:   69.138405\tkl:   26.982052\n",
      "====> Epoch: 441 Average train loss: 92.0505\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0706\n",
      "Epoch: 442 [  100/50000 ( 0%)]  \tLoss:   88.290092\trec:   63.105408\tkl:   25.184685\n",
      "Epoch: 442 [10100/50000 (20%)]  \tLoss:   90.322304\trec:   64.573700\tkl:   25.748608\n",
      "Epoch: 442 [20100/50000 (40%)]  \tLoss:   90.193298\trec:   64.172928\tkl:   26.020376\n",
      "Epoch: 442 [30100/50000 (60%)]  \tLoss:   92.308693\trec:   66.652245\tkl:   25.656445\n",
      "Epoch: 442 [40100/50000 (80%)]  \tLoss:   88.342682\trec:   62.501316\tkl:   25.841370\n",
      "====> Epoch: 442 Average train loss: 92.0434\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1429\n",
      "Epoch: 443 [  100/50000 ( 0%)]  \tLoss:   86.405777\trec:   61.319214\tkl:   25.086561\n",
      "Epoch: 443 [10100/50000 (20%)]  \tLoss:   90.467644\trec:   63.574833\tkl:   26.892809\n",
      "Epoch: 443 [20100/50000 (40%)]  \tLoss:   94.816055\trec:   68.189430\tkl:   26.626616\n",
      "Epoch: 443 [30100/50000 (60%)]  \tLoss:   92.966835\trec:   66.933846\tkl:   26.032988\n",
      "Epoch: 443 [40100/50000 (80%)]  \tLoss:   92.210236\trec:   65.807587\tkl:   26.402651\n",
      "====> Epoch: 443 Average train loss: 92.0244\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1600\n",
      "Epoch: 444 [  100/50000 ( 0%)]  \tLoss:   94.030037\trec:   67.504341\tkl:   26.525688\n",
      "Epoch: 444 [10100/50000 (20%)]  \tLoss:   91.273407\trec:   65.841377\tkl:   25.432024\n",
      "Epoch: 444 [20100/50000 (40%)]  \tLoss:   91.950348\trec:   65.401413\tkl:   26.548941\n",
      "Epoch: 444 [30100/50000 (60%)]  \tLoss:   90.997154\trec:   65.145134\tkl:   25.852026\n",
      "Epoch: 444 [40100/50000 (80%)]  \tLoss:   94.700470\trec:   68.157013\tkl:   26.543451\n",
      "====> Epoch: 444 Average train loss: 92.0239\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1399\n",
      "Epoch: 445 [  100/50000 ( 0%)]  \tLoss:   89.886879\trec:   64.842964\tkl:   25.043919\n",
      "Epoch: 445 [10100/50000 (20%)]  \tLoss:   91.724884\trec:   65.618301\tkl:   26.106581\n",
      "Epoch: 445 [20100/50000 (40%)]  \tLoss:   95.939774\trec:   69.555626\tkl:   26.384151\n",
      "Epoch: 445 [30100/50000 (60%)]  \tLoss:   91.545898\trec:   65.460320\tkl:   26.085579\n",
      "Epoch: 445 [40100/50000 (80%)]  \tLoss:   92.443031\trec:   65.640327\tkl:   26.802704\n",
      "====> Epoch: 445 Average train loss: 92.0083\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1475\n",
      "Epoch: 446 [  100/50000 ( 0%)]  \tLoss:   91.644981\trec:   65.800362\tkl:   25.844614\n",
      "Epoch: 446 [10100/50000 (20%)]  \tLoss:   90.517296\trec:   64.639473\tkl:   25.877819\n",
      "Epoch: 446 [20100/50000 (40%)]  \tLoss:   90.134048\trec:   64.216301\tkl:   25.917755\n",
      "Epoch: 446 [30100/50000 (60%)]  \tLoss:   93.236855\trec:   66.916916\tkl:   26.319946\n",
      "Epoch: 446 [40100/50000 (80%)]  \tLoss:   92.070534\trec:   66.153229\tkl:   25.917307\n",
      "====> Epoch: 446 Average train loss: 92.0147\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0169\n",
      "Epoch: 447 [  100/50000 ( 0%)]  \tLoss:   92.671150\trec:   67.450432\tkl:   25.220722\n",
      "Epoch: 447 [10100/50000 (20%)]  \tLoss:   86.089424\trec:   62.101776\tkl:   23.987644\n",
      "Epoch: 447 [20100/50000 (40%)]  \tLoss:   93.858673\trec:   68.054260\tkl:   25.804411\n",
      "Epoch: 447 [30100/50000 (60%)]  \tLoss:   92.541054\trec:   66.599731\tkl:   25.941313\n",
      "Epoch: 447 [40100/50000 (80%)]  \tLoss:   91.722031\trec:   65.671227\tkl:   26.050800\n",
      "====> Epoch: 447 Average train loss: 92.0320\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0875\n",
      "Epoch: 448 [  100/50000 ( 0%)]  \tLoss:   94.109245\trec:   67.899513\tkl:   26.209738\n",
      "Epoch: 448 [10100/50000 (20%)]  \tLoss:   89.465408\trec:   64.196159\tkl:   25.269253\n",
      "Epoch: 448 [20100/50000 (40%)]  \tLoss:   88.195190\trec:   63.511658\tkl:   24.683540\n",
      "Epoch: 448 [30100/50000 (60%)]  \tLoss:   90.092056\trec:   65.069420\tkl:   25.022636\n",
      "Epoch: 448 [40100/50000 (80%)]  \tLoss:   91.574303\trec:   65.160957\tkl:   26.413347\n",
      "====> Epoch: 448 Average train loss: 91.9879\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1354\n",
      "Epoch: 449 [  100/50000 ( 0%)]  \tLoss:   91.431404\trec:   65.097893\tkl:   26.333513\n",
      "Epoch: 449 [10100/50000 (20%)]  \tLoss:   88.648102\trec:   63.013428\tkl:   25.634674\n",
      "Epoch: 449 [20100/50000 (40%)]  \tLoss:   92.826523\trec:   66.344414\tkl:   26.482107\n",
      "Epoch: 449 [30100/50000 (60%)]  \tLoss:   96.370476\trec:   69.443260\tkl:   26.927214\n",
      "Epoch: 449 [40100/50000 (80%)]  \tLoss:   87.646309\trec:   62.711189\tkl:   24.935116\n",
      "====> Epoch: 449 Average train loss: 92.0000\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0834\n",
      "Epoch: 450 [  100/50000 ( 0%)]  \tLoss:   89.187576\trec:   64.041641\tkl:   25.145937\n",
      "Epoch: 450 [10100/50000 (20%)]  \tLoss:   90.408989\trec:   64.650749\tkl:   25.758244\n",
      "Epoch: 450 [20100/50000 (40%)]  \tLoss:   92.320290\trec:   66.812881\tkl:   25.507406\n",
      "Epoch: 450 [30100/50000 (60%)]  \tLoss:   89.835724\trec:   64.576653\tkl:   25.259069\n",
      "Epoch: 450 [40100/50000 (80%)]  \tLoss:   91.748322\trec:   65.887207\tkl:   25.861107\n",
      "====> Epoch: 450 Average train loss: 91.9827\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1485\n",
      "Epoch: 451 [  100/50000 ( 0%)]  \tLoss:   93.883797\trec:   67.141479\tkl:   26.742321\n",
      "Epoch: 451 [10100/50000 (20%)]  \tLoss:   93.466843\trec:   68.245895\tkl:   25.220943\n",
      "Epoch: 451 [20100/50000 (40%)]  \tLoss:   87.498497\trec:   62.202766\tkl:   25.295729\n",
      "Epoch: 451 [30100/50000 (60%)]  \tLoss:   94.878113\trec:   68.106071\tkl:   26.772038\n",
      "Epoch: 451 [40100/50000 (80%)]  \tLoss:   92.427315\trec:   66.077423\tkl:   26.349890\n",
      "====> Epoch: 451 Average train loss: 92.0062\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0994\n",
      "Epoch: 452 [  100/50000 ( 0%)]  \tLoss:   90.814507\trec:   64.387123\tkl:   26.427391\n",
      "Epoch: 452 [10100/50000 (20%)]  \tLoss:   95.166542\trec:   68.787476\tkl:   26.379066\n",
      "Epoch: 452 [20100/50000 (40%)]  \tLoss:   89.962311\trec:   63.493046\tkl:   26.469265\n",
      "Epoch: 452 [30100/50000 (60%)]  \tLoss:   91.013611\trec:   65.654480\tkl:   25.359127\n",
      "Epoch: 452 [40100/50000 (80%)]  \tLoss:   88.049461\trec:   62.232880\tkl:   25.816580\n",
      "====> Epoch: 452 Average train loss: 91.9933\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0874\n",
      "Epoch: 453 [  100/50000 ( 0%)]  \tLoss:   91.929901\trec:   65.776802\tkl:   26.153097\n",
      "Epoch: 453 [10100/50000 (20%)]  \tLoss:   93.691559\trec:   66.403343\tkl:   27.288216\n",
      "Epoch: 453 [20100/50000 (40%)]  \tLoss:   95.370682\trec:   69.406250\tkl:   25.964426\n",
      "Epoch: 453 [30100/50000 (60%)]  \tLoss:   91.372902\trec:   64.938545\tkl:   26.434349\n",
      "Epoch: 453 [40100/50000 (80%)]  \tLoss:   91.990479\trec:   66.183395\tkl:   25.807079\n",
      "====> Epoch: 453 Average train loss: 91.9647\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0810\n",
      "Epoch: 454 [  100/50000 ( 0%)]  \tLoss:   88.113045\trec:   62.124237\tkl:   25.988806\n",
      "Epoch: 454 [10100/50000 (20%)]  \tLoss:   93.388199\trec:   67.150505\tkl:   26.237698\n",
      "Epoch: 454 [20100/50000 (40%)]  \tLoss:   92.823067\trec:   66.926544\tkl:   25.896528\n",
      "Epoch: 454 [30100/50000 (60%)]  \tLoss:   90.239357\trec:   64.589149\tkl:   25.650200\n",
      "Epoch: 454 [40100/50000 (80%)]  \tLoss:   93.211342\trec:   66.716660\tkl:   26.494684\n",
      "====> Epoch: 454 Average train loss: 91.9660\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0083\n",
      "Epoch: 455 [  100/50000 ( 0%)]  \tLoss:   95.714294\trec:   69.180244\tkl:   26.534048\n",
      "Epoch: 455 [10100/50000 (20%)]  \tLoss:   90.417351\trec:   64.656143\tkl:   25.761206\n",
      "Epoch: 455 [20100/50000 (40%)]  \tLoss:   95.390205\trec:   68.150185\tkl:   27.240021\n",
      "Epoch: 455 [30100/50000 (60%)]  \tLoss:   92.666931\trec:   66.462029\tkl:   26.204906\n",
      "Epoch: 455 [40100/50000 (80%)]  \tLoss:   93.374741\trec:   66.752907\tkl:   26.621840\n",
      "====> Epoch: 455 Average train loss: 91.9582\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0778\n",
      "Epoch: 456 [  100/50000 ( 0%)]  \tLoss:   91.487167\trec:   65.570328\tkl:   25.916838\n",
      "Epoch: 456 [10100/50000 (20%)]  \tLoss:   94.435074\trec:   68.293350\tkl:   26.141726\n",
      "Epoch: 456 [20100/50000 (40%)]  \tLoss:   90.071327\trec:   63.965458\tkl:   26.105864\n",
      "Epoch: 456 [30100/50000 (60%)]  \tLoss:   90.101776\trec:   64.464569\tkl:   25.637207\n",
      "Epoch: 456 [40100/50000 (80%)]  \tLoss:   91.903816\trec:   66.006462\tkl:   25.897352\n",
      "====> Epoch: 456 Average train loss: 91.9749\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9339\n",
      "Epoch: 457 [  100/50000 ( 0%)]  \tLoss:   92.780876\trec:   66.374207\tkl:   26.406668\n",
      "Epoch: 457 [10100/50000 (20%)]  \tLoss:   92.797173\trec:   66.874062\tkl:   25.923117\n",
      "Epoch: 457 [20100/50000 (40%)]  \tLoss:   91.246666\trec:   64.911720\tkl:   26.334951\n",
      "Epoch: 457 [30100/50000 (60%)]  \tLoss:   90.202118\trec:   64.839088\tkl:   25.363024\n",
      "Epoch: 457 [40100/50000 (80%)]  \tLoss:   94.460106\trec:   68.269455\tkl:   26.190651\n",
      "====> Epoch: 457 Average train loss: 91.9511\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1203\n",
      "Epoch: 458 [  100/50000 ( 0%)]  \tLoss:   88.573082\trec:   63.095112\tkl:   25.477978\n",
      "Epoch: 458 [10100/50000 (20%)]  \tLoss:   92.534744\trec:   66.773331\tkl:   25.761414\n",
      "Epoch: 458 [20100/50000 (40%)]  \tLoss:   87.917595\trec:   62.864670\tkl:   25.052921\n",
      "Epoch: 458 [30100/50000 (60%)]  \tLoss:   91.526131\trec:   65.476105\tkl:   26.050032\n",
      "Epoch: 458 [40100/50000 (80%)]  \tLoss:   95.780388\trec:   68.499176\tkl:   27.281206\n",
      "====> Epoch: 458 Average train loss: 91.9349\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0650\n",
      "Epoch: 459 [  100/50000 ( 0%)]  \tLoss:   94.602295\trec:   66.702408\tkl:   27.899881\n",
      "Epoch: 459 [10100/50000 (20%)]  \tLoss:   93.129196\trec:   66.695999\tkl:   26.433193\n",
      "Epoch: 459 [20100/50000 (40%)]  \tLoss:   92.788002\trec:   65.923149\tkl:   26.864853\n",
      "Epoch: 459 [30100/50000 (60%)]  \tLoss:   89.388847\trec:   63.810310\tkl:   25.578529\n",
      "Epoch: 459 [40100/50000 (80%)]  \tLoss:   91.739502\trec:   65.331352\tkl:   26.408142\n",
      "====> Epoch: 459 Average train loss: 91.9474\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0351\n",
      "Epoch: 460 [  100/50000 ( 0%)]  \tLoss:   90.251190\trec:   64.810356\tkl:   25.440830\n",
      "Epoch: 460 [10100/50000 (20%)]  \tLoss:   90.356659\trec:   64.510040\tkl:   25.846619\n",
      "Epoch: 460 [20100/50000 (40%)]  \tLoss:   93.415779\trec:   67.185524\tkl:   26.230251\n",
      "Epoch: 460 [30100/50000 (60%)]  \tLoss:   89.625816\trec:   63.768719\tkl:   25.857103\n",
      "Epoch: 460 [40100/50000 (80%)]  \tLoss:   94.287582\trec:   67.395523\tkl:   26.892057\n",
      "====> Epoch: 460 Average train loss: 91.9361\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0360\n",
      "Epoch: 461 [  100/50000 ( 0%)]  \tLoss:   95.227089\trec:   67.953758\tkl:   27.273329\n",
      "Epoch: 461 [10100/50000 (20%)]  \tLoss:   95.410194\trec:   68.980927\tkl:   26.429268\n",
      "Epoch: 461 [20100/50000 (40%)]  \tLoss:   90.439110\trec:   64.645363\tkl:   25.793749\n",
      "Epoch: 461 [30100/50000 (60%)]  \tLoss:   95.219460\trec:   68.180855\tkl:   27.038605\n",
      "Epoch: 461 [40100/50000 (80%)]  \tLoss:   95.342072\trec:   69.017303\tkl:   26.324770\n",
      "====> Epoch: 461 Average train loss: 91.9386\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0253\n",
      "Epoch: 462 [  100/50000 ( 0%)]  \tLoss:   94.868324\trec:   68.735397\tkl:   26.132935\n",
      "Epoch: 462 [10100/50000 (20%)]  \tLoss:   92.678688\trec:   66.705795\tkl:   25.972900\n",
      "Epoch: 462 [20100/50000 (40%)]  \tLoss:   93.265877\trec:   66.737312\tkl:   26.528566\n",
      "Epoch: 462 [30100/50000 (60%)]  \tLoss:   95.633911\trec:   68.840973\tkl:   26.792946\n",
      "Epoch: 462 [40100/50000 (80%)]  \tLoss:   89.248924\trec:   62.895271\tkl:   26.353649\n",
      "====> Epoch: 462 Average train loss: 91.9311\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1198\n",
      "Epoch: 463 [  100/50000 ( 0%)]  \tLoss:   92.025955\trec:   66.214806\tkl:   25.811157\n",
      "Epoch: 463 [10100/50000 (20%)]  \tLoss:   91.339409\trec:   65.508682\tkl:   25.830732\n",
      "Epoch: 463 [20100/50000 (40%)]  \tLoss:   91.232697\trec:   65.029045\tkl:   26.203657\n",
      "Epoch: 463 [30100/50000 (60%)]  \tLoss:   92.854919\trec:   65.774765\tkl:   27.080160\n",
      "Epoch: 463 [40100/50000 (80%)]  \tLoss:   90.412384\trec:   64.782913\tkl:   25.629469\n",
      "====> Epoch: 463 Average train loss: 91.9165\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.2517\n",
      "Epoch: 464 [  100/50000 ( 0%)]  \tLoss:   92.062325\trec:   65.688232\tkl:   26.374081\n",
      "Epoch: 464 [10100/50000 (20%)]  \tLoss:   91.827248\trec:   64.299248\tkl:   27.528002\n",
      "Epoch: 464 [20100/50000 (40%)]  \tLoss:   94.266479\trec:   68.709572\tkl:   25.556910\n",
      "Epoch: 464 [30100/50000 (60%)]  \tLoss:   95.506981\trec:   68.951691\tkl:   26.555290\n",
      "Epoch: 464 [40100/50000 (80%)]  \tLoss:   88.310066\trec:   62.619392\tkl:   25.690676\n",
      "====> Epoch: 464 Average train loss: 91.9217\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9700\n",
      "Epoch: 465 [  100/50000 ( 0%)]  \tLoss:   92.348633\trec:   66.623802\tkl:   25.724827\n",
      "Epoch: 465 [10100/50000 (20%)]  \tLoss:   93.029282\trec:   66.954483\tkl:   26.074802\n",
      "Epoch: 465 [20100/50000 (40%)]  \tLoss:   92.481560\trec:   66.080772\tkl:   26.400784\n",
      "Epoch: 465 [30100/50000 (60%)]  \tLoss:   89.245796\trec:   63.706223\tkl:   25.539576\n",
      "Epoch: 465 [40100/50000 (80%)]  \tLoss:   89.570221\trec:   63.587654\tkl:   25.982571\n",
      "====> Epoch: 465 Average train loss: 91.9133\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1714\n",
      "Epoch: 466 [  100/50000 ( 0%)]  \tLoss:   90.760353\trec:   65.191895\tkl:   25.568451\n",
      "Epoch: 466 [10100/50000 (20%)]  \tLoss:   93.456329\trec:   66.384460\tkl:   27.071859\n",
      "Epoch: 466 [20100/50000 (40%)]  \tLoss:   92.741280\trec:   66.927292\tkl:   25.813986\n",
      "Epoch: 466 [30100/50000 (60%)]  \tLoss:   91.871529\trec:   65.891426\tkl:   25.980104\n",
      "Epoch: 466 [40100/50000 (80%)]  \tLoss:   95.545021\trec:   69.234901\tkl:   26.310112\n",
      "====> Epoch: 466 Average train loss: 91.8912\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1235\n",
      "Epoch: 467 [  100/50000 ( 0%)]  \tLoss:   91.610756\trec:   65.797699\tkl:   25.813066\n",
      "Epoch: 467 [10100/50000 (20%)]  \tLoss:   96.759293\trec:   69.147636\tkl:   27.611664\n",
      "Epoch: 467 [20100/50000 (40%)]  \tLoss:   89.902916\trec:   64.042633\tkl:   25.860287\n",
      "Epoch: 467 [30100/50000 (60%)]  \tLoss:   90.797905\trec:   65.240593\tkl:   25.557316\n",
      "Epoch: 467 [40100/50000 (80%)]  \tLoss:   95.397438\trec:   69.204407\tkl:   26.193037\n",
      "====> Epoch: 467 Average train loss: 91.9248\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0927\n",
      "Epoch: 468 [  100/50000 ( 0%)]  \tLoss:   90.285133\trec:   64.577477\tkl:   25.707655\n",
      "Epoch: 468 [10100/50000 (20%)]  \tLoss:   91.418983\trec:   65.788162\tkl:   25.630821\n",
      "Epoch: 468 [20100/50000 (40%)]  \tLoss:   90.148979\trec:   64.320358\tkl:   25.828621\n",
      "Epoch: 468 [30100/50000 (60%)]  \tLoss:   95.151871\trec:   69.144920\tkl:   26.006954\n",
      "Epoch: 468 [40100/50000 (80%)]  \tLoss:   89.365997\trec:   63.743916\tkl:   25.622074\n",
      "====> Epoch: 468 Average train loss: 91.8894\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0597\n",
      "Epoch: 469 [  100/50000 ( 0%)]  \tLoss:   89.500488\trec:   63.527843\tkl:   25.972643\n",
      "Epoch: 469 [10100/50000 (20%)]  \tLoss:   92.514267\trec:   66.571304\tkl:   25.942955\n",
      "Epoch: 469 [20100/50000 (40%)]  \tLoss:   96.650620\trec:   70.333466\tkl:   26.317156\n",
      "Epoch: 469 [30100/50000 (60%)]  \tLoss:   94.039734\trec:   68.349258\tkl:   25.690475\n",
      "Epoch: 469 [40100/50000 (80%)]  \tLoss:   91.076202\trec:   64.921555\tkl:   26.154642\n",
      "====> Epoch: 469 Average train loss: 91.8894\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8997\n",
      "Epoch: 470 [  100/50000 ( 0%)]  \tLoss:   89.811630\trec:   63.534912\tkl:   26.276720\n",
      "Epoch: 470 [10100/50000 (20%)]  \tLoss:   92.049675\trec:   65.861160\tkl:   26.188513\n",
      "Epoch: 470 [20100/50000 (40%)]  \tLoss:   88.956039\trec:   63.081478\tkl:   25.874567\n",
      "Epoch: 470 [30100/50000 (60%)]  \tLoss:   92.152870\trec:   66.800652\tkl:   25.352221\n",
      "Epoch: 470 [40100/50000 (80%)]  \tLoss:   93.196754\trec:   67.372215\tkl:   25.824541\n",
      "====> Epoch: 470 Average train loss: 91.8700\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0260\n",
      "Epoch: 471 [  100/50000 ( 0%)]  \tLoss:   92.581894\trec:   66.505760\tkl:   26.076128\n",
      "Epoch: 471 [10100/50000 (20%)]  \tLoss:   89.234665\trec:   64.413719\tkl:   24.820951\n",
      "Epoch: 471 [20100/50000 (40%)]  \tLoss:   89.888611\trec:   64.507668\tkl:   25.380945\n",
      "Epoch: 471 [30100/50000 (60%)]  \tLoss:   90.758911\trec:   65.178375\tkl:   25.580536\n",
      "Epoch: 471 [40100/50000 (80%)]  \tLoss:   91.585815\trec:   65.586914\tkl:   25.998901\n",
      "====> Epoch: 471 Average train loss: 91.8939\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9784\n",
      "Epoch: 472 [  100/50000 ( 0%)]  \tLoss:   90.590576\trec:   63.854691\tkl:   26.735880\n",
      "Epoch: 472 [10100/50000 (20%)]  \tLoss:   95.027863\trec:   68.623306\tkl:   26.404549\n",
      "Epoch: 472 [20100/50000 (40%)]  \tLoss:   90.640518\trec:   64.404091\tkl:   26.236423\n",
      "Epoch: 472 [30100/50000 (60%)]  \tLoss:   92.588913\trec:   66.547379\tkl:   26.041534\n",
      "Epoch: 472 [40100/50000 (80%)]  \tLoss:   93.672150\trec:   67.906227\tkl:   25.765921\n",
      "====> Epoch: 472 Average train loss: 91.8933\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9563\n",
      "Epoch: 473 [  100/50000 ( 0%)]  \tLoss:   91.804337\trec:   65.578529\tkl:   26.225800\n",
      "Epoch: 473 [10100/50000 (20%)]  \tLoss:   93.016220\trec:   66.692101\tkl:   26.324116\n",
      "Epoch: 473 [20100/50000 (40%)]  \tLoss:   95.254982\trec:   68.574120\tkl:   26.680857\n",
      "Epoch: 473 [30100/50000 (60%)]  \tLoss:   95.009384\trec:   67.840378\tkl:   27.169006\n",
      "Epoch: 473 [40100/50000 (80%)]  \tLoss:   92.721069\trec:   66.933456\tkl:   25.787615\n",
      "====> Epoch: 473 Average train loss: 91.8470\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0527\n",
      "Epoch: 474 [  100/50000 ( 0%)]  \tLoss:   91.290314\trec:   65.459099\tkl:   25.831215\n",
      "Epoch: 474 [10100/50000 (20%)]  \tLoss:   89.900978\trec:   64.616707\tkl:   25.284266\n",
      "Epoch: 474 [20100/50000 (40%)]  \tLoss:   94.564804\trec:   67.586594\tkl:   26.978218\n",
      "Epoch: 474 [30100/50000 (60%)]  \tLoss:   95.296288\trec:   68.778740\tkl:   26.517546\n",
      "Epoch: 474 [40100/50000 (80%)]  \tLoss:   91.208359\trec:   64.880547\tkl:   26.327810\n",
      "====> Epoch: 474 Average train loss: 91.8657\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0026\n",
      "Epoch: 475 [  100/50000 ( 0%)]  \tLoss:   91.602585\trec:   66.400146\tkl:   25.202444\n",
      "Epoch: 475 [10100/50000 (20%)]  \tLoss:   93.259995\trec:   66.911377\tkl:   26.348616\n",
      "Epoch: 475 [20100/50000 (40%)]  \tLoss:   93.359688\trec:   67.456451\tkl:   25.903236\n",
      "Epoch: 475 [30100/50000 (60%)]  \tLoss:   93.362320\trec:   66.970070\tkl:   26.392250\n",
      "Epoch: 475 [40100/50000 (80%)]  \tLoss:   93.640656\trec:   67.905128\tkl:   25.735527\n",
      "====> Epoch: 475 Average train loss: 91.8563\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0108\n",
      "Epoch: 476 [  100/50000 ( 0%)]  \tLoss:   92.488853\trec:   66.894516\tkl:   25.594343\n",
      "Epoch: 476 [10100/50000 (20%)]  \tLoss:   93.236343\trec:   66.730461\tkl:   26.505890\n",
      "Epoch: 476 [20100/50000 (40%)]  \tLoss:   88.427010\trec:   62.902840\tkl:   25.524174\n",
      "Epoch: 476 [30100/50000 (60%)]  \tLoss:   92.321571\trec:   65.533775\tkl:   26.787794\n",
      "Epoch: 476 [40100/50000 (80%)]  \tLoss:   86.159782\trec:   61.484905\tkl:   24.674883\n",
      "====> Epoch: 476 Average train loss: 91.8589\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0426\n",
      "Epoch: 477 [  100/50000 ( 0%)]  \tLoss:   91.489410\trec:   65.983444\tkl:   25.505972\n",
      "Epoch: 477 [10100/50000 (20%)]  \tLoss:   91.195358\trec:   65.610046\tkl:   25.585310\n",
      "Epoch: 477 [20100/50000 (40%)]  \tLoss:   92.815689\trec:   66.899231\tkl:   25.916462\n",
      "Epoch: 477 [30100/50000 (60%)]  \tLoss:   93.475769\trec:   66.873131\tkl:   26.602644\n",
      "Epoch: 477 [40100/50000 (80%)]  \tLoss:   94.971336\trec:   68.872169\tkl:   26.099167\n",
      "====> Epoch: 477 Average train loss: 91.8496\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0408\n",
      "Epoch: 478 [  100/50000 ( 0%)]  \tLoss:   92.865005\trec:   66.225616\tkl:   26.639395\n",
      "Epoch: 478 [10100/50000 (20%)]  \tLoss:   89.855888\trec:   64.815567\tkl:   25.040319\n",
      "Epoch: 478 [20100/50000 (40%)]  \tLoss:   88.258041\trec:   62.584579\tkl:   25.673464\n",
      "Epoch: 478 [30100/50000 (60%)]  \tLoss:   94.395981\trec:   68.206711\tkl:   26.189278\n",
      "Epoch: 478 [40100/50000 (80%)]  \tLoss:   87.981178\trec:   63.137985\tkl:   24.843193\n",
      "====> Epoch: 478 Average train loss: 91.8588\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.1341\n",
      "Epoch: 479 [  100/50000 ( 0%)]  \tLoss:   90.044899\trec:   64.832176\tkl:   25.212729\n",
      "Epoch: 479 [10100/50000 (20%)]  \tLoss:   93.689682\trec:   67.610748\tkl:   26.078936\n",
      "Epoch: 479 [20100/50000 (40%)]  \tLoss:   93.679451\trec:   67.193466\tkl:   26.485989\n",
      "Epoch: 479 [30100/50000 (60%)]  \tLoss:   90.192764\trec:   64.334221\tkl:   25.858538\n",
      "Epoch: 479 [40100/50000 (80%)]  \tLoss:   93.738190\trec:   66.992058\tkl:   26.746130\n",
      "====> Epoch: 479 Average train loss: 91.8265\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9560\n",
      "Epoch: 480 [  100/50000 ( 0%)]  \tLoss:   89.560135\trec:   63.598534\tkl:   25.961599\n",
      "Epoch: 480 [10100/50000 (20%)]  \tLoss:   93.077530\trec:   66.548370\tkl:   26.529158\n",
      "Epoch: 480 [20100/50000 (40%)]  \tLoss:   94.012772\trec:   67.357094\tkl:   26.655680\n",
      "Epoch: 480 [30100/50000 (60%)]  \tLoss:   90.777451\trec:   64.494934\tkl:   26.282515\n",
      "Epoch: 480 [40100/50000 (80%)]  \tLoss:   90.499863\trec:   63.984901\tkl:   26.514961\n",
      "====> Epoch: 480 Average train loss: 91.8374\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9944\n",
      "Epoch: 481 [  100/50000 ( 0%)]  \tLoss:   94.664345\trec:   68.064110\tkl:   26.600231\n",
      "Epoch: 481 [10100/50000 (20%)]  \tLoss:   94.265701\trec:   67.881767\tkl:   26.383930\n",
      "Epoch: 481 [20100/50000 (40%)]  \tLoss:   93.910065\trec:   68.538094\tkl:   25.371969\n",
      "Epoch: 481 [30100/50000 (60%)]  \tLoss:   87.126915\trec:   62.800507\tkl:   24.326408\n",
      "Epoch: 481 [40100/50000 (80%)]  \tLoss:   93.683426\trec:   67.432426\tkl:   26.251003\n",
      "====> Epoch: 481 Average train loss: 91.8231\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0140\n",
      "Epoch: 482 [  100/50000 ( 0%)]  \tLoss:   91.801033\trec:   65.871490\tkl:   25.929539\n",
      "Epoch: 482 [10100/50000 (20%)]  \tLoss:   91.429543\trec:   64.780380\tkl:   26.649160\n",
      "Epoch: 482 [20100/50000 (40%)]  \tLoss:   96.348267\trec:   69.373466\tkl:   26.974802\n",
      "Epoch: 482 [30100/50000 (60%)]  \tLoss:   89.835388\trec:   63.708778\tkl:   26.126614\n",
      "Epoch: 482 [40100/50000 (80%)]  \tLoss:   93.593765\trec:   66.994072\tkl:   26.599691\n",
      "====> Epoch: 482 Average train loss: 91.8078\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0134\n",
      "Epoch: 483 [  100/50000 ( 0%)]  \tLoss:   91.004715\trec:   64.187164\tkl:   26.817549\n",
      "Epoch: 483 [10100/50000 (20%)]  \tLoss:   88.446510\trec:   62.414940\tkl:   26.031574\n",
      "Epoch: 483 [20100/50000 (40%)]  \tLoss:   89.702911\trec:   63.752186\tkl:   25.950727\n",
      "Epoch: 483 [30100/50000 (60%)]  \tLoss:   95.141411\trec:   68.762039\tkl:   26.379377\n",
      "Epoch: 483 [40100/50000 (80%)]  \tLoss:   94.482124\trec:   68.072754\tkl:   26.409376\n",
      "====> Epoch: 483 Average train loss: 91.8245\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9364\n",
      "Epoch: 484 [  100/50000 ( 0%)]  \tLoss:   92.204765\trec:   65.721619\tkl:   26.483141\n",
      "Epoch: 484 [10100/50000 (20%)]  \tLoss:   91.076729\trec:   64.801262\tkl:   26.275469\n",
      "Epoch: 484 [20100/50000 (40%)]  \tLoss:   89.033012\trec:   62.414726\tkl:   26.618288\n",
      "Epoch: 484 [30100/50000 (60%)]  \tLoss:   94.171814\trec:   67.573479\tkl:   26.598337\n",
      "Epoch: 484 [40100/50000 (80%)]  \tLoss:   91.418915\trec:   65.825005\tkl:   25.593906\n",
      "====> Epoch: 484 Average train loss: 91.8012\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9916\n",
      "Epoch: 485 [  100/50000 ( 0%)]  \tLoss:   92.541626\trec:   65.710968\tkl:   26.830656\n",
      "Epoch: 485 [10100/50000 (20%)]  \tLoss:   94.664490\trec:   68.738808\tkl:   25.925680\n",
      "Epoch: 485 [20100/50000 (40%)]  \tLoss:   90.930283\trec:   64.985603\tkl:   25.944679\n",
      "Epoch: 485 [30100/50000 (60%)]  \tLoss:   97.446030\trec:   70.330055\tkl:   27.115980\n",
      "Epoch: 485 [40100/50000 (80%)]  \tLoss:   94.355934\trec:   67.893471\tkl:   26.462467\n",
      "====> Epoch: 485 Average train loss: 91.8029\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9465\n",
      "Epoch: 486 [  100/50000 ( 0%)]  \tLoss:   88.764282\trec:   63.120224\tkl:   25.644062\n",
      "Epoch: 486 [10100/50000 (20%)]  \tLoss:   91.908203\trec:   65.749260\tkl:   26.158945\n",
      "Epoch: 486 [20100/50000 (40%)]  \tLoss:   91.791580\trec:   65.732803\tkl:   26.058777\n",
      "Epoch: 486 [30100/50000 (60%)]  \tLoss:   96.187119\trec:   70.068604\tkl:   26.118507\n",
      "Epoch: 486 [40100/50000 (80%)]  \tLoss:   90.207802\trec:   63.972301\tkl:   26.235495\n",
      "====> Epoch: 486 Average train loss: 91.8048\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9966\n",
      "Epoch: 487 [  100/50000 ( 0%)]  \tLoss:   88.089531\trec:   62.921513\tkl:   25.168020\n",
      "Epoch: 487 [10100/50000 (20%)]  \tLoss:   90.880562\trec:   64.531631\tkl:   26.348932\n",
      "Epoch: 487 [20100/50000 (40%)]  \tLoss:   94.036423\trec:   67.546570\tkl:   26.489855\n",
      "Epoch: 487 [30100/50000 (60%)]  \tLoss:   93.047798\trec:   66.258980\tkl:   26.788826\n",
      "Epoch: 487 [40100/50000 (80%)]  \tLoss:   91.890450\trec:   65.870705\tkl:   26.019745\n",
      "====> Epoch: 487 Average train loss: 91.7886\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9856\n",
      "Epoch: 488 [  100/50000 ( 0%)]  \tLoss:   94.494911\trec:   67.366547\tkl:   27.128366\n",
      "Epoch: 488 [10100/50000 (20%)]  \tLoss:   92.920837\trec:   66.100845\tkl:   26.819992\n",
      "Epoch: 488 [20100/50000 (40%)]  \tLoss:   93.360527\trec:   67.203911\tkl:   26.156616\n",
      "Epoch: 488 [30100/50000 (60%)]  \tLoss:   90.131264\trec:   64.288116\tkl:   25.843151\n",
      "Epoch: 488 [40100/50000 (80%)]  \tLoss:   92.351501\trec:   66.323822\tkl:   26.027676\n",
      "====> Epoch: 488 Average train loss: 91.7578\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9737\n",
      "Epoch: 489 [  100/50000 ( 0%)]  \tLoss:   91.219513\trec:   65.443092\tkl:   25.776419\n",
      "Epoch: 489 [10100/50000 (20%)]  \tLoss:   94.106606\trec:   67.980766\tkl:   26.125841\n",
      "Epoch: 489 [20100/50000 (40%)]  \tLoss:   92.353973\trec:   66.554909\tkl:   25.799067\n",
      "Epoch: 489 [30100/50000 (60%)]  \tLoss:   90.739647\trec:   64.736633\tkl:   26.003017\n",
      "Epoch: 489 [40100/50000 (80%)]  \tLoss:   91.525002\trec:   66.455933\tkl:   25.069067\n",
      "====> Epoch: 489 Average train loss: 91.7599\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9773\n",
      "Epoch: 490 [  100/50000 ( 0%)]  \tLoss:   93.255234\trec:   67.311150\tkl:   25.944078\n",
      "Epoch: 490 [10100/50000 (20%)]  \tLoss:   93.209824\trec:   66.698120\tkl:   26.511698\n",
      "Epoch: 490 [20100/50000 (40%)]  \tLoss:   88.780167\trec:   63.703606\tkl:   25.076557\n",
      "Epoch: 490 [30100/50000 (60%)]  \tLoss:   91.670128\trec:   66.281960\tkl:   25.388166\n",
      "Epoch: 490 [40100/50000 (80%)]  \tLoss:   92.912163\trec:   66.615791\tkl:   26.296373\n",
      "====> Epoch: 490 Average train loss: 91.7589\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0383\n",
      "Epoch: 491 [  100/50000 ( 0%)]  \tLoss:   94.230476\trec:   67.659645\tkl:   26.570829\n",
      "Epoch: 491 [10100/50000 (20%)]  \tLoss:   90.753365\trec:   64.904137\tkl:   25.849226\n",
      "Epoch: 491 [20100/50000 (40%)]  \tLoss:   95.102745\trec:   68.792358\tkl:   26.310377\n",
      "Epoch: 491 [30100/50000 (60%)]  \tLoss:   90.500778\trec:   64.826736\tkl:   25.674040\n",
      "Epoch: 491 [40100/50000 (80%)]  \tLoss:   91.304977\trec:   64.659912\tkl:   26.645063\n",
      "====> Epoch: 491 Average train loss: 91.7841\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9379\n",
      "Epoch: 492 [  100/50000 ( 0%)]  \tLoss:   91.529610\trec:   65.092606\tkl:   26.437006\n",
      "Epoch: 492 [10100/50000 (20%)]  \tLoss:   88.456123\trec:   63.340912\tkl:   25.115210\n",
      "Epoch: 492 [20100/50000 (40%)]  \tLoss:   92.390602\trec:   66.218521\tkl:   26.172079\n",
      "Epoch: 492 [30100/50000 (60%)]  \tLoss:   90.387009\trec:   64.833633\tkl:   25.553381\n",
      "Epoch: 492 [40100/50000 (80%)]  \tLoss:   90.641113\trec:   64.968506\tkl:   25.672602\n",
      "====> Epoch: 492 Average train loss: 91.7802\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9412\n",
      "Epoch: 493 [  100/50000 ( 0%)]  \tLoss:   93.204140\trec:   67.759087\tkl:   25.445055\n",
      "Epoch: 493 [10100/50000 (20%)]  \tLoss:   88.750015\trec:   63.273335\tkl:   25.476681\n",
      "Epoch: 493 [20100/50000 (40%)]  \tLoss:   89.596924\trec:   63.383835\tkl:   26.213085\n",
      "Epoch: 493 [30100/50000 (60%)]  \tLoss:   89.876015\trec:   64.555771\tkl:   25.320238\n",
      "Epoch: 493 [40100/50000 (80%)]  \tLoss:   92.792244\trec:   65.957657\tkl:   26.834585\n",
      "====> Epoch: 493 Average train loss: 91.7498\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0610\n",
      "Epoch: 494 [  100/50000 ( 0%)]  \tLoss:   92.122696\trec:   66.096672\tkl:   26.026020\n",
      "Epoch: 494 [10100/50000 (20%)]  \tLoss:   95.494431\trec:   69.251427\tkl:   26.243010\n",
      "Epoch: 494 [20100/50000 (40%)]  \tLoss:   90.394646\trec:   64.078308\tkl:   26.316332\n",
      "Epoch: 494 [30100/50000 (60%)]  \tLoss:   90.433418\trec:   64.716064\tkl:   25.717358\n",
      "Epoch: 494 [40100/50000 (80%)]  \tLoss:   90.211220\trec:   64.362564\tkl:   25.848650\n",
      "====> Epoch: 494 Average train loss: 91.7543\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9939\n",
      "Epoch: 495 [  100/50000 ( 0%)]  \tLoss:   91.809975\trec:   65.676125\tkl:   26.133846\n",
      "Epoch: 495 [10100/50000 (20%)]  \tLoss:   93.473434\trec:   67.442764\tkl:   26.030678\n",
      "Epoch: 495 [20100/50000 (40%)]  \tLoss:   92.844452\trec:   66.463860\tkl:   26.380594\n",
      "Epoch: 495 [30100/50000 (60%)]  \tLoss:   92.848778\trec:   67.161972\tkl:   25.686804\n",
      "Epoch: 495 [40100/50000 (80%)]  \tLoss:   92.246521\trec:   65.788193\tkl:   26.458330\n",
      "====> Epoch: 495 Average train loss: 91.7489\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9632\n",
      "Epoch: 496 [  100/50000 ( 0%)]  \tLoss:   93.327934\trec:   67.260643\tkl:   26.067291\n",
      "Epoch: 496 [10100/50000 (20%)]  \tLoss:   92.132370\trec:   66.138931\tkl:   25.993437\n",
      "Epoch: 496 [20100/50000 (40%)]  \tLoss:   96.615997\trec:   69.745567\tkl:   26.870434\n",
      "Epoch: 496 [30100/50000 (60%)]  \tLoss:   88.040466\trec:   62.902401\tkl:   25.138065\n",
      "Epoch: 496 [40100/50000 (80%)]  \tLoss:   88.992569\trec:   63.325195\tkl:   25.667376\n",
      "====> Epoch: 496 Average train loss: 91.7390\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0364\n",
      "Epoch: 497 [  100/50000 ( 0%)]  \tLoss:   91.992714\trec:   65.518379\tkl:   26.474340\n",
      "Epoch: 497 [10100/50000 (20%)]  \tLoss:   92.228638\trec:   65.356163\tkl:   26.872477\n",
      "Epoch: 497 [20100/50000 (40%)]  \tLoss:   89.077339\trec:   63.654404\tkl:   25.422934\n",
      "Epoch: 497 [30100/50000 (60%)]  \tLoss:   88.821518\trec:   63.716328\tkl:   25.105200\n",
      "Epoch: 497 [40100/50000 (80%)]  \tLoss:   89.026901\trec:   63.313484\tkl:   25.713419\n",
      "====> Epoch: 497 Average train loss: 91.7470\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9532\n",
      "Epoch: 498 [  100/50000 ( 0%)]  \tLoss:   93.270073\trec:   66.700485\tkl:   26.569592\n",
      "Epoch: 498 [10100/50000 (20%)]  \tLoss:   91.498688\trec:   64.558243\tkl:   26.940447\n",
      "Epoch: 498 [20100/50000 (40%)]  \tLoss:   89.766716\trec:   63.433907\tkl:   26.332815\n",
      "Epoch: 498 [30100/50000 (60%)]  \tLoss:   87.090279\trec:   61.691288\tkl:   25.398996\n",
      "Epoch: 498 [40100/50000 (80%)]  \tLoss:   90.868111\trec:   64.985664\tkl:   25.882446\n",
      "====> Epoch: 498 Average train loss: 91.7359\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9421\n",
      "Epoch: 499 [  100/50000 ( 0%)]  \tLoss:   89.482697\trec:   64.719688\tkl:   24.763004\n",
      "Epoch: 499 [10100/50000 (20%)]  \tLoss:   93.506714\trec:   67.328110\tkl:   26.178602\n",
      "Epoch: 499 [20100/50000 (40%)]  \tLoss:   91.797615\trec:   65.720940\tkl:   26.076683\n",
      "Epoch: 499 [30100/50000 (60%)]  \tLoss:   94.617325\trec:   68.176254\tkl:   26.441074\n",
      "Epoch: 499 [40100/50000 (80%)]  \tLoss:   89.164375\trec:   63.792908\tkl:   25.371462\n",
      "====> Epoch: 499 Average train loss: 91.7135\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0457\n",
      "Epoch: 500 [  100/50000 ( 0%)]  \tLoss:   88.465195\trec:   63.005589\tkl:   25.459599\n",
      "Epoch: 500 [10100/50000 (20%)]  \tLoss:   90.848022\trec:   64.311104\tkl:   26.536917\n",
      "Epoch: 500 [20100/50000 (40%)]  \tLoss:   93.899979\trec:   67.300446\tkl:   26.599541\n",
      "Epoch: 500 [30100/50000 (60%)]  \tLoss:   88.055153\trec:   62.548851\tkl:   25.506306\n",
      "Epoch: 500 [40100/50000 (80%)]  \tLoss:   92.060333\trec:   67.156616\tkl:   24.903715\n",
      "====> Epoch: 500 Average train loss: 91.7434\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9240\n",
      "Epoch: 501 [  100/50000 ( 0%)]  \tLoss:   94.552773\trec:   68.128113\tkl:   26.424658\n",
      "Epoch: 501 [10100/50000 (20%)]  \tLoss:   94.859726\trec:   68.775085\tkl:   26.084641\n",
      "Epoch: 501 [20100/50000 (40%)]  \tLoss:   88.719147\trec:   63.287582\tkl:   25.431564\n",
      "Epoch: 501 [30100/50000 (60%)]  \tLoss:   91.773003\trec:   64.578201\tkl:   27.194799\n",
      "Epoch: 501 [40100/50000 (80%)]  \tLoss:   89.669106\trec:   63.925194\tkl:   25.743917\n",
      "====> Epoch: 501 Average train loss: 91.7114\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9711\n",
      "Epoch: 502 [  100/50000 ( 0%)]  \tLoss:   91.625847\trec:   65.846786\tkl:   25.779064\n",
      "Epoch: 502 [10100/50000 (20%)]  \tLoss:   89.794014\trec:   64.200310\tkl:   25.593699\n",
      "Epoch: 502 [20100/50000 (40%)]  \tLoss:   92.358360\trec:   65.978897\tkl:   26.379459\n",
      "Epoch: 502 [30100/50000 (60%)]  \tLoss:   93.049370\trec:   67.039009\tkl:   26.010361\n",
      "Epoch: 502 [40100/50000 (80%)]  \tLoss:   94.134003\trec:   67.705322\tkl:   26.428686\n",
      "====> Epoch: 502 Average train loss: 91.7091\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9127\n",
      "Epoch: 503 [  100/50000 ( 0%)]  \tLoss:   90.846191\trec:   64.821716\tkl:   26.024471\n",
      "Epoch: 503 [10100/50000 (20%)]  \tLoss:   93.426758\trec:   67.655365\tkl:   25.771389\n",
      "Epoch: 503 [20100/50000 (40%)]  \tLoss:   93.025215\trec:   67.274750\tkl:   25.750462\n",
      "Epoch: 503 [30100/50000 (60%)]  \tLoss:   95.214653\trec:   68.102898\tkl:   27.111759\n",
      "Epoch: 503 [40100/50000 (80%)]  \tLoss:   91.204842\trec:   65.592453\tkl:   25.612392\n",
      "====> Epoch: 503 Average train loss: 91.6838\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9160\n",
      "Epoch: 504 [  100/50000 ( 0%)]  \tLoss:   89.120216\trec:   63.698086\tkl:   25.422127\n",
      "Epoch: 504 [10100/50000 (20%)]  \tLoss:   93.374489\trec:   66.044266\tkl:   27.330229\n",
      "Epoch: 504 [20100/50000 (40%)]  \tLoss:   89.343933\trec:   63.108734\tkl:   26.235199\n",
      "Epoch: 504 [30100/50000 (60%)]  \tLoss:   91.316391\trec:   65.868958\tkl:   25.447439\n",
      "Epoch: 504 [40100/50000 (80%)]  \tLoss:   93.643181\trec:   67.641006\tkl:   26.002176\n",
      "====> Epoch: 504 Average train loss: 91.6931\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9597\n",
      "Epoch: 505 [  100/50000 ( 0%)]  \tLoss:   89.891991\trec:   63.984364\tkl:   25.907631\n",
      "Epoch: 505 [10100/50000 (20%)]  \tLoss:   87.897194\trec:   62.656498\tkl:   25.240696\n",
      "Epoch: 505 [20100/50000 (40%)]  \tLoss:   90.506958\trec:   64.256584\tkl:   26.250374\n",
      "Epoch: 505 [30100/50000 (60%)]  \tLoss:   85.543388\trec:   60.900906\tkl:   24.642483\n",
      "Epoch: 505 [40100/50000 (80%)]  \tLoss:   91.249229\trec:   65.795502\tkl:   25.453726\n",
      "====> Epoch: 505 Average train loss: 91.6784\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9956\n",
      "Epoch: 506 [  100/50000 ( 0%)]  \tLoss:   89.266899\trec:   63.187824\tkl:   26.079079\n",
      "Epoch: 506 [10100/50000 (20%)]  \tLoss:   87.582108\trec:   62.722614\tkl:   24.859489\n",
      "Epoch: 506 [20100/50000 (40%)]  \tLoss:   94.759819\trec:   67.846992\tkl:   26.912828\n",
      "Epoch: 506 [30100/50000 (60%)]  \tLoss:   94.320496\trec:   67.694168\tkl:   26.626326\n",
      "Epoch: 506 [40100/50000 (80%)]  \tLoss:   91.625854\trec:   64.982239\tkl:   26.643612\n",
      "====> Epoch: 506 Average train loss: 91.6796\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9589\n",
      "Epoch: 507 [  100/50000 ( 0%)]  \tLoss:   93.903305\trec:   67.076591\tkl:   26.826721\n",
      "Epoch: 507 [10100/50000 (20%)]  \tLoss:   89.904831\trec:   64.335030\tkl:   25.569803\n",
      "Epoch: 507 [20100/50000 (40%)]  \tLoss:   89.076309\trec:   64.055969\tkl:   25.020336\n",
      "Epoch: 507 [30100/50000 (60%)]  \tLoss:   90.742508\trec:   64.514069\tkl:   26.228439\n",
      "Epoch: 507 [40100/50000 (80%)]  \tLoss:   90.727104\trec:   64.863152\tkl:   25.863953\n",
      "====> Epoch: 507 Average train loss: 91.6857\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8995\n",
      "Epoch: 508 [  100/50000 ( 0%)]  \tLoss:   92.431305\trec:   65.699509\tkl:   26.731791\n",
      "Epoch: 508 [10100/50000 (20%)]  \tLoss:   91.257576\trec:   65.385223\tkl:   25.872353\n",
      "Epoch: 508 [20100/50000 (40%)]  \tLoss:   89.913048\trec:   63.553974\tkl:   26.359070\n",
      "Epoch: 508 [30100/50000 (60%)]  \tLoss:   89.400780\trec:   63.313744\tkl:   26.087036\n",
      "Epoch: 508 [40100/50000 (80%)]  \tLoss:   90.658218\trec:   64.387978\tkl:   26.270243\n",
      "====> Epoch: 508 Average train loss: 91.6792\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9100\n",
      "Epoch: 509 [  100/50000 ( 0%)]  \tLoss:   91.680069\trec:   65.349045\tkl:   26.331020\n",
      "Epoch: 509 [10100/50000 (20%)]  \tLoss:   91.966949\trec:   66.458496\tkl:   25.508459\n",
      "Epoch: 509 [20100/50000 (40%)]  \tLoss:   92.327461\trec:   66.656952\tkl:   25.670507\n",
      "Epoch: 509 [30100/50000 (60%)]  \tLoss:   95.744972\trec:   68.905029\tkl:   26.839943\n",
      "Epoch: 509 [40100/50000 (80%)]  \tLoss:   94.907677\trec:   67.968384\tkl:   26.939297\n",
      "====> Epoch: 509 Average train loss: 91.6793\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9018\n",
      "Epoch: 510 [  100/50000 ( 0%)]  \tLoss:   86.376862\trec:   61.810131\tkl:   24.566732\n",
      "Epoch: 510 [10100/50000 (20%)]  \tLoss:   89.841766\trec:   64.107872\tkl:   25.733898\n",
      "Epoch: 510 [20100/50000 (40%)]  \tLoss:   90.212944\trec:   65.088844\tkl:   25.124102\n",
      "Epoch: 510 [30100/50000 (60%)]  \tLoss:   90.951347\trec:   65.015450\tkl:   25.935898\n",
      "Epoch: 510 [40100/50000 (80%)]  \tLoss:   94.222130\trec:   68.094727\tkl:   26.127398\n",
      "====> Epoch: 510 Average train loss: 91.6857\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9207\n",
      "Epoch: 511 [  100/50000 ( 0%)]  \tLoss:   94.118774\trec:   68.463028\tkl:   25.655748\n",
      "Epoch: 511 [10100/50000 (20%)]  \tLoss:   88.925682\trec:   63.743309\tkl:   25.182377\n",
      "Epoch: 511 [20100/50000 (40%)]  \tLoss:   89.712051\trec:   64.090919\tkl:   25.621134\n",
      "Epoch: 511 [30100/50000 (60%)]  \tLoss:   90.638542\trec:   65.039429\tkl:   25.599108\n",
      "Epoch: 511 [40100/50000 (80%)]  \tLoss:   90.468063\trec:   64.864555\tkl:   25.603512\n",
      "====> Epoch: 511 Average train loss: 91.6686\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9221\n",
      "Epoch: 512 [  100/50000 ( 0%)]  \tLoss:   88.620834\trec:   63.347561\tkl:   25.273281\n",
      "Epoch: 512 [10100/50000 (20%)]  \tLoss:   91.103279\trec:   65.221962\tkl:   25.881323\n",
      "Epoch: 512 [20100/50000 (40%)]  \tLoss:   93.526901\trec:   67.460747\tkl:   26.066149\n",
      "Epoch: 512 [30100/50000 (60%)]  \tLoss:   88.164200\trec:   62.841198\tkl:   25.323002\n",
      "Epoch: 512 [40100/50000 (80%)]  \tLoss:   89.503555\trec:   64.311989\tkl:   25.191559\n",
      "====> Epoch: 512 Average train loss: 91.6696\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0062\n",
      "Epoch: 513 [  100/50000 ( 0%)]  \tLoss:   93.911072\trec:   67.785034\tkl:   26.126032\n",
      "Epoch: 513 [10100/50000 (20%)]  \tLoss:   91.457863\trec:   65.294601\tkl:   26.163263\n",
      "Epoch: 513 [20100/50000 (40%)]  \tLoss:   88.906876\trec:   63.029919\tkl:   25.876957\n",
      "Epoch: 513 [30100/50000 (60%)]  \tLoss:   89.316376\trec:   64.140656\tkl:   25.175722\n",
      "Epoch: 513 [40100/50000 (80%)]  \tLoss:   91.040016\trec:   64.665527\tkl:   26.374489\n",
      "====> Epoch: 513 Average train loss: 91.6490\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9261\n",
      "Epoch: 514 [  100/50000 ( 0%)]  \tLoss:   88.687325\trec:   63.607677\tkl:   25.079638\n",
      "Epoch: 514 [10100/50000 (20%)]  \tLoss:   91.714058\trec:   65.363556\tkl:   26.350502\n",
      "Epoch: 514 [20100/50000 (40%)]  \tLoss:   89.751953\trec:   64.061539\tkl:   25.690405\n",
      "Epoch: 514 [30100/50000 (60%)]  \tLoss:   93.142067\trec:   66.281555\tkl:   26.860514\n",
      "Epoch: 514 [40100/50000 (80%)]  \tLoss:   90.601059\trec:   64.153778\tkl:   26.447287\n",
      "====> Epoch: 514 Average train loss: 91.6704\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8772\n",
      "Epoch: 515 [  100/50000 ( 0%)]  \tLoss:   89.227592\trec:   62.844048\tkl:   26.383547\n",
      "Epoch: 515 [10100/50000 (20%)]  \tLoss:   89.687019\trec:   64.440376\tkl:   25.246637\n",
      "Epoch: 515 [20100/50000 (40%)]  \tLoss:   90.932030\trec:   64.968307\tkl:   25.963722\n",
      "Epoch: 515 [30100/50000 (60%)]  \tLoss:   97.403008\trec:   70.857018\tkl:   26.545996\n",
      "Epoch: 515 [40100/50000 (80%)]  \tLoss:   92.755112\trec:   67.300728\tkl:   25.454380\n",
      "====> Epoch: 515 Average train loss: 91.6462\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9398\n",
      "Epoch: 516 [  100/50000 ( 0%)]  \tLoss:   90.587769\trec:   64.630524\tkl:   25.957241\n",
      "Epoch: 516 [10100/50000 (20%)]  \tLoss:   91.021286\trec:   65.219902\tkl:   25.801384\n",
      "Epoch: 516 [20100/50000 (40%)]  \tLoss:   92.165581\trec:   66.408653\tkl:   25.756933\n",
      "Epoch: 516 [30100/50000 (60%)]  \tLoss:   91.176392\trec:   64.335197\tkl:   26.841194\n",
      "Epoch: 516 [40100/50000 (80%)]  \tLoss:   92.964851\trec:   66.717293\tkl:   26.247555\n",
      "====> Epoch: 516 Average train loss: 91.6681\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8974\n",
      "Epoch: 517 [  100/50000 ( 0%)]  \tLoss:   91.638573\trec:   65.236542\tkl:   26.402025\n",
      "Epoch: 517 [10100/50000 (20%)]  \tLoss:   93.710030\trec:   67.647629\tkl:   26.062401\n",
      "Epoch: 517 [20100/50000 (40%)]  \tLoss:   91.402748\trec:   64.832535\tkl:   26.570217\n",
      "Epoch: 517 [30100/50000 (60%)]  \tLoss:   95.300133\trec:   68.197067\tkl:   27.103067\n",
      "Epoch: 517 [40100/50000 (80%)]  \tLoss:   96.102966\trec:   68.292984\tkl:   27.809980\n",
      "====> Epoch: 517 Average train loss: 91.6361\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9831\n",
      "Epoch: 518 [  100/50000 ( 0%)]  \tLoss:   92.880135\trec:   66.720413\tkl:   26.159725\n",
      "Epoch: 518 [10100/50000 (20%)]  \tLoss:   88.966560\trec:   63.234360\tkl:   25.732197\n",
      "Epoch: 518 [20100/50000 (40%)]  \tLoss:   92.886864\trec:   65.993805\tkl:   26.893053\n",
      "Epoch: 518 [30100/50000 (60%)]  \tLoss:   90.033318\trec:   63.978134\tkl:   26.055189\n",
      "Epoch: 518 [40100/50000 (80%)]  \tLoss:   87.025002\trec:   61.868221\tkl:   25.156782\n",
      "====> Epoch: 518 Average train loss: 91.6550\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9624\n",
      "Epoch: 519 [  100/50000 ( 0%)]  \tLoss:   89.931747\trec:   63.800682\tkl:   26.131063\n",
      "Epoch: 519 [10100/50000 (20%)]  \tLoss:   87.185684\trec:   61.234783\tkl:   25.950897\n",
      "Epoch: 519 [20100/50000 (40%)]  \tLoss:   91.427040\trec:   65.246666\tkl:   26.180368\n",
      "Epoch: 519 [30100/50000 (60%)]  \tLoss:   97.059341\trec:   69.981949\tkl:   27.077389\n",
      "Epoch: 519 [40100/50000 (80%)]  \tLoss:   91.365997\trec:   65.824333\tkl:   25.541664\n",
      "====> Epoch: 519 Average train loss: 91.6043\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8899\n",
      "Epoch: 520 [  100/50000 ( 0%)]  \tLoss:   91.652214\trec:   65.089348\tkl:   26.562868\n",
      "Epoch: 520 [10100/50000 (20%)]  \tLoss:   96.214096\trec:   69.418877\tkl:   26.795219\n",
      "Epoch: 520 [20100/50000 (40%)]  \tLoss:   88.630737\trec:   63.661011\tkl:   24.969736\n",
      "Epoch: 520 [30100/50000 (60%)]  \tLoss:   91.075172\trec:   64.585777\tkl:   26.489391\n",
      "Epoch: 520 [40100/50000 (80%)]  \tLoss:   90.324829\trec:   64.685852\tkl:   25.638979\n",
      "====> Epoch: 520 Average train loss: 91.6369\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8440\n",
      "Epoch: 521 [  100/50000 ( 0%)]  \tLoss:   87.807693\trec:   61.303505\tkl:   26.504187\n",
      "Epoch: 521 [10100/50000 (20%)]  \tLoss:   90.038193\trec:   64.044357\tkl:   25.993832\n",
      "Epoch: 521 [20100/50000 (40%)]  \tLoss:   91.325584\trec:   65.810913\tkl:   25.514666\n",
      "Epoch: 521 [30100/50000 (60%)]  \tLoss:   93.618553\trec:   67.429695\tkl:   26.188862\n",
      "Epoch: 521 [40100/50000 (80%)]  \tLoss:   92.622383\trec:   66.374245\tkl:   26.248137\n",
      "====> Epoch: 521 Average train loss: 91.6197\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9564\n",
      "Epoch: 522 [  100/50000 ( 0%)]  \tLoss:   94.552635\trec:   67.579018\tkl:   26.973623\n",
      "Epoch: 522 [10100/50000 (20%)]  \tLoss:   87.336861\trec:   62.035389\tkl:   25.301477\n",
      "Epoch: 522 [20100/50000 (40%)]  \tLoss:   91.541710\trec:   65.219818\tkl:   26.321886\n",
      "Epoch: 522 [30100/50000 (60%)]  \tLoss:   91.052711\trec:   65.239548\tkl:   25.813166\n",
      "Epoch: 522 [40100/50000 (80%)]  \tLoss:   86.382469\trec:   61.311993\tkl:   25.070480\n",
      "====> Epoch: 522 Average train loss: 91.6147\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9328\n",
      "Epoch: 523 [  100/50000 ( 0%)]  \tLoss:   91.997879\trec:   65.827141\tkl:   26.170738\n",
      "Epoch: 523 [10100/50000 (20%)]  \tLoss:   95.685272\trec:   68.163040\tkl:   27.522236\n",
      "Epoch: 523 [20100/50000 (40%)]  \tLoss:   93.912735\trec:   67.513992\tkl:   26.398739\n",
      "Epoch: 523 [30100/50000 (60%)]  \tLoss:   91.092613\trec:   65.688309\tkl:   25.404306\n",
      "Epoch: 523 [40100/50000 (80%)]  \tLoss:   89.729576\trec:   64.552139\tkl:   25.177443\n",
      "====> Epoch: 523 Average train loss: 91.5866\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9925\n",
      "Epoch: 524 [  100/50000 ( 0%)]  \tLoss:   93.630470\trec:   67.767494\tkl:   25.862968\n",
      "Epoch: 524 [10100/50000 (20%)]  \tLoss:   90.990417\trec:   64.732956\tkl:   26.257460\n",
      "Epoch: 524 [20100/50000 (40%)]  \tLoss:   90.922127\trec:   64.952736\tkl:   25.969389\n",
      "Epoch: 524 [30100/50000 (60%)]  \tLoss:   93.043037\trec:   66.828835\tkl:   26.214207\n",
      "Epoch: 524 [40100/50000 (80%)]  \tLoss:   90.344490\trec:   64.618416\tkl:   25.726074\n",
      "====> Epoch: 524 Average train loss: 91.6187\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8750\n",
      "Epoch: 525 [  100/50000 ( 0%)]  \tLoss:   92.323868\trec:   65.484787\tkl:   26.839071\n",
      "Epoch: 525 [10100/50000 (20%)]  \tLoss:   90.175766\trec:   64.535515\tkl:   25.640253\n",
      "Epoch: 525 [20100/50000 (40%)]  \tLoss:   92.821297\trec:   65.965965\tkl:   26.855331\n",
      "Epoch: 525 [30100/50000 (60%)]  \tLoss:   91.875275\trec:   65.664093\tkl:   26.211176\n",
      "Epoch: 525 [40100/50000 (80%)]  \tLoss:   92.803757\trec:   66.498901\tkl:   26.304861\n",
      "====> Epoch: 525 Average train loss: 91.5925\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9239\n",
      "Epoch: 526 [  100/50000 ( 0%)]  \tLoss:   91.899063\trec:   65.250145\tkl:   26.648916\n",
      "Epoch: 526 [10100/50000 (20%)]  \tLoss:   86.697304\trec:   61.479530\tkl:   25.217775\n",
      "Epoch: 526 [20100/50000 (40%)]  \tLoss:   94.745827\trec:   68.540092\tkl:   26.205732\n",
      "Epoch: 526 [30100/50000 (60%)]  \tLoss:   88.705894\trec:   63.027790\tkl:   25.678101\n",
      "Epoch: 526 [40100/50000 (80%)]  \tLoss:   91.129509\trec:   64.682022\tkl:   26.447487\n",
      "====> Epoch: 526 Average train loss: 91.5887\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 95.0293\n",
      "Epoch: 527 [  100/50000 ( 0%)]  \tLoss:   90.866997\trec:   64.756676\tkl:   26.110319\n",
      "Epoch: 527 [10100/50000 (20%)]  \tLoss:   93.745193\trec:   68.092766\tkl:   25.652426\n",
      "Epoch: 527 [20100/50000 (40%)]  \tLoss:   91.450821\trec:   66.664719\tkl:   24.786100\n",
      "Epoch: 527 [30100/50000 (60%)]  \tLoss:   92.176071\trec:   65.742058\tkl:   26.434013\n",
      "Epoch: 527 [40100/50000 (80%)]  \tLoss:   92.953308\trec:   65.952530\tkl:   27.000780\n",
      "====> Epoch: 527 Average train loss: 91.5964\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9552\n",
      "Epoch: 528 [  100/50000 ( 0%)]  \tLoss:   92.075096\trec:   66.114258\tkl:   25.960833\n",
      "Epoch: 528 [10100/50000 (20%)]  \tLoss:   89.254158\trec:   64.146538\tkl:   25.107622\n",
      "Epoch: 528 [20100/50000 (40%)]  \tLoss:   89.385048\trec:   62.547440\tkl:   26.837605\n",
      "Epoch: 528 [30100/50000 (60%)]  \tLoss:   86.394249\trec:   62.031906\tkl:   24.362339\n",
      "Epoch: 528 [40100/50000 (80%)]  \tLoss:   95.230812\trec:   68.420631\tkl:   26.810175\n",
      "====> Epoch: 528 Average train loss: 91.5646\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8947\n",
      "Epoch: 529 [  100/50000 ( 0%)]  \tLoss:   93.461510\trec:   67.476974\tkl:   25.984541\n",
      "Epoch: 529 [10100/50000 (20%)]  \tLoss:   91.737267\trec:   65.958687\tkl:   25.778584\n",
      "Epoch: 529 [20100/50000 (40%)]  \tLoss:   89.033493\trec:   62.920570\tkl:   26.112923\n",
      "Epoch: 529 [30100/50000 (60%)]  \tLoss:   92.817520\trec:   66.532295\tkl:   26.285217\n",
      "Epoch: 529 [40100/50000 (80%)]  \tLoss:   93.829941\trec:   66.204788\tkl:   27.625149\n",
      "====> Epoch: 529 Average train loss: 91.5681\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9342\n",
      "Epoch: 530 [  100/50000 ( 0%)]  \tLoss:   93.890457\trec:   67.286774\tkl:   26.603682\n",
      "Epoch: 530 [10100/50000 (20%)]  \tLoss:   91.685722\trec:   65.463181\tkl:   26.222534\n",
      "Epoch: 530 [20100/50000 (40%)]  \tLoss:   94.805695\trec:   67.547356\tkl:   27.258337\n",
      "Epoch: 530 [30100/50000 (60%)]  \tLoss:   90.455162\trec:   64.540665\tkl:   25.914497\n",
      "Epoch: 530 [40100/50000 (80%)]  \tLoss:   87.815048\trec:   62.174938\tkl:   25.640106\n",
      "====> Epoch: 530 Average train loss: 91.5754\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8506\n",
      "Epoch: 531 [  100/50000 ( 0%)]  \tLoss:   92.361717\trec:   66.138748\tkl:   26.222963\n",
      "Epoch: 531 [10100/50000 (20%)]  \tLoss:   90.968338\trec:   65.467735\tkl:   25.500607\n",
      "Epoch: 531 [20100/50000 (40%)]  \tLoss:   89.737335\trec:   63.290955\tkl:   26.446379\n",
      "Epoch: 531 [30100/50000 (60%)]  \tLoss:   94.085693\trec:   67.462799\tkl:   26.622894\n",
      "Epoch: 531 [40100/50000 (80%)]  \tLoss:   90.501007\trec:   64.414093\tkl:   26.086910\n",
      "====> Epoch: 531 Average train loss: 91.5604\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8273\n",
      "Epoch: 532 [  100/50000 ( 0%)]  \tLoss:   91.654549\trec:   65.236359\tkl:   26.418188\n",
      "Epoch: 532 [10100/50000 (20%)]  \tLoss:   94.238594\trec:   67.964714\tkl:   26.273876\n",
      "Epoch: 532 [20100/50000 (40%)]  \tLoss:   86.934959\trec:   61.611416\tkl:   25.323547\n",
      "Epoch: 532 [30100/50000 (60%)]  \tLoss:   94.606171\trec:   67.400810\tkl:   27.205360\n",
      "Epoch: 532 [40100/50000 (80%)]  \tLoss:   93.236443\trec:   67.222717\tkl:   26.013727\n",
      "====> Epoch: 532 Average train loss: 91.5506\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8470\n",
      "Epoch: 533 [  100/50000 ( 0%)]  \tLoss:   89.690361\trec:   64.565155\tkl:   25.125204\n",
      "Epoch: 533 [10100/50000 (20%)]  \tLoss:   92.043007\trec:   65.580040\tkl:   26.462959\n",
      "Epoch: 533 [20100/50000 (40%)]  \tLoss:   94.466042\trec:   68.196945\tkl:   26.269104\n",
      "Epoch: 533 [30100/50000 (60%)]  \tLoss:   89.432564\trec:   63.494129\tkl:   25.938438\n",
      "Epoch: 533 [40100/50000 (80%)]  \tLoss:   91.405655\trec:   65.507545\tkl:   25.898108\n",
      "====> Epoch: 533 Average train loss: 91.5926\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9567\n",
      "Epoch: 534 [  100/50000 ( 0%)]  \tLoss:   91.456665\trec:   65.204735\tkl:   26.251936\n",
      "Epoch: 534 [10100/50000 (20%)]  \tLoss:   88.022636\trec:   63.182705\tkl:   24.839926\n",
      "Epoch: 534 [20100/50000 (40%)]  \tLoss:   92.189812\trec:   65.525185\tkl:   26.664629\n",
      "Epoch: 534 [30100/50000 (60%)]  \tLoss:   91.848839\trec:   65.936325\tkl:   25.912506\n",
      "Epoch: 534 [40100/50000 (80%)]  \tLoss:   89.347183\trec:   64.235542\tkl:   25.111641\n",
      "====> Epoch: 534 Average train loss: 91.5367\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9680\n",
      "Epoch: 535 [  100/50000 ( 0%)]  \tLoss:   90.443047\trec:   64.271210\tkl:   26.171833\n",
      "Epoch: 535 [10100/50000 (20%)]  \tLoss:   88.993515\trec:   63.742180\tkl:   25.251333\n",
      "Epoch: 535 [20100/50000 (40%)]  \tLoss:   89.662109\trec:   63.725388\tkl:   25.936724\n",
      "Epoch: 535 [30100/50000 (60%)]  \tLoss:   94.165215\trec:   67.468796\tkl:   26.696419\n",
      "Epoch: 535 [40100/50000 (80%)]  \tLoss:   89.677498\trec:   64.190689\tkl:   25.486811\n",
      "====> Epoch: 535 Average train loss: 91.5648\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9253\n",
      "Epoch: 536 [  100/50000 ( 0%)]  \tLoss:   94.150703\trec:   68.332047\tkl:   25.818649\n",
      "Epoch: 536 [10100/50000 (20%)]  \tLoss:   94.490524\trec:   67.630592\tkl:   26.859936\n",
      "Epoch: 536 [20100/50000 (40%)]  \tLoss:   91.263397\trec:   65.736839\tkl:   25.526552\n",
      "Epoch: 536 [30100/50000 (60%)]  \tLoss:   89.180000\trec:   62.966290\tkl:   26.213705\n",
      "Epoch: 536 [40100/50000 (80%)]  \tLoss:   88.755699\trec:   62.843250\tkl:   25.912449\n",
      "====> Epoch: 536 Average train loss: 91.5505\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8597\n",
      "Epoch: 537 [  100/50000 ( 0%)]  \tLoss:   90.799324\trec:   65.400047\tkl:   25.399275\n",
      "Epoch: 537 [10100/50000 (20%)]  \tLoss:   92.266113\trec:   65.402283\tkl:   26.863823\n",
      "Epoch: 537 [20100/50000 (40%)]  \tLoss:   91.059990\trec:   65.353882\tkl:   25.706108\n",
      "Epoch: 537 [30100/50000 (60%)]  \tLoss:   91.688873\trec:   65.225983\tkl:   26.462894\n",
      "Epoch: 537 [40100/50000 (80%)]  \tLoss:   89.281250\trec:   63.559998\tkl:   25.721247\n",
      "====> Epoch: 537 Average train loss: 91.5265\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9022\n",
      "Epoch: 538 [  100/50000 ( 0%)]  \tLoss:   91.740799\trec:   65.497665\tkl:   26.243132\n",
      "Epoch: 538 [10100/50000 (20%)]  \tLoss:   91.593102\trec:   65.177986\tkl:   26.415117\n",
      "Epoch: 538 [20100/50000 (40%)]  \tLoss:   95.156494\trec:   69.429802\tkl:   25.726696\n",
      "Epoch: 538 [30100/50000 (60%)]  \tLoss:   95.912460\trec:   68.820343\tkl:   27.092121\n",
      "Epoch: 538 [40100/50000 (80%)]  \tLoss:   86.217567\trec:   59.991482\tkl:   26.226086\n",
      "====> Epoch: 538 Average train loss: 91.5434\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9352\n",
      "Epoch: 539 [  100/50000 ( 0%)]  \tLoss:   86.848221\trec:   61.834873\tkl:   25.013353\n",
      "Epoch: 539 [10100/50000 (20%)]  \tLoss:   92.077499\trec:   66.162506\tkl:   25.914986\n",
      "Epoch: 539 [20100/50000 (40%)]  \tLoss:   91.414200\trec:   64.669121\tkl:   26.745070\n",
      "Epoch: 539 [30100/50000 (60%)]  \tLoss:   90.394707\trec:   63.693245\tkl:   26.701454\n",
      "Epoch: 539 [40100/50000 (80%)]  \tLoss:   91.741364\trec:   64.798203\tkl:   26.943169\n",
      "====> Epoch: 539 Average train loss: 91.5187\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8448\n",
      "Epoch: 540 [  100/50000 ( 0%)]  \tLoss:   91.091240\trec:   65.352554\tkl:   25.738682\n",
      "Epoch: 540 [10100/50000 (20%)]  \tLoss:   90.527306\trec:   64.691208\tkl:   25.836098\n",
      "Epoch: 540 [20100/50000 (40%)]  \tLoss:   85.015099\trec:   60.469490\tkl:   24.545610\n",
      "Epoch: 540 [30100/50000 (60%)]  \tLoss:   90.835419\trec:   64.431053\tkl:   26.404362\n",
      "Epoch: 540 [40100/50000 (80%)]  \tLoss:   94.113686\trec:   66.993561\tkl:   27.120125\n",
      "====> Epoch: 540 Average train loss: 91.5124\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8650\n",
      "Epoch: 541 [  100/50000 ( 0%)]  \tLoss:   95.209793\trec:   68.287468\tkl:   26.922323\n",
      "Epoch: 541 [10100/50000 (20%)]  \tLoss:   90.899071\trec:   64.907555\tkl:   25.991512\n",
      "Epoch: 541 [20100/50000 (40%)]  \tLoss:   91.510216\trec:   65.461769\tkl:   26.048449\n",
      "Epoch: 541 [30100/50000 (60%)]  \tLoss:   92.107582\trec:   65.865791\tkl:   26.241789\n",
      "Epoch: 541 [40100/50000 (80%)]  \tLoss:   93.778015\trec:   67.228699\tkl:   26.549318\n",
      "====> Epoch: 541 Average train loss: 91.5180\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7788\n",
      "Epoch: 542 [  100/50000 ( 0%)]  \tLoss:   89.735565\trec:   63.878429\tkl:   25.857134\n",
      "Epoch: 542 [10100/50000 (20%)]  \tLoss:   94.268387\trec:   68.531723\tkl:   25.736668\n",
      "Epoch: 542 [20100/50000 (40%)]  \tLoss:   89.502357\trec:   64.088463\tkl:   25.413897\n",
      "Epoch: 542 [30100/50000 (60%)]  \tLoss:   89.698013\trec:   64.076828\tkl:   25.621191\n",
      "Epoch: 542 [40100/50000 (80%)]  \tLoss:   92.636932\trec:   66.213707\tkl:   26.423220\n",
      "====> Epoch: 542 Average train loss: 91.5241\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9110\n",
      "Epoch: 543 [  100/50000 ( 0%)]  \tLoss:   91.614479\trec:   65.700447\tkl:   25.914036\n",
      "Epoch: 543 [10100/50000 (20%)]  \tLoss:   87.850494\trec:   62.935574\tkl:   24.914925\n",
      "Epoch: 543 [20100/50000 (40%)]  \tLoss:   89.917595\trec:   64.424774\tkl:   25.492823\n",
      "Epoch: 543 [30100/50000 (60%)]  \tLoss:   90.740105\trec:   65.288597\tkl:   25.451506\n",
      "Epoch: 543 [40100/50000 (80%)]  \tLoss:   92.920830\trec:   66.474297\tkl:   26.446533\n",
      "====> Epoch: 543 Average train loss: 91.5097\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9753\n",
      "Epoch: 544 [  100/50000 ( 0%)]  \tLoss:   92.576424\trec:   66.208755\tkl:   26.367666\n",
      "Epoch: 544 [10100/50000 (20%)]  \tLoss:   91.389786\trec:   64.697968\tkl:   26.691818\n",
      "Epoch: 544 [20100/50000 (40%)]  \tLoss:   89.619125\trec:   64.966759\tkl:   24.652370\n",
      "Epoch: 544 [30100/50000 (60%)]  \tLoss:   94.569038\trec:   68.088203\tkl:   26.480839\n",
      "Epoch: 544 [40100/50000 (80%)]  \tLoss:   94.999542\trec:   67.759521\tkl:   27.240017\n",
      "====> Epoch: 544 Average train loss: 91.4913\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9748\n",
      "Epoch: 545 [  100/50000 ( 0%)]  \tLoss:   93.824753\trec:   67.854004\tkl:   25.970743\n",
      "Epoch: 545 [10100/50000 (20%)]  \tLoss:   95.810410\trec:   68.910324\tkl:   26.900085\n",
      "Epoch: 545 [20100/50000 (40%)]  \tLoss:   91.208649\trec:   65.851456\tkl:   25.357195\n",
      "Epoch: 545 [30100/50000 (60%)]  \tLoss:   91.172325\trec:   65.008095\tkl:   26.164232\n",
      "Epoch: 545 [40100/50000 (80%)]  \tLoss:   93.073395\trec:   67.082695\tkl:   25.990705\n",
      "====> Epoch: 545 Average train loss: 91.5175\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8065\n",
      "Epoch: 546 [  100/50000 ( 0%)]  \tLoss:   93.289131\trec:   67.092522\tkl:   26.196608\n",
      "Epoch: 546 [10100/50000 (20%)]  \tLoss:   90.417671\trec:   64.968758\tkl:   25.448921\n",
      "Epoch: 546 [20100/50000 (40%)]  \tLoss:   92.285782\trec:   66.296928\tkl:   25.988850\n",
      "Epoch: 546 [30100/50000 (60%)]  \tLoss:   93.284126\trec:   67.142891\tkl:   26.141239\n",
      "Epoch: 546 [40100/50000 (80%)]  \tLoss:   90.070641\trec:   64.254585\tkl:   25.816057\n",
      "====> Epoch: 546 Average train loss: 91.4871\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9008\n",
      "Epoch: 547 [  100/50000 ( 0%)]  \tLoss:   91.997604\trec:   65.842896\tkl:   26.154707\n",
      "Epoch: 547 [10100/50000 (20%)]  \tLoss:   89.951187\trec:   64.486343\tkl:   25.464844\n",
      "Epoch: 547 [20100/50000 (40%)]  \tLoss:   93.711891\trec:   67.517616\tkl:   26.194283\n",
      "Epoch: 547 [30100/50000 (60%)]  \tLoss:   93.401894\trec:   67.489098\tkl:   25.912800\n",
      "Epoch: 547 [40100/50000 (80%)]  \tLoss:   90.220108\trec:   64.087151\tkl:   26.132957\n",
      "====> Epoch: 547 Average train loss: 91.4969\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7277\n",
      "Epoch: 548 [  100/50000 ( 0%)]  \tLoss:   94.656891\trec:   67.914650\tkl:   26.742249\n",
      "Epoch: 548 [10100/50000 (20%)]  \tLoss:   95.340485\trec:   68.213272\tkl:   27.127218\n",
      "Epoch: 548 [20100/50000 (40%)]  \tLoss:   89.277840\trec:   63.414051\tkl:   25.863787\n",
      "Epoch: 548 [30100/50000 (60%)]  \tLoss:   92.839508\trec:   66.729630\tkl:   26.109884\n",
      "Epoch: 548 [40100/50000 (80%)]  \tLoss:   90.109894\trec:   65.264938\tkl:   24.844957\n",
      "====> Epoch: 548 Average train loss: 91.4881\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8845\n",
      "Epoch: 549 [  100/50000 ( 0%)]  \tLoss:   92.564957\trec:   66.266823\tkl:   26.298136\n",
      "Epoch: 549 [10100/50000 (20%)]  \tLoss:   95.972168\trec:   69.776421\tkl:   26.195740\n",
      "Epoch: 549 [20100/50000 (40%)]  \tLoss:   87.632004\trec:   62.142288\tkl:   25.489712\n",
      "Epoch: 549 [30100/50000 (60%)]  \tLoss:   91.238632\trec:   65.939163\tkl:   25.299463\n",
      "Epoch: 549 [40100/50000 (80%)]  \tLoss:   91.349709\trec:   66.159286\tkl:   25.190420\n",
      "====> Epoch: 549 Average train loss: 91.4973\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8745\n",
      "Epoch: 550 [  100/50000 ( 0%)]  \tLoss:   93.177223\trec:   66.881104\tkl:   26.296118\n",
      "Epoch: 550 [10100/50000 (20%)]  \tLoss:   96.506454\trec:   70.161324\tkl:   26.345129\n",
      "Epoch: 550 [20100/50000 (40%)]  \tLoss:   91.665245\trec:   65.282150\tkl:   26.383095\n",
      "Epoch: 550 [30100/50000 (60%)]  \tLoss:   93.439278\trec:   67.260651\tkl:   26.178625\n",
      "Epoch: 550 [40100/50000 (80%)]  \tLoss:   92.290932\trec:   66.567810\tkl:   25.723124\n",
      "====> Epoch: 550 Average train loss: 91.4783\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8181\n",
      "Epoch: 551 [  100/50000 ( 0%)]  \tLoss:   89.144302\trec:   63.567696\tkl:   25.576611\n",
      "Epoch: 551 [10100/50000 (20%)]  \tLoss:   88.803139\trec:   62.647888\tkl:   26.155252\n",
      "Epoch: 551 [20100/50000 (40%)]  \tLoss:   90.494507\trec:   64.516533\tkl:   25.977976\n",
      "Epoch: 551 [30100/50000 (60%)]  \tLoss:   90.463432\trec:   64.049553\tkl:   26.413889\n",
      "Epoch: 551 [40100/50000 (80%)]  \tLoss:   91.152672\trec:   65.404663\tkl:   25.748007\n",
      "====> Epoch: 551 Average train loss: 91.4793\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8516\n",
      "Epoch: 552 [  100/50000 ( 0%)]  \tLoss:   92.691246\trec:   65.744606\tkl:   26.946650\n",
      "Epoch: 552 [10100/50000 (20%)]  \tLoss:   88.861763\trec:   63.657940\tkl:   25.203831\n",
      "Epoch: 552 [20100/50000 (40%)]  \tLoss:   95.579918\trec:   69.005264\tkl:   26.574650\n",
      "Epoch: 552 [30100/50000 (60%)]  \tLoss:   89.853905\trec:   64.754753\tkl:   25.099150\n",
      "Epoch: 552 [40100/50000 (80%)]  \tLoss:   90.947441\trec:   63.929691\tkl:   27.017744\n",
      "====> Epoch: 552 Average train loss: 91.4737\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9263\n",
      "Epoch: 553 [  100/50000 ( 0%)]  \tLoss:   94.826012\trec:   68.057228\tkl:   26.768791\n",
      "Epoch: 553 [10100/50000 (20%)]  \tLoss:   89.508575\trec:   63.094067\tkl:   26.414509\n",
      "Epoch: 553 [20100/50000 (40%)]  \tLoss:   92.526230\trec:   66.302689\tkl:   26.223539\n",
      "Epoch: 553 [30100/50000 (60%)]  \tLoss:   92.795898\trec:   66.542458\tkl:   26.253439\n",
      "Epoch: 553 [40100/50000 (80%)]  \tLoss:   91.084663\trec:   64.806198\tkl:   26.278461\n",
      "====> Epoch: 553 Average train loss: 91.4832\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8173\n",
      "Epoch: 554 [  100/50000 ( 0%)]  \tLoss:   92.379623\trec:   65.558418\tkl:   26.821215\n",
      "Epoch: 554 [10100/50000 (20%)]  \tLoss:   91.613419\trec:   65.569366\tkl:   26.044046\n",
      "Epoch: 554 [20100/50000 (40%)]  \tLoss:   89.480415\trec:   63.884228\tkl:   25.596191\n",
      "Epoch: 554 [30100/50000 (60%)]  \tLoss:   90.097382\trec:   63.997784\tkl:   26.099596\n",
      "Epoch: 554 [40100/50000 (80%)]  \tLoss:   93.237648\trec:   67.071861\tkl:   26.165783\n",
      "====> Epoch: 554 Average train loss: 91.4450\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8118\n",
      "Epoch: 555 [  100/50000 ( 0%)]  \tLoss:   93.406227\trec:   66.381767\tkl:   27.024460\n",
      "Epoch: 555 [10100/50000 (20%)]  \tLoss:   93.385201\trec:   67.440002\tkl:   25.945200\n",
      "Epoch: 555 [20100/50000 (40%)]  \tLoss:   87.312065\trec:   61.340576\tkl:   25.971498\n",
      "Epoch: 555 [30100/50000 (60%)]  \tLoss:   90.989220\trec:   65.703011\tkl:   25.286201\n",
      "Epoch: 555 [40100/50000 (80%)]  \tLoss:   88.635231\trec:   62.965282\tkl:   25.669945\n",
      "====> Epoch: 555 Average train loss: 91.4391\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8321\n",
      "Epoch: 556 [  100/50000 ( 0%)]  \tLoss:   87.512459\trec:   61.820984\tkl:   25.691475\n",
      "Epoch: 556 [10100/50000 (20%)]  \tLoss:   94.232941\trec:   68.382889\tkl:   25.850048\n",
      "Epoch: 556 [20100/50000 (40%)]  \tLoss:   89.818047\trec:   64.111099\tkl:   25.706944\n",
      "Epoch: 556 [30100/50000 (60%)]  \tLoss:   92.445869\trec:   66.138130\tkl:   26.307735\n",
      "Epoch: 556 [40100/50000 (80%)]  \tLoss:   95.913712\trec:   69.118416\tkl:   26.795290\n",
      "====> Epoch: 556 Average train loss: 91.4329\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9304\n",
      "Epoch: 557 [  100/50000 ( 0%)]  \tLoss:   90.259659\trec:   63.413105\tkl:   26.846550\n",
      "Epoch: 557 [10100/50000 (20%)]  \tLoss:   90.774284\trec:   64.816315\tkl:   25.957970\n",
      "Epoch: 557 [20100/50000 (40%)]  \tLoss:   92.139694\trec:   66.416420\tkl:   25.723276\n",
      "Epoch: 557 [30100/50000 (60%)]  \tLoss:   87.204079\trec:   62.607048\tkl:   24.597031\n",
      "Epoch: 557 [40100/50000 (80%)]  \tLoss:   93.653122\trec:   66.944351\tkl:   26.708763\n",
      "====> Epoch: 557 Average train loss: 91.4272\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7679\n",
      "Epoch: 558 [  100/50000 ( 0%)]  \tLoss:   93.958168\trec:   67.463638\tkl:   26.494535\n",
      "Epoch: 558 [10100/50000 (20%)]  \tLoss:   94.251579\trec:   67.548988\tkl:   26.702587\n",
      "Epoch: 558 [20100/50000 (40%)]  \tLoss:   90.178680\trec:   64.094185\tkl:   26.084494\n",
      "Epoch: 558 [30100/50000 (60%)]  \tLoss:   91.110916\trec:   64.195366\tkl:   26.915550\n",
      "Epoch: 558 [40100/50000 (80%)]  \tLoss:   91.689659\trec:   65.204887\tkl:   26.484768\n",
      "====> Epoch: 558 Average train loss: 91.4589\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8484\n",
      "Epoch: 559 [  100/50000 ( 0%)]  \tLoss:   89.675293\trec:   63.539074\tkl:   26.136221\n",
      "Epoch: 559 [10100/50000 (20%)]  \tLoss:   93.494644\trec:   66.873856\tkl:   26.620796\n",
      "Epoch: 559 [20100/50000 (40%)]  \tLoss:   92.553162\trec:   65.492828\tkl:   27.060339\n",
      "Epoch: 559 [30100/50000 (60%)]  \tLoss:   94.387672\trec:   67.684074\tkl:   26.703608\n",
      "Epoch: 559 [40100/50000 (80%)]  \tLoss:   94.412697\trec:   67.244949\tkl:   27.167746\n",
      "====> Epoch: 559 Average train loss: 91.4496\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8660\n",
      "Epoch: 560 [  100/50000 ( 0%)]  \tLoss:   93.841843\trec:   67.227348\tkl:   26.614494\n",
      "Epoch: 560 [10100/50000 (20%)]  \tLoss:   93.999275\trec:   66.914452\tkl:   27.084826\n",
      "Epoch: 560 [20100/50000 (40%)]  \tLoss:   89.204643\trec:   63.278549\tkl:   25.926100\n",
      "Epoch: 560 [30100/50000 (60%)]  \tLoss:   90.634186\trec:   65.020073\tkl:   25.614119\n",
      "Epoch: 560 [40100/50000 (80%)]  \tLoss:   93.701523\trec:   67.262131\tkl:   26.439400\n",
      "====> Epoch: 560 Average train loss: 91.4088\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8405\n",
      "Epoch: 561 [  100/50000 ( 0%)]  \tLoss:   90.006523\trec:   64.065475\tkl:   25.941053\n",
      "Epoch: 561 [10100/50000 (20%)]  \tLoss:   90.909264\trec:   65.830482\tkl:   25.078781\n",
      "Epoch: 561 [20100/50000 (40%)]  \tLoss:   88.773415\trec:   63.486137\tkl:   25.287283\n",
      "Epoch: 561 [30100/50000 (60%)]  \tLoss:   91.524033\trec:   64.900749\tkl:   26.623281\n",
      "Epoch: 561 [40100/50000 (80%)]  \tLoss:   89.899918\trec:   63.995449\tkl:   25.904467\n",
      "====> Epoch: 561 Average train loss: 91.4310\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8975\n",
      "Epoch: 562 [  100/50000 ( 0%)]  \tLoss:   93.464806\trec:   67.262840\tkl:   26.201958\n",
      "Epoch: 562 [10100/50000 (20%)]  \tLoss:   91.274696\trec:   65.032593\tkl:   26.242102\n",
      "Epoch: 562 [20100/50000 (40%)]  \tLoss:   91.044228\trec:   64.417686\tkl:   26.626535\n",
      "Epoch: 562 [30100/50000 (60%)]  \tLoss:   88.780632\trec:   63.251953\tkl:   25.528681\n",
      "Epoch: 562 [40100/50000 (80%)]  \tLoss:   91.018555\trec:   64.747215\tkl:   26.271339\n",
      "====> Epoch: 562 Average train loss: 91.4089\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8253\n",
      "Epoch: 563 [  100/50000 ( 0%)]  \tLoss:   91.694038\trec:   65.113419\tkl:   26.580629\n",
      "Epoch: 563 [10100/50000 (20%)]  \tLoss:   91.659462\trec:   65.565208\tkl:   26.094252\n",
      "Epoch: 563 [20100/50000 (40%)]  \tLoss:   91.626289\trec:   64.910942\tkl:   26.715336\n",
      "Epoch: 563 [30100/50000 (60%)]  \tLoss:   93.480873\trec:   67.169998\tkl:   26.310883\n",
      "Epoch: 563 [40100/50000 (80%)]  \tLoss:   93.657097\trec:   66.961983\tkl:   26.695114\n",
      "====> Epoch: 563 Average train loss: 91.4164\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7981\n",
      "Epoch: 564 [  100/50000 ( 0%)]  \tLoss:   90.571404\trec:   64.862701\tkl:   25.708706\n",
      "Epoch: 564 [10100/50000 (20%)]  \tLoss:   92.217087\trec:   66.962105\tkl:   25.254982\n",
      "Epoch: 564 [20100/50000 (40%)]  \tLoss:   92.733681\trec:   65.831894\tkl:   26.901789\n",
      "Epoch: 564 [30100/50000 (60%)]  \tLoss:   89.449692\trec:   63.196873\tkl:   26.252821\n",
      "Epoch: 564 [40100/50000 (80%)]  \tLoss:   89.217400\trec:   63.114876\tkl:   26.102522\n",
      "====> Epoch: 564 Average train loss: 91.4315\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8392\n",
      "Epoch: 565 [  100/50000 ( 0%)]  \tLoss:   93.070679\trec:   66.414383\tkl:   26.656301\n",
      "Epoch: 565 [10100/50000 (20%)]  \tLoss:   90.926834\trec:   65.240654\tkl:   25.686176\n",
      "Epoch: 565 [20100/50000 (40%)]  \tLoss:   93.997345\trec:   67.331215\tkl:   26.666130\n",
      "Epoch: 565 [30100/50000 (60%)]  \tLoss:   90.323692\trec:   64.638184\tkl:   25.685503\n",
      "Epoch: 565 [40100/50000 (80%)]  \tLoss:   93.729912\trec:   67.715927\tkl:   26.013985\n",
      "====> Epoch: 565 Average train loss: 91.4069\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8276\n",
      "Epoch: 566 [  100/50000 ( 0%)]  \tLoss:   90.651894\trec:   64.720573\tkl:   25.931318\n",
      "Epoch: 566 [10100/50000 (20%)]  \tLoss:   92.753571\trec:   67.838699\tkl:   24.914867\n",
      "Epoch: 566 [20100/50000 (40%)]  \tLoss:   95.005562\trec:   68.348999\tkl:   26.656565\n",
      "Epoch: 566 [30100/50000 (60%)]  \tLoss:   95.375671\trec:   68.057983\tkl:   27.317688\n",
      "Epoch: 566 [40100/50000 (80%)]  \tLoss:   92.383583\trec:   66.562828\tkl:   25.820753\n",
      "====> Epoch: 566 Average train loss: 91.4328\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7534\n",
      "Epoch: 567 [  100/50000 ( 0%)]  \tLoss:   90.013710\trec:   63.143452\tkl:   26.870256\n",
      "Epoch: 567 [10100/50000 (20%)]  \tLoss:   93.482292\trec:   67.597023\tkl:   25.885275\n",
      "Epoch: 567 [20100/50000 (40%)]  \tLoss:   95.231499\trec:   68.188576\tkl:   27.042929\n",
      "Epoch: 567 [30100/50000 (60%)]  \tLoss:   94.947144\trec:   68.423943\tkl:   26.523201\n",
      "Epoch: 567 [40100/50000 (80%)]  \tLoss:   95.223106\trec:   68.225418\tkl:   26.997690\n",
      "====> Epoch: 567 Average train loss: 91.3972\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7697\n",
      "Epoch: 568 [  100/50000 ( 0%)]  \tLoss:   86.338661\trec:   60.989216\tkl:   25.349443\n",
      "Epoch: 568 [10100/50000 (20%)]  \tLoss:   92.962929\trec:   65.683510\tkl:   27.279423\n",
      "Epoch: 568 [20100/50000 (40%)]  \tLoss:   92.267273\trec:   66.201393\tkl:   26.065880\n",
      "Epoch: 568 [30100/50000 (60%)]  \tLoss:   91.540985\trec:   65.533279\tkl:   26.007702\n",
      "Epoch: 568 [40100/50000 (80%)]  \tLoss:   89.995277\trec:   64.717743\tkl:   25.277540\n",
      "====> Epoch: 568 Average train loss: 91.4033\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7669\n",
      "Epoch: 569 [  100/50000 ( 0%)]  \tLoss:   90.602608\trec:   64.352470\tkl:   26.250132\n",
      "Epoch: 569 [10100/50000 (20%)]  \tLoss:   89.572510\trec:   63.744244\tkl:   25.828264\n",
      "Epoch: 569 [20100/50000 (40%)]  \tLoss:   92.350212\trec:   65.788086\tkl:   26.562124\n",
      "Epoch: 569 [30100/50000 (60%)]  \tLoss:   92.745781\trec:   67.075722\tkl:   25.670063\n",
      "Epoch: 569 [40100/50000 (80%)]  \tLoss:   90.433075\trec:   64.592392\tkl:   25.840681\n",
      "====> Epoch: 569 Average train loss: 91.4104\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7282\n",
      "Epoch: 570 [  100/50000 ( 0%)]  \tLoss:   89.818901\trec:   64.071312\tkl:   25.747597\n",
      "Epoch: 570 [10100/50000 (20%)]  \tLoss:   90.584946\trec:   64.129120\tkl:   26.455830\n",
      "Epoch: 570 [20100/50000 (40%)]  \tLoss:   92.081367\trec:   65.653282\tkl:   26.428087\n",
      "Epoch: 570 [30100/50000 (60%)]  \tLoss:   89.858124\trec:   63.575214\tkl:   26.282906\n",
      "Epoch: 570 [40100/50000 (80%)]  \tLoss:   92.382919\trec:   65.962891\tkl:   26.420031\n",
      "====> Epoch: 570 Average train loss: 91.3847\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8425\n",
      "Epoch: 571 [  100/50000 ( 0%)]  \tLoss:   93.615639\trec:   67.243912\tkl:   26.371723\n",
      "Epoch: 571 [10100/50000 (20%)]  \tLoss:   87.386246\trec:   62.172607\tkl:   25.213644\n",
      "Epoch: 571 [20100/50000 (40%)]  \tLoss:   94.896049\trec:   68.828255\tkl:   26.067801\n",
      "Epoch: 571 [30100/50000 (60%)]  \tLoss:   92.356529\trec:   65.630234\tkl:   26.726301\n",
      "Epoch: 571 [40100/50000 (80%)]  \tLoss:   89.497017\trec:   63.628826\tkl:   25.868195\n",
      "====> Epoch: 571 Average train loss: 91.3945\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8995\n",
      "Epoch: 572 [  100/50000 ( 0%)]  \tLoss:   92.542023\trec:   67.120972\tkl:   25.421045\n",
      "Epoch: 572 [10100/50000 (20%)]  \tLoss:   93.918701\trec:   67.965546\tkl:   25.953154\n",
      "Epoch: 572 [20100/50000 (40%)]  \tLoss:   90.812790\trec:   64.814514\tkl:   25.998274\n",
      "Epoch: 572 [30100/50000 (60%)]  \tLoss:   93.062050\trec:   65.774918\tkl:   27.287132\n",
      "Epoch: 572 [40100/50000 (80%)]  \tLoss:   91.713631\trec:   65.399918\tkl:   26.313715\n",
      "====> Epoch: 572 Average train loss: 91.3933\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8205\n",
      "Epoch: 573 [  100/50000 ( 0%)]  \tLoss:   88.277290\trec:   62.685493\tkl:   25.591801\n",
      "Epoch: 573 [10100/50000 (20%)]  \tLoss:   93.804268\trec:   66.523880\tkl:   27.280386\n",
      "Epoch: 573 [20100/50000 (40%)]  \tLoss:   92.383026\trec:   65.927361\tkl:   26.455666\n",
      "Epoch: 573 [30100/50000 (60%)]  \tLoss:   89.296127\trec:   63.050579\tkl:   26.245546\n",
      "Epoch: 573 [40100/50000 (80%)]  \tLoss:   93.060707\trec:   66.551865\tkl:   26.508844\n",
      "====> Epoch: 573 Average train loss: 91.4005\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7768\n",
      "Epoch: 574 [  100/50000 ( 0%)]  \tLoss:   88.571701\trec:   63.069893\tkl:   25.501806\n",
      "Epoch: 574 [10100/50000 (20%)]  \tLoss:   91.226196\trec:   64.404610\tkl:   26.821590\n",
      "Epoch: 574 [20100/50000 (40%)]  \tLoss:   91.905602\trec:   65.832809\tkl:   26.072790\n",
      "Epoch: 574 [30100/50000 (60%)]  \tLoss:   94.959160\trec:   68.339714\tkl:   26.619446\n",
      "Epoch: 574 [40100/50000 (80%)]  \tLoss:   89.168175\trec:   63.419758\tkl:   25.748409\n",
      "====> Epoch: 574 Average train loss: 91.4082\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8160\n",
      "Epoch: 575 [  100/50000 ( 0%)]  \tLoss:   97.043709\trec:   70.161209\tkl:   26.882502\n",
      "Epoch: 575 [10100/50000 (20%)]  \tLoss:   89.380821\trec:   63.652615\tkl:   25.728207\n",
      "Epoch: 575 [20100/50000 (40%)]  \tLoss:   91.978600\trec:   65.461922\tkl:   26.516676\n",
      "Epoch: 575 [30100/50000 (60%)]  \tLoss:   93.069633\trec:   67.269577\tkl:   25.800066\n",
      "Epoch: 575 [40100/50000 (80%)]  \tLoss:   92.610573\trec:   66.101196\tkl:   26.509373\n",
      "====> Epoch: 575 Average train loss: 91.3660\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8422\n",
      "Epoch: 576 [  100/50000 ( 0%)]  \tLoss:   91.391701\trec:   65.541550\tkl:   25.850153\n",
      "Epoch: 576 [10100/50000 (20%)]  \tLoss:   91.527290\trec:   65.041847\tkl:   26.485449\n",
      "Epoch: 576 [20100/50000 (40%)]  \tLoss:   92.875183\trec:   66.277412\tkl:   26.597773\n",
      "Epoch: 576 [30100/50000 (60%)]  \tLoss:   88.731064\trec:   63.206882\tkl:   25.524178\n",
      "Epoch: 576 [40100/50000 (80%)]  \tLoss:   93.760353\trec:   67.169861\tkl:   26.590490\n",
      "====> Epoch: 576 Average train loss: 91.3551\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7626\n",
      "Epoch: 577 [  100/50000 ( 0%)]  \tLoss:   88.634598\trec:   62.732372\tkl:   25.902224\n",
      "Epoch: 577 [10100/50000 (20%)]  \tLoss:   92.834282\trec:   66.212837\tkl:   26.621443\n",
      "Epoch: 577 [20100/50000 (40%)]  \tLoss:   95.013412\trec:   68.262146\tkl:   26.751272\n",
      "Epoch: 577 [30100/50000 (60%)]  \tLoss:   92.399689\trec:   65.928963\tkl:   26.470724\n",
      "Epoch: 577 [40100/50000 (80%)]  \tLoss:   89.487778\trec:   63.244930\tkl:   26.242851\n",
      "====> Epoch: 577 Average train loss: 91.3571\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8286\n",
      "Epoch: 578 [  100/50000 ( 0%)]  \tLoss:   90.802559\trec:   64.904861\tkl:   25.897697\n",
      "Epoch: 578 [10100/50000 (20%)]  \tLoss:   92.122292\trec:   65.904022\tkl:   26.218271\n",
      "Epoch: 578 [20100/50000 (40%)]  \tLoss:   94.693787\trec:   67.267349\tkl:   27.426430\n",
      "Epoch: 578 [30100/50000 (60%)]  \tLoss:   91.348473\trec:   65.196579\tkl:   26.151899\n",
      "Epoch: 578 [40100/50000 (80%)]  \tLoss:   93.244820\trec:   66.667793\tkl:   26.577036\n",
      "====> Epoch: 578 Average train loss: 91.3722\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8421\n",
      "Epoch: 579 [  100/50000 ( 0%)]  \tLoss:   88.546318\trec:   63.315907\tkl:   25.230410\n",
      "Epoch: 579 [10100/50000 (20%)]  \tLoss:   90.643044\trec:   64.520538\tkl:   26.122509\n",
      "Epoch: 579 [20100/50000 (40%)]  \tLoss:   92.954865\trec:   66.851898\tkl:   26.102966\n",
      "Epoch: 579 [30100/50000 (60%)]  \tLoss:   92.659409\trec:   65.748375\tkl:   26.911034\n",
      "Epoch: 579 [40100/50000 (80%)]  \tLoss:   90.699997\trec:   64.121414\tkl:   26.578583\n",
      "====> Epoch: 579 Average train loss: 91.3496\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8202\n",
      "Epoch: 580 [  100/50000 ( 0%)]  \tLoss:   93.641960\trec:   67.474976\tkl:   26.166983\n",
      "Epoch: 580 [10100/50000 (20%)]  \tLoss:   92.459236\trec:   66.314796\tkl:   26.144440\n",
      "Epoch: 580 [20100/50000 (40%)]  \tLoss:   88.478058\trec:   62.746784\tkl:   25.731266\n",
      "Epoch: 580 [30100/50000 (60%)]  \tLoss:   89.151085\trec:   63.034492\tkl:   26.116594\n",
      "Epoch: 580 [40100/50000 (80%)]  \tLoss:   88.680992\trec:   62.772816\tkl:   25.908178\n",
      "====> Epoch: 580 Average train loss: 91.3367\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7535\n",
      "Epoch: 581 [  100/50000 ( 0%)]  \tLoss:   89.203682\trec:   63.365448\tkl:   25.838232\n",
      "Epoch: 581 [10100/50000 (20%)]  \tLoss:   91.229111\trec:   64.920593\tkl:   26.308517\n",
      "Epoch: 581 [20100/50000 (40%)]  \tLoss:   90.136787\trec:   64.508736\tkl:   25.628048\n",
      "Epoch: 581 [30100/50000 (60%)]  \tLoss:   92.771042\trec:   66.346779\tkl:   26.424269\n",
      "Epoch: 581 [40100/50000 (80%)]  \tLoss:   91.201714\trec:   64.521210\tkl:   26.680511\n",
      "====> Epoch: 581 Average train loss: 91.3531\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7615\n",
      "Epoch: 582 [  100/50000 ( 0%)]  \tLoss:   92.679375\trec:   66.452820\tkl:   26.226557\n",
      "Epoch: 582 [10100/50000 (20%)]  \tLoss:   93.451561\trec:   66.662766\tkl:   26.788794\n",
      "Epoch: 582 [20100/50000 (40%)]  \tLoss:   90.078644\trec:   64.248787\tkl:   25.829851\n",
      "Epoch: 582 [30100/50000 (60%)]  \tLoss:   91.960938\trec:   65.668983\tkl:   26.291952\n",
      "Epoch: 582 [40100/50000 (80%)]  \tLoss:   92.971466\trec:   66.753166\tkl:   26.218300\n",
      "====> Epoch: 582 Average train loss: 91.3509\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8288\n",
      "Epoch: 583 [  100/50000 ( 0%)]  \tLoss:   87.927742\trec:   62.024521\tkl:   25.903225\n",
      "Epoch: 583 [10100/50000 (20%)]  \tLoss:   94.617744\trec:   67.737320\tkl:   26.880424\n",
      "Epoch: 583 [20100/50000 (40%)]  \tLoss:   94.229668\trec:   67.753098\tkl:   26.476572\n",
      "Epoch: 583 [30100/50000 (60%)]  \tLoss:   92.086823\trec:   65.574516\tkl:   26.512306\n",
      "Epoch: 583 [40100/50000 (80%)]  \tLoss:   90.671295\trec:   64.668869\tkl:   26.002426\n",
      "====> Epoch: 583 Average train loss: 91.3258\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7837\n",
      "Epoch: 584 [  100/50000 ( 0%)]  \tLoss:   85.453125\trec:   60.200932\tkl:   25.252197\n",
      "Epoch: 584 [10100/50000 (20%)]  \tLoss:   90.186180\trec:   64.661186\tkl:   25.524998\n",
      "Epoch: 584 [20100/50000 (40%)]  \tLoss:   92.889717\trec:   66.497360\tkl:   26.392355\n",
      "Epoch: 584 [30100/50000 (60%)]  \tLoss:   88.856300\trec:   62.428135\tkl:   26.428165\n",
      "Epoch: 584 [40100/50000 (80%)]  \tLoss:   92.674278\trec:   66.673538\tkl:   26.000740\n",
      "====> Epoch: 584 Average train loss: 91.3444\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7731\n",
      "Epoch: 585 [  100/50000 ( 0%)]  \tLoss:   91.719467\trec:   65.311584\tkl:   26.407881\n",
      "Epoch: 585 [10100/50000 (20%)]  \tLoss:   85.632675\trec:   60.841072\tkl:   24.791595\n",
      "Epoch: 585 [20100/50000 (40%)]  \tLoss:   94.981880\trec:   68.746574\tkl:   26.235313\n",
      "Epoch: 585 [30100/50000 (60%)]  \tLoss:   88.736397\trec:   63.147079\tkl:   25.589319\n",
      "Epoch: 585 [40100/50000 (80%)]  \tLoss:   92.918396\trec:   66.940269\tkl:   25.978123\n",
      "====> Epoch: 585 Average train loss: 91.3345\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8191\n",
      "Epoch: 586 [  100/50000 ( 0%)]  \tLoss:   91.438683\trec:   65.591309\tkl:   25.847372\n",
      "Epoch: 586 [10100/50000 (20%)]  \tLoss:   90.472153\trec:   63.640148\tkl:   26.832005\n",
      "Epoch: 586 [20100/50000 (40%)]  \tLoss:   92.693123\trec:   66.848427\tkl:   25.844692\n",
      "Epoch: 586 [30100/50000 (60%)]  \tLoss:   87.369217\trec:   61.354382\tkl:   26.014839\n",
      "Epoch: 586 [40100/50000 (80%)]  \tLoss:   88.661865\trec:   63.577332\tkl:   25.084528\n",
      "====> Epoch: 586 Average train loss: 91.3337\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8038\n",
      "Epoch: 587 [  100/50000 ( 0%)]  \tLoss:   95.869110\trec:   68.582817\tkl:   27.286295\n",
      "Epoch: 587 [10100/50000 (20%)]  \tLoss:   90.540161\trec:   64.060776\tkl:   26.479389\n",
      "Epoch: 587 [20100/50000 (40%)]  \tLoss:   92.224243\trec:   65.976608\tkl:   26.247633\n",
      "Epoch: 587 [30100/50000 (60%)]  \tLoss:   92.593887\trec:   65.824875\tkl:   26.769011\n",
      "Epoch: 587 [40100/50000 (80%)]  \tLoss:   88.724648\trec:   63.158440\tkl:   25.566202\n",
      "====> Epoch: 587 Average train loss: 91.3379\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.9113\n",
      "Epoch: 588 [  100/50000 ( 0%)]  \tLoss:   88.779724\trec:   63.180790\tkl:   25.598932\n",
      "Epoch: 588 [10100/50000 (20%)]  \tLoss:   92.354279\trec:   66.571541\tkl:   25.782736\n",
      "Epoch: 588 [20100/50000 (40%)]  \tLoss:   93.158287\trec:   66.773415\tkl:   26.384874\n",
      "Epoch: 588 [30100/50000 (60%)]  \tLoss:   93.658493\trec:   67.315964\tkl:   26.342525\n",
      "Epoch: 588 [40100/50000 (80%)]  \tLoss:   91.230431\trec:   64.831177\tkl:   26.399244\n",
      "====> Epoch: 588 Average train loss: 91.3326\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7849\n",
      "Epoch: 589 [  100/50000 ( 0%)]  \tLoss:   91.796425\trec:   66.199181\tkl:   25.597246\n",
      "Epoch: 589 [10100/50000 (20%)]  \tLoss:   87.163307\trec:   61.399540\tkl:   25.763767\n",
      "Epoch: 589 [20100/50000 (40%)]  \tLoss:   90.406967\trec:   64.950768\tkl:   25.456202\n",
      "Epoch: 589 [30100/50000 (60%)]  \tLoss:   90.651237\trec:   64.615791\tkl:   26.035440\n",
      "Epoch: 589 [40100/50000 (80%)]  \tLoss:   92.534943\trec:   66.679649\tkl:   25.855295\n",
      "====> Epoch: 589 Average train loss: 91.3277\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8412\n",
      "Epoch: 590 [  100/50000 ( 0%)]  \tLoss:   90.042030\trec:   64.564461\tkl:   25.477573\n",
      "Epoch: 590 [10100/50000 (20%)]  \tLoss:   94.127846\trec:   67.746056\tkl:   26.381802\n",
      "Epoch: 590 [20100/50000 (40%)]  \tLoss:   92.805450\trec:   66.805405\tkl:   26.000040\n",
      "Epoch: 590 [30100/50000 (60%)]  \tLoss:   94.807564\trec:   68.446037\tkl:   26.361530\n",
      "Epoch: 590 [40100/50000 (80%)]  \tLoss:   91.028618\trec:   64.510490\tkl:   26.518131\n",
      "====> Epoch: 590 Average train loss: 91.3036\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8547\n",
      "Epoch: 591 [  100/50000 ( 0%)]  \tLoss:   92.993553\trec:   66.932381\tkl:   26.061171\n",
      "Epoch: 591 [10100/50000 (20%)]  \tLoss:   86.808083\trec:   60.900330\tkl:   25.907753\n",
      "Epoch: 591 [20100/50000 (40%)]  \tLoss:   89.261841\trec:   63.717720\tkl:   25.544125\n",
      "Epoch: 591 [30100/50000 (60%)]  \tLoss:   93.226257\trec:   66.636955\tkl:   26.589306\n",
      "Epoch: 591 [40100/50000 (80%)]  \tLoss:   89.116707\trec:   63.654457\tkl:   25.462254\n",
      "====> Epoch: 591 Average train loss: 91.3036\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7214\n",
      "Epoch: 592 [  100/50000 ( 0%)]  \tLoss:   94.820663\trec:   68.384117\tkl:   26.436544\n",
      "Epoch: 592 [10100/50000 (20%)]  \tLoss:   90.906036\trec:   65.096680\tkl:   25.809349\n",
      "Epoch: 592 [20100/50000 (40%)]  \tLoss:   85.009521\trec:   59.820908\tkl:   25.188612\n",
      "Epoch: 592 [30100/50000 (60%)]  \tLoss:   95.091515\trec:   67.708290\tkl:   27.383224\n",
      "Epoch: 592 [40100/50000 (80%)]  \tLoss:   91.134674\trec:   65.459038\tkl:   25.675634\n",
      "====> Epoch: 592 Average train loss: 91.2885\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7957\n",
      "Epoch: 593 [  100/50000 ( 0%)]  \tLoss:   94.303261\trec:   68.118660\tkl:   26.184593\n",
      "Epoch: 593 [10100/50000 (20%)]  \tLoss:   91.073517\trec:   65.066139\tkl:   26.007368\n",
      "Epoch: 593 [20100/50000 (40%)]  \tLoss:   90.599686\trec:   63.879433\tkl:   26.720249\n",
      "Epoch: 593 [30100/50000 (60%)]  \tLoss:   92.986694\trec:   66.703171\tkl:   26.283520\n",
      "Epoch: 593 [40100/50000 (80%)]  \tLoss:   93.413338\trec:   66.850098\tkl:   26.563236\n",
      "====> Epoch: 593 Average train loss: 91.2837\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7728\n",
      "Epoch: 594 [  100/50000 ( 0%)]  \tLoss:   92.272179\trec:   65.536415\tkl:   26.735762\n",
      "Epoch: 594 [10100/50000 (20%)]  \tLoss:   89.934235\trec:   64.232018\tkl:   25.702221\n",
      "Epoch: 594 [20100/50000 (40%)]  \tLoss:   89.336502\trec:   63.876247\tkl:   25.460253\n",
      "Epoch: 594 [30100/50000 (60%)]  \tLoss:   91.398140\trec:   64.725723\tkl:   26.672426\n",
      "Epoch: 594 [40100/50000 (80%)]  \tLoss:   97.870796\trec:   70.434731\tkl:   27.436071\n",
      "====> Epoch: 594 Average train loss: 91.2933\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8269\n",
      "Epoch: 595 [  100/50000 ( 0%)]  \tLoss:   94.119370\trec:   67.416908\tkl:   26.702467\n",
      "Epoch: 595 [10100/50000 (20%)]  \tLoss:   90.838898\trec:   64.674927\tkl:   26.163963\n",
      "Epoch: 595 [20100/50000 (40%)]  \tLoss:   89.011444\trec:   62.958786\tkl:   26.052656\n",
      "Epoch: 595 [30100/50000 (60%)]  \tLoss:   96.481285\trec:   69.801994\tkl:   26.679298\n",
      "Epoch: 595 [40100/50000 (80%)]  \tLoss:   91.915665\trec:   65.989868\tkl:   25.925793\n",
      "====> Epoch: 595 Average train loss: 91.2841\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7824\n",
      "Epoch: 596 [  100/50000 ( 0%)]  \tLoss:   91.700356\trec:   65.379921\tkl:   26.320438\n",
      "Epoch: 596 [10100/50000 (20%)]  \tLoss:   88.823769\trec:   63.723652\tkl:   25.100119\n",
      "Epoch: 596 [20100/50000 (40%)]  \tLoss:   88.232704\trec:   61.930187\tkl:   26.302511\n",
      "Epoch: 596 [30100/50000 (60%)]  \tLoss:   90.174843\trec:   64.176567\tkl:   25.998268\n",
      "Epoch: 596 [40100/50000 (80%)]  \tLoss:   89.241882\trec:   64.503105\tkl:   24.738781\n",
      "====> Epoch: 596 Average train loss: 91.2701\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7798\n",
      "Epoch: 597 [  100/50000 ( 0%)]  \tLoss:   89.455391\trec:   63.008564\tkl:   26.446821\n",
      "Epoch: 597 [10100/50000 (20%)]  \tLoss:   90.980759\trec:   64.386658\tkl:   26.594101\n",
      "Epoch: 597 [20100/50000 (40%)]  \tLoss:   94.123657\trec:   66.983711\tkl:   27.139946\n",
      "Epoch: 597 [30100/50000 (60%)]  \tLoss:   89.336678\trec:   63.920437\tkl:   25.416239\n",
      "Epoch: 597 [40100/50000 (80%)]  \tLoss:   94.392487\trec:   67.214241\tkl:   27.178244\n",
      "====> Epoch: 597 Average train loss: 91.2660\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7425\n",
      "Epoch: 598 [  100/50000 ( 0%)]  \tLoss:   89.652382\trec:   63.559399\tkl:   26.092979\n",
      "Epoch: 598 [10100/50000 (20%)]  \tLoss:   96.408516\trec:   69.322044\tkl:   27.086466\n",
      "Epoch: 598 [20100/50000 (40%)]  \tLoss:   93.458084\trec:   66.948433\tkl:   26.509647\n",
      "Epoch: 598 [30100/50000 (60%)]  \tLoss:   86.293335\trec:   61.406235\tkl:   24.887110\n",
      "Epoch: 598 [40100/50000 (80%)]  \tLoss:   90.127350\trec:   64.825584\tkl:   25.301765\n",
      "====> Epoch: 598 Average train loss: 91.2700\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8656\n",
      "Epoch: 599 [  100/50000 ( 0%)]  \tLoss:   90.491600\trec:   64.253799\tkl:   26.237801\n",
      "Epoch: 599 [10100/50000 (20%)]  \tLoss:   93.422585\trec:   67.131119\tkl:   26.291466\n",
      "Epoch: 599 [20100/50000 (40%)]  \tLoss:   91.090088\trec:   64.314278\tkl:   26.775810\n",
      "Epoch: 599 [30100/50000 (60%)]  \tLoss:   89.680435\trec:   64.209328\tkl:   25.471109\n",
      "Epoch: 599 [40100/50000 (80%)]  \tLoss:   91.651581\trec:   65.843552\tkl:   25.808022\n",
      "====> Epoch: 599 Average train loss: 91.2769\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8278\n",
      "Epoch: 600 [  100/50000 ( 0%)]  \tLoss:   90.149986\trec:   64.405113\tkl:   25.744873\n",
      "Epoch: 600 [10100/50000 (20%)]  \tLoss:   91.213493\trec:   64.960510\tkl:   26.252981\n",
      "Epoch: 600 [20100/50000 (40%)]  \tLoss:   86.390190\trec:   60.902901\tkl:   25.487291\n",
      "Epoch: 600 [30100/50000 (60%)]  \tLoss:   95.418121\trec:   68.390656\tkl:   27.027470\n",
      "Epoch: 600 [40100/50000 (80%)]  \tLoss:   90.961967\trec:   65.234344\tkl:   25.727621\n",
      "====> Epoch: 600 Average train loss: 91.2912\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7560\n",
      "Epoch: 601 [  100/50000 ( 0%)]  \tLoss:   91.287788\trec:   65.206833\tkl:   26.080961\n",
      "Epoch: 601 [10100/50000 (20%)]  \tLoss:   88.224464\trec:   63.253654\tkl:   24.970804\n",
      "Epoch: 601 [20100/50000 (40%)]  \tLoss:   87.628769\trec:   62.084892\tkl:   25.543880\n",
      "Epoch: 601 [30100/50000 (60%)]  \tLoss:   93.565193\trec:   67.039421\tkl:   26.525776\n",
      "Epoch: 601 [40100/50000 (80%)]  \tLoss:   89.763794\trec:   63.630116\tkl:   26.133684\n",
      "====> Epoch: 601 Average train loss: 91.2660\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7988\n",
      "Epoch: 602 [  100/50000 ( 0%)]  \tLoss:   90.623787\trec:   64.468010\tkl:   26.155775\n",
      "Epoch: 602 [10100/50000 (20%)]  \tLoss:   91.791893\trec:   65.900688\tkl:   25.891205\n",
      "Epoch: 602 [20100/50000 (40%)]  \tLoss:   88.992783\trec:   62.675056\tkl:   26.317726\n",
      "Epoch: 602 [30100/50000 (60%)]  \tLoss:   88.817642\trec:   63.529156\tkl:   25.288490\n",
      "Epoch: 602 [40100/50000 (80%)]  \tLoss:   95.030800\trec:   68.083633\tkl:   26.947163\n",
      "====> Epoch: 602 Average train loss: 91.2541\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7622\n",
      "Epoch: 603 [  100/50000 ( 0%)]  \tLoss:   91.073242\trec:   65.437637\tkl:   25.635599\n",
      "Epoch: 603 [10100/50000 (20%)]  \tLoss:   94.102272\trec:   66.556953\tkl:   27.545319\n",
      "Epoch: 603 [20100/50000 (40%)]  \tLoss:   93.903046\trec:   67.552620\tkl:   26.350422\n",
      "Epoch: 603 [30100/50000 (60%)]  \tLoss:   91.822906\trec:   65.240402\tkl:   26.582504\n",
      "Epoch: 603 [40100/50000 (80%)]  \tLoss:   91.915703\trec:   65.484032\tkl:   26.431667\n",
      "====> Epoch: 603 Average train loss: 91.2464\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8097\n",
      "Epoch: 604 [  100/50000 ( 0%)]  \tLoss:   90.904999\trec:   65.480568\tkl:   25.424433\n",
      "Epoch: 604 [10100/50000 (20%)]  \tLoss:   90.482544\trec:   64.252998\tkl:   26.229549\n",
      "Epoch: 604 [20100/50000 (40%)]  \tLoss:   92.734291\trec:   66.043877\tkl:   26.690414\n",
      "Epoch: 604 [30100/50000 (60%)]  \tLoss:   90.836266\trec:   64.839371\tkl:   25.996899\n",
      "Epoch: 604 [40100/50000 (80%)]  \tLoss:   86.633942\trec:   61.678398\tkl:   24.955542\n",
      "====> Epoch: 604 Average train loss: 91.2568\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7217\n",
      "Epoch: 605 [  100/50000 ( 0%)]  \tLoss:   91.741524\trec:   65.858612\tkl:   25.882904\n",
      "Epoch: 605 [10100/50000 (20%)]  \tLoss:   90.491920\trec:   64.970024\tkl:   25.521894\n",
      "Epoch: 605 [20100/50000 (40%)]  \tLoss:   93.088768\trec:   66.774879\tkl:   26.313896\n",
      "Epoch: 605 [30100/50000 (60%)]  \tLoss:   93.760193\trec:   66.951660\tkl:   26.808533\n",
      "Epoch: 605 [40100/50000 (80%)]  \tLoss:   89.403564\trec:   63.457851\tkl:   25.945709\n",
      "====> Epoch: 605 Average train loss: 91.2452\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8049\n",
      "Epoch: 606 [  100/50000 ( 0%)]  \tLoss:   90.551727\trec:   64.410957\tkl:   26.140768\n",
      "Epoch: 606 [10100/50000 (20%)]  \tLoss:   92.215805\trec:   65.384071\tkl:   26.831738\n",
      "Epoch: 606 [20100/50000 (40%)]  \tLoss:   89.673454\trec:   63.415379\tkl:   26.258070\n",
      "Epoch: 606 [30100/50000 (60%)]  \tLoss:   87.924393\trec:   61.970932\tkl:   25.953466\n",
      "Epoch: 606 [40100/50000 (80%)]  \tLoss:   92.885727\trec:   67.207672\tkl:   25.678053\n",
      "====> Epoch: 606 Average train loss: 91.2308\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7076\n",
      "Epoch: 607 [  100/50000 ( 0%)]  \tLoss:   91.347382\trec:   65.254692\tkl:   26.092680\n",
      "Epoch: 607 [10100/50000 (20%)]  \tLoss:   90.556267\trec:   64.726799\tkl:   25.829472\n",
      "Epoch: 607 [20100/50000 (40%)]  \tLoss:   91.515778\trec:   65.196365\tkl:   26.319414\n",
      "Epoch: 607 [30100/50000 (60%)]  \tLoss:   89.781616\trec:   64.117363\tkl:   25.664255\n",
      "Epoch: 607 [40100/50000 (80%)]  \tLoss:   90.366425\trec:   64.324028\tkl:   26.042393\n",
      "====> Epoch: 607 Average train loss: 91.2564\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7746\n",
      "Epoch: 608 [  100/50000 ( 0%)]  \tLoss:   91.675430\trec:   65.597145\tkl:   26.078281\n",
      "Epoch: 608 [10100/50000 (20%)]  \tLoss:   91.188591\trec:   64.502838\tkl:   26.685753\n",
      "Epoch: 608 [20100/50000 (40%)]  \tLoss:   92.815262\trec:   67.096466\tkl:   25.718790\n",
      "Epoch: 608 [30100/50000 (60%)]  \tLoss:   91.425835\trec:   65.174362\tkl:   26.251472\n",
      "Epoch: 608 [40100/50000 (80%)]  \tLoss:   89.889404\trec:   64.083687\tkl:   25.805716\n",
      "====> Epoch: 608 Average train loss: 91.2506\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7710\n",
      "Epoch: 609 [  100/50000 ( 0%)]  \tLoss:   90.848755\trec:   65.889641\tkl:   24.959114\n",
      "Epoch: 609 [10100/50000 (20%)]  \tLoss:   90.147186\trec:   63.794453\tkl:   26.352732\n",
      "Epoch: 609 [20100/50000 (40%)]  \tLoss:   92.898338\trec:   66.928154\tkl:   25.970181\n",
      "Epoch: 609 [30100/50000 (60%)]  \tLoss:   89.561752\trec:   63.297409\tkl:   26.264339\n",
      "Epoch: 609 [40100/50000 (80%)]  \tLoss:   91.439903\trec:   64.941650\tkl:   26.498251\n",
      "====> Epoch: 609 Average train loss: 91.2227\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7638\n",
      "Epoch: 610 [  100/50000 ( 0%)]  \tLoss:   90.773560\trec:   64.770454\tkl:   26.003113\n",
      "Epoch: 610 [10100/50000 (20%)]  \tLoss:   92.583633\trec:   66.561684\tkl:   26.021948\n",
      "Epoch: 610 [20100/50000 (40%)]  \tLoss:   88.082901\trec:   62.469452\tkl:   25.613447\n",
      "Epoch: 610 [30100/50000 (60%)]  \tLoss:   90.987213\trec:   64.820549\tkl:   26.166672\n",
      "Epoch: 610 [40100/50000 (80%)]  \tLoss:   93.618553\trec:   66.892311\tkl:   26.726234\n",
      "====> Epoch: 610 Average train loss: 91.2239\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7480\n",
      "Epoch: 611 [  100/50000 ( 0%)]  \tLoss:   92.085556\trec:   65.931992\tkl:   26.153561\n",
      "Epoch: 611 [10100/50000 (20%)]  \tLoss:   89.616364\trec:   64.345192\tkl:   25.271175\n",
      "Epoch: 611 [20100/50000 (40%)]  \tLoss:   93.274460\trec:   66.730583\tkl:   26.543879\n",
      "Epoch: 611 [30100/50000 (60%)]  \tLoss:   90.359100\trec:   64.599480\tkl:   25.759613\n",
      "Epoch: 611 [40100/50000 (80%)]  \tLoss:   92.332558\trec:   66.294479\tkl:   26.038080\n",
      "====> Epoch: 611 Average train loss: 91.1930\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8416\n",
      "Epoch: 612 [  100/50000 ( 0%)]  \tLoss:   87.317017\trec:   61.782093\tkl:   25.534929\n",
      "Epoch: 612 [10100/50000 (20%)]  \tLoss:   89.864120\trec:   64.228462\tkl:   25.635658\n",
      "Epoch: 612 [20100/50000 (40%)]  \tLoss:   92.284882\trec:   65.563866\tkl:   26.721016\n",
      "Epoch: 612 [30100/50000 (60%)]  \tLoss:   90.963379\trec:   64.419121\tkl:   26.544254\n",
      "Epoch: 612 [40100/50000 (80%)]  \tLoss:   89.699509\trec:   63.508709\tkl:   26.190804\n",
      "====> Epoch: 612 Average train loss: 91.2070\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8081\n",
      "Epoch: 613 [  100/50000 ( 0%)]  \tLoss:   91.944000\trec:   65.401794\tkl:   26.542204\n",
      "Epoch: 613 [10100/50000 (20%)]  \tLoss:   91.183823\trec:   64.925041\tkl:   26.258789\n",
      "Epoch: 613 [20100/50000 (40%)]  \tLoss:   90.600304\trec:   64.729568\tkl:   25.870731\n",
      "Epoch: 613 [30100/50000 (60%)]  \tLoss:   89.887642\trec:   63.699753\tkl:   26.187893\n",
      "Epoch: 613 [40100/50000 (80%)]  \tLoss:   90.083168\trec:   64.550232\tkl:   25.532936\n",
      "====> Epoch: 613 Average train loss: 91.2084\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8305\n",
      "Epoch: 614 [  100/50000 ( 0%)]  \tLoss:   96.824982\trec:   69.816658\tkl:   27.008322\n",
      "Epoch: 614 [10100/50000 (20%)]  \tLoss:   90.447380\trec:   64.345009\tkl:   26.102375\n",
      "Epoch: 614 [20100/50000 (40%)]  \tLoss:   87.804390\trec:   62.338242\tkl:   25.466156\n",
      "Epoch: 614 [30100/50000 (60%)]  \tLoss:   89.333054\trec:   62.727547\tkl:   26.605507\n",
      "Epoch: 614 [40100/50000 (80%)]  \tLoss:   90.380943\trec:   64.390411\tkl:   25.990536\n",
      "====> Epoch: 614 Average train loss: 91.2034\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7223\n",
      "Epoch: 615 [  100/50000 ( 0%)]  \tLoss:   89.937233\trec:   64.246330\tkl:   25.690905\n",
      "Epoch: 615 [10100/50000 (20%)]  \tLoss:   88.262886\trec:   62.725330\tkl:   25.537563\n",
      "Epoch: 615 [20100/50000 (40%)]  \tLoss:   92.517281\trec:   67.241348\tkl:   25.275932\n",
      "Epoch: 615 [30100/50000 (60%)]  \tLoss:   91.898407\trec:   65.998650\tkl:   25.899755\n",
      "Epoch: 615 [40100/50000 (80%)]  \tLoss:   91.159569\trec:   64.282333\tkl:   26.877234\n",
      "====> Epoch: 615 Average train loss: 91.2307\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7346\n",
      "Epoch: 616 [  100/50000 ( 0%)]  \tLoss:   90.144646\trec:   64.375587\tkl:   25.769060\n",
      "Epoch: 616 [10100/50000 (20%)]  \tLoss:   94.258965\trec:   67.377922\tkl:   26.881039\n",
      "Epoch: 616 [20100/50000 (40%)]  \tLoss:   92.060104\trec:   65.694176\tkl:   26.365929\n",
      "Epoch: 616 [30100/50000 (60%)]  \tLoss:   93.547066\trec:   66.633904\tkl:   26.913166\n",
      "Epoch: 616 [40100/50000 (80%)]  \tLoss:   93.983887\trec:   67.485535\tkl:   26.498346\n",
      "====> Epoch: 616 Average train loss: 91.2117\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8094\n",
      "Epoch: 617 [  100/50000 ( 0%)]  \tLoss:   91.542564\trec:   64.844025\tkl:   26.698547\n",
      "Epoch: 617 [10100/50000 (20%)]  \tLoss:   91.116539\trec:   64.625961\tkl:   26.490585\n",
      "Epoch: 617 [20100/50000 (40%)]  \tLoss:   93.143768\trec:   66.599571\tkl:   26.544193\n",
      "Epoch: 617 [30100/50000 (60%)]  \tLoss:   93.010605\trec:   65.481934\tkl:   27.528673\n",
      "Epoch: 617 [40100/50000 (80%)]  \tLoss:   91.024445\trec:   64.832619\tkl:   26.191826\n",
      "====> Epoch: 617 Average train loss: 91.1937\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8290\n",
      "Epoch: 618 [  100/50000 ( 0%)]  \tLoss:   91.473579\trec:   65.386024\tkl:   26.087561\n",
      "Epoch: 618 [10100/50000 (20%)]  \tLoss:   92.641113\trec:   65.654068\tkl:   26.987038\n",
      "Epoch: 618 [20100/50000 (40%)]  \tLoss:   89.993721\trec:   64.200073\tkl:   25.793644\n",
      "Epoch: 618 [30100/50000 (60%)]  \tLoss:   92.298302\trec:   64.692192\tkl:   27.606106\n",
      "Epoch: 618 [40100/50000 (80%)]  \tLoss:   92.382225\trec:   65.782745\tkl:   26.599478\n",
      "====> Epoch: 618 Average train loss: 91.2210\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7600\n",
      "Epoch: 619 [  100/50000 ( 0%)]  \tLoss:   92.061089\trec:   65.825890\tkl:   26.235195\n",
      "Epoch: 619 [10100/50000 (20%)]  \tLoss:   88.984268\trec:   62.955574\tkl:   26.028690\n",
      "Epoch: 619 [20100/50000 (40%)]  \tLoss:   92.732712\trec:   65.874977\tkl:   26.857737\n",
      "Epoch: 619 [30100/50000 (60%)]  \tLoss:   87.927383\trec:   61.722923\tkl:   26.204458\n",
      "Epoch: 619 [40100/50000 (80%)]  \tLoss:   91.400490\trec:   64.503975\tkl:   26.896511\n",
      "====> Epoch: 619 Average train loss: 91.1876\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8095\n",
      "Epoch: 620 [  100/50000 ( 0%)]  \tLoss:   88.916824\trec:   63.387703\tkl:   25.529123\n",
      "Epoch: 620 [10100/50000 (20%)]  \tLoss:   92.847687\trec:   66.120079\tkl:   26.727608\n",
      "Epoch: 620 [20100/50000 (40%)]  \tLoss:   86.080650\trec:   61.265907\tkl:   24.814743\n",
      "Epoch: 620 [30100/50000 (60%)]  \tLoss:   87.690689\trec:   62.044903\tkl:   25.645792\n",
      "Epoch: 620 [40100/50000 (80%)]  \tLoss:   96.829185\trec:   69.531639\tkl:   27.297546\n",
      "====> Epoch: 620 Average train loss: 91.1896\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7848\n",
      "Epoch: 621 [  100/50000 ( 0%)]  \tLoss:   89.806015\trec:   63.690605\tkl:   26.115408\n",
      "Epoch: 621 [10100/50000 (20%)]  \tLoss:   93.501747\trec:   66.365677\tkl:   27.136066\n",
      "Epoch: 621 [20100/50000 (40%)]  \tLoss:   96.036278\trec:   69.464035\tkl:   26.572243\n",
      "Epoch: 621 [30100/50000 (60%)]  \tLoss:   89.722656\trec:   63.567146\tkl:   26.155512\n",
      "Epoch: 621 [40100/50000 (80%)]  \tLoss:   89.748024\trec:   63.570446\tkl:   26.177578\n",
      "====> Epoch: 621 Average train loss: 91.1805\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8334\n",
      "Epoch: 622 [  100/50000 ( 0%)]  \tLoss:   89.984627\trec:   63.963203\tkl:   26.021423\n",
      "Epoch: 622 [10100/50000 (20%)]  \tLoss:   93.701225\trec:   66.093391\tkl:   27.607834\n",
      "Epoch: 622 [20100/50000 (40%)]  \tLoss:   90.268776\trec:   64.214844\tkl:   26.053934\n",
      "Epoch: 622 [30100/50000 (60%)]  \tLoss:   90.651581\trec:   64.204620\tkl:   26.446960\n",
      "Epoch: 622 [40100/50000 (80%)]  \tLoss:   92.338661\trec:   66.049110\tkl:   26.289551\n",
      "====> Epoch: 622 Average train loss: 91.1851\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7905\n",
      "Epoch: 623 [  100/50000 ( 0%)]  \tLoss:   93.482246\trec:   67.171272\tkl:   26.310970\n",
      "Epoch: 623 [10100/50000 (20%)]  \tLoss:   90.940651\trec:   65.169067\tkl:   25.771589\n",
      "Epoch: 623 [20100/50000 (40%)]  \tLoss:   89.691231\trec:   63.270367\tkl:   26.420858\n",
      "Epoch: 623 [30100/50000 (60%)]  \tLoss:   86.858971\trec:   61.925163\tkl:   24.933805\n",
      "Epoch: 623 [40100/50000 (80%)]  \tLoss:   89.504707\trec:   64.004219\tkl:   25.500490\n",
      "====> Epoch: 623 Average train loss: 91.1936\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7085\n",
      "Epoch: 624 [  100/50000 ( 0%)]  \tLoss:   89.993980\trec:   65.040169\tkl:   24.953814\n",
      "Epoch: 624 [10100/50000 (20%)]  \tLoss:   88.173279\trec:   62.689995\tkl:   25.483280\n",
      "Epoch: 624 [20100/50000 (40%)]  \tLoss:   90.748695\trec:   64.024742\tkl:   26.723961\n",
      "Epoch: 624 [30100/50000 (60%)]  \tLoss:   91.050812\trec:   64.985161\tkl:   26.065643\n",
      "Epoch: 624 [40100/50000 (80%)]  \tLoss:   94.890381\trec:   67.948013\tkl:   26.942360\n",
      "====> Epoch: 624 Average train loss: 91.1971\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8506\n",
      "Epoch: 625 [  100/50000 ( 0%)]  \tLoss:   87.973946\trec:   62.092587\tkl:   25.881351\n",
      "Epoch: 625 [10100/50000 (20%)]  \tLoss:   91.747345\trec:   64.814476\tkl:   26.932871\n",
      "Epoch: 625 [20100/50000 (40%)]  \tLoss:   93.253181\trec:   67.068321\tkl:   26.184862\n",
      "Epoch: 625 [30100/50000 (60%)]  \tLoss:   93.350563\trec:   67.135880\tkl:   26.214684\n",
      "Epoch: 625 [40100/50000 (80%)]  \tLoss:   94.678726\trec:   69.006729\tkl:   25.672005\n",
      "====> Epoch: 625 Average train loss: 91.1553\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7823\n",
      "Epoch: 626 [  100/50000 ( 0%)]  \tLoss:   93.484665\trec:   67.259743\tkl:   26.224926\n",
      "Epoch: 626 [10100/50000 (20%)]  \tLoss:   88.010971\trec:   62.746590\tkl:   25.264389\n",
      "Epoch: 626 [20100/50000 (40%)]  \tLoss:   87.658653\trec:   62.197777\tkl:   25.460876\n",
      "Epoch: 626 [30100/50000 (60%)]  \tLoss:   92.377403\trec:   65.603546\tkl:   26.773857\n",
      "Epoch: 626 [40100/50000 (80%)]  \tLoss:   95.686951\trec:   68.942177\tkl:   26.744780\n",
      "====> Epoch: 626 Average train loss: 91.1720\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7892\n",
      "Epoch: 627 [  100/50000 ( 0%)]  \tLoss:   92.563454\trec:   65.612907\tkl:   26.950541\n",
      "Epoch: 627 [10100/50000 (20%)]  \tLoss:   90.184311\trec:   64.493378\tkl:   25.690935\n",
      "Epoch: 627 [20100/50000 (40%)]  \tLoss:   89.012329\trec:   63.846756\tkl:   25.165579\n",
      "Epoch: 627 [30100/50000 (60%)]  \tLoss:   90.279823\trec:   64.720940\tkl:   25.558889\n",
      "Epoch: 627 [40100/50000 (80%)]  \tLoss:   89.552078\trec:   63.571056\tkl:   25.981018\n",
      "====> Epoch: 627 Average train loss: 91.1334\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7003\n",
      "Epoch: 628 [  100/50000 ( 0%)]  \tLoss:   86.299179\trec:   60.286213\tkl:   26.012962\n",
      "Epoch: 628 [10100/50000 (20%)]  \tLoss:   90.111656\trec:   64.328178\tkl:   25.783474\n",
      "Epoch: 628 [20100/50000 (40%)]  \tLoss:   94.629257\trec:   68.353317\tkl:   26.275948\n",
      "Epoch: 628 [30100/50000 (60%)]  \tLoss:   90.598305\trec:   65.360985\tkl:   25.237324\n",
      "Epoch: 628 [40100/50000 (80%)]  \tLoss:   92.343513\trec:   65.486221\tkl:   26.857290\n",
      "====> Epoch: 628 Average train loss: 91.1488\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7142\n",
      "Epoch: 629 [  100/50000 ( 0%)]  \tLoss:   92.971382\trec:   66.438156\tkl:   26.533232\n",
      "Epoch: 629 [10100/50000 (20%)]  \tLoss:   87.911842\trec:   63.185379\tkl:   24.726465\n",
      "Epoch: 629 [20100/50000 (40%)]  \tLoss:   90.333771\trec:   64.769463\tkl:   25.564308\n",
      "Epoch: 629 [30100/50000 (60%)]  \tLoss:   91.020058\trec:   65.321037\tkl:   25.699018\n",
      "Epoch: 629 [40100/50000 (80%)]  \tLoss:   88.946205\trec:   63.113571\tkl:   25.832636\n",
      "====> Epoch: 629 Average train loss: 91.1707\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6893\n",
      "Epoch: 630 [  100/50000 ( 0%)]  \tLoss:   95.156387\trec:   68.205444\tkl:   26.950939\n",
      "Epoch: 630 [10100/50000 (20%)]  \tLoss:   93.307449\trec:   66.611969\tkl:   26.695482\n",
      "Epoch: 630 [20100/50000 (40%)]  \tLoss:   89.499939\trec:   63.677029\tkl:   25.822905\n",
      "Epoch: 630 [30100/50000 (60%)]  \tLoss:   91.484177\trec:   64.990768\tkl:   26.493402\n",
      "Epoch: 630 [40100/50000 (80%)]  \tLoss:   90.289413\trec:   63.607090\tkl:   26.682322\n",
      "====> Epoch: 630 Average train loss: 91.1589\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7627\n",
      "Epoch: 631 [  100/50000 ( 0%)]  \tLoss:   91.452950\trec:   65.737495\tkl:   25.715450\n",
      "Epoch: 631 [10100/50000 (20%)]  \tLoss:   88.268143\trec:   62.791660\tkl:   25.476484\n",
      "Epoch: 631 [20100/50000 (40%)]  \tLoss:   86.923309\trec:   61.262871\tkl:   25.660439\n",
      "Epoch: 631 [30100/50000 (60%)]  \tLoss:   92.114967\trec:   65.659431\tkl:   26.455534\n",
      "Epoch: 631 [40100/50000 (80%)]  \tLoss:   91.736191\trec:   65.455421\tkl:   26.280766\n",
      "====> Epoch: 631 Average train loss: 91.1329\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7886\n",
      "Epoch: 632 [  100/50000 ( 0%)]  \tLoss:   90.910522\trec:   65.015793\tkl:   25.894737\n",
      "Epoch: 632 [10100/50000 (20%)]  \tLoss:   92.366211\trec:   66.122871\tkl:   26.243343\n",
      "Epoch: 632 [20100/50000 (40%)]  \tLoss:   91.966080\trec:   65.748520\tkl:   26.217566\n",
      "Epoch: 632 [30100/50000 (60%)]  \tLoss:   89.984581\trec:   64.249306\tkl:   25.735273\n",
      "Epoch: 632 [40100/50000 (80%)]  \tLoss:   89.436905\trec:   63.846687\tkl:   25.590214\n",
      "====> Epoch: 632 Average train loss: 91.1488\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7469\n",
      "Epoch: 633 [  100/50000 ( 0%)]  \tLoss:   91.647881\trec:   65.911674\tkl:   25.736202\n",
      "Epoch: 633 [10100/50000 (20%)]  \tLoss:   92.564735\trec:   66.249825\tkl:   26.314911\n",
      "Epoch: 633 [20100/50000 (40%)]  \tLoss:   91.598404\trec:   64.957054\tkl:   26.641354\n",
      "Epoch: 633 [30100/50000 (60%)]  \tLoss:   93.148911\trec:   66.421104\tkl:   26.727810\n",
      "Epoch: 633 [40100/50000 (80%)]  \tLoss:   88.773094\trec:   63.679932\tkl:   25.093161\n",
      "====> Epoch: 633 Average train loss: 91.1229\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7174\n",
      "Epoch: 634 [  100/50000 ( 0%)]  \tLoss:   91.866890\trec:   65.572311\tkl:   26.294582\n",
      "Epoch: 634 [10100/50000 (20%)]  \tLoss:   93.170731\trec:   67.794113\tkl:   25.376617\n",
      "Epoch: 634 [20100/50000 (40%)]  \tLoss:   90.315659\trec:   64.601608\tkl:   25.714052\n",
      "Epoch: 634 [30100/50000 (60%)]  \tLoss:   88.602638\trec:   63.170612\tkl:   25.432024\n",
      "Epoch: 634 [40100/50000 (80%)]  \tLoss:   91.111618\trec:   64.130058\tkl:   26.981562\n",
      "====> Epoch: 634 Average train loss: 91.1439\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6768\n",
      "Epoch: 635 [  100/50000 ( 0%)]  \tLoss:   88.301476\trec:   62.548916\tkl:   25.752558\n",
      "Epoch: 635 [10100/50000 (20%)]  \tLoss:   91.334694\trec:   65.505737\tkl:   25.828962\n",
      "Epoch: 635 [20100/50000 (40%)]  \tLoss:   89.998863\trec:   63.462471\tkl:   26.536396\n",
      "Epoch: 635 [30100/50000 (60%)]  \tLoss:   91.289001\trec:   64.793312\tkl:   26.495697\n",
      "Epoch: 635 [40100/50000 (80%)]  \tLoss:   92.735603\trec:   66.559937\tkl:   26.175663\n",
      "====> Epoch: 635 Average train loss: 91.1423\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7874\n",
      "Epoch: 636 [  100/50000 ( 0%)]  \tLoss:   92.141457\trec:   64.825714\tkl:   27.315744\n",
      "Epoch: 636 [10100/50000 (20%)]  \tLoss:   93.790115\trec:   68.251892\tkl:   25.538225\n",
      "Epoch: 636 [20100/50000 (40%)]  \tLoss:   90.899284\trec:   65.328331\tkl:   25.570953\n",
      "Epoch: 636 [30100/50000 (60%)]  \tLoss:   94.711746\trec:   67.496513\tkl:   27.215231\n",
      "Epoch: 636 [40100/50000 (80%)]  \tLoss:   87.821541\trec:   61.884071\tkl:   25.937469\n",
      "====> Epoch: 636 Average train loss: 91.1365\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7049\n",
      "Epoch: 637 [  100/50000 ( 0%)]  \tLoss:   86.351631\trec:   60.245884\tkl:   26.105743\n",
      "Epoch: 637 [10100/50000 (20%)]  \tLoss:   90.162140\trec:   64.299316\tkl:   25.862814\n",
      "Epoch: 637 [20100/50000 (40%)]  \tLoss:   88.703491\trec:   63.368031\tkl:   25.335459\n",
      "Epoch: 637 [30100/50000 (60%)]  \tLoss:   92.819557\trec:   66.707420\tkl:   26.112139\n",
      "Epoch: 637 [40100/50000 (80%)]  \tLoss:   90.278778\trec:   64.557297\tkl:   25.721483\n",
      "====> Epoch: 637 Average train loss: 91.1322\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8014\n",
      "Epoch: 638 [  100/50000 ( 0%)]  \tLoss:   95.572495\trec:   69.014091\tkl:   26.558407\n",
      "Epoch: 638 [10100/50000 (20%)]  \tLoss:   89.613121\trec:   63.317451\tkl:   26.295668\n",
      "Epoch: 638 [20100/50000 (40%)]  \tLoss:   94.135887\trec:   67.985519\tkl:   26.150368\n",
      "Epoch: 638 [30100/50000 (60%)]  \tLoss:   92.688774\trec:   66.566757\tkl:   26.122023\n",
      "Epoch: 638 [40100/50000 (80%)]  \tLoss:   92.483414\trec:   66.120613\tkl:   26.362804\n",
      "====> Epoch: 638 Average train loss: 91.1101\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7406\n",
      "Epoch: 639 [  100/50000 ( 0%)]  \tLoss:   92.275719\trec:   65.515938\tkl:   26.759785\n",
      "Epoch: 639 [10100/50000 (20%)]  \tLoss:   86.249527\trec:   60.823669\tkl:   25.425861\n",
      "Epoch: 639 [20100/50000 (40%)]  \tLoss:   89.631256\trec:   63.982876\tkl:   25.648380\n",
      "Epoch: 639 [30100/50000 (60%)]  \tLoss:   93.689507\trec:   66.867134\tkl:   26.822374\n",
      "Epoch: 639 [40100/50000 (80%)]  \tLoss:   94.413406\trec:   67.366600\tkl:   27.046804\n",
      "====> Epoch: 639 Average train loss: 91.1159\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7386\n",
      "Epoch: 640 [  100/50000 ( 0%)]  \tLoss:   92.479721\trec:   65.829903\tkl:   26.649815\n",
      "Epoch: 640 [10100/50000 (20%)]  \tLoss:   90.533043\trec:   65.067291\tkl:   25.465755\n",
      "Epoch: 640 [20100/50000 (40%)]  \tLoss:   90.779350\trec:   64.926048\tkl:   25.853302\n",
      "Epoch: 640 [30100/50000 (60%)]  \tLoss:   96.152832\trec:   69.088654\tkl:   27.064175\n",
      "Epoch: 640 [40100/50000 (80%)]  \tLoss:   89.169502\trec:   63.756996\tkl:   25.412504\n",
      "====> Epoch: 640 Average train loss: 91.0909\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7989\n",
      "Epoch: 641 [  100/50000 ( 0%)]  \tLoss:   89.452164\trec:   62.643143\tkl:   26.809017\n",
      "Epoch: 641 [10100/50000 (20%)]  \tLoss:   92.777901\trec:   66.992912\tkl:   25.784990\n",
      "Epoch: 641 [20100/50000 (40%)]  \tLoss:   92.813835\trec:   66.766670\tkl:   26.047171\n",
      "Epoch: 641 [30100/50000 (60%)]  \tLoss:   88.463806\trec:   62.955505\tkl:   25.508305\n",
      "Epoch: 641 [40100/50000 (80%)]  \tLoss:   87.504250\trec:   62.670219\tkl:   24.834024\n",
      "====> Epoch: 641 Average train loss: 91.1145\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7272\n",
      "Epoch: 642 [  100/50000 ( 0%)]  \tLoss:   89.592712\trec:   63.363865\tkl:   26.228851\n",
      "Epoch: 642 [10100/50000 (20%)]  \tLoss:   86.116264\trec:   59.979782\tkl:   26.136484\n",
      "Epoch: 642 [20100/50000 (40%)]  \tLoss:   91.989372\trec:   65.995255\tkl:   25.994123\n",
      "Epoch: 642 [30100/50000 (60%)]  \tLoss:   92.796288\trec:   66.153214\tkl:   26.643074\n",
      "Epoch: 642 [40100/50000 (80%)]  \tLoss:   91.052315\trec:   64.828476\tkl:   26.223837\n",
      "====> Epoch: 642 Average train loss: 91.0894\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7651\n",
      "Epoch: 643 [  100/50000 ( 0%)]  \tLoss:   90.685966\trec:   64.273956\tkl:   26.412008\n",
      "Epoch: 643 [10100/50000 (20%)]  \tLoss:   92.589821\trec:   65.861290\tkl:   26.728525\n",
      "Epoch: 643 [20100/50000 (40%)]  \tLoss:   89.061432\trec:   63.130131\tkl:   25.931305\n",
      "Epoch: 643 [30100/50000 (60%)]  \tLoss:   89.815880\trec:   63.364403\tkl:   26.451473\n",
      "Epoch: 643 [40100/50000 (80%)]  \tLoss:   94.922638\trec:   67.862617\tkl:   27.060011\n",
      "====> Epoch: 643 Average train loss: 91.0981\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8125\n",
      "Epoch: 644 [  100/50000 ( 0%)]  \tLoss:   94.869614\trec:   67.874687\tkl:   26.994928\n",
      "Epoch: 644 [10100/50000 (20%)]  \tLoss:   94.973091\trec:   68.298874\tkl:   26.674221\n",
      "Epoch: 644 [20100/50000 (40%)]  \tLoss:   91.838921\trec:   65.894775\tkl:   25.944149\n",
      "Epoch: 644 [30100/50000 (60%)]  \tLoss:   88.518135\trec:   63.520790\tkl:   24.997343\n",
      "Epoch: 644 [40100/50000 (80%)]  \tLoss:   93.205612\trec:   66.613037\tkl:   26.592569\n",
      "====> Epoch: 644 Average train loss: 91.1100\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7484\n",
      "Epoch: 645 [  100/50000 ( 0%)]  \tLoss:   92.061974\trec:   65.436676\tkl:   26.625288\n",
      "Epoch: 645 [10100/50000 (20%)]  \tLoss:   90.444168\trec:   64.332291\tkl:   26.111872\n",
      "Epoch: 645 [20100/50000 (40%)]  \tLoss:   90.746910\trec:   64.923874\tkl:   25.823038\n",
      "Epoch: 645 [30100/50000 (60%)]  \tLoss:   90.914413\trec:   65.144264\tkl:   25.770149\n",
      "Epoch: 645 [40100/50000 (80%)]  \tLoss:   89.767731\trec:   63.741718\tkl:   26.026011\n",
      "====> Epoch: 645 Average train loss: 91.1109\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7454\n",
      "Epoch: 646 [  100/50000 ( 0%)]  \tLoss:   90.393745\trec:   64.672745\tkl:   25.721010\n",
      "Epoch: 646 [10100/50000 (20%)]  \tLoss:   91.231834\trec:   64.304008\tkl:   26.927816\n",
      "Epoch: 646 [20100/50000 (40%)]  \tLoss:   91.056892\trec:   64.352089\tkl:   26.704807\n",
      "Epoch: 646 [30100/50000 (60%)]  \tLoss:   91.592598\trec:   64.651245\tkl:   26.941355\n",
      "Epoch: 646 [40100/50000 (80%)]  \tLoss:   90.028984\trec:   63.774151\tkl:   26.254833\n",
      "====> Epoch: 646 Average train loss: 91.0888\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7211\n",
      "Epoch: 647 [  100/50000 ( 0%)]  \tLoss:   89.579567\trec:   63.466576\tkl:   26.112997\n",
      "Epoch: 647 [10100/50000 (20%)]  \tLoss:   87.514999\trec:   62.090378\tkl:   25.424616\n",
      "Epoch: 647 [20100/50000 (40%)]  \tLoss:   90.226173\trec:   63.858997\tkl:   26.367174\n",
      "Epoch: 647 [30100/50000 (60%)]  \tLoss:   95.932007\trec:   68.745293\tkl:   27.186716\n",
      "Epoch: 647 [40100/50000 (80%)]  \tLoss:   90.352036\trec:   63.726109\tkl:   26.625929\n",
      "====> Epoch: 647 Average train loss: 91.0853\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6942\n",
      "Epoch: 648 [  100/50000 ( 0%)]  \tLoss:   93.361206\trec:   66.636955\tkl:   26.724257\n",
      "Epoch: 648 [10100/50000 (20%)]  \tLoss:   92.670662\trec:   65.935699\tkl:   26.734962\n",
      "Epoch: 648 [20100/50000 (40%)]  \tLoss:   90.078400\trec:   63.945770\tkl:   26.132622\n",
      "Epoch: 648 [30100/50000 (60%)]  \tLoss:   89.126579\trec:   63.696259\tkl:   25.430323\n",
      "Epoch: 648 [40100/50000 (80%)]  \tLoss:   93.696091\trec:   66.487679\tkl:   27.208418\n",
      "====> Epoch: 648 Average train loss: 91.0891\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7343\n",
      "Epoch: 649 [  100/50000 ( 0%)]  \tLoss:   94.190163\trec:   67.071304\tkl:   27.118856\n",
      "Epoch: 649 [10100/50000 (20%)]  \tLoss:   92.238983\trec:   65.626404\tkl:   26.612576\n",
      "Epoch: 649 [20100/50000 (40%)]  \tLoss:   89.943291\trec:   63.731327\tkl:   26.211960\n",
      "Epoch: 649 [30100/50000 (60%)]  \tLoss:   91.643028\trec:   65.498657\tkl:   26.144365\n",
      "Epoch: 649 [40100/50000 (80%)]  \tLoss:   92.562477\trec:   66.634644\tkl:   25.927835\n",
      "====> Epoch: 649 Average train loss: 91.0654\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6262\n",
      "Epoch: 650 [  100/50000 ( 0%)]  \tLoss:   85.205750\trec:   59.540096\tkl:   25.665657\n",
      "Epoch: 650 [10100/50000 (20%)]  \tLoss:   90.645897\trec:   62.910564\tkl:   27.735331\n",
      "Epoch: 650 [20100/50000 (40%)]  \tLoss:   92.667244\trec:   66.160515\tkl:   26.506727\n",
      "Epoch: 650 [30100/50000 (60%)]  \tLoss:   90.269234\trec:   64.016029\tkl:   26.253204\n",
      "Epoch: 650 [40100/50000 (80%)]  \tLoss:   90.380455\trec:   64.860870\tkl:   25.519592\n",
      "====> Epoch: 650 Average train loss: 91.0803\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7777\n",
      "Epoch: 651 [  100/50000 ( 0%)]  \tLoss:   92.870445\trec:   67.241608\tkl:   25.628832\n",
      "Epoch: 651 [10100/50000 (20%)]  \tLoss:   89.064087\trec:   62.953571\tkl:   26.110514\n",
      "Epoch: 651 [20100/50000 (40%)]  \tLoss:   91.496407\trec:   65.323135\tkl:   26.173264\n",
      "Epoch: 651 [30100/50000 (60%)]  \tLoss:   90.889763\trec:   65.277878\tkl:   25.611889\n",
      "Epoch: 651 [40100/50000 (80%)]  \tLoss:   92.557426\trec:   66.639687\tkl:   25.917744\n",
      "====> Epoch: 651 Average train loss: 91.0860\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6660\n",
      "Epoch: 652 [  100/50000 ( 0%)]  \tLoss:   92.540466\trec:   66.772682\tkl:   25.767788\n",
      "Epoch: 652 [10100/50000 (20%)]  \tLoss:   94.089546\trec:   66.800377\tkl:   27.289169\n",
      "Epoch: 652 [20100/50000 (40%)]  \tLoss:   91.568657\trec:   66.071228\tkl:   25.497429\n",
      "Epoch: 652 [30100/50000 (60%)]  \tLoss:   92.525719\trec:   65.444977\tkl:   27.080742\n",
      "Epoch: 652 [40100/50000 (80%)]  \tLoss:   94.881958\trec:   69.090302\tkl:   25.791656\n",
      "====> Epoch: 652 Average train loss: 91.1046\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7435\n",
      "Epoch: 653 [  100/50000 ( 0%)]  \tLoss:   87.750900\trec:   62.504101\tkl:   25.246796\n",
      "Epoch: 653 [10100/50000 (20%)]  \tLoss:   89.583549\trec:   63.432995\tkl:   26.150557\n",
      "Epoch: 653 [20100/50000 (40%)]  \tLoss:   93.097221\trec:   67.388885\tkl:   25.708342\n",
      "Epoch: 653 [30100/50000 (60%)]  \tLoss:   91.134140\trec:   64.656075\tkl:   26.478064\n",
      "Epoch: 653 [40100/50000 (80%)]  \tLoss:   91.443260\trec:   65.531548\tkl:   25.911713\n",
      "====> Epoch: 653 Average train loss: 91.0501\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7391\n",
      "Epoch: 654 [  100/50000 ( 0%)]  \tLoss:   91.399292\trec:   65.079620\tkl:   26.319672\n",
      "Epoch: 654 [10100/50000 (20%)]  \tLoss:   96.190117\trec:   68.830353\tkl:   27.359770\n",
      "Epoch: 654 [20100/50000 (40%)]  \tLoss:   89.322372\trec:   63.875378\tkl:   25.446995\n",
      "Epoch: 654 [30100/50000 (60%)]  \tLoss:   88.891724\trec:   62.922234\tkl:   25.969490\n",
      "Epoch: 654 [40100/50000 (80%)]  \tLoss:   89.508438\trec:   63.180008\tkl:   26.328432\n",
      "====> Epoch: 654 Average train loss: 91.0580\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7602\n",
      "Epoch: 655 [  100/50000 ( 0%)]  \tLoss:   93.854004\trec:   67.299881\tkl:   26.554121\n",
      "Epoch: 655 [10100/50000 (20%)]  \tLoss:   91.711601\trec:   66.035439\tkl:   25.676159\n",
      "Epoch: 655 [20100/50000 (40%)]  \tLoss:   92.135620\trec:   66.003677\tkl:   26.131950\n",
      "Epoch: 655 [30100/50000 (60%)]  \tLoss:   93.307556\trec:   66.683357\tkl:   26.624203\n",
      "Epoch: 655 [40100/50000 (80%)]  \tLoss:   92.338486\trec:   66.656883\tkl:   25.681599\n",
      "====> Epoch: 655 Average train loss: 91.0588\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7609\n",
      "Epoch: 656 [  100/50000 ( 0%)]  \tLoss:   91.104782\trec:   64.826355\tkl:   26.278425\n",
      "Epoch: 656 [10100/50000 (20%)]  \tLoss:   93.649155\trec:   67.473892\tkl:   26.175272\n",
      "Epoch: 656 [20100/50000 (40%)]  \tLoss:   91.440079\trec:   65.100548\tkl:   26.339521\n",
      "Epoch: 656 [30100/50000 (60%)]  \tLoss:   92.755775\trec:   65.798874\tkl:   26.956900\n",
      "Epoch: 656 [40100/50000 (80%)]  \tLoss:   92.090897\trec:   66.209084\tkl:   25.881819\n",
      "====> Epoch: 656 Average train loss: 91.0631\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7462\n",
      "Epoch: 657 [  100/50000 ( 0%)]  \tLoss:   86.227898\trec:   60.901752\tkl:   25.326149\n",
      "Epoch: 657 [10100/50000 (20%)]  \tLoss:   94.593369\trec:   68.411346\tkl:   26.182018\n",
      "Epoch: 657 [20100/50000 (40%)]  \tLoss:   89.362389\trec:   62.941254\tkl:   26.421139\n",
      "Epoch: 657 [30100/50000 (60%)]  \tLoss:   88.206627\trec:   63.378975\tkl:   24.827658\n",
      "Epoch: 657 [40100/50000 (80%)]  \tLoss:   88.957558\trec:   62.220825\tkl:   26.736738\n",
      "====> Epoch: 657 Average train loss: 91.0512\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6932\n",
      "Epoch: 658 [  100/50000 ( 0%)]  \tLoss:   87.972794\trec:   61.732689\tkl:   26.240103\n",
      "Epoch: 658 [10100/50000 (20%)]  \tLoss:   92.239693\trec:   65.703911\tkl:   26.535778\n",
      "Epoch: 658 [20100/50000 (40%)]  \tLoss:   94.435036\trec:   66.967827\tkl:   27.467203\n",
      "Epoch: 658 [30100/50000 (60%)]  \tLoss:   96.774200\trec:   69.553001\tkl:   27.221199\n",
      "Epoch: 658 [40100/50000 (80%)]  \tLoss:   92.658989\trec:   65.811806\tkl:   26.847189\n",
      "====> Epoch: 658 Average train loss: 91.0627\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6944\n",
      "Epoch: 659 [  100/50000 ( 0%)]  \tLoss:   91.835449\trec:   65.389458\tkl:   26.445992\n",
      "Epoch: 659 [10100/50000 (20%)]  \tLoss:   91.301392\trec:   65.218575\tkl:   26.082815\n",
      "Epoch: 659 [20100/50000 (40%)]  \tLoss:   91.266479\trec:   65.077461\tkl:   26.189028\n",
      "Epoch: 659 [30100/50000 (60%)]  \tLoss:   93.576920\trec:   66.965096\tkl:   26.611826\n",
      "Epoch: 659 [40100/50000 (80%)]  \tLoss:   85.054977\trec:   60.714081\tkl:   24.340899\n",
      "====> Epoch: 659 Average train loss: 91.0302\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7431\n",
      "Epoch: 660 [  100/50000 ( 0%)]  \tLoss:   91.357498\trec:   64.823746\tkl:   26.533749\n",
      "Epoch: 660 [10100/50000 (20%)]  \tLoss:   93.377617\trec:   67.091728\tkl:   26.285892\n",
      "Epoch: 660 [20100/50000 (40%)]  \tLoss:   90.964775\trec:   63.968575\tkl:   26.996201\n",
      "Epoch: 660 [30100/50000 (60%)]  \tLoss:   86.754608\trec:   60.850243\tkl:   25.904366\n",
      "Epoch: 660 [40100/50000 (80%)]  \tLoss:   94.287994\trec:   67.148079\tkl:   27.139919\n",
      "====> Epoch: 660 Average train loss: 91.0443\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6630\n",
      "Epoch: 661 [  100/50000 ( 0%)]  \tLoss:   93.049789\trec:   66.537064\tkl:   26.512726\n",
      "Epoch: 661 [10100/50000 (20%)]  \tLoss:   93.845818\trec:   66.304436\tkl:   27.541389\n",
      "Epoch: 661 [20100/50000 (40%)]  \tLoss:   91.942909\trec:   65.698425\tkl:   26.244482\n",
      "Epoch: 661 [30100/50000 (60%)]  \tLoss:   89.777641\trec:   64.376671\tkl:   25.400976\n",
      "Epoch: 661 [40100/50000 (80%)]  \tLoss:   89.836472\trec:   63.978485\tkl:   25.857990\n",
      "====> Epoch: 661 Average train loss: 91.0442\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6041\n",
      "Epoch: 662 [  100/50000 ( 0%)]  \tLoss:   94.950508\trec:   67.703331\tkl:   27.247168\n",
      "Epoch: 662 [10100/50000 (20%)]  \tLoss:   89.325760\trec:   63.800785\tkl:   25.524981\n",
      "Epoch: 662 [20100/50000 (40%)]  \tLoss:   91.105339\trec:   64.743446\tkl:   26.361897\n",
      "Epoch: 662 [30100/50000 (60%)]  \tLoss:   91.387321\trec:   65.438644\tkl:   25.948673\n",
      "Epoch: 662 [40100/50000 (80%)]  \tLoss:   93.820068\trec:   67.644539\tkl:   26.175529\n",
      "====> Epoch: 662 Average train loss: 91.0588\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6790\n",
      "Epoch: 663 [  100/50000 ( 0%)]  \tLoss:   91.991280\trec:   66.243301\tkl:   25.747972\n",
      "Epoch: 663 [10100/50000 (20%)]  \tLoss:   89.820534\trec:   64.382385\tkl:   25.438147\n",
      "Epoch: 663 [20100/50000 (40%)]  \tLoss:   93.278397\trec:   67.061867\tkl:   26.216536\n",
      "Epoch: 663 [30100/50000 (60%)]  \tLoss:   89.111824\trec:   63.458233\tkl:   25.653593\n",
      "Epoch: 663 [40100/50000 (80%)]  \tLoss:   92.373032\trec:   65.840019\tkl:   26.533016\n",
      "====> Epoch: 663 Average train loss: 91.0230\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7668\n",
      "Epoch: 664 [  100/50000 ( 0%)]  \tLoss:   90.462563\trec:   64.132294\tkl:   26.330275\n",
      "Epoch: 664 [10100/50000 (20%)]  \tLoss:   88.931999\trec:   62.980160\tkl:   25.951838\n",
      "Epoch: 664 [20100/50000 (40%)]  \tLoss:   84.349625\trec:   59.557816\tkl:   24.791809\n",
      "Epoch: 664 [30100/50000 (60%)]  \tLoss:   92.827126\trec:   66.052383\tkl:   26.774740\n",
      "Epoch: 664 [40100/50000 (80%)]  \tLoss:   93.938477\trec:   67.054840\tkl:   26.883636\n",
      "====> Epoch: 664 Average train loss: 91.0145\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6964\n",
      "Epoch: 665 [  100/50000 ( 0%)]  \tLoss:   88.028648\trec:   62.482567\tkl:   25.546082\n",
      "Epoch: 665 [10100/50000 (20%)]  \tLoss:   92.879959\trec:   66.480202\tkl:   26.399754\n",
      "Epoch: 665 [20100/50000 (40%)]  \tLoss:   87.793884\trec:   61.822552\tkl:   25.971333\n",
      "Epoch: 665 [30100/50000 (60%)]  \tLoss:   97.719444\trec:   69.893501\tkl:   27.825939\n",
      "Epoch: 665 [40100/50000 (80%)]  \tLoss:   87.775124\trec:   61.429935\tkl:   26.345188\n",
      "====> Epoch: 665 Average train loss: 91.0296\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7355\n",
      "Epoch: 666 [  100/50000 ( 0%)]  \tLoss:   86.791611\trec:   61.875896\tkl:   24.915712\n",
      "Epoch: 666 [10100/50000 (20%)]  \tLoss:   91.164444\trec:   65.170792\tkl:   25.993654\n",
      "Epoch: 666 [20100/50000 (40%)]  \tLoss:   93.574615\trec:   66.669708\tkl:   26.904915\n",
      "Epoch: 666 [30100/50000 (60%)]  \tLoss:   92.621796\trec:   65.638756\tkl:   26.983046\n",
      "Epoch: 666 [40100/50000 (80%)]  \tLoss:   90.281990\trec:   64.086433\tkl:   26.195560\n",
      "====> Epoch: 666 Average train loss: 91.0333\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6740\n",
      "Epoch: 667 [  100/50000 ( 0%)]  \tLoss:   95.461098\trec:   67.957817\tkl:   27.503281\n",
      "Epoch: 667 [10100/50000 (20%)]  \tLoss:   89.926231\trec:   63.878963\tkl:   26.047264\n",
      "Epoch: 667 [20100/50000 (40%)]  \tLoss:   87.297684\trec:   61.888950\tkl:   25.408735\n",
      "Epoch: 667 [30100/50000 (60%)]  \tLoss:   93.309128\trec:   66.358696\tkl:   26.950432\n",
      "Epoch: 667 [40100/50000 (80%)]  \tLoss:   86.253983\trec:   60.922878\tkl:   25.331102\n",
      "====> Epoch: 667 Average train loss: 91.0217\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5956\n",
      "Epoch: 668 [  100/50000 ( 0%)]  \tLoss:   94.695328\trec:   66.827881\tkl:   27.867449\n",
      "Epoch: 668 [10100/50000 (20%)]  \tLoss:   89.515366\trec:   63.472691\tkl:   26.042681\n",
      "Epoch: 668 [20100/50000 (40%)]  \tLoss:   88.785164\trec:   63.106026\tkl:   25.679142\n",
      "Epoch: 668 [30100/50000 (60%)]  \tLoss:   94.189217\trec:   67.727371\tkl:   26.461840\n",
      "Epoch: 668 [40100/50000 (80%)]  \tLoss:   92.277702\trec:   65.874756\tkl:   26.402946\n",
      "====> Epoch: 668 Average train loss: 91.0069\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6402\n",
      "Epoch: 669 [  100/50000 ( 0%)]  \tLoss:   90.828133\trec:   64.771240\tkl:   26.056892\n",
      "Epoch: 669 [10100/50000 (20%)]  \tLoss:   90.479607\trec:   64.540092\tkl:   25.939516\n",
      "Epoch: 669 [20100/50000 (40%)]  \tLoss:   91.314049\trec:   65.295425\tkl:   26.018627\n",
      "Epoch: 669 [30100/50000 (60%)]  \tLoss:   87.130936\trec:   61.563293\tkl:   25.567638\n",
      "Epoch: 669 [40100/50000 (80%)]  \tLoss:   86.028999\trec:   60.989822\tkl:   25.039185\n",
      "====> Epoch: 669 Average train loss: 90.9998\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5898\n",
      "Epoch: 670 [  100/50000 ( 0%)]  \tLoss:   89.290268\trec:   63.100361\tkl:   26.189907\n",
      "Epoch: 670 [10100/50000 (20%)]  \tLoss:   91.170410\trec:   65.620720\tkl:   25.549683\n",
      "Epoch: 670 [20100/50000 (40%)]  \tLoss:   90.547791\trec:   63.670944\tkl:   26.876841\n",
      "Epoch: 670 [30100/50000 (60%)]  \tLoss:   89.734695\trec:   63.825764\tkl:   25.908932\n",
      "Epoch: 670 [40100/50000 (80%)]  \tLoss:   92.263458\trec:   65.435799\tkl:   26.827660\n",
      "====> Epoch: 670 Average train loss: 91.0173\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7690\n",
      "Epoch: 671 [  100/50000 ( 0%)]  \tLoss:   97.089828\trec:   70.547256\tkl:   26.542576\n",
      "Epoch: 671 [10100/50000 (20%)]  \tLoss:   90.192818\trec:   64.300308\tkl:   25.892509\n",
      "Epoch: 671 [20100/50000 (40%)]  \tLoss:   88.211052\trec:   62.337635\tkl:   25.873423\n",
      "Epoch: 671 [30100/50000 (60%)]  \tLoss:   94.416695\trec:   67.655388\tkl:   26.761305\n",
      "Epoch: 671 [40100/50000 (80%)]  \tLoss:   93.716446\trec:   66.779434\tkl:   26.937016\n",
      "====> Epoch: 671 Average train loss: 91.0023\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6646\n",
      "Epoch: 672 [  100/50000 ( 0%)]  \tLoss:   91.044518\trec:   64.996506\tkl:   26.048019\n",
      "Epoch: 672 [10100/50000 (20%)]  \tLoss:   91.130524\trec:   64.858276\tkl:   26.272251\n",
      "Epoch: 672 [20100/50000 (40%)]  \tLoss:   86.402390\trec:   61.049206\tkl:   25.353186\n",
      "Epoch: 672 [30100/50000 (60%)]  \tLoss:   92.303062\trec:   65.220993\tkl:   27.082071\n",
      "Epoch: 672 [40100/50000 (80%)]  \tLoss:   94.159584\trec:   68.111198\tkl:   26.048386\n",
      "====> Epoch: 672 Average train loss: 90.9993\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6813\n",
      "Epoch: 673 [  100/50000 ( 0%)]  \tLoss:   87.072861\trec:   61.135525\tkl:   25.937330\n",
      "Epoch: 673 [10100/50000 (20%)]  \tLoss:   88.213379\trec:   61.654411\tkl:   26.558964\n",
      "Epoch: 673 [20100/50000 (40%)]  \tLoss:   87.821114\trec:   62.395630\tkl:   25.425484\n",
      "Epoch: 673 [30100/50000 (60%)]  \tLoss:   90.004517\trec:   64.477425\tkl:   25.527096\n",
      "Epoch: 673 [40100/50000 (80%)]  \tLoss:   87.899353\trec:   62.992290\tkl:   24.907064\n",
      "====> Epoch: 673 Average train loss: 91.0096\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7839\n",
      "Epoch: 674 [  100/50000 ( 0%)]  \tLoss:   89.084068\trec:   62.949230\tkl:   26.134838\n",
      "Epoch: 674 [10100/50000 (20%)]  \tLoss:   89.800079\trec:   63.978870\tkl:   25.821203\n",
      "Epoch: 674 [20100/50000 (40%)]  \tLoss:   90.372589\trec:   65.079453\tkl:   25.293137\n",
      "Epoch: 674 [30100/50000 (60%)]  \tLoss:   91.617775\trec:   65.549713\tkl:   26.068062\n",
      "Epoch: 674 [40100/50000 (80%)]  \tLoss:   88.648064\trec:   62.877426\tkl:   25.770639\n",
      "====> Epoch: 674 Average train loss: 90.9781\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8203\n",
      "Epoch: 675 [  100/50000 ( 0%)]  \tLoss:   88.093246\trec:   62.122322\tkl:   25.970926\n",
      "Epoch: 675 [10100/50000 (20%)]  \tLoss:   94.219200\trec:   67.544312\tkl:   26.674883\n",
      "Epoch: 675 [20100/50000 (40%)]  \tLoss:   87.380402\trec:   61.632191\tkl:   25.748209\n",
      "Epoch: 675 [30100/50000 (60%)]  \tLoss:   89.988075\trec:   64.697594\tkl:   25.290476\n",
      "Epoch: 675 [40100/50000 (80%)]  \tLoss:   92.362648\trec:   66.560562\tkl:   25.802080\n",
      "====> Epoch: 675 Average train loss: 91.0001\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7489\n",
      "Epoch: 676 [  100/50000 ( 0%)]  \tLoss:   90.567505\trec:   65.039162\tkl:   25.528347\n",
      "Epoch: 676 [10100/50000 (20%)]  \tLoss:   90.232224\trec:   64.953400\tkl:   25.278830\n",
      "Epoch: 676 [20100/50000 (40%)]  \tLoss:   91.984901\trec:   66.215157\tkl:   25.769743\n",
      "Epoch: 676 [30100/50000 (60%)]  \tLoss:   92.597794\trec:   66.294586\tkl:   26.303198\n",
      "Epoch: 676 [40100/50000 (80%)]  \tLoss:   90.846817\trec:   64.323669\tkl:   26.523140\n",
      "====> Epoch: 676 Average train loss: 90.9979\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6372\n",
      "Epoch: 677 [  100/50000 ( 0%)]  \tLoss:   88.863182\trec:   63.371902\tkl:   25.491276\n",
      "Epoch: 677 [10100/50000 (20%)]  \tLoss:   89.934860\trec:   64.148888\tkl:   25.785973\n",
      "Epoch: 677 [20100/50000 (40%)]  \tLoss:   93.528687\trec:   66.857231\tkl:   26.671459\n",
      "Epoch: 677 [30100/50000 (60%)]  \tLoss:   88.067146\trec:   62.398907\tkl:   25.668236\n",
      "Epoch: 677 [40100/50000 (80%)]  \tLoss:   92.433884\trec:   66.449371\tkl:   25.984514\n",
      "====> Epoch: 677 Average train loss: 90.9885\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6593\n",
      "Epoch: 678 [  100/50000 ( 0%)]  \tLoss:   90.076820\trec:   63.638542\tkl:   26.438280\n",
      "Epoch: 678 [10100/50000 (20%)]  \tLoss:   89.647293\trec:   63.887161\tkl:   25.760128\n",
      "Epoch: 678 [20100/50000 (40%)]  \tLoss:   91.776894\trec:   65.595284\tkl:   26.181608\n",
      "Epoch: 678 [30100/50000 (60%)]  \tLoss:   88.341774\trec:   63.055202\tkl:   25.286570\n",
      "Epoch: 678 [40100/50000 (80%)]  \tLoss:   94.807755\trec:   68.162003\tkl:   26.645748\n",
      "====> Epoch: 678 Average train loss: 90.9799\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6666\n",
      "Epoch: 679 [  100/50000 ( 0%)]  \tLoss:   88.753731\trec:   62.860947\tkl:   25.892784\n",
      "Epoch: 679 [10100/50000 (20%)]  \tLoss:   88.335289\trec:   63.244804\tkl:   25.090483\n",
      "Epoch: 679 [20100/50000 (40%)]  \tLoss:   87.490654\trec:   61.956894\tkl:   25.533760\n",
      "Epoch: 679 [30100/50000 (60%)]  \tLoss:   92.472908\trec:   66.827423\tkl:   25.645481\n",
      "Epoch: 679 [40100/50000 (80%)]  \tLoss:   89.850204\trec:   63.615555\tkl:   26.234648\n",
      "====> Epoch: 679 Average train loss: 90.9764\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6957\n",
      "Epoch: 680 [  100/50000 ( 0%)]  \tLoss:   93.124161\trec:   66.565269\tkl:   26.558891\n",
      "Epoch: 680 [10100/50000 (20%)]  \tLoss:   91.442947\trec:   64.803391\tkl:   26.639555\n",
      "Epoch: 680 [20100/50000 (40%)]  \tLoss:   93.082733\trec:   66.691299\tkl:   26.391439\n",
      "Epoch: 680 [30100/50000 (60%)]  \tLoss:   87.336914\trec:   62.293110\tkl:   25.043798\n",
      "Epoch: 680 [40100/50000 (80%)]  \tLoss:   93.125526\trec:   66.719551\tkl:   26.405975\n",
      "====> Epoch: 680 Average train loss: 90.9805\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6906\n",
      "Epoch: 681 [  100/50000 ( 0%)]  \tLoss:   91.581978\trec:   65.086609\tkl:   26.495369\n",
      "Epoch: 681 [10100/50000 (20%)]  \tLoss:   94.521248\trec:   67.965065\tkl:   26.556183\n",
      "Epoch: 681 [20100/50000 (40%)]  \tLoss:   87.407486\trec:   62.234047\tkl:   25.173441\n",
      "Epoch: 681 [30100/50000 (60%)]  \tLoss:   92.037071\trec:   66.352470\tkl:   25.684597\n",
      "Epoch: 681 [40100/50000 (80%)]  \tLoss:   89.066544\trec:   63.670227\tkl:   25.396318\n",
      "====> Epoch: 681 Average train loss: 90.9735\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7319\n",
      "Epoch: 682 [  100/50000 ( 0%)]  \tLoss:   92.056931\trec:   65.196045\tkl:   26.860884\n",
      "Epoch: 682 [10100/50000 (20%)]  \tLoss:   89.267250\trec:   63.232735\tkl:   26.034523\n",
      "Epoch: 682 [20100/50000 (40%)]  \tLoss:   88.986046\trec:   62.454967\tkl:   26.531071\n",
      "Epoch: 682 [30100/50000 (60%)]  \tLoss:   92.635719\trec:   66.093163\tkl:   26.542562\n",
      "Epoch: 682 [40100/50000 (80%)]  \tLoss:   89.564743\trec:   63.352077\tkl:   26.212669\n",
      "====> Epoch: 682 Average train loss: 90.9674\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6220\n",
      "Epoch: 683 [  100/50000 ( 0%)]  \tLoss:   90.250587\trec:   64.461662\tkl:   25.788927\n",
      "Epoch: 683 [10100/50000 (20%)]  \tLoss:   90.361092\trec:   64.214256\tkl:   26.146837\n",
      "Epoch: 683 [20100/50000 (40%)]  \tLoss:   88.470001\trec:   62.860703\tkl:   25.609291\n",
      "Epoch: 683 [30100/50000 (60%)]  \tLoss:   88.041832\trec:   63.030388\tkl:   25.011440\n",
      "Epoch: 683 [40100/50000 (80%)]  \tLoss:   93.016930\trec:   65.651016\tkl:   27.365913\n",
      "====> Epoch: 683 Average train loss: 90.9495\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6185\n",
      "Epoch: 684 [  100/50000 ( 0%)]  \tLoss:   88.719688\trec:   62.335850\tkl:   26.383835\n",
      "Epoch: 684 [10100/50000 (20%)]  \tLoss:   90.261101\trec:   64.518700\tkl:   25.742405\n",
      "Epoch: 684 [20100/50000 (40%)]  \tLoss:   88.198677\trec:   62.504520\tkl:   25.694162\n",
      "Epoch: 684 [30100/50000 (60%)]  \tLoss:   94.765633\trec:   66.979469\tkl:   27.786163\n",
      "Epoch: 684 [40100/50000 (80%)]  \tLoss:   93.064835\trec:   66.428947\tkl:   26.635891\n",
      "====> Epoch: 684 Average train loss: 90.9738\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6700\n",
      "Epoch: 685 [  100/50000 ( 0%)]  \tLoss:   91.709000\trec:   65.390312\tkl:   26.318695\n",
      "Epoch: 685 [10100/50000 (20%)]  \tLoss:   88.218399\trec:   62.517986\tkl:   25.700413\n",
      "Epoch: 685 [20100/50000 (40%)]  \tLoss:   90.002319\trec:   63.405190\tkl:   26.597136\n",
      "Epoch: 685 [30100/50000 (60%)]  \tLoss:   93.544106\trec:   67.356880\tkl:   26.187223\n",
      "Epoch: 685 [40100/50000 (80%)]  \tLoss:   90.715866\trec:   63.838467\tkl:   26.877399\n",
      "====> Epoch: 685 Average train loss: 90.9439\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6980\n",
      "Epoch: 686 [  100/50000 ( 0%)]  \tLoss:   87.514435\trec:   61.635937\tkl:   25.878490\n",
      "Epoch: 686 [10100/50000 (20%)]  \tLoss:   89.659012\trec:   63.059052\tkl:   26.599960\n",
      "Epoch: 686 [20100/50000 (40%)]  \tLoss:   90.369980\trec:   64.296158\tkl:   26.073820\n",
      "Epoch: 686 [30100/50000 (60%)]  \tLoss:   91.810112\trec:   66.120720\tkl:   25.689394\n",
      "Epoch: 686 [40100/50000 (80%)]  \tLoss:   90.602547\trec:   65.075325\tkl:   25.527218\n",
      "====> Epoch: 686 Average train loss: 90.9671\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6055\n",
      "Epoch: 687 [  100/50000 ( 0%)]  \tLoss:   88.726189\trec:   62.547375\tkl:   26.178812\n",
      "Epoch: 687 [10100/50000 (20%)]  \tLoss:   92.297539\trec:   65.625099\tkl:   26.672438\n",
      "Epoch: 687 [20100/50000 (40%)]  \tLoss:   88.966209\trec:   63.066513\tkl:   25.899693\n",
      "Epoch: 687 [30100/50000 (60%)]  \tLoss:   94.900490\trec:   67.934715\tkl:   26.965771\n",
      "Epoch: 687 [40100/50000 (80%)]  \tLoss:   91.810097\trec:   64.946457\tkl:   26.863640\n",
      "====> Epoch: 687 Average train loss: 90.9566\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7178\n",
      "Epoch: 688 [  100/50000 ( 0%)]  \tLoss:   90.388123\trec:   63.887451\tkl:   26.500679\n",
      "Epoch: 688 [10100/50000 (20%)]  \tLoss:   88.355499\trec:   61.620888\tkl:   26.734606\n",
      "Epoch: 688 [20100/50000 (40%)]  \tLoss:   92.482208\trec:   65.930786\tkl:   26.551413\n",
      "Epoch: 688 [30100/50000 (60%)]  \tLoss:   96.618004\trec:   69.801109\tkl:   26.816898\n",
      "Epoch: 688 [40100/50000 (80%)]  \tLoss:   91.504578\trec:   65.278458\tkl:   26.226126\n",
      "====> Epoch: 688 Average train loss: 90.9652\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6197\n",
      "Epoch: 689 [  100/50000 ( 0%)]  \tLoss:   89.858803\trec:   63.762493\tkl:   26.096313\n",
      "Epoch: 689 [10100/50000 (20%)]  \tLoss:   91.868057\trec:   64.982742\tkl:   26.885309\n",
      "Epoch: 689 [20100/50000 (40%)]  \tLoss:   90.965645\trec:   65.660446\tkl:   25.305195\n",
      "Epoch: 689 [30100/50000 (60%)]  \tLoss:   91.691238\trec:   65.668190\tkl:   26.023048\n",
      "Epoch: 689 [40100/50000 (80%)]  \tLoss:   90.984116\trec:   64.628418\tkl:   26.355696\n",
      "====> Epoch: 689 Average train loss: 90.9578\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6959\n",
      "Epoch: 690 [  100/50000 ( 0%)]  \tLoss:   92.725548\trec:   65.798447\tkl:   26.927101\n",
      "Epoch: 690 [10100/50000 (20%)]  \tLoss:   90.970596\trec:   65.302361\tkl:   25.668232\n",
      "Epoch: 690 [20100/50000 (40%)]  \tLoss:   88.776237\trec:   62.212109\tkl:   26.564131\n",
      "Epoch: 690 [30100/50000 (60%)]  \tLoss:   89.411636\trec:   63.946346\tkl:   25.465288\n",
      "Epoch: 690 [40100/50000 (80%)]  \tLoss:   88.134399\trec:   62.798141\tkl:   25.336256\n",
      "====> Epoch: 690 Average train loss: 90.9509\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7487\n",
      "Epoch: 691 [  100/50000 ( 0%)]  \tLoss:   92.900673\trec:   66.870941\tkl:   26.029732\n",
      "Epoch: 691 [10100/50000 (20%)]  \tLoss:   91.050156\trec:   64.723602\tkl:   26.326550\n",
      "Epoch: 691 [20100/50000 (40%)]  \tLoss:   95.344376\trec:   68.491684\tkl:   26.852695\n",
      "Epoch: 691 [30100/50000 (60%)]  \tLoss:   91.058746\trec:   64.777168\tkl:   26.281574\n",
      "Epoch: 691 [40100/50000 (80%)]  \tLoss:   92.393417\trec:   65.981270\tkl:   26.412149\n",
      "====> Epoch: 691 Average train loss: 90.9478\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8148\n",
      "Epoch: 692 [  100/50000 ( 0%)]  \tLoss:   92.489967\trec:   66.100128\tkl:   26.389841\n",
      "Epoch: 692 [10100/50000 (20%)]  \tLoss:   90.635483\trec:   64.584099\tkl:   26.051388\n",
      "Epoch: 692 [20100/50000 (40%)]  \tLoss:   89.069588\trec:   63.723789\tkl:   25.345795\n",
      "Epoch: 692 [30100/50000 (60%)]  \tLoss:   90.347740\trec:   64.112144\tkl:   26.235603\n",
      "Epoch: 692 [40100/50000 (80%)]  \tLoss:   91.486954\trec:   64.694473\tkl:   26.792475\n",
      "====> Epoch: 692 Average train loss: 90.9262\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6411\n",
      "Epoch: 693 [  100/50000 ( 0%)]  \tLoss:   87.009399\trec:   62.089016\tkl:   24.920383\n",
      "Epoch: 693 [10100/50000 (20%)]  \tLoss:   88.234978\trec:   62.838947\tkl:   25.396030\n",
      "Epoch: 693 [20100/50000 (40%)]  \tLoss:   90.235504\trec:   63.958374\tkl:   26.277130\n",
      "Epoch: 693 [30100/50000 (60%)]  \tLoss:   90.427567\trec:   63.937492\tkl:   26.490072\n",
      "Epoch: 693 [40100/50000 (80%)]  \tLoss:   95.367950\trec:   67.754982\tkl:   27.612967\n",
      "====> Epoch: 693 Average train loss: 90.9158\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6942\n",
      "Epoch: 694 [  100/50000 ( 0%)]  \tLoss:   91.858643\trec:   65.344879\tkl:   26.513760\n",
      "Epoch: 694 [10100/50000 (20%)]  \tLoss:   87.484283\trec:   61.989773\tkl:   25.494511\n",
      "Epoch: 694 [20100/50000 (40%)]  \tLoss:   86.676559\trec:   60.855038\tkl:   25.821524\n",
      "Epoch: 694 [30100/50000 (60%)]  \tLoss:   90.669510\trec:   64.825867\tkl:   25.843637\n",
      "Epoch: 694 [40100/50000 (80%)]  \tLoss:   88.675606\trec:   62.703110\tkl:   25.972500\n",
      "====> Epoch: 694 Average train loss: 90.9235\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6885\n",
      "Epoch: 695 [  100/50000 ( 0%)]  \tLoss:   91.587624\trec:   66.016769\tkl:   25.570856\n",
      "Epoch: 695 [10100/50000 (20%)]  \tLoss:   90.893517\trec:   63.986385\tkl:   26.907133\n",
      "Epoch: 695 [20100/50000 (40%)]  \tLoss:   86.573143\trec:   60.755760\tkl:   25.817377\n",
      "Epoch: 695 [30100/50000 (60%)]  \tLoss:   91.111137\trec:   64.825714\tkl:   26.285429\n",
      "Epoch: 695 [40100/50000 (80%)]  \tLoss:   88.451523\trec:   62.834492\tkl:   25.617031\n",
      "====> Epoch: 695 Average train loss: 90.9301\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6889\n",
      "Epoch: 696 [  100/50000 ( 0%)]  \tLoss:   93.896469\trec:   66.887177\tkl:   27.009296\n",
      "Epoch: 696 [10100/50000 (20%)]  \tLoss:   88.259277\trec:   62.126328\tkl:   26.132946\n",
      "Epoch: 696 [20100/50000 (40%)]  \tLoss:   87.403023\trec:   61.673122\tkl:   25.729902\n",
      "Epoch: 696 [30100/50000 (60%)]  \tLoss:   93.702545\trec:   66.810211\tkl:   26.892330\n",
      "Epoch: 696 [40100/50000 (80%)]  \tLoss:   90.433540\trec:   64.384621\tkl:   26.048923\n",
      "====> Epoch: 696 Average train loss: 90.9330\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6809\n",
      "Epoch: 697 [  100/50000 ( 0%)]  \tLoss:   90.843651\trec:   65.470451\tkl:   25.373194\n",
      "Epoch: 697 [10100/50000 (20%)]  \tLoss:   89.454201\trec:   63.164978\tkl:   26.289215\n",
      "Epoch: 697 [20100/50000 (40%)]  \tLoss:   92.652199\trec:   66.152023\tkl:   26.500175\n",
      "Epoch: 697 [30100/50000 (60%)]  \tLoss:   91.246010\trec:   65.288414\tkl:   25.957594\n",
      "Epoch: 697 [40100/50000 (80%)]  \tLoss:   91.979195\trec:   65.804489\tkl:   26.174711\n",
      "====> Epoch: 697 Average train loss: 90.9074\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6453\n",
      "Epoch: 698 [  100/50000 ( 0%)]  \tLoss:   93.856339\trec:   66.999336\tkl:   26.857002\n",
      "Epoch: 698 [10100/50000 (20%)]  \tLoss:   89.725388\trec:   64.539139\tkl:   25.186247\n",
      "Epoch: 698 [20100/50000 (40%)]  \tLoss:   91.435585\trec:   65.144684\tkl:   26.290897\n",
      "Epoch: 698 [30100/50000 (60%)]  \tLoss:   92.841049\trec:   65.379082\tkl:   27.461967\n",
      "Epoch: 698 [40100/50000 (80%)]  \tLoss:   91.299904\trec:   65.364067\tkl:   25.935827\n",
      "====> Epoch: 698 Average train loss: 90.8998\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7774\n",
      "Epoch: 699 [  100/50000 ( 0%)]  \tLoss:   91.573601\trec:   65.437431\tkl:   26.136173\n",
      "Epoch: 699 [10100/50000 (20%)]  \tLoss:   92.517632\trec:   66.145607\tkl:   26.372032\n",
      "Epoch: 699 [20100/50000 (40%)]  \tLoss:   87.826210\trec:   62.221283\tkl:   25.604929\n",
      "Epoch: 699 [30100/50000 (60%)]  \tLoss:   91.215858\trec:   64.829651\tkl:   26.386209\n",
      "Epoch: 699 [40100/50000 (80%)]  \tLoss:   92.149940\trec:   66.161934\tkl:   25.988005\n",
      "====> Epoch: 699 Average train loss: 90.9347\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7125\n",
      "Epoch: 700 [  100/50000 ( 0%)]  \tLoss:   89.327492\trec:   63.136719\tkl:   26.190771\n",
      "Epoch: 700 [10100/50000 (20%)]  \tLoss:   89.585068\trec:   64.387939\tkl:   25.197121\n",
      "Epoch: 700 [20100/50000 (40%)]  \tLoss:   91.057266\trec:   65.212273\tkl:   25.844997\n",
      "Epoch: 700 [30100/50000 (60%)]  \tLoss:   91.533409\trec:   65.390984\tkl:   26.142429\n",
      "Epoch: 700 [40100/50000 (80%)]  \tLoss:   94.559715\trec:   67.902237\tkl:   26.657478\n",
      "====> Epoch: 700 Average train loss: 90.9028\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7613\n",
      "Epoch: 701 [  100/50000 ( 0%)]  \tLoss:   88.745132\trec:   63.064098\tkl:   25.681030\n",
      "Epoch: 701 [10100/50000 (20%)]  \tLoss:   87.666512\trec:   61.412796\tkl:   26.253717\n",
      "Epoch: 701 [20100/50000 (40%)]  \tLoss:   89.554726\trec:   63.133636\tkl:   26.421089\n",
      "Epoch: 701 [30100/50000 (60%)]  \tLoss:   93.232666\trec:   66.688339\tkl:   26.544325\n",
      "Epoch: 701 [40100/50000 (80%)]  \tLoss:   93.841209\trec:   66.370636\tkl:   27.470577\n",
      "====> Epoch: 701 Average train loss: 90.8985\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6364\n",
      "Epoch: 702 [  100/50000 ( 0%)]  \tLoss:   92.381035\trec:   65.772163\tkl:   26.608870\n",
      "Epoch: 702 [10100/50000 (20%)]  \tLoss:   90.596367\trec:   63.842464\tkl:   26.753899\n",
      "Epoch: 702 [20100/50000 (40%)]  \tLoss:   91.176834\trec:   65.584953\tkl:   25.591887\n",
      "Epoch: 702 [30100/50000 (60%)]  \tLoss:   88.874901\trec:   64.005028\tkl:   24.869867\n",
      "Epoch: 702 [40100/50000 (80%)]  \tLoss:   90.443153\trec:   64.492180\tkl:   25.950975\n",
      "====> Epoch: 702 Average train loss: 90.9363\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6633\n",
      "Epoch: 703 [  100/50000 ( 0%)]  \tLoss:   93.417625\trec:   66.179070\tkl:   27.238556\n",
      "Epoch: 703 [10100/50000 (20%)]  \tLoss:   88.327164\trec:   63.190155\tkl:   25.137009\n",
      "Epoch: 703 [20100/50000 (40%)]  \tLoss:   90.038025\trec:   63.789951\tkl:   26.248075\n",
      "Epoch: 703 [30100/50000 (60%)]  \tLoss:   90.846092\trec:   64.524269\tkl:   26.321825\n",
      "Epoch: 703 [40100/50000 (80%)]  \tLoss:   91.044334\trec:   65.320282\tkl:   25.724051\n",
      "====> Epoch: 703 Average train loss: 90.9212\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6071\n",
      "Epoch: 704 [  100/50000 ( 0%)]  \tLoss:   89.885811\trec:   64.000786\tkl:   25.885017\n",
      "Epoch: 704 [10100/50000 (20%)]  \tLoss:   91.668594\trec:   65.837730\tkl:   25.830856\n",
      "Epoch: 704 [20100/50000 (40%)]  \tLoss:   91.609116\trec:   65.444595\tkl:   26.164516\n",
      "Epoch: 704 [30100/50000 (60%)]  \tLoss:   90.452599\trec:   64.472549\tkl:   25.980049\n",
      "Epoch: 704 [40100/50000 (80%)]  \tLoss:   88.326187\trec:   62.770271\tkl:   25.555920\n",
      "====> Epoch: 704 Average train loss: 90.9139\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7014\n",
      "Epoch: 705 [  100/50000 ( 0%)]  \tLoss:   88.717583\trec:   62.551796\tkl:   26.165792\n",
      "Epoch: 705 [10100/50000 (20%)]  \tLoss:   89.945908\trec:   64.034470\tkl:   25.911440\n",
      "Epoch: 705 [20100/50000 (40%)]  \tLoss:   89.206352\trec:   63.497810\tkl:   25.708542\n",
      "Epoch: 705 [30100/50000 (60%)]  \tLoss:   90.430122\trec:   64.424118\tkl:   26.006008\n",
      "Epoch: 705 [40100/50000 (80%)]  \tLoss:   89.251831\trec:   63.913895\tkl:   25.337938\n",
      "====> Epoch: 705 Average train loss: 90.9057\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6334\n",
      "Epoch: 706 [  100/50000 ( 0%)]  \tLoss:   90.497437\trec:   64.611549\tkl:   25.885899\n",
      "Epoch: 706 [10100/50000 (20%)]  \tLoss:   91.589073\trec:   64.681892\tkl:   26.907179\n",
      "Epoch: 706 [20100/50000 (40%)]  \tLoss:   91.718857\trec:   65.532272\tkl:   26.186579\n",
      "Epoch: 706 [30100/50000 (60%)]  \tLoss:   91.922752\trec:   65.579842\tkl:   26.342915\n",
      "Epoch: 706 [40100/50000 (80%)]  \tLoss:   90.190468\trec:   64.469795\tkl:   25.720676\n",
      "====> Epoch: 706 Average train loss: 90.8946\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7037\n",
      "Epoch: 707 [  100/50000 ( 0%)]  \tLoss:   92.989830\trec:   66.647850\tkl:   26.341980\n",
      "Epoch: 707 [10100/50000 (20%)]  \tLoss:   92.255287\trec:   65.712883\tkl:   26.542410\n",
      "Epoch: 707 [20100/50000 (40%)]  \tLoss:   95.918396\trec:   69.328484\tkl:   26.589916\n",
      "Epoch: 707 [30100/50000 (60%)]  \tLoss:   94.974281\trec:   68.417915\tkl:   26.556366\n",
      "Epoch: 707 [40100/50000 (80%)]  \tLoss:   92.827263\trec:   66.526726\tkl:   26.300541\n",
      "====> Epoch: 707 Average train loss: 90.8798\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6563\n",
      "Epoch: 708 [  100/50000 ( 0%)]  \tLoss:   88.750992\trec:   62.486374\tkl:   26.264624\n",
      "Epoch: 708 [10100/50000 (20%)]  \tLoss:   92.989464\trec:   66.541924\tkl:   26.447540\n",
      "Epoch: 708 [20100/50000 (40%)]  \tLoss:   88.025085\trec:   62.244404\tkl:   25.780684\n",
      "Epoch: 708 [30100/50000 (60%)]  \tLoss:   90.561554\trec:   64.564529\tkl:   25.997019\n",
      "Epoch: 708 [40100/50000 (80%)]  \tLoss:   90.136055\trec:   64.832672\tkl:   25.303379\n",
      "====> Epoch: 708 Average train loss: 90.9097\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7793\n",
      "Epoch: 709 [  100/50000 ( 0%)]  \tLoss:   90.220573\trec:   63.741867\tkl:   26.478708\n",
      "Epoch: 709 [10100/50000 (20%)]  \tLoss:   91.559013\trec:   65.800827\tkl:   25.758181\n",
      "Epoch: 709 [20100/50000 (40%)]  \tLoss:   91.369751\trec:   64.671066\tkl:   26.698683\n",
      "Epoch: 709 [30100/50000 (60%)]  \tLoss:   92.338554\trec:   66.011360\tkl:   26.327187\n",
      "Epoch: 709 [40100/50000 (80%)]  \tLoss:   85.917732\trec:   60.900780\tkl:   25.016954\n",
      "====> Epoch: 709 Average train loss: 90.8541\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7652\n",
      "Epoch: 710 [  100/50000 ( 0%)]  \tLoss:   89.272179\trec:   63.872662\tkl:   25.399515\n",
      "Epoch: 710 [10100/50000 (20%)]  \tLoss:   87.739265\trec:   62.237373\tkl:   25.501892\n",
      "Epoch: 710 [20100/50000 (40%)]  \tLoss:   88.275215\trec:   62.076035\tkl:   26.199177\n",
      "Epoch: 710 [30100/50000 (60%)]  \tLoss:   91.535095\trec:   64.930321\tkl:   26.604771\n",
      "Epoch: 710 [40100/50000 (80%)]  \tLoss:   91.039215\trec:   64.763153\tkl:   26.276058\n",
      "====> Epoch: 710 Average train loss: 90.8610\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7961\n",
      "Epoch: 711 [  100/50000 ( 0%)]  \tLoss:   93.253677\trec:   66.029701\tkl:   27.223978\n",
      "Epoch: 711 [10100/50000 (20%)]  \tLoss:   89.026413\trec:   62.979256\tkl:   26.047159\n",
      "Epoch: 711 [20100/50000 (40%)]  \tLoss:   90.752235\trec:   64.672867\tkl:   26.079367\n",
      "Epoch: 711 [30100/50000 (60%)]  \tLoss:   92.529266\trec:   65.408325\tkl:   27.120945\n",
      "Epoch: 711 [40100/50000 (80%)]  \tLoss:   89.780434\trec:   64.024132\tkl:   25.756306\n",
      "====> Epoch: 711 Average train loss: 90.8876\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5879\n",
      "Epoch: 712 [  100/50000 ( 0%)]  \tLoss:   89.148766\trec:   63.361542\tkl:   25.787226\n",
      "Epoch: 712 [10100/50000 (20%)]  \tLoss:   89.631104\trec:   63.142811\tkl:   26.488289\n",
      "Epoch: 712 [20100/50000 (40%)]  \tLoss:   91.043472\trec:   64.403954\tkl:   26.639517\n",
      "Epoch: 712 [30100/50000 (60%)]  \tLoss:   88.402695\trec:   63.162239\tkl:   25.240458\n",
      "Epoch: 712 [40100/50000 (80%)]  \tLoss:   94.081757\trec:   67.436676\tkl:   26.645084\n",
      "====> Epoch: 712 Average train loss: 90.8604\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6472\n",
      "Epoch: 713 [  100/50000 ( 0%)]  \tLoss:   92.377808\trec:   65.393974\tkl:   26.983835\n",
      "Epoch: 713 [10100/50000 (20%)]  \tLoss:   93.260742\trec:   67.196144\tkl:   26.064602\n",
      "Epoch: 713 [20100/50000 (40%)]  \tLoss:   92.897560\trec:   66.247734\tkl:   26.649820\n",
      "Epoch: 713 [30100/50000 (60%)]  \tLoss:   87.587769\trec:   61.647938\tkl:   25.939829\n",
      "Epoch: 713 [40100/50000 (80%)]  \tLoss:   88.251053\trec:   62.206326\tkl:   26.044731\n",
      "====> Epoch: 713 Average train loss: 90.8646\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7095\n",
      "Epoch: 714 [  100/50000 ( 0%)]  \tLoss:   91.041512\trec:   64.941093\tkl:   26.100422\n",
      "Epoch: 714 [10100/50000 (20%)]  \tLoss:   92.439156\trec:   66.448303\tkl:   25.990858\n",
      "Epoch: 714 [20100/50000 (40%)]  \tLoss:   84.302429\trec:   58.877659\tkl:   25.424770\n",
      "Epoch: 714 [30100/50000 (60%)]  \tLoss:   88.973923\trec:   63.316196\tkl:   25.657726\n",
      "Epoch: 714 [40100/50000 (80%)]  \tLoss:   92.554176\trec:   66.556030\tkl:   25.998148\n",
      "====> Epoch: 714 Average train loss: 90.8609\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6263\n",
      "Epoch: 715 [  100/50000 ( 0%)]  \tLoss:   88.817612\trec:   63.034988\tkl:   25.782623\n",
      "Epoch: 715 [10100/50000 (20%)]  \tLoss:   95.361923\trec:   68.246239\tkl:   27.115683\n",
      "Epoch: 715 [20100/50000 (40%)]  \tLoss:   93.943588\trec:   67.274353\tkl:   26.669233\n",
      "Epoch: 715 [30100/50000 (60%)]  \tLoss:   90.561089\trec:   65.076324\tkl:   25.484760\n",
      "Epoch: 715 [40100/50000 (80%)]  \tLoss:   91.941719\trec:   65.391075\tkl:   26.550638\n",
      "====> Epoch: 715 Average train loss: 90.8348\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6916\n",
      "Epoch: 716 [  100/50000 ( 0%)]  \tLoss:   92.429596\trec:   66.852654\tkl:   25.576944\n",
      "Epoch: 716 [10100/50000 (20%)]  \tLoss:   90.593239\trec:   64.748558\tkl:   25.844685\n",
      "Epoch: 716 [20100/50000 (40%)]  \tLoss:   93.678886\trec:   67.269165\tkl:   26.409716\n",
      "Epoch: 716 [30100/50000 (60%)]  \tLoss:   86.101852\trec:   61.244511\tkl:   24.857346\n",
      "Epoch: 716 [40100/50000 (80%)]  \tLoss:   90.080879\trec:   63.978474\tkl:   26.102407\n",
      "====> Epoch: 716 Average train loss: 90.8683\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6212\n",
      "Epoch: 717 [  100/50000 ( 0%)]  \tLoss:   89.422752\trec:   62.938816\tkl:   26.483940\n",
      "Epoch: 717 [10100/50000 (20%)]  \tLoss:   87.862770\trec:   61.553318\tkl:   26.309458\n",
      "Epoch: 717 [20100/50000 (40%)]  \tLoss:   95.594246\trec:   69.856934\tkl:   25.737314\n",
      "Epoch: 717 [30100/50000 (60%)]  \tLoss:   93.308182\trec:   66.793243\tkl:   26.514936\n",
      "Epoch: 717 [40100/50000 (80%)]  \tLoss:   88.324333\trec:   62.191971\tkl:   26.132364\n",
      "====> Epoch: 717 Average train loss: 90.8362\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7404\n",
      "Epoch: 718 [  100/50000 ( 0%)]  \tLoss:   91.134499\trec:   64.759743\tkl:   26.374760\n",
      "Epoch: 718 [10100/50000 (20%)]  \tLoss:   89.527832\trec:   63.266670\tkl:   26.261166\n",
      "Epoch: 718 [20100/50000 (40%)]  \tLoss:   89.643608\trec:   64.143906\tkl:   25.499712\n",
      "Epoch: 718 [30100/50000 (60%)]  \tLoss:   91.848320\trec:   65.295685\tkl:   26.552631\n",
      "Epoch: 718 [40100/50000 (80%)]  \tLoss:   81.208481\trec:   56.822605\tkl:   24.385881\n",
      "====> Epoch: 718 Average train loss: 90.8704\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7622\n",
      "Epoch: 719 [  100/50000 ( 0%)]  \tLoss:   90.242340\trec:   63.890663\tkl:   26.351677\n",
      "Epoch: 719 [10100/50000 (20%)]  \tLoss:   95.512383\trec:   69.334679\tkl:   26.177702\n",
      "Epoch: 719 [20100/50000 (40%)]  \tLoss:   89.941307\trec:   63.915115\tkl:   26.026194\n",
      "Epoch: 719 [30100/50000 (60%)]  \tLoss:   94.080582\trec:   67.467323\tkl:   26.613256\n",
      "Epoch: 719 [40100/50000 (80%)]  \tLoss:   90.264923\trec:   63.583923\tkl:   26.680998\n",
      "====> Epoch: 719 Average train loss: 90.8264\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6900\n",
      "Epoch: 720 [  100/50000 ( 0%)]  \tLoss:   91.269402\trec:   64.863983\tkl:   26.405416\n",
      "Epoch: 720 [10100/50000 (20%)]  \tLoss:   90.309303\trec:   63.481186\tkl:   26.828121\n",
      "Epoch: 720 [20100/50000 (40%)]  \tLoss:   91.488022\trec:   64.926811\tkl:   26.561220\n",
      "Epoch: 720 [30100/50000 (60%)]  \tLoss:   90.217262\trec:   63.731800\tkl:   26.485464\n",
      "Epoch: 720 [40100/50000 (80%)]  \tLoss:   90.637711\trec:   64.934837\tkl:   25.702877\n",
      "====> Epoch: 720 Average train loss: 90.8658\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6500\n",
      "Epoch: 721 [  100/50000 ( 0%)]  \tLoss:   91.537338\trec:   65.483047\tkl:   26.054287\n",
      "Epoch: 721 [10100/50000 (20%)]  \tLoss:   91.330017\trec:   65.139351\tkl:   26.190660\n",
      "Epoch: 721 [20100/50000 (40%)]  \tLoss:   90.823647\trec:   64.817261\tkl:   26.006382\n",
      "Epoch: 721 [30100/50000 (60%)]  \tLoss:   90.419518\trec:   64.971085\tkl:   25.448437\n",
      "Epoch: 721 [40100/50000 (80%)]  \tLoss:   93.820694\trec:   66.462868\tkl:   27.357822\n",
      "====> Epoch: 721 Average train loss: 90.8567\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6209\n",
      "Epoch: 722 [  100/50000 ( 0%)]  \tLoss:   93.228943\trec:   67.158325\tkl:   26.070620\n",
      "Epoch: 722 [10100/50000 (20%)]  \tLoss:   90.120987\trec:   63.412186\tkl:   26.708801\n",
      "Epoch: 722 [20100/50000 (40%)]  \tLoss:   87.823517\trec:   60.892002\tkl:   26.931519\n",
      "Epoch: 722 [30100/50000 (60%)]  \tLoss:   91.254547\trec:   64.856064\tkl:   26.398481\n",
      "Epoch: 722 [40100/50000 (80%)]  \tLoss:   94.784470\trec:   67.730736\tkl:   27.053734\n",
      "====> Epoch: 722 Average train loss: 90.8538\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6412\n",
      "Epoch: 723 [  100/50000 ( 0%)]  \tLoss:   91.732597\trec:   65.045586\tkl:   26.687014\n",
      "Epoch: 723 [10100/50000 (20%)]  \tLoss:   88.699112\trec:   62.894608\tkl:   25.804501\n",
      "Epoch: 723 [20100/50000 (40%)]  \tLoss:   92.131935\trec:   66.209435\tkl:   25.922497\n",
      "Epoch: 723 [30100/50000 (60%)]  \tLoss:   91.975143\trec:   65.614632\tkl:   26.360512\n",
      "Epoch: 723 [40100/50000 (80%)]  \tLoss:   92.700386\trec:   66.221397\tkl:   26.478994\n",
      "====> Epoch: 723 Average train loss: 90.8587\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6292\n",
      "Epoch: 724 [  100/50000 ( 0%)]  \tLoss:   91.523544\trec:   65.475037\tkl:   26.048502\n",
      "Epoch: 724 [10100/50000 (20%)]  \tLoss:   95.812462\trec:   68.607185\tkl:   27.205275\n",
      "Epoch: 724 [20100/50000 (40%)]  \tLoss:   86.574257\trec:   62.053600\tkl:   24.520649\n",
      "Epoch: 724 [30100/50000 (60%)]  \tLoss:   90.918526\trec:   64.863762\tkl:   26.054766\n",
      "Epoch: 724 [40100/50000 (80%)]  \tLoss:   85.522125\trec:   59.524944\tkl:   25.997183\n",
      "====> Epoch: 724 Average train loss: 90.8289\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6381\n",
      "Epoch: 725 [  100/50000 ( 0%)]  \tLoss:   91.544571\trec:   64.784386\tkl:   26.760176\n",
      "Epoch: 725 [10100/50000 (20%)]  \tLoss:   87.836128\trec:   61.737419\tkl:   26.098711\n",
      "Epoch: 725 [20100/50000 (40%)]  \tLoss:   91.700661\trec:   65.863960\tkl:   25.836704\n",
      "Epoch: 725 [30100/50000 (60%)]  \tLoss:   93.662315\trec:   66.615471\tkl:   27.046844\n",
      "Epoch: 725 [40100/50000 (80%)]  \tLoss:   82.462479\trec:   57.600437\tkl:   24.862045\n",
      "====> Epoch: 725 Average train loss: 90.8221\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6605\n",
      "Epoch: 726 [  100/50000 ( 0%)]  \tLoss:   92.668770\trec:   66.337036\tkl:   26.331722\n",
      "Epoch: 726 [10100/50000 (20%)]  \tLoss:   96.799995\trec:   69.317429\tkl:   27.482563\n",
      "Epoch: 726 [20100/50000 (40%)]  \tLoss:   93.231140\trec:   66.842781\tkl:   26.388361\n",
      "Epoch: 726 [30100/50000 (60%)]  \tLoss:   92.747543\trec:   65.510735\tkl:   27.236818\n",
      "Epoch: 726 [40100/50000 (80%)]  \tLoss:   88.036911\trec:   62.721455\tkl:   25.315454\n",
      "====> Epoch: 726 Average train loss: 90.8427\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6760\n",
      "Epoch: 727 [  100/50000 ( 0%)]  \tLoss:   89.383476\trec:   63.213623\tkl:   26.169847\n",
      "Epoch: 727 [10100/50000 (20%)]  \tLoss:   93.864662\trec:   66.839905\tkl:   27.024763\n",
      "Epoch: 727 [20100/50000 (40%)]  \tLoss:   89.492531\trec:   64.505127\tkl:   24.987396\n",
      "Epoch: 727 [30100/50000 (60%)]  \tLoss:   89.626259\trec:   63.865978\tkl:   25.760277\n",
      "Epoch: 727 [40100/50000 (80%)]  \tLoss:   86.784676\trec:   61.328651\tkl:   25.456022\n",
      "====> Epoch: 727 Average train loss: 90.8232\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6778\n",
      "Epoch: 728 [  100/50000 ( 0%)]  \tLoss:   90.040123\trec:   64.176445\tkl:   25.863684\n",
      "Epoch: 728 [10100/50000 (20%)]  \tLoss:   91.041069\trec:   64.639755\tkl:   26.401314\n",
      "Epoch: 728 [20100/50000 (40%)]  \tLoss:   91.087311\trec:   65.059090\tkl:   26.028221\n",
      "Epoch: 728 [30100/50000 (60%)]  \tLoss:   88.669533\trec:   63.435505\tkl:   25.234022\n",
      "Epoch: 728 [40100/50000 (80%)]  \tLoss:   90.547325\trec:   64.903069\tkl:   25.644249\n",
      "====> Epoch: 728 Average train loss: 90.8133\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5425\n",
      "Epoch: 729 [  100/50000 ( 0%)]  \tLoss:   90.975044\trec:   64.574181\tkl:   26.400866\n",
      "Epoch: 729 [10100/50000 (20%)]  \tLoss:   89.718895\trec:   63.643730\tkl:   26.075163\n",
      "Epoch: 729 [20100/50000 (40%)]  \tLoss:   89.932281\trec:   63.995155\tkl:   25.937134\n",
      "Epoch: 729 [30100/50000 (60%)]  \tLoss:   93.928436\trec:   67.090240\tkl:   26.838200\n",
      "Epoch: 729 [40100/50000 (80%)]  \tLoss:   93.607849\trec:   67.026382\tkl:   26.581469\n",
      "====> Epoch: 729 Average train loss: 90.8195\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6730\n",
      "Epoch: 730 [  100/50000 ( 0%)]  \tLoss:   84.526512\trec:   60.301872\tkl:   24.224638\n",
      "Epoch: 730 [10100/50000 (20%)]  \tLoss:   94.884117\trec:   67.098595\tkl:   27.785524\n",
      "Epoch: 730 [20100/50000 (40%)]  \tLoss:   86.647926\trec:   61.176811\tkl:   25.471117\n",
      "Epoch: 730 [30100/50000 (60%)]  \tLoss:   93.367401\trec:   66.094322\tkl:   27.273071\n",
      "Epoch: 730 [40100/50000 (80%)]  \tLoss:   90.058556\trec:   63.675556\tkl:   26.382999\n",
      "====> Epoch: 730 Average train loss: 90.8421\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7101\n",
      "Epoch: 731 [  100/50000 ( 0%)]  \tLoss:   91.368881\trec:   64.394287\tkl:   26.974600\n",
      "Epoch: 731 [10100/50000 (20%)]  \tLoss:   90.299332\trec:   63.543633\tkl:   26.755699\n",
      "Epoch: 731 [20100/50000 (40%)]  \tLoss:   94.263756\trec:   66.584557\tkl:   27.679207\n",
      "Epoch: 731 [30100/50000 (60%)]  \tLoss:   89.793472\trec:   63.766537\tkl:   26.026936\n",
      "Epoch: 731 [40100/50000 (80%)]  \tLoss:   90.568336\trec:   64.320374\tkl:   26.247965\n",
      "====> Epoch: 731 Average train loss: 90.7929\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5970\n",
      "Epoch: 732 [  100/50000 ( 0%)]  \tLoss:   88.257324\trec:   62.849598\tkl:   25.407726\n",
      "Epoch: 732 [10100/50000 (20%)]  \tLoss:   90.310562\trec:   64.758331\tkl:   25.552240\n",
      "Epoch: 732 [20100/50000 (40%)]  \tLoss:   93.823952\trec:   66.751190\tkl:   27.072763\n",
      "Epoch: 732 [30100/50000 (60%)]  \tLoss:   88.416115\trec:   62.591152\tkl:   25.824965\n",
      "Epoch: 732 [40100/50000 (80%)]  \tLoss:   90.938934\trec:   64.365036\tkl:   26.573893\n",
      "====> Epoch: 732 Average train loss: 90.8108\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6677\n",
      "Epoch: 733 [  100/50000 ( 0%)]  \tLoss:   91.061951\trec:   64.910553\tkl:   26.151394\n",
      "Epoch: 733 [10100/50000 (20%)]  \tLoss:   88.099152\trec:   62.023621\tkl:   26.075523\n",
      "Epoch: 733 [20100/50000 (40%)]  \tLoss:   88.547363\trec:   62.664272\tkl:   25.883087\n",
      "Epoch: 733 [30100/50000 (60%)]  \tLoss:   95.653824\trec:   68.517082\tkl:   27.136753\n",
      "Epoch: 733 [40100/50000 (80%)]  \tLoss:   91.616440\trec:   65.254776\tkl:   26.361664\n",
      "====> Epoch: 733 Average train loss: 90.7931\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6023\n",
      "Epoch: 734 [  100/50000 ( 0%)]  \tLoss:   88.730057\trec:   62.521679\tkl:   26.208374\n",
      "Epoch: 734 [10100/50000 (20%)]  \tLoss:   89.769295\trec:   63.671062\tkl:   26.098228\n",
      "Epoch: 734 [20100/50000 (40%)]  \tLoss:   90.375542\trec:   63.964294\tkl:   26.411249\n",
      "Epoch: 734 [30100/50000 (60%)]  \tLoss:   88.071152\trec:   61.838379\tkl:   26.232775\n",
      "Epoch: 734 [40100/50000 (80%)]  \tLoss:   90.525429\trec:   64.521652\tkl:   26.003775\n",
      "====> Epoch: 734 Average train loss: 90.8087\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7575\n",
      "Epoch: 735 [  100/50000 ( 0%)]  \tLoss:   88.751411\trec:   62.572392\tkl:   26.179024\n",
      "Epoch: 735 [10100/50000 (20%)]  \tLoss:   90.686592\trec:   63.717663\tkl:   26.968924\n",
      "Epoch: 735 [20100/50000 (40%)]  \tLoss:   92.252098\trec:   66.086967\tkl:   26.165127\n",
      "Epoch: 735 [30100/50000 (60%)]  \tLoss:   89.695053\trec:   64.490067\tkl:   25.204992\n",
      "Epoch: 735 [40100/50000 (80%)]  \tLoss:   93.228767\trec:   66.970070\tkl:   26.258701\n",
      "====> Epoch: 735 Average train loss: 90.7956\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6955\n",
      "Epoch: 736 [  100/50000 ( 0%)]  \tLoss:   89.941765\trec:   64.207115\tkl:   25.734646\n",
      "Epoch: 736 [10100/50000 (20%)]  \tLoss:   89.552979\trec:   63.374249\tkl:   26.178728\n",
      "Epoch: 736 [20100/50000 (40%)]  \tLoss:   92.333954\trec:   66.072365\tkl:   26.261589\n",
      "Epoch: 736 [30100/50000 (60%)]  \tLoss:   89.455536\trec:   62.502705\tkl:   26.952833\n",
      "Epoch: 736 [40100/50000 (80%)]  \tLoss:   86.511307\trec:   61.708660\tkl:   24.802650\n",
      "====> Epoch: 736 Average train loss: 90.8057\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6453\n",
      "Epoch: 737 [  100/50000 ( 0%)]  \tLoss:   91.188934\trec:   65.130951\tkl:   26.057980\n",
      "Epoch: 737 [10100/50000 (20%)]  \tLoss:   84.452515\trec:   59.627937\tkl:   24.824585\n",
      "Epoch: 737 [20100/50000 (40%)]  \tLoss:   94.074722\trec:   67.262444\tkl:   26.812290\n",
      "Epoch: 737 [30100/50000 (60%)]  \tLoss:   89.374863\trec:   63.812840\tkl:   25.562016\n",
      "Epoch: 737 [40100/50000 (80%)]  \tLoss:   89.416718\trec:   63.174995\tkl:   26.241726\n",
      "====> Epoch: 737 Average train loss: 90.7995\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6322\n",
      "Epoch: 738 [  100/50000 ( 0%)]  \tLoss:   89.275253\trec:   63.370312\tkl:   25.904942\n",
      "Epoch: 738 [10100/50000 (20%)]  \tLoss:   95.547302\trec:   68.677666\tkl:   26.869640\n",
      "Epoch: 738 [20100/50000 (40%)]  \tLoss:   91.396759\trec:   65.249832\tkl:   26.146925\n",
      "Epoch: 738 [30100/50000 (60%)]  \tLoss:   91.149918\trec:   64.671562\tkl:   26.478359\n",
      "Epoch: 738 [40100/50000 (80%)]  \tLoss:   88.997574\trec:   62.786579\tkl:   26.210993\n",
      "====> Epoch: 738 Average train loss: 90.7877\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6662\n",
      "Epoch: 739 [  100/50000 ( 0%)]  \tLoss:   84.622284\trec:   58.902111\tkl:   25.720171\n",
      "Epoch: 739 [10100/50000 (20%)]  \tLoss:   90.455704\trec:   64.043472\tkl:   26.412224\n",
      "Epoch: 739 [20100/50000 (40%)]  \tLoss:   88.741402\trec:   62.932274\tkl:   25.809135\n",
      "Epoch: 739 [30100/50000 (60%)]  \tLoss:   93.811226\trec:   66.824181\tkl:   26.987047\n",
      "Epoch: 739 [40100/50000 (80%)]  \tLoss:   94.014320\trec:   66.486450\tkl:   27.527878\n",
      "====> Epoch: 739 Average train loss: 90.7946\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5828\n",
      "Epoch: 740 [  100/50000 ( 0%)]  \tLoss:   90.685898\trec:   64.608742\tkl:   26.077156\n",
      "Epoch: 740 [10100/50000 (20%)]  \tLoss:   90.062851\trec:   64.867386\tkl:   25.195469\n",
      "Epoch: 740 [20100/50000 (40%)]  \tLoss:   89.190720\trec:   62.811062\tkl:   26.379662\n",
      "Epoch: 740 [30100/50000 (60%)]  \tLoss:   91.863960\trec:   65.026237\tkl:   26.837729\n",
      "Epoch: 740 [40100/50000 (80%)]  \tLoss:   90.930077\trec:   64.812302\tkl:   26.117775\n",
      "====> Epoch: 740 Average train loss: 90.7832\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5756\n",
      "Epoch: 741 [  100/50000 ( 0%)]  \tLoss:   90.186882\trec:   64.741455\tkl:   25.445431\n",
      "Epoch: 741 [10100/50000 (20%)]  \tLoss:   87.640450\trec:   62.005047\tkl:   25.635403\n",
      "Epoch: 741 [20100/50000 (40%)]  \tLoss:   88.834549\trec:   61.953442\tkl:   26.881107\n",
      "Epoch: 741 [30100/50000 (60%)]  \tLoss:   90.717514\trec:   64.858032\tkl:   25.859484\n",
      "Epoch: 741 [40100/50000 (80%)]  \tLoss:   91.352798\trec:   65.569298\tkl:   25.783508\n",
      "====> Epoch: 741 Average train loss: 90.8142\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5660\n",
      "Epoch: 742 [  100/50000 ( 0%)]  \tLoss:   92.196548\trec:   65.881622\tkl:   26.314932\n",
      "Epoch: 742 [10100/50000 (20%)]  \tLoss:   94.268517\trec:   67.455307\tkl:   26.813208\n",
      "Epoch: 742 [20100/50000 (40%)]  \tLoss:   84.315933\trec:   58.789883\tkl:   25.526058\n",
      "Epoch: 742 [30100/50000 (60%)]  \tLoss:   92.823730\trec:   65.644745\tkl:   27.178986\n",
      "Epoch: 742 [40100/50000 (80%)]  \tLoss:   90.655792\trec:   64.456688\tkl:   26.199100\n",
      "====> Epoch: 742 Average train loss: 90.7845\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6857\n",
      "Epoch: 743 [  100/50000 ( 0%)]  \tLoss:   90.664360\trec:   64.275192\tkl:   26.389168\n",
      "Epoch: 743 [10100/50000 (20%)]  \tLoss:   91.528778\trec:   65.811974\tkl:   25.716801\n",
      "Epoch: 743 [20100/50000 (40%)]  \tLoss:   90.742416\trec:   64.233910\tkl:   26.508505\n",
      "Epoch: 743 [30100/50000 (60%)]  \tLoss:   93.828804\trec:   67.052986\tkl:   26.775814\n",
      "Epoch: 743 [40100/50000 (80%)]  \tLoss:   90.497574\trec:   64.523193\tkl:   25.974379\n",
      "====> Epoch: 743 Average train loss: 90.7890\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6799\n",
      "Epoch: 744 [  100/50000 ( 0%)]  \tLoss:   90.717072\trec:   64.761734\tkl:   25.955336\n",
      "Epoch: 744 [10100/50000 (20%)]  \tLoss:   91.097672\trec:   64.580338\tkl:   26.517336\n",
      "Epoch: 744 [20100/50000 (40%)]  \tLoss:   91.454453\trec:   64.883575\tkl:   26.570879\n",
      "Epoch: 744 [30100/50000 (60%)]  \tLoss:   92.639839\trec:   65.783043\tkl:   26.856791\n",
      "Epoch: 744 [40100/50000 (80%)]  \tLoss:   90.185272\trec:   63.771729\tkl:   26.413540\n",
      "====> Epoch: 744 Average train loss: 90.7864\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6115\n",
      "Epoch: 745 [  100/50000 ( 0%)]  \tLoss:   92.507195\trec:   65.350456\tkl:   27.156740\n",
      "Epoch: 745 [10100/50000 (20%)]  \tLoss:   88.089081\trec:   62.362709\tkl:   25.726366\n",
      "Epoch: 745 [20100/50000 (40%)]  \tLoss:   88.441483\trec:   62.626942\tkl:   25.814535\n",
      "Epoch: 745 [30100/50000 (60%)]  \tLoss:   90.863113\trec:   63.266640\tkl:   27.596476\n",
      "Epoch: 745 [40100/50000 (80%)]  \tLoss:   89.301544\trec:   63.656567\tkl:   25.644972\n",
      "====> Epoch: 745 Average train loss: 90.7878\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6620\n",
      "Epoch: 746 [  100/50000 ( 0%)]  \tLoss:   89.703163\trec:   63.862698\tkl:   25.840467\n",
      "Epoch: 746 [10100/50000 (20%)]  \tLoss:   91.034431\trec:   65.044334\tkl:   25.990095\n",
      "Epoch: 746 [20100/50000 (40%)]  \tLoss:   89.008263\trec:   63.768845\tkl:   25.239414\n",
      "Epoch: 746 [30100/50000 (60%)]  \tLoss:   93.731422\trec:   66.296425\tkl:   27.435005\n",
      "Epoch: 746 [40100/50000 (80%)]  \tLoss:   88.753593\trec:   62.775742\tkl:   25.977852\n",
      "====> Epoch: 746 Average train loss: 90.7731\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6479\n",
      "Epoch: 747 [  100/50000 ( 0%)]  \tLoss:   89.971893\trec:   63.506592\tkl:   26.465302\n",
      "Epoch: 747 [10100/50000 (20%)]  \tLoss:   92.208633\trec:   65.186317\tkl:   27.022316\n",
      "Epoch: 747 [20100/50000 (40%)]  \tLoss:   92.644028\trec:   65.663322\tkl:   26.980709\n",
      "Epoch: 747 [30100/50000 (60%)]  \tLoss:   88.813332\trec:   62.856819\tkl:   25.956509\n",
      "Epoch: 747 [40100/50000 (80%)]  \tLoss:   92.739975\trec:   66.703865\tkl:   26.036116\n",
      "====> Epoch: 747 Average train loss: 90.7691\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5959\n",
      "Epoch: 748 [  100/50000 ( 0%)]  \tLoss:   87.883141\trec:   61.898510\tkl:   25.984632\n",
      "Epoch: 748 [10100/50000 (20%)]  \tLoss:   93.704140\trec:   66.586021\tkl:   27.118120\n",
      "Epoch: 748 [20100/50000 (40%)]  \tLoss:   87.729416\trec:   61.765110\tkl:   25.964302\n",
      "Epoch: 748 [30100/50000 (60%)]  \tLoss:   89.905838\trec:   64.703552\tkl:   25.202288\n",
      "Epoch: 748 [40100/50000 (80%)]  \tLoss:   94.743393\trec:   66.872368\tkl:   27.871029\n",
      "====> Epoch: 748 Average train loss: 90.7500\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6574\n",
      "Epoch: 749 [  100/50000 ( 0%)]  \tLoss:   91.220528\trec:   64.457130\tkl:   26.763401\n",
      "Epoch: 749 [10100/50000 (20%)]  \tLoss:   90.261063\trec:   64.892059\tkl:   25.369005\n",
      "Epoch: 749 [20100/50000 (40%)]  \tLoss:   92.227325\trec:   66.062088\tkl:   26.165232\n",
      "Epoch: 749 [30100/50000 (60%)]  \tLoss:   91.332321\trec:   65.089462\tkl:   26.242867\n",
      "Epoch: 749 [40100/50000 (80%)]  \tLoss:   87.558693\trec:   61.573357\tkl:   25.985334\n",
      "====> Epoch: 749 Average train loss: 90.7380\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5997\n",
      "Epoch: 750 [  100/50000 ( 0%)]  \tLoss:   95.388771\trec:   68.363869\tkl:   27.024904\n",
      "Epoch: 750 [10100/50000 (20%)]  \tLoss:   85.046944\trec:   60.000107\tkl:   25.046835\n",
      "Epoch: 750 [20100/50000 (40%)]  \tLoss:   91.060532\trec:   65.179405\tkl:   25.881126\n",
      "Epoch: 750 [30100/50000 (60%)]  \tLoss:   88.431206\trec:   62.729218\tkl:   25.701994\n",
      "Epoch: 750 [40100/50000 (80%)]  \tLoss:   89.160416\trec:   63.481483\tkl:   25.678934\n",
      "====> Epoch: 750 Average train loss: 90.7579\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6508\n",
      "Epoch: 751 [  100/50000 ( 0%)]  \tLoss:   89.504753\trec:   64.260506\tkl:   25.244246\n",
      "Epoch: 751 [10100/50000 (20%)]  \tLoss:   92.338844\trec:   66.266823\tkl:   26.072021\n",
      "Epoch: 751 [20100/50000 (40%)]  \tLoss:   88.715385\trec:   63.494164\tkl:   25.221228\n",
      "Epoch: 751 [30100/50000 (60%)]  \tLoss:   89.946739\trec:   63.588173\tkl:   26.358566\n",
      "Epoch: 751 [40100/50000 (80%)]  \tLoss:   88.896988\trec:   62.308167\tkl:   26.588823\n",
      "====> Epoch: 751 Average train loss: 90.7448\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7401\n",
      "Epoch: 752 [  100/50000 ( 0%)]  \tLoss:   91.584267\trec:   64.585114\tkl:   26.999149\n",
      "Epoch: 752 [10100/50000 (20%)]  \tLoss:   89.410858\trec:   62.367031\tkl:   27.043827\n",
      "Epoch: 752 [20100/50000 (40%)]  \tLoss:   92.442459\trec:   65.510559\tkl:   26.931894\n",
      "Epoch: 752 [30100/50000 (60%)]  \tLoss:   88.124176\trec:   62.128773\tkl:   25.995399\n",
      "Epoch: 752 [40100/50000 (80%)]  \tLoss:   88.711365\trec:   62.914539\tkl:   25.796827\n",
      "====> Epoch: 752 Average train loss: 90.7558\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6707\n",
      "Epoch: 753 [  100/50000 ( 0%)]  \tLoss:   91.330856\trec:   64.451447\tkl:   26.879412\n",
      "Epoch: 753 [10100/50000 (20%)]  \tLoss:   91.027916\trec:   65.004936\tkl:   26.022980\n",
      "Epoch: 753 [20100/50000 (40%)]  \tLoss:   89.382233\trec:   63.586502\tkl:   25.795729\n",
      "Epoch: 753 [30100/50000 (60%)]  \tLoss:   94.830933\trec:   68.515305\tkl:   26.315634\n",
      "Epoch: 753 [40100/50000 (80%)]  \tLoss:   93.018326\trec:   66.121849\tkl:   26.896479\n",
      "====> Epoch: 753 Average train loss: 90.7489\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6637\n",
      "Epoch: 754 [  100/50000 ( 0%)]  \tLoss:   87.643631\trec:   62.134827\tkl:   25.508801\n",
      "Epoch: 754 [10100/50000 (20%)]  \tLoss:   90.244102\trec:   64.948624\tkl:   25.295475\n",
      "Epoch: 754 [20100/50000 (40%)]  \tLoss:   89.165359\trec:   63.276405\tkl:   25.888952\n",
      "Epoch: 754 [30100/50000 (60%)]  \tLoss:   90.464722\trec:   63.847172\tkl:   26.617554\n",
      "Epoch: 754 [40100/50000 (80%)]  \tLoss:   87.877266\trec:   61.864529\tkl:   26.012739\n",
      "====> Epoch: 754 Average train loss: 90.7456\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6405\n",
      "Epoch: 755 [  100/50000 ( 0%)]  \tLoss:   87.252670\trec:   61.832287\tkl:   25.420387\n",
      "Epoch: 755 [10100/50000 (20%)]  \tLoss:   89.989792\trec:   63.738708\tkl:   26.251081\n",
      "Epoch: 755 [20100/50000 (40%)]  \tLoss:   93.765236\trec:   66.489601\tkl:   27.275639\n",
      "Epoch: 755 [30100/50000 (60%)]  \tLoss:   89.194786\trec:   62.648827\tkl:   26.545952\n",
      "Epoch: 755 [40100/50000 (80%)]  \tLoss:   88.594254\trec:   62.073418\tkl:   26.520840\n",
      "====> Epoch: 755 Average train loss: 90.7273\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6328\n",
      "Epoch: 756 [  100/50000 ( 0%)]  \tLoss:   90.223778\trec:   64.464066\tkl:   25.759712\n",
      "Epoch: 756 [10100/50000 (20%)]  \tLoss:   94.773338\trec:   67.066154\tkl:   27.707191\n",
      "Epoch: 756 [20100/50000 (40%)]  \tLoss:   93.295113\trec:   67.157242\tkl:   26.137875\n",
      "Epoch: 756 [30100/50000 (60%)]  \tLoss:   91.379410\trec:   65.031837\tkl:   26.347582\n",
      "Epoch: 756 [40100/50000 (80%)]  \tLoss:   96.230347\trec:   69.739845\tkl:   26.490513\n",
      "====> Epoch: 756 Average train loss: 90.7355\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6796\n",
      "Epoch: 757 [  100/50000 ( 0%)]  \tLoss:   91.199471\trec:   64.936974\tkl:   26.262505\n",
      "Epoch: 757 [10100/50000 (20%)]  \tLoss:   85.426231\trec:   60.126396\tkl:   25.299833\n",
      "Epoch: 757 [20100/50000 (40%)]  \tLoss:   88.464058\trec:   62.702431\tkl:   25.761625\n",
      "Epoch: 757 [30100/50000 (60%)]  \tLoss:   91.119011\trec:   64.523788\tkl:   26.595224\n",
      "Epoch: 757 [40100/50000 (80%)]  \tLoss:   91.134621\trec:   64.864441\tkl:   26.270172\n",
      "====> Epoch: 757 Average train loss: 90.7355\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6631\n",
      "Epoch: 758 [  100/50000 ( 0%)]  \tLoss:   87.577393\trec:   61.968140\tkl:   25.609253\n",
      "Epoch: 758 [10100/50000 (20%)]  \tLoss:   91.003906\trec:   64.225525\tkl:   26.778381\n",
      "Epoch: 758 [20100/50000 (40%)]  \tLoss:   90.449860\trec:   64.276588\tkl:   26.173275\n",
      "Epoch: 758 [30100/50000 (60%)]  \tLoss:   95.225655\trec:   68.757271\tkl:   26.468386\n",
      "Epoch: 758 [40100/50000 (80%)]  \tLoss:   91.486832\trec:   65.457733\tkl:   26.029104\n",
      "====> Epoch: 758 Average train loss: 90.7384\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6254\n",
      "Epoch: 759 [  100/50000 ( 0%)]  \tLoss:   89.462029\trec:   63.342537\tkl:   26.119493\n",
      "Epoch: 759 [10100/50000 (20%)]  \tLoss:   91.617203\trec:   65.205032\tkl:   26.412167\n",
      "Epoch: 759 [20100/50000 (40%)]  \tLoss:   89.127457\trec:   62.957958\tkl:   26.169506\n",
      "Epoch: 759 [30100/50000 (60%)]  \tLoss:   93.448776\trec:   66.699013\tkl:   26.749767\n",
      "Epoch: 759 [40100/50000 (80%)]  \tLoss:   88.164986\trec:   62.567245\tkl:   25.597742\n",
      "====> Epoch: 759 Average train loss: 90.7314\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5729\n",
      "Epoch: 760 [  100/50000 ( 0%)]  \tLoss:   90.888748\trec:   64.457146\tkl:   26.431606\n",
      "Epoch: 760 [10100/50000 (20%)]  \tLoss:   84.688446\trec:   58.769485\tkl:   25.918962\n",
      "Epoch: 760 [20100/50000 (40%)]  \tLoss:   91.060486\trec:   64.264900\tkl:   26.795584\n",
      "Epoch: 760 [30100/50000 (60%)]  \tLoss:   91.941635\trec:   64.983551\tkl:   26.958090\n",
      "Epoch: 760 [40100/50000 (80%)]  \tLoss:   91.712410\trec:   65.408218\tkl:   26.304190\n",
      "====> Epoch: 760 Average train loss: 90.7281\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5807\n",
      "Epoch: 761 [  100/50000 ( 0%)]  \tLoss:   92.531929\trec:   66.548576\tkl:   25.983358\n",
      "Epoch: 761 [10100/50000 (20%)]  \tLoss:   90.969902\trec:   64.946198\tkl:   26.023703\n",
      "Epoch: 761 [20100/50000 (40%)]  \tLoss:   93.931923\trec:   67.557236\tkl:   26.374687\n",
      "Epoch: 761 [30100/50000 (60%)]  \tLoss:   87.345192\trec:   61.915634\tkl:   25.429562\n",
      "Epoch: 761 [40100/50000 (80%)]  \tLoss:   92.459198\trec:   66.259193\tkl:   26.200003\n",
      "====> Epoch: 761 Average train loss: 90.6929\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6137\n",
      "Epoch: 762 [  100/50000 ( 0%)]  \tLoss:   90.039383\trec:   64.559624\tkl:   25.479753\n",
      "Epoch: 762 [10100/50000 (20%)]  \tLoss:   93.976196\trec:   68.174973\tkl:   25.801229\n",
      "Epoch: 762 [20100/50000 (40%)]  \tLoss:   91.109283\trec:   64.942436\tkl:   26.166843\n",
      "Epoch: 762 [30100/50000 (60%)]  \tLoss:   90.422195\trec:   64.893257\tkl:   25.528933\n",
      "Epoch: 762 [40100/50000 (80%)]  \tLoss:   93.967148\trec:   67.035187\tkl:   26.931963\n",
      "====> Epoch: 762 Average train loss: 90.7202\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6462\n",
      "Epoch: 763 [  100/50000 ( 0%)]  \tLoss:   91.328865\trec:   63.942825\tkl:   27.386044\n",
      "Epoch: 763 [10100/50000 (20%)]  \tLoss:   92.879204\trec:   65.843819\tkl:   27.035387\n",
      "Epoch: 763 [20100/50000 (40%)]  \tLoss:   88.196548\trec:   61.796581\tkl:   26.399973\n",
      "Epoch: 763 [30100/50000 (60%)]  \tLoss:   93.470718\trec:   66.568794\tkl:   26.901928\n",
      "Epoch: 763 [40100/50000 (80%)]  \tLoss:   94.718063\trec:   67.101631\tkl:   27.616430\n",
      "====> Epoch: 763 Average train loss: 90.7079\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6323\n",
      "Epoch: 764 [  100/50000 ( 0%)]  \tLoss:   89.795235\trec:   64.070595\tkl:   25.724644\n",
      "Epoch: 764 [10100/50000 (20%)]  \tLoss:   91.336983\trec:   65.367577\tkl:   25.969404\n",
      "Epoch: 764 [20100/50000 (40%)]  \tLoss:   85.840866\trec:   60.507381\tkl:   25.333488\n",
      "Epoch: 764 [30100/50000 (60%)]  \tLoss:   91.309029\trec:   64.632378\tkl:   26.676657\n",
      "Epoch: 764 [40100/50000 (80%)]  \tLoss:   89.526718\trec:   64.072021\tkl:   25.454699\n",
      "====> Epoch: 764 Average train loss: 90.7385\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6911\n",
      "Epoch: 765 [  100/50000 ( 0%)]  \tLoss:   91.919792\trec:   65.657524\tkl:   26.262270\n",
      "Epoch: 765 [10100/50000 (20%)]  \tLoss:   93.791100\trec:   65.867088\tkl:   27.924015\n",
      "Epoch: 765 [20100/50000 (40%)]  \tLoss:   91.788765\trec:   65.435898\tkl:   26.352867\n",
      "Epoch: 765 [30100/50000 (60%)]  \tLoss:   90.998589\trec:   65.232475\tkl:   25.766108\n",
      "Epoch: 765 [40100/50000 (80%)]  \tLoss:   88.139969\trec:   62.353680\tkl:   25.786289\n",
      "====> Epoch: 765 Average train loss: 90.7425\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6350\n",
      "Epoch: 766 [  100/50000 ( 0%)]  \tLoss:   91.356453\trec:   65.202171\tkl:   26.154282\n",
      "Epoch: 766 [10100/50000 (20%)]  \tLoss:   89.970848\trec:   64.260147\tkl:   25.710705\n",
      "Epoch: 766 [20100/50000 (40%)]  \tLoss:   88.449028\trec:   62.756794\tkl:   25.692238\n",
      "Epoch: 766 [30100/50000 (60%)]  \tLoss:   94.665421\trec:   67.840363\tkl:   26.825060\n",
      "Epoch: 766 [40100/50000 (80%)]  \tLoss:   90.761879\trec:   64.335060\tkl:   26.426826\n",
      "====> Epoch: 766 Average train loss: 90.7205\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5755\n",
      "Epoch: 767 [  100/50000 ( 0%)]  \tLoss:   90.748398\trec:   64.329140\tkl:   26.419254\n",
      "Epoch: 767 [10100/50000 (20%)]  \tLoss:   93.612831\trec:   66.040764\tkl:   27.572067\n",
      "Epoch: 767 [20100/50000 (40%)]  \tLoss:   86.413864\trec:   61.344540\tkl:   25.069323\n",
      "Epoch: 767 [30100/50000 (60%)]  \tLoss:   90.954819\trec:   64.022034\tkl:   26.932791\n",
      "Epoch: 767 [40100/50000 (80%)]  \tLoss:   88.925713\trec:   63.020252\tkl:   25.905460\n",
      "====> Epoch: 767 Average train loss: 90.7244\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6708\n",
      "Epoch: 768 [  100/50000 ( 0%)]  \tLoss:   90.284409\trec:   64.047600\tkl:   26.236820\n",
      "Epoch: 768 [10100/50000 (20%)]  \tLoss:   89.212479\trec:   63.151421\tkl:   26.061054\n",
      "Epoch: 768 [20100/50000 (40%)]  \tLoss:   89.501991\trec:   63.163944\tkl:   26.338047\n",
      "Epoch: 768 [30100/50000 (60%)]  \tLoss:   87.218163\trec:   61.689480\tkl:   25.528687\n",
      "Epoch: 768 [40100/50000 (80%)]  \tLoss:   90.303062\trec:   64.760674\tkl:   25.542387\n",
      "====> Epoch: 768 Average train loss: 90.7162\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6536\n",
      "Epoch: 769 [  100/50000 ( 0%)]  \tLoss:   91.390816\trec:   64.660439\tkl:   26.730377\n",
      "Epoch: 769 [10100/50000 (20%)]  \tLoss:   91.654930\trec:   65.127274\tkl:   26.527658\n",
      "Epoch: 769 [20100/50000 (40%)]  \tLoss:   88.818291\trec:   63.104511\tkl:   25.713779\n",
      "Epoch: 769 [30100/50000 (60%)]  \tLoss:   94.135391\trec:   68.410995\tkl:   25.724396\n",
      "Epoch: 769 [40100/50000 (80%)]  \tLoss:   90.328453\trec:   63.997948\tkl:   26.330507\n",
      "====> Epoch: 769 Average train loss: 90.7184\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6991\n",
      "Epoch: 770 [  100/50000 ( 0%)]  \tLoss:   93.140732\trec:   66.362038\tkl:   26.778696\n",
      "Epoch: 770 [10100/50000 (20%)]  \tLoss:   92.706192\trec:   65.988594\tkl:   26.717592\n",
      "Epoch: 770 [20100/50000 (40%)]  \tLoss:   87.987946\trec:   61.131760\tkl:   26.856184\n",
      "Epoch: 770 [30100/50000 (60%)]  \tLoss:   87.775681\trec:   62.069004\tkl:   25.706678\n",
      "Epoch: 770 [40100/50000 (80%)]  \tLoss:   91.985146\trec:   64.954979\tkl:   27.030169\n",
      "====> Epoch: 770 Average train loss: 90.6888\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5277\n",
      "Epoch: 771 [  100/50000 ( 0%)]  \tLoss:   89.017838\trec:   63.482742\tkl:   25.535095\n",
      "Epoch: 771 [10100/50000 (20%)]  \tLoss:   87.992470\trec:   61.414692\tkl:   26.577780\n",
      "Epoch: 771 [20100/50000 (40%)]  \tLoss:   93.542252\trec:   67.497391\tkl:   26.044863\n",
      "Epoch: 771 [30100/50000 (60%)]  \tLoss:   85.981461\trec:   60.397217\tkl:   25.584246\n",
      "Epoch: 771 [40100/50000 (80%)]  \tLoss:   88.382927\trec:   62.190144\tkl:   26.192778\n",
      "====> Epoch: 771 Average train loss: 90.6948\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5558\n",
      "Epoch: 772 [  100/50000 ( 0%)]  \tLoss:   91.105614\trec:   64.849953\tkl:   26.255661\n",
      "Epoch: 772 [10100/50000 (20%)]  \tLoss:   90.710487\trec:   64.829933\tkl:   25.880556\n",
      "Epoch: 772 [20100/50000 (40%)]  \tLoss:   89.727127\trec:   63.445805\tkl:   26.281322\n",
      "Epoch: 772 [30100/50000 (60%)]  \tLoss:   93.073540\trec:   66.156151\tkl:   26.917395\n",
      "Epoch: 772 [40100/50000 (80%)]  \tLoss:   92.292030\trec:   66.140724\tkl:   26.151310\n",
      "====> Epoch: 772 Average train loss: 90.7061\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6197\n",
      "Epoch: 773 [  100/50000 ( 0%)]  \tLoss:   93.918175\trec:   67.233055\tkl:   26.685114\n",
      "Epoch: 773 [10100/50000 (20%)]  \tLoss:   89.677650\trec:   63.283855\tkl:   26.393799\n",
      "Epoch: 773 [20100/50000 (40%)]  \tLoss:   91.569450\trec:   65.171989\tkl:   26.397470\n",
      "Epoch: 773 [30100/50000 (60%)]  \tLoss:   90.299629\trec:   64.491486\tkl:   25.808140\n",
      "Epoch: 773 [40100/50000 (80%)]  \tLoss:   91.057915\trec:   64.746780\tkl:   26.311136\n",
      "====> Epoch: 773 Average train loss: 90.6637\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5782\n",
      "Epoch: 774 [  100/50000 ( 0%)]  \tLoss:   89.439308\trec:   63.285126\tkl:   26.154182\n",
      "Epoch: 774 [10100/50000 (20%)]  \tLoss:   91.802048\trec:   65.406044\tkl:   26.396009\n",
      "Epoch: 774 [20100/50000 (40%)]  \tLoss:   87.952995\trec:   62.386196\tkl:   25.566803\n",
      "Epoch: 774 [30100/50000 (60%)]  \tLoss:   91.657471\trec:   65.505165\tkl:   26.152306\n",
      "Epoch: 774 [40100/50000 (80%)]  \tLoss:   89.319122\trec:   63.599567\tkl:   25.719545\n",
      "====> Epoch: 774 Average train loss: 90.6939\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6388\n",
      "Epoch: 775 [  100/50000 ( 0%)]  \tLoss:   91.510742\trec:   64.610893\tkl:   26.899845\n",
      "Epoch: 775 [10100/50000 (20%)]  \tLoss:   87.421707\trec:   61.313801\tkl:   26.107904\n",
      "Epoch: 775 [20100/50000 (40%)]  \tLoss:   91.506470\trec:   64.575989\tkl:   26.930483\n",
      "Epoch: 775 [30100/50000 (60%)]  \tLoss:   90.197891\trec:   63.491283\tkl:   26.706612\n",
      "Epoch: 775 [40100/50000 (80%)]  \tLoss:   91.288918\trec:   64.336357\tkl:   26.952555\n",
      "====> Epoch: 775 Average train loss: 90.6946\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6155\n",
      "Epoch: 776 [  100/50000 ( 0%)]  \tLoss:   93.702019\trec:   67.381584\tkl:   26.320435\n",
      "Epoch: 776 [10100/50000 (20%)]  \tLoss:   87.240349\trec:   62.204472\tkl:   25.035873\n",
      "Epoch: 776 [20100/50000 (40%)]  \tLoss:   88.508492\trec:   61.732830\tkl:   26.775663\n",
      "Epoch: 776 [30100/50000 (60%)]  \tLoss:   93.517197\trec:   66.832069\tkl:   26.685123\n",
      "Epoch: 776 [40100/50000 (80%)]  \tLoss:   90.219849\trec:   63.923080\tkl:   26.296774\n",
      "====> Epoch: 776 Average train loss: 90.6827\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6542\n",
      "Epoch: 777 [  100/50000 ( 0%)]  \tLoss:   97.916412\trec:   70.625282\tkl:   27.291134\n",
      "Epoch: 777 [10100/50000 (20%)]  \tLoss:   91.403114\trec:   64.701935\tkl:   26.701183\n",
      "Epoch: 777 [20100/50000 (40%)]  \tLoss:   93.581841\trec:   66.909149\tkl:   26.672691\n",
      "Epoch: 777 [30100/50000 (60%)]  \tLoss:   89.769852\trec:   63.359364\tkl:   26.410484\n",
      "Epoch: 777 [40100/50000 (80%)]  \tLoss:   92.959137\trec:   66.249802\tkl:   26.709330\n",
      "====> Epoch: 777 Average train loss: 90.6453\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6805\n",
      "Epoch: 778 [  100/50000 ( 0%)]  \tLoss:   85.367729\trec:   60.247967\tkl:   25.119770\n",
      "Epoch: 778 [10100/50000 (20%)]  \tLoss:   94.994194\trec:   67.669983\tkl:   27.324219\n",
      "Epoch: 778 [20100/50000 (40%)]  \tLoss:   92.194702\trec:   65.667656\tkl:   26.527056\n",
      "Epoch: 778 [30100/50000 (60%)]  \tLoss:   91.411362\trec:   65.299210\tkl:   26.112152\n",
      "Epoch: 778 [40100/50000 (80%)]  \tLoss:   94.503456\trec:   67.986572\tkl:   26.516884\n",
      "====> Epoch: 778 Average train loss: 90.6801\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6435\n",
      "Epoch: 779 [  100/50000 ( 0%)]  \tLoss:   89.721695\trec:   63.499271\tkl:   26.222425\n",
      "Epoch: 779 [10100/50000 (20%)]  \tLoss:   91.676872\trec:   65.165215\tkl:   26.511660\n",
      "Epoch: 779 [20100/50000 (40%)]  \tLoss:   93.306259\trec:   66.353683\tkl:   26.952578\n",
      "Epoch: 779 [30100/50000 (60%)]  \tLoss:   92.157478\trec:   65.532921\tkl:   26.624557\n",
      "Epoch: 779 [40100/50000 (80%)]  \tLoss:   90.446205\trec:   64.577301\tkl:   25.868914\n",
      "====> Epoch: 779 Average train loss: 90.6603\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6538\n",
      "Epoch: 780 [  100/50000 ( 0%)]  \tLoss:   87.032578\trec:   61.337269\tkl:   25.695305\n",
      "Epoch: 780 [10100/50000 (20%)]  \tLoss:   91.589272\trec:   64.987320\tkl:   26.601957\n",
      "Epoch: 780 [20100/50000 (40%)]  \tLoss:   92.461739\trec:   66.367851\tkl:   26.093885\n",
      "Epoch: 780 [30100/50000 (60%)]  \tLoss:   89.718613\trec:   63.330921\tkl:   26.387690\n",
      "Epoch: 780 [40100/50000 (80%)]  \tLoss:   91.035873\trec:   64.394806\tkl:   26.641075\n",
      "====> Epoch: 780 Average train loss: 90.6759\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6220\n",
      "Epoch: 781 [  100/50000 ( 0%)]  \tLoss:   89.318443\trec:   62.839764\tkl:   26.478678\n",
      "Epoch: 781 [10100/50000 (20%)]  \tLoss:   87.788605\trec:   62.512089\tkl:   25.276516\n",
      "Epoch: 781 [20100/50000 (40%)]  \tLoss:   90.838921\trec:   64.615974\tkl:   26.222946\n",
      "Epoch: 781 [30100/50000 (60%)]  \tLoss:   89.485527\trec:   63.504917\tkl:   25.980608\n",
      "Epoch: 781 [40100/50000 (80%)]  \tLoss:   92.975571\trec:   65.830269\tkl:   27.145304\n",
      "====> Epoch: 781 Average train loss: 90.6368\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6527\n",
      "Epoch: 782 [  100/50000 ( 0%)]  \tLoss:   88.396660\trec:   61.238338\tkl:   27.158319\n",
      "Epoch: 782 [10100/50000 (20%)]  \tLoss:   87.014664\trec:   61.186111\tkl:   25.828554\n",
      "Epoch: 782 [20100/50000 (40%)]  \tLoss:   88.242363\trec:   61.824589\tkl:   26.417768\n",
      "Epoch: 782 [30100/50000 (60%)]  \tLoss:   94.143738\trec:   66.662750\tkl:   27.480988\n",
      "Epoch: 782 [40100/50000 (80%)]  \tLoss:   86.238327\trec:   60.643623\tkl:   25.594704\n",
      "====> Epoch: 782 Average train loss: 90.6703\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7304\n",
      "Epoch: 783 [  100/50000 ( 0%)]  \tLoss:   89.706383\trec:   63.559467\tkl:   26.146915\n",
      "Epoch: 783 [10100/50000 (20%)]  \tLoss:   89.913513\trec:   63.969055\tkl:   25.944462\n",
      "Epoch: 783 [20100/50000 (40%)]  \tLoss:   93.905769\trec:   66.792656\tkl:   27.113111\n",
      "Epoch: 783 [30100/50000 (60%)]  \tLoss:   92.922722\trec:   66.136887\tkl:   26.785837\n",
      "Epoch: 783 [40100/50000 (80%)]  \tLoss:   95.786095\trec:   68.847099\tkl:   26.938992\n",
      "====> Epoch: 783 Average train loss: 90.6668\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6267\n",
      "Epoch: 784 [  100/50000 ( 0%)]  \tLoss:   93.284645\trec:   67.077446\tkl:   26.207191\n",
      "Epoch: 784 [10100/50000 (20%)]  \tLoss:   90.088661\trec:   63.435184\tkl:   26.653477\n",
      "Epoch: 784 [20100/50000 (40%)]  \tLoss:   91.246597\trec:   65.745758\tkl:   25.500845\n",
      "Epoch: 784 [30100/50000 (60%)]  \tLoss:   90.884567\trec:   64.431725\tkl:   26.452837\n",
      "Epoch: 784 [40100/50000 (80%)]  \tLoss:   91.844727\trec:   65.708321\tkl:   26.136410\n",
      "====> Epoch: 784 Average train loss: 90.6648\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6716\n",
      "Epoch: 785 [  100/50000 ( 0%)]  \tLoss:   92.315186\trec:   65.085693\tkl:   27.229488\n",
      "Epoch: 785 [10100/50000 (20%)]  \tLoss:   90.232712\trec:   63.518036\tkl:   26.714674\n",
      "Epoch: 785 [20100/50000 (40%)]  \tLoss:   92.980095\trec:   66.302841\tkl:   26.677256\n",
      "Epoch: 785 [30100/50000 (60%)]  \tLoss:   87.318512\trec:   61.205254\tkl:   26.113256\n",
      "Epoch: 785 [40100/50000 (80%)]  \tLoss:   91.542938\trec:   65.097466\tkl:   26.445471\n",
      "====> Epoch: 785 Average train loss: 90.6657\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6400\n",
      "Epoch: 786 [  100/50000 ( 0%)]  \tLoss:   88.797951\trec:   62.806622\tkl:   25.991323\n",
      "Epoch: 786 [10100/50000 (20%)]  \tLoss:   90.203468\trec:   63.943470\tkl:   26.259993\n",
      "Epoch: 786 [20100/50000 (40%)]  \tLoss:   87.623093\trec:   61.785179\tkl:   25.837917\n",
      "Epoch: 786 [30100/50000 (60%)]  \tLoss:   89.944389\trec:   63.984715\tkl:   25.959682\n",
      "Epoch: 786 [40100/50000 (80%)]  \tLoss:   86.155212\trec:   61.181763\tkl:   24.973452\n",
      "====> Epoch: 786 Average train loss: 90.6464\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6629\n",
      "Epoch: 787 [  100/50000 ( 0%)]  \tLoss:   89.391151\trec:   64.013824\tkl:   25.377319\n",
      "Epoch: 787 [10100/50000 (20%)]  \tLoss:   94.336494\trec:   67.449364\tkl:   26.887131\n",
      "Epoch: 787 [20100/50000 (40%)]  \tLoss:   88.063896\trec:   62.766159\tkl:   25.297733\n",
      "Epoch: 787 [30100/50000 (60%)]  \tLoss:   92.099159\trec:   65.287987\tkl:   26.811176\n",
      "Epoch: 787 [40100/50000 (80%)]  \tLoss:   94.773804\trec:   67.644112\tkl:   27.129692\n",
      "====> Epoch: 787 Average train loss: 90.6805\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5167\n",
      "Epoch: 788 [  100/50000 ( 0%)]  \tLoss:   92.078712\trec:   65.304558\tkl:   26.774153\n",
      "Epoch: 788 [10100/50000 (20%)]  \tLoss:   89.690285\trec:   64.059357\tkl:   25.630924\n",
      "Epoch: 788 [20100/50000 (40%)]  \tLoss:   90.119194\trec:   63.889202\tkl:   26.230000\n",
      "Epoch: 788 [30100/50000 (60%)]  \tLoss:   93.725380\trec:   66.901329\tkl:   26.824049\n",
      "Epoch: 788 [40100/50000 (80%)]  \tLoss:   92.241264\trec:   65.301041\tkl:   26.940233\n",
      "====> Epoch: 788 Average train loss: 90.6437\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6004\n",
      "Epoch: 789 [  100/50000 ( 0%)]  \tLoss:   87.878387\trec:   61.838017\tkl:   26.040367\n",
      "Epoch: 789 [10100/50000 (20%)]  \tLoss:   91.931519\trec:   65.876480\tkl:   26.055044\n",
      "Epoch: 789 [20100/50000 (40%)]  \tLoss:   90.913330\trec:   64.637459\tkl:   26.275866\n",
      "Epoch: 789 [30100/50000 (60%)]  \tLoss:   86.621071\trec:   60.320381\tkl:   26.300697\n",
      "Epoch: 789 [40100/50000 (80%)]  \tLoss:   91.011238\trec:   64.649879\tkl:   26.361362\n",
      "====> Epoch: 789 Average train loss: 90.6572\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5807\n",
      "Epoch: 790 [  100/50000 ( 0%)]  \tLoss:   89.879982\trec:   64.327843\tkl:   25.552128\n",
      "Epoch: 790 [10100/50000 (20%)]  \tLoss:   88.092003\trec:   62.260681\tkl:   25.831316\n",
      "Epoch: 790 [20100/50000 (40%)]  \tLoss:   94.545807\trec:   66.869850\tkl:   27.675957\n",
      "Epoch: 790 [30100/50000 (60%)]  \tLoss:   83.539986\trec:   58.394375\tkl:   25.145617\n",
      "Epoch: 790 [40100/50000 (80%)]  \tLoss:   88.957909\trec:   63.546326\tkl:   25.411587\n",
      "====> Epoch: 790 Average train loss: 90.6564\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7090\n",
      "Epoch: 791 [  100/50000 ( 0%)]  \tLoss:   91.157852\trec:   64.665031\tkl:   26.492819\n",
      "Epoch: 791 [10100/50000 (20%)]  \tLoss:   89.520874\trec:   62.946709\tkl:   26.574171\n",
      "Epoch: 791 [20100/50000 (40%)]  \tLoss:   94.421822\trec:   67.835808\tkl:   26.586018\n",
      "Epoch: 791 [30100/50000 (60%)]  \tLoss:   93.829636\trec:   67.309685\tkl:   26.519951\n",
      "Epoch: 791 [40100/50000 (80%)]  \tLoss:   91.795624\trec:   65.201775\tkl:   26.593842\n",
      "====> Epoch: 791 Average train loss: 90.6366\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5783\n",
      "Epoch: 792 [  100/50000 ( 0%)]  \tLoss:   91.802818\trec:   64.749001\tkl:   27.053820\n",
      "Epoch: 792 [10100/50000 (20%)]  \tLoss:   88.761284\trec:   63.189667\tkl:   25.571621\n",
      "Epoch: 792 [20100/50000 (40%)]  \tLoss:   90.816444\trec:   65.151054\tkl:   25.665390\n",
      "Epoch: 792 [30100/50000 (60%)]  \tLoss:   93.555702\trec:   67.135109\tkl:   26.420593\n",
      "Epoch: 792 [40100/50000 (80%)]  \tLoss:   90.047714\trec:   63.711994\tkl:   26.335720\n",
      "====> Epoch: 792 Average train loss: 90.6352\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6350\n",
      "Epoch: 793 [  100/50000 ( 0%)]  \tLoss:   91.818375\trec:   65.112900\tkl:   26.705481\n",
      "Epoch: 793 [10100/50000 (20%)]  \tLoss:   91.578300\trec:   65.275986\tkl:   26.302309\n",
      "Epoch: 793 [20100/50000 (40%)]  \tLoss:   92.387123\trec:   65.180794\tkl:   27.206335\n",
      "Epoch: 793 [30100/50000 (60%)]  \tLoss:   89.462013\trec:   63.564423\tkl:   25.897587\n",
      "Epoch: 793 [40100/50000 (80%)]  \tLoss:   90.763512\trec:   64.091309\tkl:   26.672211\n",
      "====> Epoch: 793 Average train loss: 90.6454\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5797\n",
      "Epoch: 794 [  100/50000 ( 0%)]  \tLoss:   93.560005\trec:   67.409500\tkl:   26.150505\n",
      "Epoch: 794 [10100/50000 (20%)]  \tLoss:   90.739220\trec:   64.904953\tkl:   25.834263\n",
      "Epoch: 794 [20100/50000 (40%)]  \tLoss:   94.094444\trec:   67.724152\tkl:   26.370285\n",
      "Epoch: 794 [30100/50000 (60%)]  \tLoss:   88.480751\trec:   62.573750\tkl:   25.907003\n",
      "Epoch: 794 [40100/50000 (80%)]  \tLoss:   92.735107\trec:   66.385109\tkl:   26.350002\n",
      "====> Epoch: 794 Average train loss: 90.6363\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6588\n",
      "Epoch: 795 [  100/50000 ( 0%)]  \tLoss:   87.349785\trec:   62.265800\tkl:   25.083981\n",
      "Epoch: 795 [10100/50000 (20%)]  \tLoss:   89.369049\trec:   63.142761\tkl:   26.226292\n",
      "Epoch: 795 [20100/50000 (40%)]  \tLoss:   92.999962\trec:   65.813454\tkl:   27.186514\n",
      "Epoch: 795 [30100/50000 (60%)]  \tLoss:   85.978348\trec:   60.308926\tkl:   25.669424\n",
      "Epoch: 795 [40100/50000 (80%)]  \tLoss:   91.336510\trec:   64.075157\tkl:   27.261354\n",
      "====> Epoch: 795 Average train loss: 90.6001\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6749\n",
      "Epoch: 796 [  100/50000 ( 0%)]  \tLoss:   92.540085\trec:   65.884850\tkl:   26.655233\n",
      "Epoch: 796 [10100/50000 (20%)]  \tLoss:   92.752968\trec:   66.050026\tkl:   26.702946\n",
      "Epoch: 796 [20100/50000 (40%)]  \tLoss:   93.318687\trec:   66.518074\tkl:   26.800615\n",
      "Epoch: 796 [30100/50000 (60%)]  \tLoss:   91.472664\trec:   65.149673\tkl:   26.322990\n",
      "Epoch: 796 [40100/50000 (80%)]  \tLoss:   90.026024\trec:   64.314178\tkl:   25.711847\n",
      "====> Epoch: 796 Average train loss: 90.6214\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6337\n",
      "Epoch: 797 [  100/50000 ( 0%)]  \tLoss:   91.148064\trec:   63.827118\tkl:   27.320950\n",
      "Epoch: 797 [10100/50000 (20%)]  \tLoss:   86.644508\trec:   60.710644\tkl:   25.933868\n",
      "Epoch: 797 [20100/50000 (40%)]  \tLoss:   87.268570\trec:   62.149014\tkl:   25.119560\n",
      "Epoch: 797 [30100/50000 (60%)]  \tLoss:   93.193886\trec:   67.157898\tkl:   26.035980\n",
      "Epoch: 797 [40100/50000 (80%)]  \tLoss:   91.021500\trec:   65.220314\tkl:   25.801184\n",
      "====> Epoch: 797 Average train loss: 90.6299\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6186\n",
      "Epoch: 798 [  100/50000 ( 0%)]  \tLoss:   90.630928\trec:   63.777023\tkl:   26.853899\n",
      "Epoch: 798 [10100/50000 (20%)]  \tLoss:   92.764015\trec:   65.923775\tkl:   26.840237\n",
      "Epoch: 798 [20100/50000 (40%)]  \tLoss:   91.031166\trec:   65.170792\tkl:   25.860373\n",
      "Epoch: 798 [30100/50000 (60%)]  \tLoss:   94.424530\trec:   68.460152\tkl:   25.964373\n",
      "Epoch: 798 [40100/50000 (80%)]  \tLoss:   90.004295\trec:   64.366920\tkl:   25.637375\n",
      "====> Epoch: 798 Average train loss: 90.6042\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5054\n",
      "Epoch: 799 [  100/50000 ( 0%)]  \tLoss:   90.690643\trec:   65.196709\tkl:   25.493933\n",
      "Epoch: 799 [10100/50000 (20%)]  \tLoss:   94.740662\trec:   68.230820\tkl:   26.509848\n",
      "Epoch: 799 [20100/50000 (40%)]  \tLoss:   91.115311\trec:   65.301445\tkl:   25.813864\n",
      "Epoch: 799 [30100/50000 (60%)]  \tLoss:   90.170212\trec:   63.942295\tkl:   26.227922\n",
      "Epoch: 799 [40100/50000 (80%)]  \tLoss:   90.184975\trec:   64.315674\tkl:   25.869307\n",
      "====> Epoch: 799 Average train loss: 90.6250\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6353\n",
      "Epoch: 800 [  100/50000 ( 0%)]  \tLoss:   91.656754\trec:   65.165504\tkl:   26.491247\n",
      "Epoch: 800 [10100/50000 (20%)]  \tLoss:   90.099792\trec:   63.607491\tkl:   26.492302\n",
      "Epoch: 800 [20100/50000 (40%)]  \tLoss:   88.223427\trec:   61.771416\tkl:   26.452011\n",
      "Epoch: 800 [30100/50000 (60%)]  \tLoss:   89.378632\trec:   62.953396\tkl:   26.425236\n",
      "Epoch: 800 [40100/50000 (80%)]  \tLoss:   93.417274\trec:   67.303551\tkl:   26.113720\n",
      "====> Epoch: 800 Average train loss: 90.6282\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6486\n",
      "Epoch: 801 [  100/50000 ( 0%)]  \tLoss:   88.945816\trec:   63.164684\tkl:   25.781137\n",
      "Epoch: 801 [10100/50000 (20%)]  \tLoss:   88.164780\trec:   63.080170\tkl:   25.084612\n",
      "Epoch: 801 [20100/50000 (40%)]  \tLoss:   91.707466\trec:   65.144043\tkl:   26.563427\n",
      "Epoch: 801 [30100/50000 (60%)]  \tLoss:   96.314835\trec:   69.669876\tkl:   26.644955\n",
      "Epoch: 801 [40100/50000 (80%)]  \tLoss:   94.729980\trec:   67.104286\tkl:   27.625698\n",
      "====> Epoch: 801 Average train loss: 90.6230\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6491\n",
      "Epoch: 802 [  100/50000 ( 0%)]  \tLoss:   90.576385\trec:   63.394958\tkl:   27.181431\n",
      "Epoch: 802 [10100/50000 (20%)]  \tLoss:   90.658913\trec:   65.064903\tkl:   25.594013\n",
      "Epoch: 802 [20100/50000 (40%)]  \tLoss:   89.839684\trec:   62.866142\tkl:   26.973547\n",
      "Epoch: 802 [30100/50000 (60%)]  \tLoss:   93.898048\trec:   66.473511\tkl:   27.424526\n",
      "Epoch: 802 [40100/50000 (80%)]  \tLoss:   92.816437\trec:   66.336632\tkl:   26.479801\n",
      "====> Epoch: 802 Average train loss: 90.5996\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7343\n",
      "Epoch: 803 [  100/50000 ( 0%)]  \tLoss:   88.760216\trec:   62.881767\tkl:   25.878441\n",
      "Epoch: 803 [10100/50000 (20%)]  \tLoss:   88.408989\trec:   62.490246\tkl:   25.918745\n",
      "Epoch: 803 [20100/50000 (40%)]  \tLoss:   90.697617\trec:   64.184830\tkl:   26.512779\n",
      "Epoch: 803 [30100/50000 (60%)]  \tLoss:   93.543983\trec:   66.151474\tkl:   27.392513\n",
      "Epoch: 803 [40100/50000 (80%)]  \tLoss:   93.052437\trec:   65.867088\tkl:   27.185350\n",
      "====> Epoch: 803 Average train loss: 90.5838\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6081\n",
      "Epoch: 804 [  100/50000 ( 0%)]  \tLoss:   90.422722\trec:   63.946209\tkl:   26.476513\n",
      "Epoch: 804 [10100/50000 (20%)]  \tLoss:   93.120468\trec:   66.553276\tkl:   26.567194\n",
      "Epoch: 804 [20100/50000 (40%)]  \tLoss:   90.138252\trec:   63.982956\tkl:   26.155293\n",
      "Epoch: 804 [30100/50000 (60%)]  \tLoss:   88.306625\trec:   62.420601\tkl:   25.886030\n",
      "Epoch: 804 [40100/50000 (80%)]  \tLoss:   90.638847\trec:   64.749519\tkl:   25.889326\n",
      "====> Epoch: 804 Average train loss: 90.6166\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5928\n",
      "Epoch: 805 [  100/50000 ( 0%)]  \tLoss:   90.639099\trec:   64.471031\tkl:   26.168066\n",
      "Epoch: 805 [10100/50000 (20%)]  \tLoss:   92.056267\trec:   66.199333\tkl:   25.856937\n",
      "Epoch: 805 [20100/50000 (40%)]  \tLoss:   89.304550\trec:   62.793728\tkl:   26.510822\n",
      "Epoch: 805 [30100/50000 (60%)]  \tLoss:   89.107506\trec:   63.684814\tkl:   25.422693\n",
      "Epoch: 805 [40100/50000 (80%)]  \tLoss:   93.346054\trec:   66.794777\tkl:   26.551279\n",
      "====> Epoch: 805 Average train loss: 90.6086\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7107\n",
      "Epoch: 806 [  100/50000 ( 0%)]  \tLoss:   92.226540\trec:   65.717316\tkl:   26.509226\n",
      "Epoch: 806 [10100/50000 (20%)]  \tLoss:   87.326637\trec:   61.532509\tkl:   25.794125\n",
      "Epoch: 806 [20100/50000 (40%)]  \tLoss:   90.525055\trec:   64.143204\tkl:   26.381845\n",
      "Epoch: 806 [30100/50000 (60%)]  \tLoss:   88.368942\trec:   61.678486\tkl:   26.690456\n",
      "Epoch: 806 [40100/50000 (80%)]  \tLoss:   91.555878\trec:   65.575401\tkl:   25.980482\n",
      "====> Epoch: 806 Average train loss: 90.5788\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6888\n",
      "Epoch: 807 [  100/50000 ( 0%)]  \tLoss:   90.581726\trec:   64.521774\tkl:   26.059954\n",
      "Epoch: 807 [10100/50000 (20%)]  \tLoss:   92.816483\trec:   66.805664\tkl:   26.010817\n",
      "Epoch: 807 [20100/50000 (40%)]  \tLoss:   88.219978\trec:   62.428383\tkl:   25.791594\n",
      "Epoch: 807 [30100/50000 (60%)]  \tLoss:   89.719521\trec:   63.721016\tkl:   25.998508\n",
      "Epoch: 807 [40100/50000 (80%)]  \tLoss:   89.587578\trec:   63.404793\tkl:   26.182783\n",
      "====> Epoch: 807 Average train loss: 90.6049\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5831\n",
      "Epoch: 808 [  100/50000 ( 0%)]  \tLoss:   91.654366\trec:   65.112167\tkl:   26.542196\n",
      "Epoch: 808 [10100/50000 (20%)]  \tLoss:   94.671188\trec:   68.892609\tkl:   25.778578\n",
      "Epoch: 808 [20100/50000 (40%)]  \tLoss:   85.730118\trec:   60.294861\tkl:   25.435259\n",
      "Epoch: 808 [30100/50000 (60%)]  \tLoss:   90.806328\trec:   64.819122\tkl:   25.987196\n",
      "Epoch: 808 [40100/50000 (80%)]  \tLoss:   89.805145\trec:   64.059982\tkl:   25.745163\n",
      "====> Epoch: 808 Average train loss: 90.5945\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6622\n",
      "Epoch: 809 [  100/50000 ( 0%)]  \tLoss:   90.636635\trec:   64.677658\tkl:   25.958986\n",
      "Epoch: 809 [10100/50000 (20%)]  \tLoss:   90.503647\trec:   64.227516\tkl:   26.276136\n",
      "Epoch: 809 [20100/50000 (40%)]  \tLoss:   93.062057\trec:   66.171356\tkl:   26.890699\n",
      "Epoch: 809 [30100/50000 (60%)]  \tLoss:   90.309830\trec:   64.163330\tkl:   26.146502\n",
      "Epoch: 809 [40100/50000 (80%)]  \tLoss:   94.019203\trec:   67.286690\tkl:   26.732513\n",
      "====> Epoch: 809 Average train loss: 90.5993\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6272\n",
      "Epoch: 810 [  100/50000 ( 0%)]  \tLoss:   92.908356\trec:   66.246819\tkl:   26.661547\n",
      "Epoch: 810 [10100/50000 (20%)]  \tLoss:   89.890457\trec:   62.939873\tkl:   26.950583\n",
      "Epoch: 810 [20100/50000 (40%)]  \tLoss:   90.248451\trec:   64.194153\tkl:   26.054300\n",
      "Epoch: 810 [30100/50000 (60%)]  \tLoss:   90.535995\trec:   64.445938\tkl:   26.090061\n",
      "Epoch: 810 [40100/50000 (80%)]  \tLoss:   94.540291\trec:   67.852348\tkl:   26.687939\n",
      "====> Epoch: 810 Average train loss: 90.6000\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5700\n",
      "Epoch: 811 [  100/50000 ( 0%)]  \tLoss:   84.720398\trec:   59.374268\tkl:   25.346132\n",
      "Epoch: 811 [10100/50000 (20%)]  \tLoss:   89.752892\trec:   63.231091\tkl:   26.521793\n",
      "Epoch: 811 [20100/50000 (40%)]  \tLoss:   88.909950\trec:   63.571053\tkl:   25.338898\n",
      "Epoch: 811 [30100/50000 (60%)]  \tLoss:   92.002380\trec:   65.901978\tkl:   26.100399\n",
      "Epoch: 811 [40100/50000 (80%)]  \tLoss:   90.944473\trec:   65.113007\tkl:   25.831461\n",
      "====> Epoch: 811 Average train loss: 90.5935\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6699\n",
      "Epoch: 812 [  100/50000 ( 0%)]  \tLoss:   90.523979\trec:   63.796055\tkl:   26.727926\n",
      "Epoch: 812 [10100/50000 (20%)]  \tLoss:   91.742905\trec:   64.249718\tkl:   27.493187\n",
      "Epoch: 812 [20100/50000 (40%)]  \tLoss:   94.009315\trec:   67.159874\tkl:   26.849443\n",
      "Epoch: 812 [30100/50000 (60%)]  \tLoss:   85.023277\trec:   59.818699\tkl:   25.204580\n",
      "Epoch: 812 [40100/50000 (80%)]  \tLoss:   89.975945\trec:   64.092018\tkl:   25.883930\n",
      "====> Epoch: 812 Average train loss: 90.6016\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7114\n",
      "Epoch: 813 [  100/50000 ( 0%)]  \tLoss:   93.356010\trec:   67.298416\tkl:   26.057602\n",
      "Epoch: 813 [10100/50000 (20%)]  \tLoss:   92.988441\trec:   67.190201\tkl:   25.798244\n",
      "Epoch: 813 [20100/50000 (40%)]  \tLoss:   90.887901\trec:   63.751308\tkl:   27.136589\n",
      "Epoch: 813 [30100/50000 (60%)]  \tLoss:   92.416290\trec:   66.003510\tkl:   26.412777\n",
      "Epoch: 813 [40100/50000 (80%)]  \tLoss:   87.192360\trec:   61.995335\tkl:   25.197031\n",
      "====> Epoch: 813 Average train loss: 90.5661\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6097\n",
      "Epoch: 814 [  100/50000 ( 0%)]  \tLoss:   90.326523\trec:   64.532265\tkl:   25.794262\n",
      "Epoch: 814 [10100/50000 (20%)]  \tLoss:   90.353271\trec:   64.414825\tkl:   25.938450\n",
      "Epoch: 814 [20100/50000 (40%)]  \tLoss:   89.260284\trec:   63.420422\tkl:   25.839857\n",
      "Epoch: 814 [30100/50000 (60%)]  \tLoss:   88.334908\trec:   62.495068\tkl:   25.839840\n",
      "Epoch: 814 [40100/50000 (80%)]  \tLoss:   94.906395\trec:   67.321411\tkl:   27.584978\n",
      "====> Epoch: 814 Average train loss: 90.5964\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.8193\n",
      "Epoch: 815 [  100/50000 ( 0%)]  \tLoss:   93.963295\trec:   67.201286\tkl:   26.762007\n",
      "Epoch: 815 [10100/50000 (20%)]  \tLoss:   93.416443\trec:   66.419342\tkl:   26.997097\n",
      "Epoch: 815 [20100/50000 (40%)]  \tLoss:   88.976479\trec:   63.504395\tkl:   25.472094\n",
      "Epoch: 815 [30100/50000 (60%)]  \tLoss:   91.278152\trec:   65.069107\tkl:   26.209047\n",
      "Epoch: 815 [40100/50000 (80%)]  \tLoss:   91.279366\trec:   65.070946\tkl:   26.208418\n",
      "====> Epoch: 815 Average train loss: 90.5839\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6966\n",
      "Epoch: 816 [  100/50000 ( 0%)]  \tLoss:   88.383316\trec:   63.196693\tkl:   25.186630\n",
      "Epoch: 816 [10100/50000 (20%)]  \tLoss:   92.572853\trec:   66.418854\tkl:   26.153999\n",
      "Epoch: 816 [20100/50000 (40%)]  \tLoss:   91.115929\trec:   64.418427\tkl:   26.697496\n",
      "Epoch: 816 [30100/50000 (60%)]  \tLoss:   91.603325\trec:   64.127289\tkl:   27.476042\n",
      "Epoch: 816 [40100/50000 (80%)]  \tLoss:   85.850395\trec:   59.948963\tkl:   25.901436\n",
      "====> Epoch: 816 Average train loss: 90.5942\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7318\n",
      "Epoch: 817 [  100/50000 ( 0%)]  \tLoss:   90.126747\trec:   63.992119\tkl:   26.134626\n",
      "Epoch: 817 [10100/50000 (20%)]  \tLoss:   89.101784\trec:   63.885731\tkl:   25.216051\n",
      "Epoch: 817 [20100/50000 (40%)]  \tLoss:   87.923340\trec:   61.819084\tkl:   26.104248\n",
      "Epoch: 817 [30100/50000 (60%)]  \tLoss:   90.345505\trec:   64.157745\tkl:   26.187767\n",
      "Epoch: 817 [40100/50000 (80%)]  \tLoss:   88.664497\trec:   62.451679\tkl:   26.212820\n",
      "====> Epoch: 817 Average train loss: 90.5981\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6388\n",
      "Epoch: 818 [  100/50000 ( 0%)]  \tLoss:   93.326431\trec:   66.980751\tkl:   26.345680\n",
      "Epoch: 818 [10100/50000 (20%)]  \tLoss:   88.115585\trec:   62.315594\tkl:   25.799990\n",
      "Epoch: 818 [20100/50000 (40%)]  \tLoss:   91.082558\trec:   64.748047\tkl:   26.334509\n",
      "Epoch: 818 [30100/50000 (60%)]  \tLoss:   89.816055\trec:   64.260307\tkl:   25.555744\n",
      "Epoch: 818 [40100/50000 (80%)]  \tLoss:   90.149704\trec:   64.271988\tkl:   25.877720\n",
      "====> Epoch: 818 Average train loss: 90.5716\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5477\n",
      "Epoch: 819 [  100/50000 ( 0%)]  \tLoss:   87.304520\trec:   61.918221\tkl:   25.386295\n",
      "Epoch: 819 [10100/50000 (20%)]  \tLoss:   93.803810\trec:   67.431007\tkl:   26.372799\n",
      "Epoch: 819 [20100/50000 (40%)]  \tLoss:   91.112938\trec:   64.318619\tkl:   26.794321\n",
      "Epoch: 819 [30100/50000 (60%)]  \tLoss:   88.724724\trec:   61.650848\tkl:   27.073874\n",
      "Epoch: 819 [40100/50000 (80%)]  \tLoss:   92.145660\trec:   65.410667\tkl:   26.734995\n",
      "====> Epoch: 819 Average train loss: 90.5634\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6505\n",
      "Epoch: 820 [  100/50000 ( 0%)]  \tLoss:   85.759689\trec:   60.945126\tkl:   24.814566\n",
      "Epoch: 820 [10100/50000 (20%)]  \tLoss:   92.858887\trec:   67.267159\tkl:   25.591732\n",
      "Epoch: 820 [20100/50000 (40%)]  \tLoss:   87.751167\trec:   62.124046\tkl:   25.627119\n",
      "Epoch: 820 [30100/50000 (60%)]  \tLoss:   87.556541\trec:   61.328049\tkl:   26.228495\n",
      "Epoch: 820 [40100/50000 (80%)]  \tLoss:   92.068336\trec:   66.029106\tkl:   26.039228\n",
      "====> Epoch: 820 Average train loss: 90.5562\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5947\n",
      "Epoch: 821 [  100/50000 ( 0%)]  \tLoss:   91.872856\trec:   65.132675\tkl:   26.740187\n",
      "Epoch: 821 [10100/50000 (20%)]  \tLoss:   88.250801\trec:   61.828934\tkl:   26.421869\n",
      "Epoch: 821 [20100/50000 (40%)]  \tLoss:   90.702255\trec:   64.872742\tkl:   25.829508\n",
      "Epoch: 821 [30100/50000 (60%)]  \tLoss:   92.514610\trec:   65.608765\tkl:   26.905836\n",
      "Epoch: 821 [40100/50000 (80%)]  \tLoss:   86.827675\trec:   61.164150\tkl:   25.663527\n",
      "====> Epoch: 821 Average train loss: 90.5656\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5848\n",
      "Epoch: 822 [  100/50000 ( 0%)]  \tLoss:   95.493774\trec:   67.889595\tkl:   27.604177\n",
      "Epoch: 822 [10100/50000 (20%)]  \tLoss:   92.155182\trec:   65.131378\tkl:   27.023800\n",
      "Epoch: 822 [20100/50000 (40%)]  \tLoss:   88.741531\trec:   62.747829\tkl:   25.993704\n",
      "Epoch: 822 [30100/50000 (60%)]  \tLoss:   86.871750\trec:   61.635159\tkl:   25.236589\n",
      "Epoch: 822 [40100/50000 (80%)]  \tLoss:   92.023140\trec:   64.938202\tkl:   27.084936\n",
      "====> Epoch: 822 Average train loss: 90.5715\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6041\n",
      "Epoch: 823 [  100/50000 ( 0%)]  \tLoss:   89.521309\trec:   63.457798\tkl:   26.063507\n",
      "Epoch: 823 [10100/50000 (20%)]  \tLoss:   89.254921\trec:   61.818718\tkl:   27.436197\n",
      "Epoch: 823 [20100/50000 (40%)]  \tLoss:   89.364853\trec:   62.851738\tkl:   26.513115\n",
      "Epoch: 823 [30100/50000 (60%)]  \tLoss:   88.435455\trec:   62.540691\tkl:   25.894768\n",
      "Epoch: 823 [40100/50000 (80%)]  \tLoss:   93.027771\trec:   67.028755\tkl:   25.999016\n",
      "====> Epoch: 823 Average train loss: 90.5382\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5725\n",
      "Epoch: 824 [  100/50000 ( 0%)]  \tLoss:   91.319740\trec:   64.983322\tkl:   26.336426\n",
      "Epoch: 824 [10100/50000 (20%)]  \tLoss:   87.157524\trec:   61.382404\tkl:   25.775120\n",
      "Epoch: 824 [20100/50000 (40%)]  \tLoss:   91.518074\trec:   65.538536\tkl:   25.979532\n",
      "Epoch: 824 [30100/50000 (60%)]  \tLoss:   89.632050\trec:   63.725731\tkl:   25.906315\n",
      "Epoch: 824 [40100/50000 (80%)]  \tLoss:   95.786949\trec:   69.118752\tkl:   26.668192\n",
      "====> Epoch: 824 Average train loss: 90.5644\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5850\n",
      "Epoch: 825 [  100/50000 ( 0%)]  \tLoss:   91.679100\trec:   64.785545\tkl:   26.893551\n",
      "Epoch: 825 [10100/50000 (20%)]  \tLoss:   87.098885\trec:   60.833115\tkl:   26.265768\n",
      "Epoch: 825 [20100/50000 (40%)]  \tLoss:   91.426231\trec:   64.432655\tkl:   26.993578\n",
      "Epoch: 825 [30100/50000 (60%)]  \tLoss:   92.157784\trec:   65.059853\tkl:   27.097931\n",
      "Epoch: 825 [40100/50000 (80%)]  \tLoss:   93.992149\trec:   66.545158\tkl:   27.446991\n",
      "====> Epoch: 825 Average train loss: 90.5480\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7125\n",
      "Epoch: 826 [  100/50000 ( 0%)]  \tLoss:   90.099571\trec:   62.993710\tkl:   27.105864\n",
      "Epoch: 826 [10100/50000 (20%)]  \tLoss:   91.663002\trec:   64.468803\tkl:   27.194193\n",
      "Epoch: 826 [20100/50000 (40%)]  \tLoss:   92.715515\trec:   66.011772\tkl:   26.703737\n",
      "Epoch: 826 [30100/50000 (60%)]  \tLoss:   89.675369\trec:   63.252598\tkl:   26.422770\n",
      "Epoch: 826 [40100/50000 (80%)]  \tLoss:   91.455612\trec:   64.928986\tkl:   26.526632\n",
      "====> Epoch: 826 Average train loss: 90.5405\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6558\n",
      "Epoch: 827 [  100/50000 ( 0%)]  \tLoss:   90.649918\trec:   63.961815\tkl:   26.688110\n",
      "Epoch: 827 [10100/50000 (20%)]  \tLoss:   90.277451\trec:   64.179993\tkl:   26.097456\n",
      "Epoch: 827 [20100/50000 (40%)]  \tLoss:   91.945137\trec:   65.475197\tkl:   26.469933\n",
      "Epoch: 827 [30100/50000 (60%)]  \tLoss:   92.736511\trec:   66.322937\tkl:   26.413570\n",
      "Epoch: 827 [40100/50000 (80%)]  \tLoss:   85.780701\trec:   59.620312\tkl:   26.160387\n",
      "====> Epoch: 827 Average train loss: 90.5487\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4920\n",
      "Epoch: 828 [  100/50000 ( 0%)]  \tLoss:   85.228127\trec:   60.025059\tkl:   25.203060\n",
      "Epoch: 828 [10100/50000 (20%)]  \tLoss:   94.696411\trec:   67.759529\tkl:   26.936886\n",
      "Epoch: 828 [20100/50000 (40%)]  \tLoss:   90.952652\trec:   65.378975\tkl:   25.573681\n",
      "Epoch: 828 [30100/50000 (60%)]  \tLoss:   91.150879\trec:   63.881084\tkl:   27.269796\n",
      "Epoch: 828 [40100/50000 (80%)]  \tLoss:   93.945084\trec:   66.453018\tkl:   27.492073\n",
      "====> Epoch: 828 Average train loss: 90.5170\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5696\n",
      "Epoch: 829 [  100/50000 ( 0%)]  \tLoss:   89.756622\trec:   63.797127\tkl:   25.959490\n",
      "Epoch: 829 [10100/50000 (20%)]  \tLoss:   90.076004\trec:   63.861179\tkl:   26.214821\n",
      "Epoch: 829 [20100/50000 (40%)]  \tLoss:   91.457382\trec:   65.197189\tkl:   26.260189\n",
      "Epoch: 829 [30100/50000 (60%)]  \tLoss:   89.193413\trec:   63.092773\tkl:   26.100647\n",
      "Epoch: 829 [40100/50000 (80%)]  \tLoss:   87.456734\trec:   61.708221\tkl:   25.748520\n",
      "====> Epoch: 829 Average train loss: 90.5251\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6006\n",
      "Epoch: 830 [  100/50000 ( 0%)]  \tLoss:   91.715797\trec:   65.245941\tkl:   26.469856\n",
      "Epoch: 830 [10100/50000 (20%)]  \tLoss:   90.249702\trec:   63.585293\tkl:   26.664419\n",
      "Epoch: 830 [20100/50000 (40%)]  \tLoss:   92.717896\trec:   65.483177\tkl:   27.234718\n",
      "Epoch: 830 [30100/50000 (60%)]  \tLoss:   87.417305\trec:   61.257526\tkl:   26.159777\n",
      "Epoch: 830 [40100/50000 (80%)]  \tLoss:   88.345123\trec:   61.857079\tkl:   26.488047\n",
      "====> Epoch: 830 Average train loss: 90.5595\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6520\n",
      "Epoch: 831 [  100/50000 ( 0%)]  \tLoss:   91.910332\trec:   65.909401\tkl:   26.000937\n",
      "Epoch: 831 [10100/50000 (20%)]  \tLoss:   89.813820\trec:   63.864140\tkl:   25.949675\n",
      "Epoch: 831 [20100/50000 (40%)]  \tLoss:   90.232811\trec:   64.656158\tkl:   25.576651\n",
      "Epoch: 831 [30100/50000 (60%)]  \tLoss:   90.983231\trec:   64.663132\tkl:   26.320095\n",
      "Epoch: 831 [40100/50000 (80%)]  \tLoss:   95.827225\trec:   68.838074\tkl:   26.989147\n",
      "====> Epoch: 831 Average train loss: 90.5366\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6227\n",
      "Epoch: 832 [  100/50000 ( 0%)]  \tLoss:   88.343399\trec:   61.999653\tkl:   26.343746\n",
      "Epoch: 832 [10100/50000 (20%)]  \tLoss:   92.635757\trec:   66.649834\tkl:   25.985924\n",
      "Epoch: 832 [20100/50000 (40%)]  \tLoss:   92.862007\trec:   66.565605\tkl:   26.296410\n",
      "Epoch: 832 [30100/50000 (60%)]  \tLoss:   91.937775\trec:   64.941628\tkl:   26.996145\n",
      "Epoch: 832 [40100/50000 (80%)]  \tLoss:   87.942970\trec:   62.278591\tkl:   25.664375\n",
      "====> Epoch: 832 Average train loss: 90.5439\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5192\n",
      "Epoch: 833 [  100/50000 ( 0%)]  \tLoss:   88.728928\trec:   62.103977\tkl:   26.624950\n",
      "Epoch: 833 [10100/50000 (20%)]  \tLoss:   88.926773\trec:   63.086903\tkl:   25.839872\n",
      "Epoch: 833 [20100/50000 (40%)]  \tLoss:   92.381020\trec:   66.170250\tkl:   26.210768\n",
      "Epoch: 833 [30100/50000 (60%)]  \tLoss:   89.421631\trec:   62.784447\tkl:   26.637180\n",
      "Epoch: 833 [40100/50000 (80%)]  \tLoss:   87.118515\trec:   61.479755\tkl:   25.638760\n",
      "====> Epoch: 833 Average train loss: 90.5341\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6645\n",
      "Epoch: 834 [  100/50000 ( 0%)]  \tLoss:   90.694931\trec:   64.502579\tkl:   26.192356\n",
      "Epoch: 834 [10100/50000 (20%)]  \tLoss:   90.400192\trec:   63.722473\tkl:   26.677719\n",
      "Epoch: 834 [20100/50000 (40%)]  \tLoss:   93.272987\trec:   66.563980\tkl:   26.709007\n",
      "Epoch: 834 [30100/50000 (60%)]  \tLoss:   93.413300\trec:   66.700073\tkl:   26.713223\n",
      "Epoch: 834 [40100/50000 (80%)]  \tLoss:   90.153786\trec:   64.060448\tkl:   26.093344\n",
      "====> Epoch: 834 Average train loss: 90.4948\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6092\n",
      "Epoch: 835 [  100/50000 ( 0%)]  \tLoss:   89.708496\trec:   64.524269\tkl:   25.184233\n",
      "Epoch: 835 [10100/50000 (20%)]  \tLoss:   93.034874\trec:   66.399185\tkl:   26.635683\n",
      "Epoch: 835 [20100/50000 (40%)]  \tLoss:   88.783066\trec:   62.472839\tkl:   26.310225\n",
      "Epoch: 835 [30100/50000 (60%)]  \tLoss:   91.401329\trec:   65.330841\tkl:   26.070482\n",
      "Epoch: 835 [40100/50000 (80%)]  \tLoss:   93.585411\trec:   67.409065\tkl:   26.176342\n",
      "====> Epoch: 835 Average train loss: 90.5393\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6044\n",
      "Epoch: 836 [  100/50000 ( 0%)]  \tLoss:   88.862564\trec:   62.922737\tkl:   25.939829\n",
      "Epoch: 836 [10100/50000 (20%)]  \tLoss:   91.747993\trec:   64.915092\tkl:   26.832903\n",
      "Epoch: 836 [20100/50000 (40%)]  \tLoss:   90.178162\trec:   63.908943\tkl:   26.269217\n",
      "Epoch: 836 [30100/50000 (60%)]  \tLoss:   89.072479\trec:   63.020054\tkl:   26.052425\n",
      "Epoch: 836 [40100/50000 (80%)]  \tLoss:   91.406952\trec:   64.575249\tkl:   26.831701\n",
      "====> Epoch: 836 Average train loss: 90.5443\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6273\n",
      "Epoch: 837 [  100/50000 ( 0%)]  \tLoss:   92.580307\trec:   65.569817\tkl:   27.010494\n",
      "Epoch: 837 [10100/50000 (20%)]  \tLoss:   88.632622\trec:   62.659267\tkl:   25.973356\n",
      "Epoch: 837 [20100/50000 (40%)]  \tLoss:   92.726044\trec:   65.729416\tkl:   26.996630\n",
      "Epoch: 837 [30100/50000 (60%)]  \tLoss:   88.005974\trec:   61.573879\tkl:   26.432089\n",
      "Epoch: 837 [40100/50000 (80%)]  \tLoss:   84.630554\trec:   60.974201\tkl:   23.656349\n",
      "====> Epoch: 837 Average train loss: 90.5261\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5851\n",
      "Epoch: 838 [  100/50000 ( 0%)]  \tLoss:   89.627052\trec:   64.490433\tkl:   25.136616\n",
      "Epoch: 838 [10100/50000 (20%)]  \tLoss:   92.136070\trec:   65.503387\tkl:   26.632690\n",
      "Epoch: 838 [20100/50000 (40%)]  \tLoss:   91.361336\trec:   65.171089\tkl:   26.190243\n",
      "Epoch: 838 [30100/50000 (60%)]  \tLoss:   89.162704\trec:   62.633797\tkl:   26.528908\n",
      "Epoch: 838 [40100/50000 (80%)]  \tLoss:   92.846268\trec:   65.904160\tkl:   26.942108\n",
      "====> Epoch: 838 Average train loss: 90.5398\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5796\n",
      "Epoch: 839 [  100/50000 ( 0%)]  \tLoss:   84.503296\trec:   59.339115\tkl:   25.164186\n",
      "Epoch: 839 [10100/50000 (20%)]  \tLoss:   92.663071\trec:   67.074562\tkl:   25.588512\n",
      "Epoch: 839 [20100/50000 (40%)]  \tLoss:   93.052208\trec:   67.234734\tkl:   25.817467\n",
      "Epoch: 839 [30100/50000 (60%)]  \tLoss:   88.856949\trec:   62.760742\tkl:   26.096205\n",
      "Epoch: 839 [40100/50000 (80%)]  \tLoss:   89.907143\trec:   63.680496\tkl:   26.226646\n",
      "====> Epoch: 839 Average train loss: 90.5153\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5827\n",
      "Epoch: 840 [  100/50000 ( 0%)]  \tLoss:   91.723282\trec:   64.596542\tkl:   27.126736\n",
      "Epoch: 840 [10100/50000 (20%)]  \tLoss:   87.845558\trec:   61.945782\tkl:   25.899776\n",
      "Epoch: 840 [20100/50000 (40%)]  \tLoss:   93.406876\trec:   67.159027\tkl:   26.247843\n",
      "Epoch: 840 [30100/50000 (60%)]  \tLoss:   95.550369\trec:   68.505028\tkl:   27.045343\n",
      "Epoch: 840 [40100/50000 (80%)]  \tLoss:   91.077148\trec:   64.316597\tkl:   26.760544\n",
      "====> Epoch: 840 Average train loss: 90.5466\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5489\n",
      "Epoch: 841 [  100/50000 ( 0%)]  \tLoss:   96.291573\trec:   68.821434\tkl:   27.470137\n",
      "Epoch: 841 [10100/50000 (20%)]  \tLoss:   88.045586\trec:   61.357788\tkl:   26.687798\n",
      "Epoch: 841 [20100/50000 (40%)]  \tLoss:   91.993164\trec:   65.762604\tkl:   26.230558\n",
      "Epoch: 841 [30100/50000 (60%)]  \tLoss:   93.207237\trec:   66.714165\tkl:   26.493073\n",
      "Epoch: 841 [40100/50000 (80%)]  \tLoss:   89.032814\trec:   62.410645\tkl:   26.622166\n",
      "====> Epoch: 841 Average train loss: 90.5096\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5765\n",
      "Epoch: 842 [  100/50000 ( 0%)]  \tLoss:   90.189919\trec:   64.117409\tkl:   26.072517\n",
      "Epoch: 842 [10100/50000 (20%)]  \tLoss:   91.907547\trec:   64.543854\tkl:   27.363691\n",
      "Epoch: 842 [20100/50000 (40%)]  \tLoss:   90.590332\trec:   64.059006\tkl:   26.531322\n",
      "Epoch: 842 [30100/50000 (60%)]  \tLoss:   88.247421\trec:   61.901043\tkl:   26.346375\n",
      "Epoch: 842 [40100/50000 (80%)]  \tLoss:   90.108429\trec:   64.531876\tkl:   25.576551\n",
      "====> Epoch: 842 Average train loss: 90.5249\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5302\n",
      "Epoch: 843 [  100/50000 ( 0%)]  \tLoss:   91.739334\trec:   65.433128\tkl:   26.306204\n",
      "Epoch: 843 [10100/50000 (20%)]  \tLoss:   88.331306\trec:   61.786228\tkl:   26.545082\n",
      "Epoch: 843 [20100/50000 (40%)]  \tLoss:   94.022408\trec:   66.792770\tkl:   27.229635\n",
      "Epoch: 843 [30100/50000 (60%)]  \tLoss:   90.493340\trec:   64.036827\tkl:   26.456503\n",
      "Epoch: 843 [40100/50000 (80%)]  \tLoss:   92.237350\trec:   65.639595\tkl:   26.597754\n",
      "====> Epoch: 843 Average train loss: 90.5007\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6064\n",
      "Epoch: 844 [  100/50000 ( 0%)]  \tLoss:   91.543686\trec:   64.660591\tkl:   26.883091\n",
      "Epoch: 844 [10100/50000 (20%)]  \tLoss:   90.805153\trec:   64.465302\tkl:   26.339848\n",
      "Epoch: 844 [20100/50000 (40%)]  \tLoss:   90.222672\trec:   63.997475\tkl:   26.225203\n",
      "Epoch: 844 [30100/50000 (60%)]  \tLoss:   87.996826\trec:   62.496017\tkl:   25.500805\n",
      "Epoch: 844 [40100/50000 (80%)]  \tLoss:   86.076759\trec:   60.221020\tkl:   25.855732\n",
      "====> Epoch: 844 Average train loss: 90.5152\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5252\n",
      "Epoch: 845 [  100/50000 ( 0%)]  \tLoss:   89.650772\trec:   64.012512\tkl:   25.638254\n",
      "Epoch: 845 [10100/50000 (20%)]  \tLoss:   88.683647\trec:   62.522766\tkl:   26.160889\n",
      "Epoch: 845 [20100/50000 (40%)]  \tLoss:   87.046021\trec:   61.056282\tkl:   25.989742\n",
      "Epoch: 845 [30100/50000 (60%)]  \tLoss:   85.907852\trec:   59.729694\tkl:   26.178148\n",
      "Epoch: 845 [40100/50000 (80%)]  \tLoss:   92.165268\trec:   65.714622\tkl:   26.450649\n",
      "====> Epoch: 845 Average train loss: 90.4962\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7899\n",
      "Epoch: 846 [  100/50000 ( 0%)]  \tLoss:   91.091034\trec:   64.827179\tkl:   26.263855\n",
      "Epoch: 846 [10100/50000 (20%)]  \tLoss:   90.257988\trec:   64.632805\tkl:   25.625191\n",
      "Epoch: 846 [20100/50000 (40%)]  \tLoss:   87.951584\trec:   62.599823\tkl:   25.351753\n",
      "Epoch: 846 [30100/50000 (60%)]  \tLoss:   88.924088\trec:   62.652813\tkl:   26.271276\n",
      "Epoch: 846 [40100/50000 (80%)]  \tLoss:   91.962227\trec:   64.484337\tkl:   27.477890\n",
      "====> Epoch: 846 Average train loss: 90.4839\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6172\n",
      "Epoch: 847 [  100/50000 ( 0%)]  \tLoss:   90.248375\trec:   62.671432\tkl:   27.576939\n",
      "Epoch: 847 [10100/50000 (20%)]  \tLoss:   89.262383\trec:   63.271420\tkl:   25.990963\n",
      "Epoch: 847 [20100/50000 (40%)]  \tLoss:   90.052185\trec:   63.923809\tkl:   26.128376\n",
      "Epoch: 847 [30100/50000 (60%)]  \tLoss:   91.673569\trec:   65.902657\tkl:   25.770918\n",
      "Epoch: 847 [40100/50000 (80%)]  \tLoss:   91.080513\trec:   66.108528\tkl:   24.971989\n",
      "====> Epoch: 847 Average train loss: 90.5109\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6848\n",
      "Epoch: 848 [  100/50000 ( 0%)]  \tLoss:   89.903358\trec:   63.739353\tkl:   26.163998\n",
      "Epoch: 848 [10100/50000 (20%)]  \tLoss:   90.006958\trec:   63.261745\tkl:   26.745216\n",
      "Epoch: 848 [20100/50000 (40%)]  \tLoss:   91.909561\trec:   65.186348\tkl:   26.723209\n",
      "Epoch: 848 [30100/50000 (60%)]  \tLoss:   92.963142\trec:   65.767868\tkl:   27.195271\n",
      "Epoch: 848 [40100/50000 (80%)]  \tLoss:   91.441689\trec:   65.377541\tkl:   26.064144\n",
      "====> Epoch: 848 Average train loss: 90.4854\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5802\n",
      "Epoch: 849 [  100/50000 ( 0%)]  \tLoss:   94.067123\trec:   67.819511\tkl:   26.247612\n",
      "Epoch: 849 [10100/50000 (20%)]  \tLoss:   90.341217\trec:   64.483955\tkl:   25.857260\n",
      "Epoch: 849 [20100/50000 (40%)]  \tLoss:   91.833267\trec:   65.388901\tkl:   26.444372\n",
      "Epoch: 849 [30100/50000 (60%)]  \tLoss:   93.064430\trec:   66.654663\tkl:   26.409767\n",
      "Epoch: 849 [40100/50000 (80%)]  \tLoss:   91.448364\trec:   65.737228\tkl:   25.711143\n",
      "====> Epoch: 849 Average train loss: 90.5060\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6404\n",
      "Epoch: 850 [  100/50000 ( 0%)]  \tLoss:   92.588280\trec:   65.209541\tkl:   27.378742\n",
      "Epoch: 850 [10100/50000 (20%)]  \tLoss:   95.284355\trec:   67.508492\tkl:   27.775856\n",
      "Epoch: 850 [20100/50000 (40%)]  \tLoss:   88.989113\trec:   63.607101\tkl:   25.382004\n",
      "Epoch: 850 [30100/50000 (60%)]  \tLoss:   93.006180\trec:   66.627701\tkl:   26.378479\n",
      "Epoch: 850 [40100/50000 (80%)]  \tLoss:   88.990334\trec:   63.240360\tkl:   25.749966\n",
      "====> Epoch: 850 Average train loss: 90.4915\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6428\n",
      "Epoch: 851 [  100/50000 ( 0%)]  \tLoss:   86.241844\trec:   61.073494\tkl:   25.168346\n",
      "Epoch: 851 [10100/50000 (20%)]  \tLoss:   92.423615\trec:   65.209282\tkl:   27.214321\n",
      "Epoch: 851 [20100/50000 (40%)]  \tLoss:   90.987244\trec:   64.984039\tkl:   26.003212\n",
      "Epoch: 851 [30100/50000 (60%)]  \tLoss:   86.279472\trec:   61.364624\tkl:   24.914846\n",
      "Epoch: 851 [40100/50000 (80%)]  \tLoss:   88.865730\trec:   63.517178\tkl:   25.348555\n",
      "====> Epoch: 851 Average train loss: 90.4949\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5788\n",
      "Epoch: 852 [  100/50000 ( 0%)]  \tLoss:   90.726059\trec:   64.101822\tkl:   26.624235\n",
      "Epoch: 852 [10100/50000 (20%)]  \tLoss:   91.579666\trec:   65.030510\tkl:   26.549164\n",
      "Epoch: 852 [20100/50000 (40%)]  \tLoss:   90.007362\trec:   64.483833\tkl:   25.523523\n",
      "Epoch: 852 [30100/50000 (60%)]  \tLoss:   92.869354\trec:   65.737457\tkl:   27.131899\n",
      "Epoch: 852 [40100/50000 (80%)]  \tLoss:   89.003960\trec:   62.920288\tkl:   26.083681\n",
      "====> Epoch: 852 Average train loss: 90.4898\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5014\n",
      "Epoch: 853 [  100/50000 ( 0%)]  \tLoss:   90.415329\trec:   63.013515\tkl:   27.401814\n",
      "Epoch: 853 [10100/50000 (20%)]  \tLoss:   89.989258\trec:   63.335361\tkl:   26.653893\n",
      "Epoch: 853 [20100/50000 (40%)]  \tLoss:   93.293999\trec:   66.770248\tkl:   26.523750\n",
      "Epoch: 853 [30100/50000 (60%)]  \tLoss:   90.442169\trec:   64.243073\tkl:   26.199089\n",
      "Epoch: 853 [40100/50000 (80%)]  \tLoss:   91.345856\trec:   65.614235\tkl:   25.731615\n",
      "====> Epoch: 853 Average train loss: 90.4715\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5622\n",
      "Epoch: 854 [  100/50000 ( 0%)]  \tLoss:   89.431717\trec:   62.874668\tkl:   26.557051\n",
      "Epoch: 854 [10100/50000 (20%)]  \tLoss:   90.122955\trec:   63.739033\tkl:   26.383928\n",
      "Epoch: 854 [20100/50000 (40%)]  \tLoss:   93.518120\trec:   66.830864\tkl:   26.687250\n",
      "Epoch: 854 [30100/50000 (60%)]  \tLoss:   90.657585\trec:   63.575817\tkl:   27.081766\n",
      "Epoch: 854 [40100/50000 (80%)]  \tLoss:   91.955109\trec:   65.215767\tkl:   26.739342\n",
      "====> Epoch: 854 Average train loss: 90.4879\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6054\n",
      "Epoch: 855 [  100/50000 ( 0%)]  \tLoss:   88.022812\trec:   62.309242\tkl:   25.713564\n",
      "Epoch: 855 [10100/50000 (20%)]  \tLoss:   86.846367\trec:   61.150429\tkl:   25.695932\n",
      "Epoch: 855 [20100/50000 (40%)]  \tLoss:   90.335747\trec:   63.976631\tkl:   26.359123\n",
      "Epoch: 855 [30100/50000 (60%)]  \tLoss:   94.653687\trec:   67.682472\tkl:   26.971226\n",
      "Epoch: 855 [40100/50000 (80%)]  \tLoss:   89.831131\trec:   63.474724\tkl:   26.356401\n",
      "====> Epoch: 855 Average train loss: 90.4776\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6182\n",
      "Epoch: 856 [  100/50000 ( 0%)]  \tLoss:   90.766388\trec:   64.667297\tkl:   26.099091\n",
      "Epoch: 856 [10100/50000 (20%)]  \tLoss:   91.683815\trec:   66.305832\tkl:   25.377987\n",
      "Epoch: 856 [20100/50000 (40%)]  \tLoss:   88.130661\trec:   62.702759\tkl:   25.427904\n",
      "Epoch: 856 [30100/50000 (60%)]  \tLoss:   93.724892\trec:   66.669289\tkl:   27.055599\n",
      "Epoch: 856 [40100/50000 (80%)]  \tLoss:   89.975006\trec:   63.351044\tkl:   26.623966\n",
      "====> Epoch: 856 Average train loss: 90.4803\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5518\n",
      "Epoch: 857 [  100/50000 ( 0%)]  \tLoss:   89.218132\trec:   62.685291\tkl:   26.532839\n",
      "Epoch: 857 [10100/50000 (20%)]  \tLoss:   91.224792\trec:   65.231194\tkl:   25.993605\n",
      "Epoch: 857 [20100/50000 (40%)]  \tLoss:   90.159775\trec:   64.365562\tkl:   25.794210\n",
      "Epoch: 857 [30100/50000 (60%)]  \tLoss:   88.058769\trec:   61.845604\tkl:   26.213165\n",
      "Epoch: 857 [40100/50000 (80%)]  \tLoss:   92.875221\trec:   65.580162\tkl:   27.295057\n",
      "====> Epoch: 857 Average train loss: 90.5025\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6105\n",
      "Epoch: 858 [  100/50000 ( 0%)]  \tLoss:   91.390625\trec:   64.534172\tkl:   26.856459\n",
      "Epoch: 858 [10100/50000 (20%)]  \tLoss:   88.689629\trec:   62.366299\tkl:   26.323324\n",
      "Epoch: 858 [20100/50000 (40%)]  \tLoss:   89.461502\trec:   63.824200\tkl:   25.637302\n",
      "Epoch: 858 [30100/50000 (60%)]  \tLoss:   95.608986\trec:   67.941170\tkl:   27.667809\n",
      "Epoch: 858 [40100/50000 (80%)]  \tLoss:   93.914467\trec:   66.362877\tkl:   27.551586\n",
      "====> Epoch: 858 Average train loss: 90.4737\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6357\n",
      "Epoch: 859 [  100/50000 ( 0%)]  \tLoss:   90.714142\trec:   64.180046\tkl:   26.534086\n",
      "Epoch: 859 [10100/50000 (20%)]  \tLoss:   92.188766\trec:   65.885796\tkl:   26.302973\n",
      "Epoch: 859 [20100/50000 (40%)]  \tLoss:   87.483627\trec:   62.843426\tkl:   24.640207\n",
      "Epoch: 859 [30100/50000 (60%)]  \tLoss:   89.116043\trec:   63.165466\tkl:   25.950579\n",
      "Epoch: 859 [40100/50000 (80%)]  \tLoss:   91.447227\trec:   65.316513\tkl:   26.130711\n",
      "====> Epoch: 859 Average train loss: 90.4795\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5943\n",
      "Epoch: 860 [  100/50000 ( 0%)]  \tLoss:   92.268341\trec:   65.521561\tkl:   26.746777\n",
      "Epoch: 860 [10100/50000 (20%)]  \tLoss:   88.843727\trec:   62.421009\tkl:   26.422714\n",
      "Epoch: 860 [20100/50000 (40%)]  \tLoss:   88.770126\trec:   62.167736\tkl:   26.602386\n",
      "Epoch: 860 [30100/50000 (60%)]  \tLoss:   93.079994\trec:   65.903435\tkl:   27.176567\n",
      "Epoch: 860 [40100/50000 (80%)]  \tLoss:   94.514229\trec:   67.467026\tkl:   27.047199\n",
      "====> Epoch: 860 Average train loss: 90.4717\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5675\n",
      "Epoch: 861 [  100/50000 ( 0%)]  \tLoss:   90.030449\trec:   63.730888\tkl:   26.299557\n",
      "Epoch: 861 [10100/50000 (20%)]  \tLoss:   90.512909\trec:   63.995808\tkl:   26.517099\n",
      "Epoch: 861 [20100/50000 (40%)]  \tLoss:   92.942467\trec:   66.063972\tkl:   26.878492\n",
      "Epoch: 861 [30100/50000 (60%)]  \tLoss:   87.941948\trec:   61.849743\tkl:   26.092207\n",
      "Epoch: 861 [40100/50000 (80%)]  \tLoss:   92.501831\trec:   66.288849\tkl:   26.212988\n",
      "====> Epoch: 861 Average train loss: 90.4499\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6008\n",
      "Epoch: 862 [  100/50000 ( 0%)]  \tLoss:   89.229286\trec:   62.688366\tkl:   26.540915\n",
      "Epoch: 862 [10100/50000 (20%)]  \tLoss:   93.411446\trec:   66.043571\tkl:   27.367872\n",
      "Epoch: 862 [20100/50000 (40%)]  \tLoss:   89.640213\trec:   63.571968\tkl:   26.068241\n",
      "Epoch: 862 [30100/50000 (60%)]  \tLoss:   93.706345\trec:   67.119072\tkl:   26.587267\n",
      "Epoch: 862 [40100/50000 (80%)]  \tLoss:   91.185707\trec:   65.178223\tkl:   26.007490\n",
      "====> Epoch: 862 Average train loss: 90.4622\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6031\n",
      "Epoch: 863 [  100/50000 ( 0%)]  \tLoss:   90.395370\trec:   64.099403\tkl:   26.295961\n",
      "Epoch: 863 [10100/50000 (20%)]  \tLoss:   87.970200\trec:   62.162292\tkl:   25.807913\n",
      "Epoch: 863 [20100/50000 (40%)]  \tLoss:   91.333458\trec:   65.100510\tkl:   26.232946\n",
      "Epoch: 863 [30100/50000 (60%)]  \tLoss:   88.407059\trec:   63.012928\tkl:   25.394133\n",
      "Epoch: 863 [40100/50000 (80%)]  \tLoss:   89.620239\trec:   62.764019\tkl:   26.856228\n",
      "====> Epoch: 863 Average train loss: 90.4817\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5615\n",
      "Epoch: 864 [  100/50000 ( 0%)]  \tLoss:   89.614746\trec:   63.178024\tkl:   26.436714\n",
      "Epoch: 864 [10100/50000 (20%)]  \tLoss:   87.584610\trec:   61.353554\tkl:   26.231056\n",
      "Epoch: 864 [20100/50000 (40%)]  \tLoss:   88.848495\trec:   62.728222\tkl:   26.120275\n",
      "Epoch: 864 [30100/50000 (60%)]  \tLoss:   87.264938\trec:   61.174900\tkl:   26.090040\n",
      "Epoch: 864 [40100/50000 (80%)]  \tLoss:   92.069519\trec:   65.505402\tkl:   26.564121\n",
      "====> Epoch: 864 Average train loss: 90.4550\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5877\n",
      "Epoch: 865 [  100/50000 ( 0%)]  \tLoss:   87.104668\trec:   61.070526\tkl:   26.034140\n",
      "Epoch: 865 [10100/50000 (20%)]  \tLoss:   89.231384\trec:   63.430603\tkl:   25.800781\n",
      "Epoch: 865 [20100/50000 (40%)]  \tLoss:   93.145096\trec:   65.910927\tkl:   27.234169\n",
      "Epoch: 865 [30100/50000 (60%)]  \tLoss:   90.472878\trec:   64.487350\tkl:   25.985525\n",
      "Epoch: 865 [40100/50000 (80%)]  \tLoss:   88.147324\trec:   62.239971\tkl:   25.907351\n",
      "====> Epoch: 865 Average train loss: 90.4884\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5728\n",
      "Epoch: 866 [  100/50000 ( 0%)]  \tLoss:   95.469803\trec:   68.427765\tkl:   27.042036\n",
      "Epoch: 866 [10100/50000 (20%)]  \tLoss:   88.654152\trec:   62.872509\tkl:   25.781637\n",
      "Epoch: 866 [20100/50000 (40%)]  \tLoss:   97.247803\trec:   68.859146\tkl:   28.388651\n",
      "Epoch: 866 [30100/50000 (60%)]  \tLoss:   92.338921\trec:   65.449127\tkl:   26.889793\n",
      "Epoch: 866 [40100/50000 (80%)]  \tLoss:   94.911110\trec:   67.553658\tkl:   27.357460\n",
      "====> Epoch: 866 Average train loss: 90.4589\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5310\n",
      "Epoch: 867 [  100/50000 ( 0%)]  \tLoss:   89.800079\trec:   64.150490\tkl:   25.649580\n",
      "Epoch: 867 [10100/50000 (20%)]  \tLoss:   92.813576\trec:   66.038139\tkl:   26.775427\n",
      "Epoch: 867 [20100/50000 (40%)]  \tLoss:   86.585991\trec:   61.477428\tkl:   25.108568\n",
      "Epoch: 867 [30100/50000 (60%)]  \tLoss:   89.872017\trec:   63.940918\tkl:   25.931101\n",
      "Epoch: 867 [40100/50000 (80%)]  \tLoss:   91.713234\trec:   65.774002\tkl:   25.939226\n",
      "====> Epoch: 867 Average train loss: 90.4569\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6535\n",
      "Epoch: 868 [  100/50000 ( 0%)]  \tLoss:   95.635231\trec:   67.777588\tkl:   27.857641\n",
      "Epoch: 868 [10100/50000 (20%)]  \tLoss:   87.382866\trec:   61.189373\tkl:   26.193491\n",
      "Epoch: 868 [20100/50000 (40%)]  \tLoss:   95.730873\trec:   68.822075\tkl:   26.908800\n",
      "Epoch: 868 [30100/50000 (60%)]  \tLoss:   88.055565\trec:   62.203918\tkl:   25.851646\n",
      "Epoch: 868 [40100/50000 (80%)]  \tLoss:   89.987289\trec:   63.661716\tkl:   26.325575\n",
      "====> Epoch: 868 Average train loss: 90.4476\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6090\n",
      "Epoch: 869 [  100/50000 ( 0%)]  \tLoss:   88.676910\trec:   62.583309\tkl:   26.093597\n",
      "Epoch: 869 [10100/50000 (20%)]  \tLoss:   93.680275\trec:   66.780739\tkl:   26.899527\n",
      "Epoch: 869 [20100/50000 (40%)]  \tLoss:   94.172890\trec:   67.623230\tkl:   26.549658\n",
      "Epoch: 869 [30100/50000 (60%)]  \tLoss:   87.935661\trec:   62.110706\tkl:   25.824955\n",
      "Epoch: 869 [40100/50000 (80%)]  \tLoss:   89.951065\trec:   63.926186\tkl:   26.024878\n",
      "====> Epoch: 869 Average train loss: 90.4700\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5790\n",
      "Epoch: 870 [  100/50000 ( 0%)]  \tLoss:   90.331718\trec:   64.553902\tkl:   25.777819\n",
      "Epoch: 870 [10100/50000 (20%)]  \tLoss:   89.497253\trec:   63.142159\tkl:   26.355099\n",
      "Epoch: 870 [20100/50000 (40%)]  \tLoss:   93.494835\trec:   66.002525\tkl:   27.492304\n",
      "Epoch: 870 [30100/50000 (60%)]  \tLoss:   95.183182\trec:   68.650024\tkl:   26.533155\n",
      "Epoch: 870 [40100/50000 (80%)]  \tLoss:   89.745171\trec:   63.287884\tkl:   26.457287\n",
      "====> Epoch: 870 Average train loss: 90.4202\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5895\n",
      "Epoch: 871 [  100/50000 ( 0%)]  \tLoss:   90.914101\trec:   64.344307\tkl:   26.569792\n",
      "Epoch: 871 [10100/50000 (20%)]  \tLoss:   89.280724\trec:   63.007172\tkl:   26.273550\n",
      "Epoch: 871 [20100/50000 (40%)]  \tLoss:   91.396500\trec:   65.220558\tkl:   26.175947\n",
      "Epoch: 871 [30100/50000 (60%)]  \tLoss:   91.586113\trec:   65.573975\tkl:   26.012136\n",
      "Epoch: 871 [40100/50000 (80%)]  \tLoss:   89.873390\trec:   63.488472\tkl:   26.384914\n",
      "====> Epoch: 871 Average train loss: 90.4530\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5334\n",
      "Epoch: 872 [  100/50000 ( 0%)]  \tLoss:   85.824043\trec:   60.613045\tkl:   25.210999\n",
      "Epoch: 872 [10100/50000 (20%)]  \tLoss:   90.568634\trec:   64.770805\tkl:   25.797825\n",
      "Epoch: 872 [20100/50000 (40%)]  \tLoss:   93.088882\trec:   66.538048\tkl:   26.550835\n",
      "Epoch: 872 [30100/50000 (60%)]  \tLoss:   88.393280\trec:   62.756470\tkl:   25.636816\n",
      "Epoch: 872 [40100/50000 (80%)]  \tLoss:   90.520073\trec:   64.322159\tkl:   26.197924\n",
      "====> Epoch: 872 Average train loss: 90.4300\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5845\n",
      "Epoch: 873 [  100/50000 ( 0%)]  \tLoss:   90.479897\trec:   64.050415\tkl:   26.429483\n",
      "Epoch: 873 [10100/50000 (20%)]  \tLoss:   92.997818\trec:   66.541855\tkl:   26.455967\n",
      "Epoch: 873 [20100/50000 (40%)]  \tLoss:   90.265320\trec:   63.552551\tkl:   26.712770\n",
      "Epoch: 873 [30100/50000 (60%)]  \tLoss:   91.220718\trec:   64.313622\tkl:   26.907097\n",
      "Epoch: 873 [40100/50000 (80%)]  \tLoss:   94.300835\trec:   67.769424\tkl:   26.531412\n",
      "====> Epoch: 873 Average train loss: 90.4356\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4321\n",
      "Epoch: 874 [  100/50000 ( 0%)]  \tLoss:   89.839355\trec:   63.353466\tkl:   26.485888\n",
      "Epoch: 874 [10100/50000 (20%)]  \tLoss:   90.489166\trec:   63.847679\tkl:   26.641489\n",
      "Epoch: 874 [20100/50000 (40%)]  \tLoss:   89.653725\trec:   63.431171\tkl:   26.222555\n",
      "Epoch: 874 [30100/50000 (60%)]  \tLoss:   93.068298\trec:   66.176949\tkl:   26.891348\n",
      "Epoch: 874 [40100/50000 (80%)]  \tLoss:   89.448158\trec:   63.619919\tkl:   25.828241\n",
      "====> Epoch: 874 Average train loss: 90.4500\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6007\n",
      "Epoch: 875 [  100/50000 ( 0%)]  \tLoss:   87.327621\trec:   62.085266\tkl:   25.242355\n",
      "Epoch: 875 [10100/50000 (20%)]  \tLoss:   88.642479\trec:   62.240864\tkl:   26.401615\n",
      "Epoch: 875 [20100/50000 (40%)]  \tLoss:   90.080444\trec:   63.326462\tkl:   26.753979\n",
      "Epoch: 875 [30100/50000 (60%)]  \tLoss:   92.207703\trec:   66.029343\tkl:   26.178366\n",
      "Epoch: 875 [40100/50000 (80%)]  \tLoss:   94.392334\trec:   68.240265\tkl:   26.152067\n",
      "====> Epoch: 875 Average train loss: 90.4261\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6482\n",
      "Epoch: 876 [  100/50000 ( 0%)]  \tLoss:   85.441193\trec:   60.713699\tkl:   24.727491\n",
      "Epoch: 876 [10100/50000 (20%)]  \tLoss:   90.276680\trec:   64.031471\tkl:   26.245205\n",
      "Epoch: 876 [20100/50000 (40%)]  \tLoss:   93.203896\trec:   66.160690\tkl:   27.043209\n",
      "Epoch: 876 [30100/50000 (60%)]  \tLoss:   89.180870\trec:   63.253830\tkl:   25.927032\n",
      "Epoch: 876 [40100/50000 (80%)]  \tLoss:   84.882195\trec:   59.924297\tkl:   24.957899\n",
      "====> Epoch: 876 Average train loss: 90.4362\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6006\n",
      "Epoch: 877 [  100/50000 ( 0%)]  \tLoss:   93.839081\trec:   67.066315\tkl:   26.772770\n",
      "Epoch: 877 [10100/50000 (20%)]  \tLoss:   89.055176\trec:   62.816414\tkl:   26.238764\n",
      "Epoch: 877 [20100/50000 (40%)]  \tLoss:   93.968666\trec:   65.842812\tkl:   28.125864\n",
      "Epoch: 877 [30100/50000 (60%)]  \tLoss:   90.263191\trec:   63.766449\tkl:   26.496740\n",
      "Epoch: 877 [40100/50000 (80%)]  \tLoss:   91.655373\trec:   65.149254\tkl:   26.506115\n",
      "====> Epoch: 877 Average train loss: 90.4187\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6512\n",
      "Epoch: 878 [  100/50000 ( 0%)]  \tLoss:   89.029381\trec:   62.027088\tkl:   27.002293\n",
      "Epoch: 878 [10100/50000 (20%)]  \tLoss:   89.523376\trec:   64.245628\tkl:   25.277750\n",
      "Epoch: 878 [20100/50000 (40%)]  \tLoss:   89.082527\trec:   63.408520\tkl:   25.674006\n",
      "Epoch: 878 [30100/50000 (60%)]  \tLoss:   91.396744\trec:   65.437897\tkl:   25.958845\n",
      "Epoch: 878 [40100/50000 (80%)]  \tLoss:   95.746826\trec:   69.155708\tkl:   26.591118\n",
      "====> Epoch: 878 Average train loss: 90.4197\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6448\n",
      "Epoch: 879 [  100/50000 ( 0%)]  \tLoss:   92.237312\trec:   65.220161\tkl:   27.017151\n",
      "Epoch: 879 [10100/50000 (20%)]  \tLoss:   90.315407\trec:   64.388283\tkl:   25.927134\n",
      "Epoch: 879 [20100/50000 (40%)]  \tLoss:   91.213661\trec:   64.638702\tkl:   26.574963\n",
      "Epoch: 879 [30100/50000 (60%)]  \tLoss:   95.410057\trec:   67.783691\tkl:   27.626362\n",
      "Epoch: 879 [40100/50000 (80%)]  \tLoss:   88.009819\trec:   62.270065\tkl:   25.739758\n",
      "====> Epoch: 879 Average train loss: 90.4359\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6072\n",
      "Epoch: 880 [  100/50000 ( 0%)]  \tLoss:   89.949608\trec:   63.761971\tkl:   26.187634\n",
      "Epoch: 880 [10100/50000 (20%)]  \tLoss:   89.224060\trec:   63.898029\tkl:   25.326025\n",
      "Epoch: 880 [20100/50000 (40%)]  \tLoss:   94.161285\trec:   68.267769\tkl:   25.893513\n",
      "Epoch: 880 [30100/50000 (60%)]  \tLoss:   89.279961\trec:   63.940212\tkl:   25.339750\n",
      "Epoch: 880 [40100/50000 (80%)]  \tLoss:   87.391075\trec:   61.689800\tkl:   25.701273\n",
      "====> Epoch: 880 Average train loss: 90.4258\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5919\n",
      "Epoch: 881 [  100/50000 ( 0%)]  \tLoss:   93.204666\trec:   65.636223\tkl:   27.568451\n",
      "Epoch: 881 [10100/50000 (20%)]  \tLoss:   89.512917\trec:   62.909763\tkl:   26.603153\n",
      "Epoch: 881 [20100/50000 (40%)]  \tLoss:   87.821053\trec:   62.462238\tkl:   25.358810\n",
      "Epoch: 881 [30100/50000 (60%)]  \tLoss:   90.214569\trec:   63.979832\tkl:   26.234737\n",
      "Epoch: 881 [40100/50000 (80%)]  \tLoss:   91.394150\trec:   64.079117\tkl:   27.315025\n",
      "====> Epoch: 881 Average train loss: 90.4207\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6751\n",
      "Epoch: 882 [  100/50000 ( 0%)]  \tLoss:   89.521095\trec:   63.901978\tkl:   25.619120\n",
      "Epoch: 882 [10100/50000 (20%)]  \tLoss:   88.507080\trec:   62.468533\tkl:   26.038548\n",
      "Epoch: 882 [20100/50000 (40%)]  \tLoss:   88.226326\trec:   63.409779\tkl:   24.816542\n",
      "Epoch: 882 [30100/50000 (60%)]  \tLoss:   92.127441\trec:   65.351189\tkl:   26.776255\n",
      "Epoch: 882 [40100/50000 (80%)]  \tLoss:   86.688591\trec:   60.042725\tkl:   26.645866\n",
      "====> Epoch: 882 Average train loss: 90.4224\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5031\n",
      "Epoch: 883 [  100/50000 ( 0%)]  \tLoss:   90.369850\trec:   64.170235\tkl:   26.199615\n",
      "Epoch: 883 [10100/50000 (20%)]  \tLoss:   90.801659\trec:   65.583282\tkl:   25.218384\n",
      "Epoch: 883 [20100/50000 (40%)]  \tLoss:   93.695709\trec:   66.973175\tkl:   26.722538\n",
      "Epoch: 883 [30100/50000 (60%)]  \tLoss:   90.934647\trec:   65.127495\tkl:   25.807152\n",
      "Epoch: 883 [40100/50000 (80%)]  \tLoss:   88.305695\trec:   62.050312\tkl:   26.255379\n",
      "====> Epoch: 883 Average train loss: 90.4291\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6333\n",
      "Epoch: 884 [  100/50000 ( 0%)]  \tLoss:   92.220779\trec:   65.732048\tkl:   26.488726\n",
      "Epoch: 884 [10100/50000 (20%)]  \tLoss:   92.580254\trec:   66.215027\tkl:   26.365221\n",
      "Epoch: 884 [20100/50000 (40%)]  \tLoss:   91.654625\trec:   64.702820\tkl:   26.951805\n",
      "Epoch: 884 [30100/50000 (60%)]  \tLoss:   89.314926\trec:   63.068592\tkl:   26.246340\n",
      "Epoch: 884 [40100/50000 (80%)]  \tLoss:   91.393730\trec:   64.536064\tkl:   26.857660\n",
      "====> Epoch: 884 Average train loss: 90.4179\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6009\n",
      "Epoch: 885 [  100/50000 ( 0%)]  \tLoss:   89.782997\trec:   64.109138\tkl:   25.673859\n",
      "Epoch: 885 [10100/50000 (20%)]  \tLoss:   94.490997\trec:   67.544487\tkl:   26.946508\n",
      "Epoch: 885 [20100/50000 (40%)]  \tLoss:   92.206520\trec:   65.205788\tkl:   27.000736\n",
      "Epoch: 885 [30100/50000 (60%)]  \tLoss:   90.642731\trec:   63.806278\tkl:   26.836460\n",
      "Epoch: 885 [40100/50000 (80%)]  \tLoss:   93.090858\trec:   66.498611\tkl:   26.592245\n",
      "====> Epoch: 885 Average train loss: 90.3941\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5570\n",
      "Epoch: 886 [  100/50000 ( 0%)]  \tLoss:   89.071617\trec:   63.194065\tkl:   25.877558\n",
      "Epoch: 886 [10100/50000 (20%)]  \tLoss:   87.700508\trec:   61.852242\tkl:   25.848270\n",
      "Epoch: 886 [20100/50000 (40%)]  \tLoss:   89.886108\trec:   63.531601\tkl:   26.354513\n",
      "Epoch: 886 [30100/50000 (60%)]  \tLoss:   90.500031\trec:   64.470924\tkl:   26.029104\n",
      "Epoch: 886 [40100/50000 (80%)]  \tLoss:   91.397934\trec:   65.171333\tkl:   26.226603\n",
      "====> Epoch: 886 Average train loss: 90.3900\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5941\n",
      "Epoch: 887 [  100/50000 ( 0%)]  \tLoss:   90.522301\trec:   62.819870\tkl:   27.702431\n",
      "Epoch: 887 [10100/50000 (20%)]  \tLoss:   92.463783\trec:   65.579811\tkl:   26.883968\n",
      "Epoch: 887 [20100/50000 (40%)]  \tLoss:   88.097580\trec:   61.249371\tkl:   26.848202\n",
      "Epoch: 887 [30100/50000 (60%)]  \tLoss:   91.332848\trec:   64.863228\tkl:   26.469624\n",
      "Epoch: 887 [40100/50000 (80%)]  \tLoss:   88.168755\trec:   62.358582\tkl:   25.810173\n",
      "====> Epoch: 887 Average train loss: 90.4369\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5142\n",
      "Epoch: 888 [  100/50000 ( 0%)]  \tLoss:   88.200714\trec:   62.624149\tkl:   25.576565\n",
      "Epoch: 888 [10100/50000 (20%)]  \tLoss:   87.011314\trec:   60.935459\tkl:   26.075859\n",
      "Epoch: 888 [20100/50000 (40%)]  \tLoss:   91.178833\trec:   64.633804\tkl:   26.545029\n",
      "Epoch: 888 [30100/50000 (60%)]  \tLoss:   93.015350\trec:   66.933617\tkl:   26.081734\n",
      "Epoch: 888 [40100/50000 (80%)]  \tLoss:   89.489449\trec:   64.060020\tkl:   25.429430\n",
      "====> Epoch: 888 Average train loss: 90.4162\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5600\n",
      "Epoch: 889 [  100/50000 ( 0%)]  \tLoss:   87.804939\trec:   61.465221\tkl:   26.339716\n",
      "Epoch: 889 [10100/50000 (20%)]  \tLoss:   91.137741\trec:   64.328964\tkl:   26.808781\n",
      "Epoch: 889 [20100/50000 (40%)]  \tLoss:   91.010040\trec:   63.715378\tkl:   27.294662\n",
      "Epoch: 889 [30100/50000 (60%)]  \tLoss:   88.529716\trec:   62.068398\tkl:   26.461315\n",
      "Epoch: 889 [40100/50000 (80%)]  \tLoss:   90.361900\trec:   63.790760\tkl:   26.571140\n",
      "====> Epoch: 889 Average train loss: 90.3916\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6053\n",
      "Epoch: 890 [  100/50000 ( 0%)]  \tLoss:   93.157234\trec:   66.296532\tkl:   26.860703\n",
      "Epoch: 890 [10100/50000 (20%)]  \tLoss:   90.563263\trec:   64.501045\tkl:   26.062216\n",
      "Epoch: 890 [20100/50000 (40%)]  \tLoss:   87.001320\trec:   61.761829\tkl:   25.239489\n",
      "Epoch: 890 [30100/50000 (60%)]  \tLoss:   94.497490\trec:   67.234634\tkl:   27.262852\n",
      "Epoch: 890 [40100/50000 (80%)]  \tLoss:   92.413689\trec:   65.050354\tkl:   27.363335\n",
      "====> Epoch: 890 Average train loss: 90.3905\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5958\n",
      "Epoch: 891 [  100/50000 ( 0%)]  \tLoss:   93.392509\trec:   66.050858\tkl:   27.341652\n",
      "Epoch: 891 [10100/50000 (20%)]  \tLoss:   92.012794\trec:   65.399323\tkl:   26.613472\n",
      "Epoch: 891 [20100/50000 (40%)]  \tLoss:   91.674370\trec:   65.253914\tkl:   26.420456\n",
      "Epoch: 891 [30100/50000 (60%)]  \tLoss:   92.788788\trec:   67.061081\tkl:   25.727705\n",
      "Epoch: 891 [40100/50000 (80%)]  \tLoss:   89.890572\trec:   63.933830\tkl:   25.956739\n",
      "====> Epoch: 891 Average train loss: 90.3870\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5764\n",
      "Epoch: 892 [  100/50000 ( 0%)]  \tLoss:   87.505615\trec:   60.963291\tkl:   26.542324\n",
      "Epoch: 892 [10100/50000 (20%)]  \tLoss:   88.644165\trec:   62.510456\tkl:   26.133713\n",
      "Epoch: 892 [20100/50000 (40%)]  \tLoss:   94.790443\trec:   67.483757\tkl:   27.306698\n",
      "Epoch: 892 [30100/50000 (60%)]  \tLoss:   90.715073\trec:   64.260529\tkl:   26.454556\n",
      "Epoch: 892 [40100/50000 (80%)]  \tLoss:   91.988487\trec:   65.448166\tkl:   26.540314\n",
      "====> Epoch: 892 Average train loss: 90.4079\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5952\n",
      "Epoch: 893 [  100/50000 ( 0%)]  \tLoss:   88.916267\trec:   62.701653\tkl:   26.214619\n",
      "Epoch: 893 [10100/50000 (20%)]  \tLoss:   87.005455\trec:   61.726074\tkl:   25.279383\n",
      "Epoch: 893 [20100/50000 (40%)]  \tLoss:   94.348007\trec:   67.170250\tkl:   27.177752\n",
      "Epoch: 893 [30100/50000 (60%)]  \tLoss:   87.982994\trec:   61.882629\tkl:   26.100365\n",
      "Epoch: 893 [40100/50000 (80%)]  \tLoss:   89.363335\trec:   63.728729\tkl:   25.634605\n",
      "====> Epoch: 893 Average train loss: 90.4066\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6643\n",
      "Epoch: 894 [  100/50000 ( 0%)]  \tLoss:   88.865250\trec:   63.576157\tkl:   25.289101\n",
      "Epoch: 894 [10100/50000 (20%)]  \tLoss:   89.737282\trec:   64.808990\tkl:   24.928293\n",
      "Epoch: 894 [20100/50000 (40%)]  \tLoss:   93.562340\trec:   66.904045\tkl:   26.658295\n",
      "Epoch: 894 [30100/50000 (60%)]  \tLoss:   88.242638\trec:   62.683601\tkl:   25.559038\n",
      "Epoch: 894 [40100/50000 (80%)]  \tLoss:   93.522598\trec:   66.915245\tkl:   26.607353\n",
      "====> Epoch: 894 Average train loss: 90.3960\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5869\n",
      "Epoch: 895 [  100/50000 ( 0%)]  \tLoss:   94.111778\trec:   66.507507\tkl:   27.604265\n",
      "Epoch: 895 [10100/50000 (20%)]  \tLoss:   87.858986\trec:   62.297718\tkl:   25.561262\n",
      "Epoch: 895 [20100/50000 (40%)]  \tLoss:   91.057777\trec:   64.543060\tkl:   26.514719\n",
      "Epoch: 895 [30100/50000 (60%)]  \tLoss:   88.095917\trec:   62.458328\tkl:   25.637587\n",
      "Epoch: 895 [40100/50000 (80%)]  \tLoss:   89.129936\trec:   63.358711\tkl:   25.771231\n",
      "====> Epoch: 895 Average train loss: 90.3768\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5653\n",
      "Epoch: 896 [  100/50000 ( 0%)]  \tLoss:   94.374825\trec:   67.632416\tkl:   26.742405\n",
      "Epoch: 896 [10100/50000 (20%)]  \tLoss:   87.332314\trec:   61.854515\tkl:   25.477797\n",
      "Epoch: 896 [20100/50000 (40%)]  \tLoss:   88.586815\trec:   62.832928\tkl:   25.753881\n",
      "Epoch: 896 [30100/50000 (60%)]  \tLoss:   87.320915\trec:   61.888905\tkl:   25.432011\n",
      "Epoch: 896 [40100/50000 (80%)]  \tLoss:   92.173279\trec:   65.234184\tkl:   26.939091\n",
      "====> Epoch: 896 Average train loss: 90.3938\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5225\n",
      "Epoch: 897 [  100/50000 ( 0%)]  \tLoss:   89.896416\trec:   63.263500\tkl:   26.632917\n",
      "Epoch: 897 [10100/50000 (20%)]  \tLoss:   86.237274\trec:   60.707577\tkl:   25.529694\n",
      "Epoch: 897 [20100/50000 (40%)]  \tLoss:   89.225624\trec:   63.119900\tkl:   26.105717\n",
      "Epoch: 897 [30100/50000 (60%)]  \tLoss:   95.520027\trec:   69.115105\tkl:   26.404919\n",
      "Epoch: 897 [40100/50000 (80%)]  \tLoss:   88.703003\trec:   63.252186\tkl:   25.450823\n",
      "====> Epoch: 897 Average train loss: 90.3802\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6008\n",
      "Epoch: 898 [  100/50000 ( 0%)]  \tLoss:   90.987442\trec:   64.862778\tkl:   26.124660\n",
      "Epoch: 898 [10100/50000 (20%)]  \tLoss:   90.267456\trec:   63.529804\tkl:   26.737650\n",
      "Epoch: 898 [20100/50000 (40%)]  \tLoss:   92.583572\trec:   64.891602\tkl:   27.691969\n",
      "Epoch: 898 [30100/50000 (60%)]  \tLoss:   89.985039\trec:   64.642204\tkl:   25.342827\n",
      "Epoch: 898 [40100/50000 (80%)]  \tLoss:   94.301910\trec:   66.541412\tkl:   27.760502\n",
      "====> Epoch: 898 Average train loss: 90.3973\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5553\n",
      "Epoch: 899 [  100/50000 ( 0%)]  \tLoss:   88.858543\trec:   62.335594\tkl:   26.522951\n",
      "Epoch: 899 [10100/50000 (20%)]  \tLoss:   91.674255\trec:   65.173660\tkl:   26.500589\n",
      "Epoch: 899 [20100/50000 (40%)]  \tLoss:   89.273575\trec:   63.394489\tkl:   25.879080\n",
      "Epoch: 899 [30100/50000 (60%)]  \tLoss:   87.418083\trec:   61.774887\tkl:   25.643202\n",
      "Epoch: 899 [40100/50000 (80%)]  \tLoss:   87.959732\trec:   62.751022\tkl:   25.208710\n",
      "====> Epoch: 899 Average train loss: 90.3829\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5858\n",
      "Epoch: 900 [  100/50000 ( 0%)]  \tLoss:   89.490448\trec:   63.409519\tkl:   26.080923\n",
      "Epoch: 900 [10100/50000 (20%)]  \tLoss:   87.857292\trec:   62.030262\tkl:   25.827034\n",
      "Epoch: 900 [20100/50000 (40%)]  \tLoss:   91.272110\trec:   64.808708\tkl:   26.463396\n",
      "Epoch: 900 [30100/50000 (60%)]  \tLoss:   85.671486\trec:   60.868652\tkl:   24.802834\n",
      "Epoch: 900 [40100/50000 (80%)]  \tLoss:   87.655136\trec:   61.757900\tkl:   25.897230\n",
      "====> Epoch: 900 Average train loss: 90.3951\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5535\n",
      "Epoch: 901 [  100/50000 ( 0%)]  \tLoss:   96.249062\trec:   69.350655\tkl:   26.898407\n",
      "Epoch: 901 [10100/50000 (20%)]  \tLoss:   91.819489\trec:   65.121277\tkl:   26.698212\n",
      "Epoch: 901 [20100/50000 (40%)]  \tLoss:   85.535271\trec:   59.499878\tkl:   26.035393\n",
      "Epoch: 901 [30100/50000 (60%)]  \tLoss:   88.720215\trec:   62.682621\tkl:   26.037592\n",
      "Epoch: 901 [40100/50000 (80%)]  \tLoss:   93.202019\trec:   65.560280\tkl:   27.641741\n",
      "====> Epoch: 901 Average train loss: 90.3613\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5338\n",
      "Epoch: 902 [  100/50000 ( 0%)]  \tLoss:   87.705284\trec:   61.964432\tkl:   25.740852\n",
      "Epoch: 902 [10100/50000 (20%)]  \tLoss:   89.819527\trec:   63.483208\tkl:   26.336317\n",
      "Epoch: 902 [20100/50000 (40%)]  \tLoss:   90.751991\trec:   64.485466\tkl:   26.266525\n",
      "Epoch: 902 [30100/50000 (60%)]  \tLoss:   89.962555\trec:   63.784042\tkl:   26.178520\n",
      "Epoch: 902 [40100/50000 (80%)]  \tLoss:   89.518066\trec:   63.336010\tkl:   26.182058\n",
      "====> Epoch: 902 Average train loss: 90.3711\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5361\n",
      "Epoch: 903 [  100/50000 ( 0%)]  \tLoss:   86.839523\trec:   60.882069\tkl:   25.957453\n",
      "Epoch: 903 [10100/50000 (20%)]  \tLoss:   92.441948\trec:   65.874268\tkl:   26.567688\n",
      "Epoch: 903 [20100/50000 (40%)]  \tLoss:   86.121216\trec:   60.745815\tkl:   25.375404\n",
      "Epoch: 903 [30100/50000 (60%)]  \tLoss:   87.993332\trec:   61.981735\tkl:   26.011591\n",
      "Epoch: 903 [40100/50000 (80%)]  \tLoss:   90.237869\trec:   64.267448\tkl:   25.970421\n",
      "====> Epoch: 903 Average train loss: 90.3771\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5714\n",
      "Epoch: 904 [  100/50000 ( 0%)]  \tLoss:   90.578133\trec:   63.896736\tkl:   26.681398\n",
      "Epoch: 904 [10100/50000 (20%)]  \tLoss:   91.787514\trec:   65.387558\tkl:   26.399960\n",
      "Epoch: 904 [20100/50000 (40%)]  \tLoss:   87.737175\trec:   62.012859\tkl:   25.724314\n",
      "Epoch: 904 [30100/50000 (60%)]  \tLoss:   91.372833\trec:   64.328293\tkl:   27.044538\n",
      "Epoch: 904 [40100/50000 (80%)]  \tLoss:   90.910057\trec:   64.627647\tkl:   26.282413\n",
      "====> Epoch: 904 Average train loss: 90.3673\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6867\n",
      "Epoch: 905 [  100/50000 ( 0%)]  \tLoss:   88.376236\trec:   62.217796\tkl:   26.158440\n",
      "Epoch: 905 [10100/50000 (20%)]  \tLoss:   91.057381\trec:   64.809250\tkl:   26.248129\n",
      "Epoch: 905 [20100/50000 (40%)]  \tLoss:   94.997536\trec:   68.087563\tkl:   26.909969\n",
      "Epoch: 905 [30100/50000 (60%)]  \tLoss:   92.660919\trec:   66.199913\tkl:   26.461006\n",
      "Epoch: 905 [40100/50000 (80%)]  \tLoss:   86.213028\trec:   61.632004\tkl:   24.581020\n",
      "====> Epoch: 905 Average train loss: 90.3565\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4502\n",
      "Epoch: 906 [  100/50000 ( 0%)]  \tLoss:   90.874542\trec:   64.299957\tkl:   26.574577\n",
      "Epoch: 906 [10100/50000 (20%)]  \tLoss:   86.366066\trec:   60.344902\tkl:   26.021164\n",
      "Epoch: 906 [20100/50000 (40%)]  \tLoss:   90.142906\trec:   63.632187\tkl:   26.510721\n",
      "Epoch: 906 [30100/50000 (60%)]  \tLoss:   88.402946\trec:   62.943962\tkl:   25.458988\n",
      "Epoch: 906 [40100/50000 (80%)]  \tLoss:   94.267365\trec:   68.042458\tkl:   26.224907\n",
      "====> Epoch: 906 Average train loss: 90.3569\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5165\n",
      "Epoch: 907 [  100/50000 ( 0%)]  \tLoss:   91.434242\trec:   64.374557\tkl:   27.059689\n",
      "Epoch: 907 [10100/50000 (20%)]  \tLoss:   92.566635\trec:   66.539665\tkl:   26.026974\n",
      "Epoch: 907 [20100/50000 (40%)]  \tLoss:   91.098907\trec:   64.213737\tkl:   26.885176\n",
      "Epoch: 907 [30100/50000 (60%)]  \tLoss:   91.382645\trec:   65.017906\tkl:   26.364737\n",
      "Epoch: 907 [40100/50000 (80%)]  \tLoss:   86.559669\trec:   60.901459\tkl:   25.658207\n",
      "====> Epoch: 907 Average train loss: 90.3742\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6315\n",
      "Epoch: 908 [  100/50000 ( 0%)]  \tLoss:   89.651283\trec:   63.181000\tkl:   26.470293\n",
      "Epoch: 908 [10100/50000 (20%)]  \tLoss:   88.859535\trec:   62.162811\tkl:   26.696728\n",
      "Epoch: 908 [20100/50000 (40%)]  \tLoss:   89.444771\trec:   63.924229\tkl:   25.520548\n",
      "Epoch: 908 [30100/50000 (60%)]  \tLoss:   87.937866\trec:   62.393612\tkl:   25.544260\n",
      "Epoch: 908 [40100/50000 (80%)]  \tLoss:   89.574539\trec:   63.765427\tkl:   25.809113\n",
      "====> Epoch: 908 Average train loss: 90.3687\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4498\n",
      "Epoch: 909 [  100/50000 ( 0%)]  \tLoss:   89.413849\trec:   63.311230\tkl:   26.102612\n",
      "Epoch: 909 [10100/50000 (20%)]  \tLoss:   90.556847\trec:   63.508755\tkl:   27.048090\n",
      "Epoch: 909 [20100/50000 (40%)]  \tLoss:   86.586792\trec:   60.553242\tkl:   26.033554\n",
      "Epoch: 909 [30100/50000 (60%)]  \tLoss:   87.964531\trec:   61.931843\tkl:   26.032684\n",
      "Epoch: 909 [40100/50000 (80%)]  \tLoss:   88.687759\trec:   62.910248\tkl:   25.777512\n",
      "====> Epoch: 909 Average train loss: 90.3534\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5920\n",
      "Epoch: 910 [  100/50000 ( 0%)]  \tLoss:   91.114395\trec:   65.206993\tkl:   25.907402\n",
      "Epoch: 910 [10100/50000 (20%)]  \tLoss:   86.546768\trec:   60.732613\tkl:   25.814152\n",
      "Epoch: 910 [20100/50000 (40%)]  \tLoss:   90.018120\trec:   63.722431\tkl:   26.295698\n",
      "Epoch: 910 [30100/50000 (60%)]  \tLoss:   90.436798\trec:   64.078957\tkl:   26.357836\n",
      "Epoch: 910 [40100/50000 (80%)]  \tLoss:   88.612808\trec:   62.782528\tkl:   25.830288\n",
      "====> Epoch: 910 Average train loss: 90.3520\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5226\n",
      "Epoch: 911 [  100/50000 ( 0%)]  \tLoss:   85.983429\trec:   59.945057\tkl:   26.038370\n",
      "Epoch: 911 [10100/50000 (20%)]  \tLoss:   93.414581\trec:   67.241989\tkl:   26.172586\n",
      "Epoch: 911 [20100/50000 (40%)]  \tLoss:   89.096718\trec:   63.241062\tkl:   25.855648\n",
      "Epoch: 911 [30100/50000 (60%)]  \tLoss:   89.063553\trec:   62.327831\tkl:   26.735727\n",
      "Epoch: 911 [40100/50000 (80%)]  \tLoss:   94.503334\trec:   67.558128\tkl:   26.945211\n",
      "====> Epoch: 911 Average train loss: 90.3663\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5224\n",
      "Epoch: 912 [  100/50000 ( 0%)]  \tLoss:   89.506149\trec:   62.989414\tkl:   26.516737\n",
      "Epoch: 912 [10100/50000 (20%)]  \tLoss:   94.339615\trec:   67.230637\tkl:   27.108984\n",
      "Epoch: 912 [20100/50000 (40%)]  \tLoss:   92.267349\trec:   65.617966\tkl:   26.649391\n",
      "Epoch: 912 [30100/50000 (60%)]  \tLoss:   91.503075\trec:   65.396515\tkl:   26.106564\n",
      "Epoch: 912 [40100/50000 (80%)]  \tLoss:   92.064835\trec:   65.331688\tkl:   26.733141\n",
      "====> Epoch: 912 Average train loss: 90.3196\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5372\n",
      "Epoch: 913 [  100/50000 ( 0%)]  \tLoss:   93.942413\trec:   67.168907\tkl:   26.773502\n",
      "Epoch: 913 [10100/50000 (20%)]  \tLoss:   89.976601\trec:   63.232368\tkl:   26.744230\n",
      "Epoch: 913 [20100/50000 (40%)]  \tLoss:   94.076378\trec:   67.093506\tkl:   26.982872\n",
      "Epoch: 913 [30100/50000 (60%)]  \tLoss:   90.425964\trec:   63.762741\tkl:   26.663221\n",
      "Epoch: 913 [40100/50000 (80%)]  \tLoss:   89.694801\trec:   64.147003\tkl:   25.547798\n",
      "====> Epoch: 913 Average train loss: 90.3477\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5827\n",
      "Epoch: 914 [  100/50000 ( 0%)]  \tLoss:   90.914215\trec:   64.473083\tkl:   26.441135\n",
      "Epoch: 914 [10100/50000 (20%)]  \tLoss:   90.145416\trec:   63.514946\tkl:   26.630472\n",
      "Epoch: 914 [20100/50000 (40%)]  \tLoss:   91.387833\trec:   64.706505\tkl:   26.681328\n",
      "Epoch: 914 [30100/50000 (60%)]  \tLoss:   92.876137\trec:   65.231926\tkl:   27.644218\n",
      "Epoch: 914 [40100/50000 (80%)]  \tLoss:   93.888046\trec:   67.157455\tkl:   26.730593\n",
      "====> Epoch: 914 Average train loss: 90.3516\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5544\n",
      "Epoch: 915 [  100/50000 ( 0%)]  \tLoss:   88.262833\trec:   61.823215\tkl:   26.439610\n",
      "Epoch: 915 [10100/50000 (20%)]  \tLoss:   92.099060\trec:   65.227295\tkl:   26.871773\n",
      "Epoch: 915 [20100/50000 (40%)]  \tLoss:   90.057930\trec:   64.093849\tkl:   25.964077\n",
      "Epoch: 915 [30100/50000 (60%)]  \tLoss:   91.204865\trec:   65.134941\tkl:   26.069925\n",
      "Epoch: 915 [40100/50000 (80%)]  \tLoss:   87.882553\trec:   61.840374\tkl:   26.042177\n",
      "====> Epoch: 915 Average train loss: 90.3371\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5048\n",
      "Epoch: 916 [  100/50000 ( 0%)]  \tLoss:   89.561066\trec:   62.602665\tkl:   26.958401\n",
      "Epoch: 916 [10100/50000 (20%)]  \tLoss:   93.299706\trec:   66.232193\tkl:   27.067513\n",
      "Epoch: 916 [20100/50000 (40%)]  \tLoss:   88.035683\trec:   62.058598\tkl:   25.977089\n",
      "Epoch: 916 [30100/50000 (60%)]  \tLoss:   92.804764\trec:   65.837578\tkl:   26.967184\n",
      "Epoch: 916 [40100/50000 (80%)]  \tLoss:   92.884048\trec:   65.949715\tkl:   26.934334\n",
      "====> Epoch: 916 Average train loss: 90.3343\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5538\n",
      "Epoch: 917 [  100/50000 ( 0%)]  \tLoss:   89.793671\trec:   63.281719\tkl:   26.511948\n",
      "Epoch: 917 [10100/50000 (20%)]  \tLoss:   90.033302\trec:   63.559170\tkl:   26.474131\n",
      "Epoch: 917 [20100/50000 (40%)]  \tLoss:   92.692108\trec:   66.045059\tkl:   26.647045\n",
      "Epoch: 917 [30100/50000 (60%)]  \tLoss:   90.799904\trec:   65.626945\tkl:   25.172953\n",
      "Epoch: 917 [40100/50000 (80%)]  \tLoss:   91.760445\trec:   64.795944\tkl:   26.964499\n",
      "====> Epoch: 917 Average train loss: 90.3208\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5776\n",
      "Epoch: 918 [  100/50000 ( 0%)]  \tLoss:   84.545517\trec:   59.493046\tkl:   25.052467\n",
      "Epoch: 918 [10100/50000 (20%)]  \tLoss:   89.170937\trec:   62.508190\tkl:   26.662746\n",
      "Epoch: 918 [20100/50000 (40%)]  \tLoss:   90.812080\trec:   64.327126\tkl:   26.484951\n",
      "Epoch: 918 [30100/50000 (60%)]  \tLoss:   87.924454\trec:   61.659061\tkl:   26.265385\n",
      "Epoch: 918 [40100/50000 (80%)]  \tLoss:   90.451332\trec:   63.865936\tkl:   26.585400\n",
      "====> Epoch: 918 Average train loss: 90.3287\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5988\n",
      "Epoch: 919 [  100/50000 ( 0%)]  \tLoss:   90.389870\trec:   63.921726\tkl:   26.468147\n",
      "Epoch: 919 [10100/50000 (20%)]  \tLoss:   90.147598\trec:   63.908337\tkl:   26.239254\n",
      "Epoch: 919 [20100/50000 (40%)]  \tLoss:   94.574150\trec:   67.325081\tkl:   27.249069\n",
      "Epoch: 919 [30100/50000 (60%)]  \tLoss:   92.061119\trec:   65.387268\tkl:   26.673849\n",
      "Epoch: 919 [40100/50000 (80%)]  \tLoss:   86.369804\trec:   60.767090\tkl:   25.602713\n",
      "====> Epoch: 919 Average train loss: 90.3301\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6212\n",
      "Epoch: 920 [  100/50000 ( 0%)]  \tLoss:   89.071693\trec:   62.894787\tkl:   26.176910\n",
      "Epoch: 920 [10100/50000 (20%)]  \tLoss:   87.107361\trec:   61.573772\tkl:   25.533587\n",
      "Epoch: 920 [20100/50000 (40%)]  \tLoss:   90.683258\trec:   64.478615\tkl:   26.204647\n",
      "Epoch: 920 [30100/50000 (60%)]  \tLoss:   88.446289\trec:   62.224678\tkl:   26.221613\n",
      "Epoch: 920 [40100/50000 (80%)]  \tLoss:   90.233208\trec:   64.080376\tkl:   26.152836\n",
      "====> Epoch: 920 Average train loss: 90.3282\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5606\n",
      "Epoch: 921 [  100/50000 ( 0%)]  \tLoss:   90.924316\trec:   64.212440\tkl:   26.711880\n",
      "Epoch: 921 [10100/50000 (20%)]  \tLoss:   92.250404\trec:   65.910774\tkl:   26.339630\n",
      "Epoch: 921 [20100/50000 (40%)]  \tLoss:   93.602814\trec:   66.824966\tkl:   26.777849\n",
      "Epoch: 921 [30100/50000 (60%)]  \tLoss:   93.385796\trec:   66.210106\tkl:   27.175694\n",
      "Epoch: 921 [40100/50000 (80%)]  \tLoss:   85.258026\trec:   60.758499\tkl:   24.499521\n",
      "====> Epoch: 921 Average train loss: 90.3256\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5836\n",
      "Epoch: 922 [  100/50000 ( 0%)]  \tLoss:   90.695869\trec:   64.771973\tkl:   25.923893\n",
      "Epoch: 922 [10100/50000 (20%)]  \tLoss:   88.468277\trec:   62.076363\tkl:   26.391914\n",
      "Epoch: 922 [20100/50000 (40%)]  \tLoss:   89.268219\trec:   62.845928\tkl:   26.422295\n",
      "Epoch: 922 [30100/50000 (60%)]  \tLoss:   91.363113\trec:   64.401810\tkl:   26.961300\n",
      "Epoch: 922 [40100/50000 (80%)]  \tLoss:   90.004799\trec:   64.565750\tkl:   25.439054\n",
      "====> Epoch: 922 Average train loss: 90.3361\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5887\n",
      "Epoch: 923 [  100/50000 ( 0%)]  \tLoss:   87.354927\trec:   61.553890\tkl:   25.801043\n",
      "Epoch: 923 [10100/50000 (20%)]  \tLoss:   88.309723\trec:   62.056385\tkl:   26.253336\n",
      "Epoch: 923 [20100/50000 (40%)]  \tLoss:   89.399780\trec:   62.918396\tkl:   26.481380\n",
      "Epoch: 923 [30100/50000 (60%)]  \tLoss:   94.061432\trec:   66.948906\tkl:   27.112528\n",
      "Epoch: 923 [40100/50000 (80%)]  \tLoss:   90.702362\trec:   63.619366\tkl:   27.082993\n",
      "====> Epoch: 923 Average train loss: 90.3343\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5779\n",
      "Epoch: 924 [  100/50000 ( 0%)]  \tLoss:   90.415176\trec:   64.155342\tkl:   26.259830\n",
      "Epoch: 924 [10100/50000 (20%)]  \tLoss:   95.453690\trec:   67.480705\tkl:   27.972988\n",
      "Epoch: 924 [20100/50000 (40%)]  \tLoss:   90.616348\trec:   64.113678\tkl:   26.502670\n",
      "Epoch: 924 [30100/50000 (60%)]  \tLoss:   88.949852\trec:   62.420010\tkl:   26.529840\n",
      "Epoch: 924 [40100/50000 (80%)]  \tLoss:   87.743530\trec:   61.946709\tkl:   25.796825\n",
      "====> Epoch: 924 Average train loss: 90.3060\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5567\n",
      "Epoch: 925 [  100/50000 ( 0%)]  \tLoss:   85.588905\trec:   60.064342\tkl:   25.524555\n",
      "Epoch: 925 [10100/50000 (20%)]  \tLoss:   92.572227\trec:   66.188072\tkl:   26.384155\n",
      "Epoch: 925 [20100/50000 (40%)]  \tLoss:   92.297310\trec:   65.573082\tkl:   26.724230\n",
      "Epoch: 925 [30100/50000 (60%)]  \tLoss:   92.052017\trec:   65.406036\tkl:   26.645988\n",
      "Epoch: 925 [40100/50000 (80%)]  \tLoss:   91.703232\trec:   65.409859\tkl:   26.293373\n",
      "====> Epoch: 925 Average train loss: 90.3200\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6005\n",
      "Epoch: 926 [  100/50000 ( 0%)]  \tLoss:   91.561600\trec:   65.248672\tkl:   26.312927\n",
      "Epoch: 926 [10100/50000 (20%)]  \tLoss:   90.435349\trec:   63.882557\tkl:   26.552788\n",
      "Epoch: 926 [20100/50000 (40%)]  \tLoss:   93.526688\trec:   66.455734\tkl:   27.070959\n",
      "Epoch: 926 [30100/50000 (60%)]  \tLoss:   89.541618\trec:   63.750000\tkl:   25.791626\n",
      "Epoch: 926 [40100/50000 (80%)]  \tLoss:   91.850662\trec:   64.973557\tkl:   26.877102\n",
      "====> Epoch: 926 Average train loss: 90.2983\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4792\n",
      "Epoch: 927 [  100/50000 ( 0%)]  \tLoss:   89.225815\trec:   63.891003\tkl:   25.334814\n",
      "Epoch: 927 [10100/50000 (20%)]  \tLoss:   93.098511\trec:   66.202095\tkl:   26.896423\n",
      "Epoch: 927 [20100/50000 (40%)]  \tLoss:   91.563004\trec:   64.803658\tkl:   26.759348\n",
      "Epoch: 927 [30100/50000 (60%)]  \tLoss:   88.388023\trec:   63.127567\tkl:   25.260456\n",
      "Epoch: 927 [40100/50000 (80%)]  \tLoss:   87.085152\trec:   60.565506\tkl:   26.519648\n",
      "====> Epoch: 927 Average train loss: 90.3075\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6045\n",
      "Epoch: 928 [  100/50000 ( 0%)]  \tLoss:   92.159439\trec:   64.882553\tkl:   27.276890\n",
      "Epoch: 928 [10100/50000 (20%)]  \tLoss:   88.648865\trec:   62.659012\tkl:   25.989847\n",
      "Epoch: 928 [20100/50000 (40%)]  \tLoss:   88.471443\trec:   62.440857\tkl:   26.030588\n",
      "Epoch: 928 [30100/50000 (60%)]  \tLoss:   88.588524\trec:   62.514633\tkl:   26.073891\n",
      "Epoch: 928 [40100/50000 (80%)]  \tLoss:   90.088501\trec:   64.250137\tkl:   25.838371\n",
      "====> Epoch: 928 Average train loss: 90.3242\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4563\n",
      "Epoch: 929 [  100/50000 ( 0%)]  \tLoss:   92.746422\trec:   66.358063\tkl:   26.388363\n",
      "Epoch: 929 [10100/50000 (20%)]  \tLoss:   84.881813\trec:   59.941341\tkl:   24.940477\n",
      "Epoch: 929 [20100/50000 (40%)]  \tLoss:   86.955933\trec:   61.222500\tkl:   25.733442\n",
      "Epoch: 929 [30100/50000 (60%)]  \tLoss:   86.947433\trec:   62.324608\tkl:   24.622820\n",
      "Epoch: 929 [40100/50000 (80%)]  \tLoss:   89.772217\trec:   63.495766\tkl:   26.276451\n",
      "====> Epoch: 929 Average train loss: 90.3360\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6192\n",
      "Epoch: 930 [  100/50000 ( 0%)]  \tLoss:   88.205078\trec:   61.762608\tkl:   26.442465\n",
      "Epoch: 930 [10100/50000 (20%)]  \tLoss:   90.825294\trec:   63.832371\tkl:   26.992914\n",
      "Epoch: 930 [20100/50000 (40%)]  \tLoss:   91.711266\trec:   64.997490\tkl:   26.713779\n",
      "Epoch: 930 [30100/50000 (60%)]  \tLoss:   88.881950\trec:   62.553757\tkl:   26.328194\n",
      "Epoch: 930 [40100/50000 (80%)]  \tLoss:   90.423241\trec:   63.680115\tkl:   26.743120\n",
      "====> Epoch: 930 Average train loss: 90.3221\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6170\n",
      "Epoch: 931 [  100/50000 ( 0%)]  \tLoss:   87.078941\trec:   61.298279\tkl:   25.780668\n",
      "Epoch: 931 [10100/50000 (20%)]  \tLoss:   92.046776\trec:   66.307724\tkl:   25.739050\n",
      "Epoch: 931 [20100/50000 (40%)]  \tLoss:   92.978615\trec:   65.659966\tkl:   27.318642\n",
      "Epoch: 931 [30100/50000 (60%)]  \tLoss:   88.682472\trec:   62.762798\tkl:   25.919670\n",
      "Epoch: 931 [40100/50000 (80%)]  \tLoss:   92.608398\trec:   66.521606\tkl:   26.086784\n",
      "====> Epoch: 931 Average train loss: 90.3309\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5945\n",
      "Epoch: 932 [  100/50000 ( 0%)]  \tLoss:   91.560112\trec:   64.461845\tkl:   27.098261\n",
      "Epoch: 932 [10100/50000 (20%)]  \tLoss:   87.434311\trec:   61.501675\tkl:   25.932644\n",
      "Epoch: 932 [20100/50000 (40%)]  \tLoss:   91.221893\trec:   64.915474\tkl:   26.306419\n",
      "Epoch: 932 [30100/50000 (60%)]  \tLoss:   93.602867\trec:   66.440918\tkl:   27.161949\n",
      "Epoch: 932 [40100/50000 (80%)]  \tLoss:   87.199783\trec:   61.696865\tkl:   25.502920\n",
      "====> Epoch: 932 Average train loss: 90.3395\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5162\n",
      "Epoch: 933 [  100/50000 ( 0%)]  \tLoss:   92.676666\trec:   66.579117\tkl:   26.097546\n",
      "Epoch: 933 [10100/50000 (20%)]  \tLoss:   86.708786\trec:   61.593174\tkl:   25.115612\n",
      "Epoch: 933 [20100/50000 (40%)]  \tLoss:   92.715927\trec:   65.830910\tkl:   26.885014\n",
      "Epoch: 933 [30100/50000 (60%)]  \tLoss:   87.575798\trec:   62.319149\tkl:   25.256655\n",
      "Epoch: 933 [40100/50000 (80%)]  \tLoss:   91.471169\trec:   65.084114\tkl:   26.387060\n",
      "====> Epoch: 933 Average train loss: 90.2901\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4312\n",
      "Epoch: 934 [  100/50000 ( 0%)]  \tLoss:   89.447098\trec:   63.972393\tkl:   25.474707\n",
      "Epoch: 934 [10100/50000 (20%)]  \tLoss:   90.841591\trec:   63.748894\tkl:   27.092691\n",
      "Epoch: 934 [20100/50000 (40%)]  \tLoss:   87.054695\trec:   61.856953\tkl:   25.197746\n",
      "Epoch: 934 [30100/50000 (60%)]  \tLoss:   92.747421\trec:   65.900673\tkl:   26.846750\n",
      "Epoch: 934 [40100/50000 (80%)]  \tLoss:   92.177734\trec:   65.485420\tkl:   26.692316\n",
      "====> Epoch: 934 Average train loss: 90.3083\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5754\n",
      "Epoch: 935 [  100/50000 ( 0%)]  \tLoss:   88.675301\trec:   63.105583\tkl:   25.569715\n",
      "Epoch: 935 [10100/50000 (20%)]  \tLoss:   91.254982\trec:   64.244446\tkl:   27.010536\n",
      "Epoch: 935 [20100/50000 (40%)]  \tLoss:   89.600639\trec:   63.466160\tkl:   26.134487\n",
      "Epoch: 935 [30100/50000 (60%)]  \tLoss:   91.481178\trec:   65.012703\tkl:   26.468479\n",
      "Epoch: 935 [40100/50000 (80%)]  \tLoss:   89.186844\trec:   62.926716\tkl:   26.260126\n",
      "====> Epoch: 935 Average train loss: 90.3049\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6795\n",
      "Epoch: 936 [  100/50000 ( 0%)]  \tLoss:   88.368507\trec:   62.438416\tkl:   25.930084\n",
      "Epoch: 936 [10100/50000 (20%)]  \tLoss:   87.348907\trec:   61.143593\tkl:   26.205309\n",
      "Epoch: 936 [20100/50000 (40%)]  \tLoss:   91.657295\trec:   64.529373\tkl:   27.127916\n",
      "Epoch: 936 [30100/50000 (60%)]  \tLoss:   90.612175\trec:   64.559540\tkl:   26.052635\n",
      "Epoch: 936 [40100/50000 (80%)]  \tLoss:   89.369240\trec:   63.462292\tkl:   25.906940\n",
      "====> Epoch: 936 Average train loss: 90.2996\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4544\n",
      "Epoch: 937 [  100/50000 ( 0%)]  \tLoss:   94.352028\trec:   68.233826\tkl:   26.118208\n",
      "Epoch: 937 [10100/50000 (20%)]  \tLoss:   89.843124\trec:   63.555683\tkl:   26.287437\n",
      "Epoch: 937 [20100/50000 (40%)]  \tLoss:   88.605064\trec:   62.821381\tkl:   25.783684\n",
      "Epoch: 937 [30100/50000 (60%)]  \tLoss:   91.423706\trec:   65.322746\tkl:   26.100962\n",
      "Epoch: 937 [40100/50000 (80%)]  \tLoss:   92.185173\trec:   65.701065\tkl:   26.484104\n",
      "====> Epoch: 937 Average train loss: 90.2717\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5353\n",
      "Epoch: 938 [  100/50000 ( 0%)]  \tLoss:   90.685699\trec:   63.151054\tkl:   27.534651\n",
      "Epoch: 938 [10100/50000 (20%)]  \tLoss:   89.238953\trec:   62.669270\tkl:   26.569685\n",
      "Epoch: 938 [20100/50000 (40%)]  \tLoss:   92.147339\trec:   65.220642\tkl:   26.926701\n",
      "Epoch: 938 [30100/50000 (60%)]  \tLoss:   84.368263\trec:   59.169922\tkl:   25.198343\n",
      "Epoch: 938 [40100/50000 (80%)]  \tLoss:   89.967575\trec:   63.442608\tkl:   26.524967\n",
      "====> Epoch: 938 Average train loss: 90.2806\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5885\n",
      "Epoch: 939 [  100/50000 ( 0%)]  \tLoss:   86.549316\trec:   60.820671\tkl:   25.728640\n",
      "Epoch: 939 [10100/50000 (20%)]  \tLoss:   91.706161\trec:   65.005859\tkl:   26.700300\n",
      "Epoch: 939 [20100/50000 (40%)]  \tLoss:   91.376396\trec:   65.185471\tkl:   26.190928\n",
      "Epoch: 939 [30100/50000 (60%)]  \tLoss:   91.723732\trec:   65.529816\tkl:   26.193913\n",
      "Epoch: 939 [40100/50000 (80%)]  \tLoss:   91.219704\trec:   65.399902\tkl:   25.819799\n",
      "====> Epoch: 939 Average train loss: 90.3105\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6205\n",
      "Epoch: 940 [  100/50000 ( 0%)]  \tLoss:   92.277832\trec:   65.371262\tkl:   26.906574\n",
      "Epoch: 940 [10100/50000 (20%)]  \tLoss:   92.221840\trec:   65.371460\tkl:   26.850380\n",
      "Epoch: 940 [20100/50000 (40%)]  \tLoss:   89.514389\trec:   63.647839\tkl:   25.866556\n",
      "Epoch: 940 [30100/50000 (60%)]  \tLoss:   86.553535\trec:   60.909058\tkl:   25.644482\n",
      "Epoch: 940 [40100/50000 (80%)]  \tLoss:   86.214920\trec:   60.943413\tkl:   25.271505\n",
      "====> Epoch: 940 Average train loss: 90.3029\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7262\n",
      "Epoch: 941 [  100/50000 ( 0%)]  \tLoss:   91.208038\trec:   64.754082\tkl:   26.453949\n",
      "Epoch: 941 [10100/50000 (20%)]  \tLoss:   91.363594\trec:   65.237099\tkl:   26.126495\n",
      "Epoch: 941 [20100/50000 (40%)]  \tLoss:   92.050751\trec:   65.528641\tkl:   26.522106\n",
      "Epoch: 941 [30100/50000 (60%)]  \tLoss:   97.148888\trec:   68.972412\tkl:   28.176472\n",
      "Epoch: 941 [40100/50000 (80%)]  \tLoss:   93.952393\trec:   67.016266\tkl:   26.936123\n",
      "====> Epoch: 941 Average train loss: 90.2861\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5826\n",
      "Epoch: 942 [  100/50000 ( 0%)]  \tLoss:   90.778923\trec:   64.669312\tkl:   26.109612\n",
      "Epoch: 942 [10100/50000 (20%)]  \tLoss:   88.551285\trec:   62.959179\tkl:   25.592113\n",
      "Epoch: 942 [20100/50000 (40%)]  \tLoss:   90.330948\trec:   64.088684\tkl:   26.242260\n",
      "Epoch: 942 [30100/50000 (60%)]  \tLoss:   92.009193\trec:   65.992813\tkl:   26.016392\n",
      "Epoch: 942 [40100/50000 (80%)]  \tLoss:   87.670555\trec:   61.031353\tkl:   26.639200\n",
      "====> Epoch: 942 Average train loss: 90.2591\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6004\n",
      "Epoch: 943 [  100/50000 ( 0%)]  \tLoss:   91.521835\trec:   63.752899\tkl:   27.768930\n",
      "Epoch: 943 [10100/50000 (20%)]  \tLoss:   93.744629\trec:   66.582970\tkl:   27.161657\n",
      "Epoch: 943 [20100/50000 (40%)]  \tLoss:   91.733925\trec:   64.998405\tkl:   26.735512\n",
      "Epoch: 943 [30100/50000 (60%)]  \tLoss:   91.911987\trec:   64.627403\tkl:   27.284590\n",
      "Epoch: 943 [40100/50000 (80%)]  \tLoss:   91.383484\trec:   65.170143\tkl:   26.213337\n",
      "====> Epoch: 943 Average train loss: 90.2876\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6101\n",
      "Epoch: 944 [  100/50000 ( 0%)]  \tLoss:   88.629089\trec:   63.520290\tkl:   25.108801\n",
      "Epoch: 944 [10100/50000 (20%)]  \tLoss:   88.587540\trec:   62.172703\tkl:   26.414831\n",
      "Epoch: 944 [20100/50000 (40%)]  \tLoss:   89.491837\trec:   63.819183\tkl:   25.672653\n",
      "Epoch: 944 [30100/50000 (60%)]  \tLoss:   87.291069\trec:   60.692696\tkl:   26.598381\n",
      "Epoch: 944 [40100/50000 (80%)]  \tLoss:   92.205429\trec:   65.489853\tkl:   26.715570\n",
      "====> Epoch: 944 Average train loss: 90.3002\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.7418\n",
      "Epoch: 945 [  100/50000 ( 0%)]  \tLoss:   86.938385\trec:   61.349609\tkl:   25.588779\n",
      "Epoch: 945 [10100/50000 (20%)]  \tLoss:   89.663330\trec:   63.456127\tkl:   26.207201\n",
      "Epoch: 945 [20100/50000 (40%)]  \tLoss:   88.783104\trec:   62.750565\tkl:   26.032534\n",
      "Epoch: 945 [30100/50000 (60%)]  \tLoss:   93.859566\trec:   67.035408\tkl:   26.824165\n",
      "Epoch: 945 [40100/50000 (80%)]  \tLoss:   91.959061\trec:   65.606239\tkl:   26.352827\n",
      "====> Epoch: 945 Average train loss: 90.2635\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5758\n",
      "Epoch: 946 [  100/50000 ( 0%)]  \tLoss:   89.226036\trec:   63.171616\tkl:   26.054413\n",
      "Epoch: 946 [10100/50000 (20%)]  \tLoss:   88.302330\trec:   62.799717\tkl:   25.502619\n",
      "Epoch: 946 [20100/50000 (40%)]  \tLoss:   89.536598\trec:   63.171757\tkl:   26.364843\n",
      "Epoch: 946 [30100/50000 (60%)]  \tLoss:   88.101906\trec:   62.283760\tkl:   25.818146\n",
      "Epoch: 946 [40100/50000 (80%)]  \tLoss:   89.371620\trec:   62.887489\tkl:   26.484131\n",
      "====> Epoch: 946 Average train loss: 90.2665\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6452\n",
      "Epoch: 947 [  100/50000 ( 0%)]  \tLoss:   91.051254\trec:   64.410477\tkl:   26.640778\n",
      "Epoch: 947 [10100/50000 (20%)]  \tLoss:   92.312218\trec:   65.857384\tkl:   26.454830\n",
      "Epoch: 947 [20100/50000 (40%)]  \tLoss:   89.942657\trec:   63.829189\tkl:   26.113466\n",
      "Epoch: 947 [30100/50000 (60%)]  \tLoss:   91.517204\trec:   64.696213\tkl:   26.820995\n",
      "Epoch: 947 [40100/50000 (80%)]  \tLoss:   85.884956\trec:   60.216209\tkl:   25.668753\n",
      "====> Epoch: 947 Average train loss: 90.2565\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5117\n",
      "Epoch: 948 [  100/50000 ( 0%)]  \tLoss:   93.997498\trec:   67.241821\tkl:   26.755684\n",
      "Epoch: 948 [10100/50000 (20%)]  \tLoss:   93.326675\trec:   66.487457\tkl:   26.839218\n",
      "Epoch: 948 [20100/50000 (40%)]  \tLoss:   94.402740\trec:   66.987823\tkl:   27.414925\n",
      "Epoch: 948 [30100/50000 (60%)]  \tLoss:   88.938354\trec:   62.835663\tkl:   26.102692\n",
      "Epoch: 948 [40100/50000 (80%)]  \tLoss:   88.681656\trec:   62.106186\tkl:   26.575470\n",
      "====> Epoch: 948 Average train loss: 90.2816\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5570\n",
      "Epoch: 949 [  100/50000 ( 0%)]  \tLoss:   89.205688\trec:   62.896141\tkl:   26.309549\n",
      "Epoch: 949 [10100/50000 (20%)]  \tLoss:   90.685837\trec:   64.761452\tkl:   25.924381\n",
      "Epoch: 949 [20100/50000 (40%)]  \tLoss:   93.125961\trec:   66.680527\tkl:   26.445436\n",
      "Epoch: 949 [30100/50000 (60%)]  \tLoss:   87.179787\trec:   62.146072\tkl:   25.033716\n",
      "Epoch: 949 [40100/50000 (80%)]  \tLoss:   92.410919\trec:   65.625000\tkl:   26.785915\n",
      "====> Epoch: 949 Average train loss: 90.2826\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6701\n",
      "Epoch: 950 [  100/50000 ( 0%)]  \tLoss:   89.252594\trec:   63.118504\tkl:   26.134089\n",
      "Epoch: 950 [10100/50000 (20%)]  \tLoss:   88.637039\trec:   62.183018\tkl:   26.454023\n",
      "Epoch: 950 [20100/50000 (40%)]  \tLoss:   92.439575\trec:   65.954277\tkl:   26.485302\n",
      "Epoch: 950 [30100/50000 (60%)]  \tLoss:   89.577461\trec:   62.894714\tkl:   26.682749\n",
      "Epoch: 950 [40100/50000 (80%)]  \tLoss:   91.726013\trec:   65.246819\tkl:   26.479197\n",
      "====> Epoch: 950 Average train loss: 90.2710\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6270\n",
      "Epoch: 951 [  100/50000 ( 0%)]  \tLoss:   89.459549\trec:   63.210583\tkl:   26.248959\n",
      "Epoch: 951 [10100/50000 (20%)]  \tLoss:   89.959892\trec:   63.333992\tkl:   26.625898\n",
      "Epoch: 951 [20100/50000 (40%)]  \tLoss:   91.068954\trec:   64.786964\tkl:   26.281990\n",
      "Epoch: 951 [30100/50000 (60%)]  \tLoss:   96.417625\trec:   69.337318\tkl:   27.080307\n",
      "Epoch: 951 [40100/50000 (80%)]  \tLoss:   90.479774\trec:   63.997692\tkl:   26.482082\n",
      "====> Epoch: 951 Average train loss: 90.2752\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6006\n",
      "Epoch: 952 [  100/50000 ( 0%)]  \tLoss:   90.518219\trec:   64.182732\tkl:   26.335489\n",
      "Epoch: 952 [10100/50000 (20%)]  \tLoss:   89.193367\trec:   63.420067\tkl:   25.773298\n",
      "Epoch: 952 [20100/50000 (40%)]  \tLoss:   93.697899\trec:   67.097313\tkl:   26.600586\n",
      "Epoch: 952 [30100/50000 (60%)]  \tLoss:   90.040382\trec:   63.844341\tkl:   26.196039\n",
      "Epoch: 952 [40100/50000 (80%)]  \tLoss:   88.072617\trec:   61.966667\tkl:   26.105947\n",
      "====> Epoch: 952 Average train loss: 90.2578\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5345\n",
      "Epoch: 953 [  100/50000 ( 0%)]  \tLoss:   92.121925\trec:   64.882652\tkl:   27.239269\n",
      "Epoch: 953 [10100/50000 (20%)]  \tLoss:   90.908707\trec:   64.344475\tkl:   26.564240\n",
      "Epoch: 953 [20100/50000 (40%)]  \tLoss:   92.503136\trec:   65.461731\tkl:   27.041401\n",
      "Epoch: 953 [30100/50000 (60%)]  \tLoss:   89.528778\trec:   63.918457\tkl:   25.610319\n",
      "Epoch: 953 [40100/50000 (80%)]  \tLoss:   91.403534\trec:   64.485970\tkl:   26.917564\n",
      "====> Epoch: 953 Average train loss: 90.2748\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4905\n",
      "Epoch: 954 [  100/50000 ( 0%)]  \tLoss:   90.465271\trec:   64.467430\tkl:   25.997843\n",
      "Epoch: 954 [10100/50000 (20%)]  \tLoss:   88.684723\trec:   62.066418\tkl:   26.618309\n",
      "Epoch: 954 [20100/50000 (40%)]  \tLoss:   92.983025\trec:   66.052086\tkl:   26.930943\n",
      "Epoch: 954 [30100/50000 (60%)]  \tLoss:   87.371353\trec:   61.735344\tkl:   25.636007\n",
      "Epoch: 954 [40100/50000 (80%)]  \tLoss:   92.122505\trec:   66.107750\tkl:   26.014753\n",
      "====> Epoch: 954 Average train loss: 90.2623\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5538\n",
      "Epoch: 955 [  100/50000 ( 0%)]  \tLoss:   88.737808\trec:   62.598835\tkl:   26.138971\n",
      "Epoch: 955 [10100/50000 (20%)]  \tLoss:   89.818901\trec:   63.058792\tkl:   26.760107\n",
      "Epoch: 955 [20100/50000 (40%)]  \tLoss:   92.714607\trec:   65.674080\tkl:   27.040524\n",
      "Epoch: 955 [30100/50000 (60%)]  \tLoss:   92.569344\trec:   66.460846\tkl:   26.108496\n",
      "Epoch: 955 [40100/50000 (80%)]  \tLoss:   93.652519\trec:   67.159378\tkl:   26.493139\n",
      "====> Epoch: 955 Average train loss: 90.2675\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4923\n",
      "Epoch: 956 [  100/50000 ( 0%)]  \tLoss:   91.224396\trec:   64.430252\tkl:   26.794140\n",
      "Epoch: 956 [10100/50000 (20%)]  \tLoss:   89.232651\trec:   63.470581\tkl:   25.762075\n",
      "Epoch: 956 [20100/50000 (40%)]  \tLoss:   91.867950\trec:   65.423088\tkl:   26.444859\n",
      "Epoch: 956 [30100/50000 (60%)]  \tLoss:   91.738823\trec:   64.371124\tkl:   27.367697\n",
      "Epoch: 956 [40100/50000 (80%)]  \tLoss:   91.760323\trec:   65.885826\tkl:   25.874491\n",
      "====> Epoch: 956 Average train loss: 90.2763\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5578\n",
      "Epoch: 957 [  100/50000 ( 0%)]  \tLoss:   88.383926\trec:   62.309158\tkl:   26.074760\n",
      "Epoch: 957 [10100/50000 (20%)]  \tLoss:   90.028954\trec:   63.551846\tkl:   26.477106\n",
      "Epoch: 957 [20100/50000 (40%)]  \tLoss:   89.099342\trec:   63.484951\tkl:   25.614397\n",
      "Epoch: 957 [30100/50000 (60%)]  \tLoss:   90.911133\trec:   64.569405\tkl:   26.341728\n",
      "Epoch: 957 [40100/50000 (80%)]  \tLoss:   88.999100\trec:   62.856522\tkl:   26.142580\n",
      "====> Epoch: 957 Average train loss: 90.2551\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5142\n",
      "Epoch: 958 [  100/50000 ( 0%)]  \tLoss:   88.924034\trec:   62.855213\tkl:   26.068821\n",
      "Epoch: 958 [10100/50000 (20%)]  \tLoss:   90.473984\trec:   63.546535\tkl:   26.927441\n",
      "Epoch: 958 [20100/50000 (40%)]  \tLoss:   89.089607\trec:   63.302670\tkl:   25.786934\n",
      "Epoch: 958 [30100/50000 (60%)]  \tLoss:   87.566338\trec:   61.168316\tkl:   26.398022\n",
      "Epoch: 958 [40100/50000 (80%)]  \tLoss:   93.590347\trec:   67.123016\tkl:   26.467331\n",
      "====> Epoch: 958 Average train loss: 90.2394\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5499\n",
      "Epoch: 959 [  100/50000 ( 0%)]  \tLoss:   87.659142\trec:   61.930771\tkl:   25.728373\n",
      "Epoch: 959 [10100/50000 (20%)]  \tLoss:   90.770798\trec:   63.718220\tkl:   27.052582\n",
      "Epoch: 959 [20100/50000 (40%)]  \tLoss:   87.640114\trec:   61.619160\tkl:   26.020962\n",
      "Epoch: 959 [30100/50000 (60%)]  \tLoss:   91.795464\trec:   64.652725\tkl:   27.142736\n",
      "Epoch: 959 [40100/50000 (80%)]  \tLoss:   88.420654\trec:   62.489250\tkl:   25.931398\n",
      "====> Epoch: 959 Average train loss: 90.2397\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5706\n",
      "Epoch: 960 [  100/50000 ( 0%)]  \tLoss:   89.531616\trec:   62.981350\tkl:   26.550262\n",
      "Epoch: 960 [10100/50000 (20%)]  \tLoss:   91.135956\trec:   64.775246\tkl:   26.360714\n",
      "Epoch: 960 [20100/50000 (40%)]  \tLoss:   91.598892\trec:   64.930946\tkl:   26.667948\n",
      "Epoch: 960 [30100/50000 (60%)]  \tLoss:   92.356560\trec:   65.547188\tkl:   26.809372\n",
      "Epoch: 960 [40100/50000 (80%)]  \tLoss:   94.106659\trec:   67.066231\tkl:   27.040419\n",
      "====> Epoch: 960 Average train loss: 90.2429\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5794\n",
      "Epoch: 961 [  100/50000 ( 0%)]  \tLoss:   91.704033\trec:   65.552246\tkl:   26.151787\n",
      "Epoch: 961 [10100/50000 (20%)]  \tLoss:   91.239021\trec:   64.110626\tkl:   27.128397\n",
      "Epoch: 961 [20100/50000 (40%)]  \tLoss:   87.725349\trec:   61.921131\tkl:   25.804222\n",
      "Epoch: 961 [30100/50000 (60%)]  \tLoss:   91.846710\trec:   65.523117\tkl:   26.323593\n",
      "Epoch: 961 [40100/50000 (80%)]  \tLoss:   90.827911\trec:   64.158394\tkl:   26.669508\n",
      "====> Epoch: 961 Average train loss: 90.2430\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6163\n",
      "Epoch: 962 [  100/50000 ( 0%)]  \tLoss:   93.183449\trec:   66.633698\tkl:   26.549746\n",
      "Epoch: 962 [10100/50000 (20%)]  \tLoss:   88.727402\trec:   61.687424\tkl:   27.039978\n",
      "Epoch: 962 [20100/50000 (40%)]  \tLoss:   94.311920\trec:   67.536377\tkl:   26.775541\n",
      "Epoch: 962 [30100/50000 (60%)]  \tLoss:   86.151695\trec:   61.254108\tkl:   24.897593\n",
      "Epoch: 962 [40100/50000 (80%)]  \tLoss:   90.566299\trec:   64.640060\tkl:   25.926237\n",
      "====> Epoch: 962 Average train loss: 90.2368\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5369\n",
      "Epoch: 963 [  100/50000 ( 0%)]  \tLoss:   89.259216\trec:   62.659100\tkl:   26.600115\n",
      "Epoch: 963 [10100/50000 (20%)]  \tLoss:   88.631775\trec:   62.786980\tkl:   25.844795\n",
      "Epoch: 963 [20100/50000 (40%)]  \tLoss:   91.499039\trec:   64.884567\tkl:   26.614477\n",
      "Epoch: 963 [30100/50000 (60%)]  \tLoss:   88.649117\trec:   62.319431\tkl:   26.329681\n",
      "Epoch: 963 [40100/50000 (80%)]  \tLoss:   89.739449\trec:   63.898384\tkl:   25.841068\n",
      "====> Epoch: 963 Average train loss: 90.2470\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5805\n",
      "Epoch: 964 [  100/50000 ( 0%)]  \tLoss:   86.913818\trec:   60.613213\tkl:   26.300608\n",
      "Epoch: 964 [10100/50000 (20%)]  \tLoss:   89.275917\trec:   63.124748\tkl:   26.151163\n",
      "Epoch: 964 [20100/50000 (40%)]  \tLoss:   91.101929\trec:   64.473068\tkl:   26.628857\n",
      "Epoch: 964 [30100/50000 (60%)]  \tLoss:   90.382744\trec:   63.928818\tkl:   26.453926\n",
      "Epoch: 964 [40100/50000 (80%)]  \tLoss:   88.481590\trec:   62.769539\tkl:   25.712048\n",
      "====> Epoch: 964 Average train loss: 90.2569\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6976\n",
      "Epoch: 965 [  100/50000 ( 0%)]  \tLoss:   88.689873\trec:   62.459793\tkl:   26.230078\n",
      "Epoch: 965 [10100/50000 (20%)]  \tLoss:   86.253677\trec:   60.363205\tkl:   25.890476\n",
      "Epoch: 965 [20100/50000 (40%)]  \tLoss:   87.823555\trec:   62.041916\tkl:   25.781639\n",
      "Epoch: 965 [30100/50000 (60%)]  \tLoss:   94.272156\trec:   68.142937\tkl:   26.129215\n",
      "Epoch: 965 [40100/50000 (80%)]  \tLoss:   89.916237\trec:   63.452900\tkl:   26.463339\n",
      "====> Epoch: 965 Average train loss: 90.2311\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5094\n",
      "Epoch: 966 [  100/50000 ( 0%)]  \tLoss:   91.027176\trec:   64.883408\tkl:   26.143766\n",
      "Epoch: 966 [10100/50000 (20%)]  \tLoss:   90.141129\trec:   64.280327\tkl:   25.860809\n",
      "Epoch: 966 [20100/50000 (40%)]  \tLoss:   86.288399\trec:   60.569424\tkl:   25.718971\n",
      "Epoch: 966 [30100/50000 (60%)]  \tLoss:   93.000954\trec:   66.230339\tkl:   26.770613\n",
      "Epoch: 966 [40100/50000 (80%)]  \tLoss:   91.950111\trec:   65.000053\tkl:   26.950052\n",
      "====> Epoch: 966 Average train loss: 90.2382\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6254\n",
      "Epoch: 967 [  100/50000 ( 0%)]  \tLoss:   91.228844\trec:   65.674538\tkl:   25.554306\n",
      "Epoch: 967 [10100/50000 (20%)]  \tLoss:   86.549919\trec:   61.171623\tkl:   25.378296\n",
      "Epoch: 967 [20100/50000 (40%)]  \tLoss:   90.714493\trec:   64.111076\tkl:   26.603422\n",
      "Epoch: 967 [30100/50000 (60%)]  \tLoss:   90.606094\trec:   63.667938\tkl:   26.938158\n",
      "Epoch: 967 [40100/50000 (80%)]  \tLoss:   89.370567\trec:   63.203625\tkl:   26.166933\n",
      "====> Epoch: 967 Average train loss: 90.2378\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5738\n",
      "Epoch: 968 [  100/50000 ( 0%)]  \tLoss:   89.894753\trec:   63.759682\tkl:   26.135073\n",
      "Epoch: 968 [10100/50000 (20%)]  \tLoss:   89.135231\trec:   62.545490\tkl:   26.589741\n",
      "Epoch: 968 [20100/50000 (40%)]  \tLoss:   91.208839\trec:   64.557137\tkl:   26.651695\n",
      "Epoch: 968 [30100/50000 (60%)]  \tLoss:   89.434013\trec:   63.286209\tkl:   26.147802\n",
      "Epoch: 968 [40100/50000 (80%)]  \tLoss:   88.735184\trec:   63.050606\tkl:   25.684580\n",
      "====> Epoch: 968 Average train loss: 90.2478\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5192\n",
      "Epoch: 969 [  100/50000 ( 0%)]  \tLoss:   86.589226\trec:   61.052494\tkl:   25.536736\n",
      "Epoch: 969 [10100/50000 (20%)]  \tLoss:   93.018448\trec:   66.392029\tkl:   26.626415\n",
      "Epoch: 969 [20100/50000 (40%)]  \tLoss:   86.752052\trec:   60.665432\tkl:   26.086611\n",
      "Epoch: 969 [30100/50000 (60%)]  \tLoss:   95.069420\trec:   68.559364\tkl:   26.510061\n",
      "Epoch: 969 [40100/50000 (80%)]  \tLoss:   90.109657\trec:   62.931442\tkl:   27.178209\n",
      "====> Epoch: 969 Average train loss: 90.2094\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6515\n",
      "Epoch: 970 [  100/50000 ( 0%)]  \tLoss:   88.148506\trec:   62.121658\tkl:   26.026842\n",
      "Epoch: 970 [10100/50000 (20%)]  \tLoss:   89.448456\trec:   63.197407\tkl:   26.251045\n",
      "Epoch: 970 [20100/50000 (40%)]  \tLoss:   86.575050\trec:   60.024792\tkl:   26.550255\n",
      "Epoch: 970 [30100/50000 (60%)]  \tLoss:   91.899872\trec:   64.936630\tkl:   26.963242\n",
      "Epoch: 970 [40100/50000 (80%)]  \tLoss:   92.280327\trec:   65.729332\tkl:   26.551004\n",
      "====> Epoch: 970 Average train loss: 90.2356\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5220\n",
      "Epoch: 971 [  100/50000 ( 0%)]  \tLoss:   89.421959\trec:   63.281040\tkl:   26.140923\n",
      "Epoch: 971 [10100/50000 (20%)]  \tLoss:   89.168953\trec:   63.209343\tkl:   25.959608\n",
      "Epoch: 971 [20100/50000 (40%)]  \tLoss:   92.294487\trec:   65.617424\tkl:   26.677065\n",
      "Epoch: 971 [30100/50000 (60%)]  \tLoss:   93.086128\trec:   65.827728\tkl:   27.258406\n",
      "Epoch: 971 [40100/50000 (80%)]  \tLoss:   88.352852\trec:   61.549911\tkl:   26.802938\n",
      "====> Epoch: 971 Average train loss: 90.2107\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4892\n",
      "Epoch: 972 [  100/50000 ( 0%)]  \tLoss:   91.308258\trec:   64.497246\tkl:   26.811012\n",
      "Epoch: 972 [10100/50000 (20%)]  \tLoss:   92.797462\trec:   65.354469\tkl:   27.442987\n",
      "Epoch: 972 [20100/50000 (40%)]  \tLoss:   90.024155\trec:   63.649998\tkl:   26.374163\n",
      "Epoch: 972 [30100/50000 (60%)]  \tLoss:   92.638611\trec:   66.351425\tkl:   26.287184\n",
      "Epoch: 972 [40100/50000 (80%)]  \tLoss:   88.170280\trec:   62.660683\tkl:   25.509600\n",
      "====> Epoch: 972 Average train loss: 90.2246\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5848\n",
      "Epoch: 973 [  100/50000 ( 0%)]  \tLoss:   88.307762\trec:   61.780987\tkl:   26.526773\n",
      "Epoch: 973 [10100/50000 (20%)]  \tLoss:   90.882607\trec:   63.737671\tkl:   27.144934\n",
      "Epoch: 973 [20100/50000 (40%)]  \tLoss:   88.287086\trec:   61.862175\tkl:   26.424911\n",
      "Epoch: 973 [30100/50000 (60%)]  \tLoss:   88.860054\trec:   62.817265\tkl:   26.042788\n",
      "Epoch: 973 [40100/50000 (80%)]  \tLoss:   90.490135\trec:   63.884899\tkl:   26.605238\n",
      "====> Epoch: 973 Average train loss: 90.2196\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5163\n",
      "Epoch: 974 [  100/50000 ( 0%)]  \tLoss:   91.586098\trec:   64.396774\tkl:   27.189327\n",
      "Epoch: 974 [10100/50000 (20%)]  \tLoss:   91.089943\trec:   64.007690\tkl:   27.082245\n",
      "Epoch: 974 [20100/50000 (40%)]  \tLoss:   91.365334\trec:   64.771408\tkl:   26.593927\n",
      "Epoch: 974 [30100/50000 (60%)]  \tLoss:   86.782448\trec:   61.031273\tkl:   25.751177\n",
      "Epoch: 974 [40100/50000 (80%)]  \tLoss:   92.838814\trec:   66.199188\tkl:   26.639626\n",
      "====> Epoch: 974 Average train loss: 90.2019\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5119\n",
      "Epoch: 975 [  100/50000 ( 0%)]  \tLoss:   88.994392\trec:   62.989700\tkl:   26.004694\n",
      "Epoch: 975 [10100/50000 (20%)]  \tLoss:   86.246056\trec:   60.559921\tkl:   25.686132\n",
      "Epoch: 975 [20100/50000 (40%)]  \tLoss:   88.871071\trec:   63.185200\tkl:   25.685873\n",
      "Epoch: 975 [30100/50000 (60%)]  \tLoss:   90.796143\trec:   64.203964\tkl:   26.592180\n",
      "Epoch: 975 [40100/50000 (80%)]  \tLoss:   89.982460\trec:   63.764832\tkl:   26.217627\n",
      "====> Epoch: 975 Average train loss: 90.2294\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5112\n",
      "Epoch: 976 [  100/50000 ( 0%)]  \tLoss:   92.848663\trec:   66.542648\tkl:   26.306013\n",
      "Epoch: 976 [10100/50000 (20%)]  \tLoss:   87.889175\trec:   62.326942\tkl:   25.562239\n",
      "Epoch: 976 [20100/50000 (40%)]  \tLoss:   87.359604\trec:   61.028152\tkl:   26.331450\n",
      "Epoch: 976 [30100/50000 (60%)]  \tLoss:   91.708893\trec:   65.539383\tkl:   26.169512\n",
      "Epoch: 976 [40100/50000 (80%)]  \tLoss:   88.597534\trec:   62.141155\tkl:   26.456381\n",
      "====> Epoch: 976 Average train loss: 90.2167\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5120\n",
      "Epoch: 977 [  100/50000 ( 0%)]  \tLoss:   93.288490\trec:   65.911407\tkl:   27.377087\n",
      "Epoch: 977 [10100/50000 (20%)]  \tLoss:   89.030975\trec:   63.082333\tkl:   25.948645\n",
      "Epoch: 977 [20100/50000 (40%)]  \tLoss:   89.636688\trec:   63.583904\tkl:   26.052786\n",
      "Epoch: 977 [30100/50000 (60%)]  \tLoss:   93.086815\trec:   65.806412\tkl:   27.280405\n",
      "Epoch: 977 [40100/50000 (80%)]  \tLoss:   89.782478\trec:   63.913269\tkl:   25.869205\n",
      "====> Epoch: 977 Average train loss: 90.1848\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6155\n",
      "Epoch: 978 [  100/50000 ( 0%)]  \tLoss:   94.388794\trec:   67.093307\tkl:   27.295488\n",
      "Epoch: 978 [10100/50000 (20%)]  \tLoss:   87.348007\trec:   60.897961\tkl:   26.450048\n",
      "Epoch: 978 [20100/50000 (40%)]  \tLoss:   89.257530\trec:   62.708340\tkl:   26.549191\n",
      "Epoch: 978 [30100/50000 (60%)]  \tLoss:   90.965248\trec:   64.745728\tkl:   26.219521\n",
      "Epoch: 978 [40100/50000 (80%)]  \tLoss:   88.018379\trec:   61.805111\tkl:   26.213270\n",
      "====> Epoch: 978 Average train loss: 90.2380\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5686\n",
      "Epoch: 979 [  100/50000 ( 0%)]  \tLoss:   91.099686\trec:   64.086349\tkl:   27.013334\n",
      "Epoch: 979 [10100/50000 (20%)]  \tLoss:   87.978371\trec:   62.244148\tkl:   25.734219\n",
      "Epoch: 979 [20100/50000 (40%)]  \tLoss:   87.631660\trec:   61.082745\tkl:   26.548916\n",
      "Epoch: 979 [30100/50000 (60%)]  \tLoss:   88.082657\trec:   62.001354\tkl:   26.081301\n",
      "Epoch: 979 [40100/50000 (80%)]  \tLoss:   90.506973\trec:   63.972553\tkl:   26.534420\n",
      "====> Epoch: 979 Average train loss: 90.1936\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5282\n",
      "Epoch: 980 [  100/50000 ( 0%)]  \tLoss:   93.465340\trec:   67.741646\tkl:   25.723696\n",
      "Epoch: 980 [10100/50000 (20%)]  \tLoss:   91.612381\trec:   64.472198\tkl:   27.140184\n",
      "Epoch: 980 [20100/50000 (40%)]  \tLoss:   89.235641\trec:   62.932274\tkl:   26.303373\n",
      "Epoch: 980 [30100/50000 (60%)]  \tLoss:   92.473320\trec:   64.813675\tkl:   27.659647\n",
      "Epoch: 980 [40100/50000 (80%)]  \tLoss:   90.721405\trec:   64.058067\tkl:   26.663338\n",
      "====> Epoch: 980 Average train loss: 90.1980\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5785\n",
      "Epoch: 981 [  100/50000 ( 0%)]  \tLoss:   89.799118\trec:   63.296745\tkl:   26.502377\n",
      "Epoch: 981 [10100/50000 (20%)]  \tLoss:   86.245903\trec:   60.268379\tkl:   25.977530\n",
      "Epoch: 981 [20100/50000 (40%)]  \tLoss:   91.899246\trec:   64.999001\tkl:   26.900251\n",
      "Epoch: 981 [30100/50000 (60%)]  \tLoss:   91.782654\trec:   64.984474\tkl:   26.798183\n",
      "Epoch: 981 [40100/50000 (80%)]  \tLoss:   88.089882\trec:   61.915352\tkl:   26.174534\n",
      "====> Epoch: 981 Average train loss: 90.2073\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6630\n",
      "Epoch: 982 [  100/50000 ( 0%)]  \tLoss:   89.095818\trec:   62.714348\tkl:   26.381474\n",
      "Epoch: 982 [10100/50000 (20%)]  \tLoss:   89.175781\trec:   62.706989\tkl:   26.468794\n",
      "Epoch: 982 [20100/50000 (40%)]  \tLoss:   87.672165\trec:   61.309208\tkl:   26.362955\n",
      "Epoch: 982 [30100/50000 (60%)]  \tLoss:   86.095856\trec:   60.095860\tkl:   25.999994\n",
      "Epoch: 982 [40100/50000 (80%)]  \tLoss:   95.274536\trec:   67.371971\tkl:   27.902567\n",
      "====> Epoch: 982 Average train loss: 90.2092\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5991\n",
      "Epoch: 983 [  100/50000 ( 0%)]  \tLoss:   91.879921\trec:   65.220695\tkl:   26.659225\n",
      "Epoch: 983 [10100/50000 (20%)]  \tLoss:   91.069763\trec:   64.380768\tkl:   26.688993\n",
      "Epoch: 983 [20100/50000 (40%)]  \tLoss:   89.311081\trec:   63.908314\tkl:   25.402765\n",
      "Epoch: 983 [30100/50000 (60%)]  \tLoss:   90.600601\trec:   64.264267\tkl:   26.336332\n",
      "Epoch: 983 [40100/50000 (80%)]  \tLoss:   88.216118\trec:   62.057724\tkl:   26.158400\n",
      "====> Epoch: 983 Average train loss: 90.2104\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4548\n",
      "Epoch: 984 [  100/50000 ( 0%)]  \tLoss:   88.490532\trec:   62.538826\tkl:   25.951712\n",
      "Epoch: 984 [10100/50000 (20%)]  \tLoss:   90.887360\trec:   64.185760\tkl:   26.701599\n",
      "Epoch: 984 [20100/50000 (40%)]  \tLoss:   90.223663\trec:   62.972557\tkl:   27.251102\n",
      "Epoch: 984 [30100/50000 (60%)]  \tLoss:   95.673798\trec:   68.482788\tkl:   27.191008\n",
      "Epoch: 984 [40100/50000 (80%)]  \tLoss:   93.376198\trec:   66.086914\tkl:   27.289286\n",
      "====> Epoch: 984 Average train loss: 90.2036\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5867\n",
      "Epoch: 985 [  100/50000 ( 0%)]  \tLoss:   87.795387\trec:   62.246902\tkl:   25.548481\n",
      "Epoch: 985 [10100/50000 (20%)]  \tLoss:   87.244698\trec:   60.909023\tkl:   26.335676\n",
      "Epoch: 985 [20100/50000 (40%)]  \tLoss:   89.911446\trec:   63.304699\tkl:   26.606747\n",
      "Epoch: 985 [30100/50000 (60%)]  \tLoss:   91.381050\trec:   64.715942\tkl:   26.665102\n",
      "Epoch: 985 [40100/50000 (80%)]  \tLoss:   92.529297\trec:   65.752739\tkl:   26.776558\n",
      "====> Epoch: 985 Average train loss: 90.1952\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5490\n",
      "Epoch: 986 [  100/50000 ( 0%)]  \tLoss:   85.860489\trec:   60.089489\tkl:   25.771000\n",
      "Epoch: 986 [10100/50000 (20%)]  \tLoss:   91.472908\trec:   65.430984\tkl:   26.041925\n",
      "Epoch: 986 [20100/50000 (40%)]  \tLoss:   92.609375\trec:   66.092506\tkl:   26.516863\n",
      "Epoch: 986 [30100/50000 (60%)]  \tLoss:   87.798233\trec:   61.672909\tkl:   26.125322\n",
      "Epoch: 986 [40100/50000 (80%)]  \tLoss:   90.855606\trec:   64.545265\tkl:   26.310337\n",
      "====> Epoch: 986 Average train loss: 90.1925\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6431\n",
      "Epoch: 987 [  100/50000 ( 0%)]  \tLoss:   90.289406\trec:   63.989872\tkl:   26.299530\n",
      "Epoch: 987 [10100/50000 (20%)]  \tLoss:   90.656738\trec:   64.774101\tkl:   25.882626\n",
      "Epoch: 987 [20100/50000 (40%)]  \tLoss:   92.781227\trec:   66.397430\tkl:   26.383802\n",
      "Epoch: 987 [30100/50000 (60%)]  \tLoss:   92.248268\trec:   66.031700\tkl:   26.216570\n",
      "Epoch: 987 [40100/50000 (80%)]  \tLoss:   94.411972\trec:   66.872208\tkl:   27.539768\n",
      "====> Epoch: 987 Average train loss: 90.2018\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5926\n",
      "Epoch: 988 [  100/50000 ( 0%)]  \tLoss:   92.723885\trec:   66.373642\tkl:   26.350243\n",
      "Epoch: 988 [10100/50000 (20%)]  \tLoss:   89.531967\trec:   63.904942\tkl:   25.627035\n",
      "Epoch: 988 [20100/50000 (40%)]  \tLoss:   88.236923\trec:   62.753738\tkl:   25.483183\n",
      "Epoch: 988 [30100/50000 (60%)]  \tLoss:   88.030762\trec:   61.445786\tkl:   26.584978\n",
      "Epoch: 988 [40100/50000 (80%)]  \tLoss:   92.257355\trec:   65.761818\tkl:   26.495531\n",
      "====> Epoch: 988 Average train loss: 90.2033\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5985\n",
      "Epoch: 989 [  100/50000 ( 0%)]  \tLoss:   89.921349\trec:   63.304314\tkl:   26.617031\n",
      "Epoch: 989 [10100/50000 (20%)]  \tLoss:   91.145798\trec:   63.848179\tkl:   27.297626\n",
      "Epoch: 989 [20100/50000 (40%)]  \tLoss:   86.281044\trec:   60.874737\tkl:   25.406307\n",
      "Epoch: 989 [30100/50000 (60%)]  \tLoss:   90.234894\trec:   64.049217\tkl:   26.185677\n",
      "Epoch: 989 [40100/50000 (80%)]  \tLoss:   91.455009\trec:   65.045563\tkl:   26.409441\n",
      "====> Epoch: 989 Average train loss: 90.1802\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4948\n",
      "Epoch: 990 [  100/50000 ( 0%)]  \tLoss:   90.838501\trec:   63.787060\tkl:   27.051443\n",
      "Epoch: 990 [10100/50000 (20%)]  \tLoss:   87.171532\trec:   60.990158\tkl:   26.181372\n",
      "Epoch: 990 [20100/50000 (40%)]  \tLoss:   91.743355\trec:   65.724167\tkl:   26.019186\n",
      "Epoch: 990 [30100/50000 (60%)]  \tLoss:   92.266945\trec:   65.059807\tkl:   27.207136\n",
      "Epoch: 990 [40100/50000 (80%)]  \tLoss:   85.810974\trec:   59.846279\tkl:   25.964701\n",
      "====> Epoch: 990 Average train loss: 90.2015\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5189\n",
      "Epoch: 991 [  100/50000 ( 0%)]  \tLoss:   87.408150\trec:   60.875107\tkl:   26.533047\n",
      "Epoch: 991 [10100/50000 (20%)]  \tLoss:   88.836296\trec:   62.639904\tkl:   26.196390\n",
      "Epoch: 991 [20100/50000 (40%)]  \tLoss:   89.417061\trec:   62.902378\tkl:   26.514679\n",
      "Epoch: 991 [30100/50000 (60%)]  \tLoss:   87.373756\trec:   61.118046\tkl:   26.255711\n",
      "Epoch: 991 [40100/50000 (80%)]  \tLoss:   90.169952\trec:   63.357094\tkl:   26.812853\n",
      "====> Epoch: 991 Average train loss: 90.1932\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5553\n",
      "Epoch: 992 [  100/50000 ( 0%)]  \tLoss:   94.404694\trec:   68.072212\tkl:   26.332479\n",
      "Epoch: 992 [10100/50000 (20%)]  \tLoss:   85.941406\trec:   60.451775\tkl:   25.489628\n",
      "Epoch: 992 [20100/50000 (40%)]  \tLoss:   93.264275\trec:   66.153183\tkl:   27.111092\n",
      "Epoch: 992 [30100/50000 (60%)]  \tLoss:   92.101173\trec:   65.571564\tkl:   26.529610\n",
      "Epoch: 992 [40100/50000 (80%)]  \tLoss:   91.733757\trec:   64.838966\tkl:   26.894794\n",
      "====> Epoch: 992 Average train loss: 90.1860\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5943\n",
      "Epoch: 993 [  100/50000 ( 0%)]  \tLoss:   89.442398\trec:   62.614929\tkl:   26.827465\n",
      "Epoch: 993 [10100/50000 (20%)]  \tLoss:   88.534927\trec:   62.616478\tkl:   25.918451\n",
      "Epoch: 993 [20100/50000 (40%)]  \tLoss:   90.628433\trec:   63.540928\tkl:   27.087503\n",
      "Epoch: 993 [30100/50000 (60%)]  \tLoss:   91.020027\trec:   64.199593\tkl:   26.820436\n",
      "Epoch: 993 [40100/50000 (80%)]  \tLoss:   91.049370\trec:   64.381889\tkl:   26.667477\n",
      "====> Epoch: 993 Average train loss: 90.1771\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4889\n",
      "Epoch: 994 [  100/50000 ( 0%)]  \tLoss:   90.462646\trec:   63.962704\tkl:   26.499943\n",
      "Epoch: 994 [10100/50000 (20%)]  \tLoss:   88.121147\trec:   61.807888\tkl:   26.313259\n",
      "Epoch: 994 [20100/50000 (40%)]  \tLoss:   86.771431\trec:   60.233791\tkl:   26.537642\n",
      "Epoch: 994 [30100/50000 (60%)]  \tLoss:   93.881363\trec:   66.797783\tkl:   27.083580\n",
      "Epoch: 994 [40100/50000 (80%)]  \tLoss:   88.304253\trec:   61.585560\tkl:   26.718700\n",
      "====> Epoch: 994 Average train loss: 90.1912\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.6974\n",
      "Epoch: 995 [  100/50000 ( 0%)]  \tLoss:   88.333824\trec:   61.917778\tkl:   26.416044\n",
      "Epoch: 995 [10100/50000 (20%)]  \tLoss:   86.132759\trec:   60.355984\tkl:   25.776779\n",
      "Epoch: 995 [20100/50000 (40%)]  \tLoss:   86.351387\trec:   61.047024\tkl:   25.304358\n",
      "Epoch: 995 [30100/50000 (60%)]  \tLoss:   89.537712\trec:   63.955410\tkl:   25.582302\n",
      "Epoch: 995 [40100/50000 (80%)]  \tLoss:   89.974106\trec:   62.909885\tkl:   27.064222\n",
      "====> Epoch: 995 Average train loss: 90.1804\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.4925\n",
      "Epoch: 996 [  100/50000 ( 0%)]  \tLoss:   92.027634\trec:   65.553787\tkl:   26.473850\n",
      "Epoch: 996 [10100/50000 (20%)]  \tLoss:   88.664040\trec:   63.183735\tkl:   25.480312\n",
      "Epoch: 996 [20100/50000 (40%)]  \tLoss:   90.043541\trec:   63.976299\tkl:   26.067245\n",
      "Epoch: 996 [30100/50000 (60%)]  \tLoss:   86.265411\trec:   60.170658\tkl:   26.094755\n",
      "Epoch: 996 [40100/50000 (80%)]  \tLoss:   88.876335\trec:   62.561787\tkl:   26.314550\n",
      "====> Epoch: 996 Average train loss: 90.1687\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5022\n",
      "Epoch: 997 [  100/50000 ( 0%)]  \tLoss:   88.021736\trec:   61.649883\tkl:   26.371853\n",
      "Epoch: 997 [10100/50000 (20%)]  \tLoss:   89.566772\trec:   63.101574\tkl:   26.465200\n",
      "Epoch: 997 [20100/50000 (40%)]  \tLoss:   93.996483\trec:   67.211693\tkl:   26.784794\n",
      "Epoch: 997 [30100/50000 (60%)]  \tLoss:   87.258644\trec:   61.148064\tkl:   26.110579\n",
      "Epoch: 997 [40100/50000 (80%)]  \tLoss:   91.500183\trec:   64.941940\tkl:   26.558241\n",
      "====> Epoch: 997 Average train loss: 90.1814\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5676\n",
      "Epoch: 998 [  100/50000 ( 0%)]  \tLoss:   93.830132\trec:   66.695663\tkl:   27.134470\n",
      "Epoch: 998 [10100/50000 (20%)]  \tLoss:   84.968506\trec:   59.502808\tkl:   25.465694\n",
      "Epoch: 998 [20100/50000 (40%)]  \tLoss:   88.442009\trec:   62.961750\tkl:   25.480263\n",
      "Epoch: 998 [30100/50000 (60%)]  \tLoss:   91.754295\trec:   64.878815\tkl:   26.875481\n",
      "Epoch: 998 [40100/50000 (80%)]  \tLoss:   87.824120\trec:   61.741467\tkl:   26.082651\n",
      "====> Epoch: 998 Average train loss: 90.1712\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5897\n",
      "Epoch: 999 [  100/50000 ( 0%)]  \tLoss:   94.660606\trec:   67.494934\tkl:   27.165668\n",
      "Epoch: 999 [10100/50000 (20%)]  \tLoss:   96.413437\trec:   69.195854\tkl:   27.217583\n",
      "Epoch: 999 [20100/50000 (40%)]  \tLoss:   89.745346\trec:   63.790817\tkl:   25.954531\n",
      "Epoch: 999 [30100/50000 (60%)]  \tLoss:   94.107704\trec:   67.539932\tkl:   26.567776\n",
      "Epoch: 999 [40100/50000 (80%)]  \tLoss:   92.294434\trec:   66.156219\tkl:   26.138206\n",
      "====> Epoch: 999 Average train loss: 90.1751\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5780\n",
      "Epoch: 1000 [  100/50000 ( 0%)]  \tLoss:   89.807716\trec:   64.208130\tkl:   25.599579\n",
      "Epoch: 1000 [10100/50000 (20%)]  \tLoss:   93.236092\trec:   65.905701\tkl:   27.330395\n",
      "Epoch: 1000 [20100/50000 (40%)]  \tLoss:   95.030762\trec:   67.313126\tkl:   27.717634\n",
      "Epoch: 1000 [30100/50000 (60%)]  \tLoss:   88.334122\trec:   61.972313\tkl:   26.361801\n",
      "Epoch: 1000 [40100/50000 (80%)]  \tLoss:   90.594383\trec:   64.207306\tkl:   26.387077\n",
      "====> Epoch: 1000 Average train loss: 90.1885\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.5420\n",
      "====> Validation set loss: 94.5933\n",
      "Computing log-likelihood on test set\n",
      "Progress: 0.00%\n",
      "Progress: 10.00%\n",
      "Progress: 20.00%\n",
      "Progress: 30.00%\n",
      "Progress: 40.00%\n",
      "Progress: 50.00%\n",
      "Progress: 60.00%\n",
      "Progress: 70.00%\n",
      "Progress: 80.00%\n",
      "Progress: 90.00%\n",
      "====> Test set loss: 93.6319\n",
      "====> Test set log-likelihood: 89.1330\n"
     ]
    }
   ],
   "source": [
    "%run main_experiment_VAE.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO+6eAcKFIMEnTRFcueILSV",
   "name": "test10_third.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
