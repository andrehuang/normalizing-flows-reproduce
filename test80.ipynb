{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1078,
     "status": "ok",
     "timestamp": 1616352072718,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "fNJNenu3mbOs",
    "outputId": "8160e24c-d9bc-4d0c-d3e5-975aecf47863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2791,
     "status": "ok",
     "timestamp": 1616352074438,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "ueTs9SJumgG-",
    "outputId": "d75737eb-8a8a-4142-cc65-9b417d6f6dc6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2788,
     "status": "ok",
     "timestamp": 1616352074439,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "QxRpIeccmhYQ",
    "outputId": "6dc2b499-63b7-4c7f-8498-5609bda6b774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "cd \"/content/drive/MyDrive/Colab Notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34542955,
     "status": "ok",
     "timestamp": 1616386614610,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "wMWZQwPkmi2u",
    "outputId": "c9475006-26f6-4755-c638-52c8867d06ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8185\n",
      "Epoch: 378 [  100/50000 ( 0%)]  \tLoss:   90.689125\trec:   62.999043\tkl:   27.690086\n",
      "Epoch: 378 [10100/50000 (20%)]  \tLoss:   91.034119\trec:   63.343513\tkl:   27.690599\n",
      "Epoch: 378 [20100/50000 (40%)]  \tLoss:   89.492760\trec:   62.267792\tkl:   27.224972\n",
      "Epoch: 378 [30100/50000 (60%)]  \tLoss:   86.254318\trec:   59.447029\tkl:   26.807285\n",
      "Epoch: 378 [40100/50000 (80%)]  \tLoss:   92.038574\trec:   64.811127\tkl:   27.227440\n",
      "====> Epoch: 378 Average train loss: 90.6971\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7377\n",
      "Epoch: 379 [  100/50000 ( 0%)]  \tLoss:   91.855606\trec:   63.769245\tkl:   28.086361\n",
      "Epoch: 379 [10100/50000 (20%)]  \tLoss:   92.403809\trec:   65.068329\tkl:   27.335484\n",
      "Epoch: 379 [20100/50000 (40%)]  \tLoss:   90.437927\trec:   63.267441\tkl:   27.170488\n",
      "Epoch: 379 [30100/50000 (60%)]  \tLoss:   88.719620\trec:   62.256664\tkl:   26.462954\n",
      "Epoch: 379 [40100/50000 (80%)]  \tLoss:   91.472443\trec:   64.454834\tkl:   27.017607\n",
      "====> Epoch: 379 Average train loss: 90.6768\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.9313\n",
      "Epoch: 380 [  100/50000 ( 0%)]  \tLoss:   91.942177\trec:   64.100525\tkl:   27.841650\n",
      "Epoch: 380 [10100/50000 (20%)]  \tLoss:   90.304482\trec:   61.887928\tkl:   28.416552\n",
      "Epoch: 380 [20100/50000 (40%)]  \tLoss:   89.710869\trec:   63.214443\tkl:   26.496428\n",
      "Epoch: 380 [30100/50000 (60%)]  \tLoss:   89.942146\trec:   63.104267\tkl:   26.837885\n",
      "Epoch: 380 [40100/50000 (80%)]  \tLoss:   90.487526\trec:   63.241013\tkl:   27.246513\n",
      "====> Epoch: 380 Average train loss: 90.6891\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8902\n",
      "Epoch: 381 [  100/50000 ( 0%)]  \tLoss:   93.665894\trec:   66.476242\tkl:   27.189648\n",
      "Epoch: 381 [10100/50000 (20%)]  \tLoss:   91.080154\trec:   63.248337\tkl:   27.831810\n",
      "Epoch: 381 [20100/50000 (40%)]  \tLoss:   86.735680\trec:   59.855595\tkl:   26.880087\n",
      "Epoch: 381 [30100/50000 (60%)]  \tLoss:   85.825584\trec:   58.691208\tkl:   27.134377\n",
      "Epoch: 381 [40100/50000 (80%)]  \tLoss:   92.992554\trec:   64.189873\tkl:   28.802691\n",
      "====> Epoch: 381 Average train loss: 90.6913\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.9354\n",
      "Epoch: 382 [  100/50000 ( 0%)]  \tLoss:   91.277626\trec:   63.922695\tkl:   27.354931\n",
      "Epoch: 382 [10100/50000 (20%)]  \tLoss:   91.651657\trec:   63.455906\tkl:   28.195757\n",
      "Epoch: 382 [20100/50000 (40%)]  \tLoss:   92.507324\trec:   64.928047\tkl:   27.579277\n",
      "Epoch: 382 [30100/50000 (60%)]  \tLoss:   90.450485\trec:   62.480625\tkl:   27.969868\n",
      "Epoch: 382 [40100/50000 (80%)]  \tLoss:   89.188828\trec:   62.229206\tkl:   26.959614\n",
      "====> Epoch: 382 Average train loss: 90.6724\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8285\n",
      "Epoch: 383 [  100/50000 ( 0%)]  \tLoss:   91.883530\trec:   64.673943\tkl:   27.209585\n",
      "Epoch: 383 [10100/50000 (20%)]  \tLoss:   90.529442\trec:   62.516937\tkl:   28.012505\n",
      "Epoch: 383 [20100/50000 (40%)]  \tLoss:   89.319611\trec:   61.823280\tkl:   27.496323\n",
      "Epoch: 383 [30100/50000 (60%)]  \tLoss:   91.945328\trec:   64.194824\tkl:   27.750502\n",
      "Epoch: 383 [40100/50000 (80%)]  \tLoss:   93.055824\trec:   64.556442\tkl:   28.499384\n",
      "====> Epoch: 383 Average train loss: 90.6487\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7725\n",
      "Epoch: 384 [  100/50000 ( 0%)]  \tLoss:   91.618736\trec:   63.660862\tkl:   27.957876\n",
      "Epoch: 384 [10100/50000 (20%)]  \tLoss:   89.478065\trec:   61.993591\tkl:   27.484476\n",
      "Epoch: 384 [20100/50000 (40%)]  \tLoss:   91.549644\trec:   63.047104\tkl:   28.502541\n",
      "Epoch: 384 [30100/50000 (60%)]  \tLoss:   85.791084\trec:   58.116219\tkl:   27.674862\n",
      "Epoch: 384 [40100/50000 (80%)]  \tLoss:   89.814842\trec:   62.292107\tkl:   27.522736\n",
      "====> Epoch: 384 Average train loss: 90.6587\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7879\n",
      "Epoch: 385 [  100/50000 ( 0%)]  \tLoss:   89.446320\trec:   61.520359\tkl:   27.925959\n",
      "Epoch: 385 [10100/50000 (20%)]  \tLoss:   95.002113\trec:   66.662369\tkl:   28.339745\n",
      "Epoch: 385 [20100/50000 (40%)]  \tLoss:   89.065971\trec:   61.437840\tkl:   27.628134\n",
      "Epoch: 385 [30100/50000 (60%)]  \tLoss:   92.672707\trec:   64.481461\tkl:   28.191240\n",
      "Epoch: 385 [40100/50000 (80%)]  \tLoss:   88.834137\trec:   61.299961\tkl:   27.534180\n",
      "====> Epoch: 385 Average train loss: 90.6529\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8771\n",
      "Epoch: 386 [  100/50000 ( 0%)]  \tLoss:   93.328102\trec:   65.864960\tkl:   27.463148\n",
      "Epoch: 386 [10100/50000 (20%)]  \tLoss:   89.383827\trec:   62.006630\tkl:   27.377201\n",
      "Epoch: 386 [20100/50000 (40%)]  \tLoss:   91.418129\trec:   63.795506\tkl:   27.622627\n",
      "Epoch: 386 [30100/50000 (60%)]  \tLoss:   89.152351\trec:   62.024384\tkl:   27.127970\n",
      "Epoch: 386 [40100/50000 (80%)]  \tLoss:   95.140114\trec:   67.072517\tkl:   28.067593\n",
      "====> Epoch: 386 Average train loss: 90.6274\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7741\n",
      "Epoch: 387 [  100/50000 ( 0%)]  \tLoss:   91.066170\trec:   63.035145\tkl:   28.031019\n",
      "Epoch: 387 [10100/50000 (20%)]  \tLoss:   90.415993\trec:   62.098133\tkl:   28.317863\n",
      "Epoch: 387 [20100/50000 (40%)]  \tLoss:   89.378052\trec:   61.233490\tkl:   28.144567\n",
      "Epoch: 387 [30100/50000 (60%)]  \tLoss:   86.754807\trec:   59.644279\tkl:   27.110521\n",
      "Epoch: 387 [40100/50000 (80%)]  \tLoss:   92.843620\trec:   65.058777\tkl:   27.784842\n",
      "====> Epoch: 387 Average train loss: 90.6306\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8160\n",
      "Epoch: 388 [  100/50000 ( 0%)]  \tLoss:   88.328835\trec:   60.238457\tkl:   28.090380\n",
      "Epoch: 388 [10100/50000 (20%)]  \tLoss:   90.062889\trec:   61.785789\tkl:   28.277102\n",
      "Epoch: 388 [20100/50000 (40%)]  \tLoss:   90.774467\trec:   63.238083\tkl:   27.536386\n",
      "Epoch: 388 [30100/50000 (60%)]  \tLoss:   91.959877\trec:   64.785088\tkl:   27.174795\n",
      "Epoch: 388 [40100/50000 (80%)]  \tLoss:   87.671387\trec:   60.059364\tkl:   27.612019\n",
      "====> Epoch: 388 Average train loss: 90.6485\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7959\n",
      "Epoch: 389 [  100/50000 ( 0%)]  \tLoss:   91.327431\trec:   63.477386\tkl:   27.850044\n",
      "Epoch: 389 [10100/50000 (20%)]  \tLoss:   93.334785\trec:   64.806114\tkl:   28.528666\n",
      "Epoch: 389 [20100/50000 (40%)]  \tLoss:   90.474861\trec:   63.359573\tkl:   27.115288\n",
      "Epoch: 389 [30100/50000 (60%)]  \tLoss:   93.570122\trec:   65.960411\tkl:   27.609709\n",
      "Epoch: 389 [40100/50000 (80%)]  \tLoss:   88.328316\trec:   61.304722\tkl:   27.023603\n",
      "====> Epoch: 389 Average train loss: 90.6113\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6670\n",
      "Epoch: 390 [  100/50000 ( 0%)]  \tLoss:   90.041115\trec:   62.191860\tkl:   27.849247\n",
      "Epoch: 390 [10100/50000 (20%)]  \tLoss:   91.829918\trec:   63.202084\tkl:   28.627838\n",
      "Epoch: 390 [20100/50000 (40%)]  \tLoss:   88.464355\trec:   61.660332\tkl:   26.804026\n",
      "Epoch: 390 [30100/50000 (60%)]  \tLoss:   91.281128\trec:   63.548832\tkl:   27.732302\n",
      "Epoch: 390 [40100/50000 (80%)]  \tLoss:   91.808823\trec:   63.949627\tkl:   27.859194\n",
      "====> Epoch: 390 Average train loss: 90.6145\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8513\n",
      "Epoch: 391 [  100/50000 ( 0%)]  \tLoss:   91.617691\trec:   63.698215\tkl:   27.919479\n",
      "Epoch: 391 [10100/50000 (20%)]  \tLoss:   90.973022\trec:   63.405964\tkl:   27.567055\n",
      "Epoch: 391 [20100/50000 (40%)]  \tLoss:   88.331940\trec:   60.957100\tkl:   27.374844\n",
      "Epoch: 391 [30100/50000 (60%)]  \tLoss:   91.458672\trec:   63.645245\tkl:   27.813433\n",
      "Epoch: 391 [40100/50000 (80%)]  \tLoss:   92.258766\trec:   64.826584\tkl:   27.432192\n",
      "====> Epoch: 391 Average train loss: 90.6097\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8260\n",
      "Epoch: 392 [  100/50000 ( 0%)]  \tLoss:   89.249336\trec:   61.670624\tkl:   27.578714\n",
      "Epoch: 392 [10100/50000 (20%)]  \tLoss:   92.984344\trec:   65.120216\tkl:   27.864126\n",
      "Epoch: 392 [20100/50000 (40%)]  \tLoss:   86.751717\trec:   60.025414\tkl:   26.726309\n",
      "Epoch: 392 [30100/50000 (60%)]  \tLoss:   88.314072\trec:   60.527981\tkl:   27.786091\n",
      "Epoch: 392 [40100/50000 (80%)]  \tLoss:   91.172546\trec:   63.269665\tkl:   27.902880\n",
      "====> Epoch: 392 Average train loss: 90.6005\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8003\n",
      "Epoch: 393 [  100/50000 ( 0%)]  \tLoss:   96.035973\trec:   67.054131\tkl:   28.981850\n",
      "Epoch: 393 [10100/50000 (20%)]  \tLoss:   93.566025\trec:   65.198097\tkl:   28.367924\n",
      "Epoch: 393 [20100/50000 (40%)]  \tLoss:   92.623962\trec:   64.849533\tkl:   27.774427\n",
      "Epoch: 393 [30100/50000 (60%)]  \tLoss:   90.052917\trec:   62.985649\tkl:   27.067272\n",
      "Epoch: 393 [40100/50000 (80%)]  \tLoss:   94.084595\trec:   65.058243\tkl:   29.026358\n",
      "====> Epoch: 393 Average train loss: 90.5895\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7716\n",
      "Epoch: 394 [  100/50000 ( 0%)]  \tLoss:   90.623222\trec:   62.122128\tkl:   28.501091\n",
      "Epoch: 394 [10100/50000 (20%)]  \tLoss:   88.978622\trec:   61.522175\tkl:   27.456444\n",
      "Epoch: 394 [20100/50000 (40%)]  \tLoss:   93.080956\trec:   65.947006\tkl:   27.133951\n",
      "Epoch: 394 [30100/50000 (60%)]  \tLoss:   89.793152\trec:   62.224392\tkl:   27.568762\n",
      "Epoch: 394 [40100/50000 (80%)]  \tLoss:   94.778023\trec:   66.945244\tkl:   27.832777\n",
      "====> Epoch: 394 Average train loss: 90.5970\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8204\n",
      "Epoch: 395 [  100/50000 ( 0%)]  \tLoss:   90.916054\trec:   62.767597\tkl:   28.148460\n",
      "Epoch: 395 [10100/50000 (20%)]  \tLoss:   88.548859\trec:   60.586555\tkl:   27.962297\n",
      "Epoch: 395 [20100/50000 (40%)]  \tLoss:   91.884308\trec:   64.150200\tkl:   27.734098\n",
      "Epoch: 395 [30100/50000 (60%)]  \tLoss:   91.237633\trec:   63.584003\tkl:   27.653627\n",
      "Epoch: 395 [40100/50000 (80%)]  \tLoss:   92.739273\trec:   64.708496\tkl:   28.030775\n",
      "====> Epoch: 395 Average train loss: 90.5730\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7591\n",
      "Epoch: 396 [  100/50000 ( 0%)]  \tLoss:   89.395622\trec:   61.953533\tkl:   27.442091\n",
      "Epoch: 396 [10100/50000 (20%)]  \tLoss:   89.081696\trec:   61.648788\tkl:   27.432907\n",
      "Epoch: 396 [20100/50000 (40%)]  \tLoss:   90.160049\trec:   62.348133\tkl:   27.811913\n",
      "Epoch: 396 [30100/50000 (60%)]  \tLoss:   89.852638\trec:   62.231533\tkl:   27.621105\n",
      "Epoch: 396 [40100/50000 (80%)]  \tLoss:   91.230202\trec:   63.200432\tkl:   28.029770\n",
      "====> Epoch: 396 Average train loss: 90.5860\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8078\n",
      "Epoch: 397 [  100/50000 ( 0%)]  \tLoss:   89.434219\trec:   61.263699\tkl:   28.170517\n",
      "Epoch: 397 [10100/50000 (20%)]  \tLoss:   87.954918\trec:   60.464245\tkl:   27.490679\n",
      "Epoch: 397 [20100/50000 (40%)]  \tLoss:   92.379997\trec:   64.138611\tkl:   28.241390\n",
      "Epoch: 397 [30100/50000 (60%)]  \tLoss:   94.767403\trec:   66.711555\tkl:   28.055841\n",
      "Epoch: 397 [40100/50000 (80%)]  \tLoss:   87.651031\trec:   60.714554\tkl:   26.936483\n",
      "====> Epoch: 397 Average train loss: 90.5592\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7232\n",
      "Epoch: 398 [  100/50000 ( 0%)]  \tLoss:   86.761795\trec:   60.120068\tkl:   26.641733\n",
      "Epoch: 398 [10100/50000 (20%)]  \tLoss:   88.579323\trec:   61.576523\tkl:   27.002802\n",
      "Epoch: 398 [20100/50000 (40%)]  \tLoss:   88.958145\trec:   62.475952\tkl:   26.482197\n",
      "Epoch: 398 [30100/50000 (60%)]  \tLoss:   91.651398\trec:   63.403278\tkl:   28.248116\n",
      "Epoch: 398 [40100/50000 (80%)]  \tLoss:   91.476578\trec:   63.302525\tkl:   28.174059\n",
      "====> Epoch: 398 Average train loss: 90.5680\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7851\n",
      "Epoch: 399 [  100/50000 ( 0%)]  \tLoss:   92.989174\trec:   65.234581\tkl:   27.754591\n",
      "Epoch: 399 [10100/50000 (20%)]  \tLoss:   87.333435\trec:   60.254890\tkl:   27.078539\n",
      "Epoch: 399 [20100/50000 (40%)]  \tLoss:   92.641304\trec:   64.168121\tkl:   28.473179\n",
      "Epoch: 399 [30100/50000 (60%)]  \tLoss:   86.695175\trec:   60.459743\tkl:   26.235430\n",
      "Epoch: 399 [40100/50000 (80%)]  \tLoss:   91.646538\trec:   63.232430\tkl:   28.414108\n",
      "====> Epoch: 399 Average train loss: 90.5591\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8489\n",
      "Epoch: 400 [  100/50000 ( 0%)]  \tLoss:   95.632797\trec:   67.704567\tkl:   27.928230\n",
      "Epoch: 400 [10100/50000 (20%)]  \tLoss:   90.424019\trec:   62.700317\tkl:   27.723703\n",
      "Epoch: 400 [20100/50000 (40%)]  \tLoss:   91.790718\trec:   63.412617\tkl:   28.378105\n",
      "Epoch: 400 [30100/50000 (60%)]  \tLoss:   90.545998\trec:   63.562592\tkl:   26.983398\n",
      "Epoch: 400 [40100/50000 (80%)]  \tLoss:   90.535469\trec:   62.442890\tkl:   28.092581\n",
      "====> Epoch: 400 Average train loss: 90.5597\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8128\n",
      "Epoch: 401 [  100/50000 ( 0%)]  \tLoss:   92.302887\trec:   63.855839\tkl:   28.447050\n",
      "Epoch: 401 [10100/50000 (20%)]  \tLoss:   90.903816\trec:   62.572205\tkl:   28.331612\n",
      "Epoch: 401 [20100/50000 (40%)]  \tLoss:   89.480858\trec:   61.333557\tkl:   28.147297\n",
      "Epoch: 401 [30100/50000 (60%)]  \tLoss:   93.652809\trec:   64.486496\tkl:   29.166313\n",
      "Epoch: 401 [40100/50000 (80%)]  \tLoss:   91.645576\trec:   64.621811\tkl:   27.023764\n",
      "====> Epoch: 401 Average train loss: 90.5561\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7233\n",
      "Epoch: 402 [  100/50000 ( 0%)]  \tLoss:   92.681862\trec:   64.189255\tkl:   28.492607\n",
      "Epoch: 402 [10100/50000 (20%)]  \tLoss:   89.618866\trec:   62.292957\tkl:   27.325905\n",
      "Epoch: 402 [20100/50000 (40%)]  \tLoss:   90.269341\trec:   61.849411\tkl:   28.419931\n",
      "Epoch: 402 [30100/50000 (60%)]  \tLoss:   95.104782\trec:   66.769043\tkl:   28.335737\n",
      "Epoch: 402 [40100/50000 (80%)]  \tLoss:   89.710037\trec:   62.521130\tkl:   27.188900\n",
      "====> Epoch: 402 Average train loss: 90.5329\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.8557\n",
      "Epoch: 403 [  100/50000 ( 0%)]  \tLoss:   93.386383\trec:   65.240410\tkl:   28.145971\n",
      "Epoch: 403 [10100/50000 (20%)]  \tLoss:   90.415565\trec:   62.239571\tkl:   28.176001\n",
      "Epoch: 403 [20100/50000 (40%)]  \tLoss:   92.883530\trec:   64.339432\tkl:   28.544102\n",
      "Epoch: 403 [30100/50000 (60%)]  \tLoss:   87.897896\trec:   60.545300\tkl:   27.352600\n",
      "Epoch: 403 [40100/50000 (80%)]  \tLoss:   95.764435\trec:   67.154335\tkl:   28.610092\n",
      "====> Epoch: 403 Average train loss: 90.5265\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6710\n",
      "Epoch: 404 [  100/50000 ( 0%)]  \tLoss:   91.678291\trec:   63.644306\tkl:   28.033983\n",
      "Epoch: 404 [10100/50000 (20%)]  \tLoss:   91.318413\trec:   63.429150\tkl:   27.889261\n",
      "Epoch: 404 [20100/50000 (40%)]  \tLoss:   92.106758\trec:   64.095215\tkl:   28.011543\n",
      "Epoch: 404 [30100/50000 (60%)]  \tLoss:   89.956696\trec:   62.769363\tkl:   27.187338\n",
      "Epoch: 404 [40100/50000 (80%)]  \tLoss:   95.092888\trec:   66.229622\tkl:   28.863260\n",
      "====> Epoch: 404 Average train loss: 90.4907\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6969\n",
      "Epoch: 405 [  100/50000 ( 0%)]  \tLoss:   86.781128\trec:   59.730370\tkl:   27.050766\n",
      "Epoch: 405 [10100/50000 (20%)]  \tLoss:   89.473083\trec:   61.704277\tkl:   27.768803\n",
      "Epoch: 405 [20100/50000 (40%)]  \tLoss:   90.869766\trec:   62.479492\tkl:   28.390272\n",
      "Epoch: 405 [30100/50000 (60%)]  \tLoss:   89.697418\trec:   62.027473\tkl:   27.669941\n",
      "Epoch: 405 [40100/50000 (80%)]  \tLoss:   90.589890\trec:   62.757717\tkl:   27.832170\n",
      "====> Epoch: 405 Average train loss: 90.5241\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7333\n",
      "Epoch: 406 [  100/50000 ( 0%)]  \tLoss:   92.311958\trec:   64.200546\tkl:   28.111412\n",
      "Epoch: 406 [10100/50000 (20%)]  \tLoss:   86.285332\trec:   59.345615\tkl:   26.939716\n",
      "Epoch: 406 [20100/50000 (40%)]  \tLoss:   90.746620\trec:   63.642239\tkl:   27.104380\n",
      "Epoch: 406 [30100/50000 (60%)]  \tLoss:   94.560234\trec:   66.097824\tkl:   28.462412\n",
      "Epoch: 406 [40100/50000 (80%)]  \tLoss:   92.350708\trec:   63.789101\tkl:   28.561611\n",
      "====> Epoch: 406 Average train loss: 90.5116\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7346\n",
      "Epoch: 407 [  100/50000 ( 0%)]  \tLoss:   90.493904\trec:   62.705292\tkl:   27.788609\n",
      "Epoch: 407 [10100/50000 (20%)]  \tLoss:   90.279892\trec:   63.024891\tkl:   27.255001\n",
      "Epoch: 407 [20100/50000 (40%)]  \tLoss:   93.741753\trec:   65.497574\tkl:   28.244175\n",
      "Epoch: 407 [30100/50000 (60%)]  \tLoss:   89.078545\trec:   61.929611\tkl:   27.148930\n",
      "Epoch: 407 [40100/50000 (80%)]  \tLoss:   91.156212\trec:   63.507622\tkl:   27.648582\n",
      "====> Epoch: 407 Average train loss: 90.5133\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5830\n",
      "Epoch: 408 [  100/50000 ( 0%)]  \tLoss:   90.845337\trec:   63.112816\tkl:   27.732523\n",
      "Epoch: 408 [10100/50000 (20%)]  \tLoss:   85.499138\trec:   59.086407\tkl:   26.412731\n",
      "Epoch: 408 [20100/50000 (40%)]  \tLoss:   89.821495\trec:   62.399910\tkl:   27.421579\n",
      "Epoch: 408 [30100/50000 (60%)]  \tLoss:   89.673447\trec:   61.951469\tkl:   27.721977\n",
      "Epoch: 408 [40100/50000 (80%)]  \tLoss:   88.871010\trec:   61.708454\tkl:   27.162558\n",
      "====> Epoch: 408 Average train loss: 90.4877\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6410\n",
      "Epoch: 409 [  100/50000 ( 0%)]  \tLoss:   89.863007\trec:   62.923271\tkl:   26.939734\n",
      "Epoch: 409 [10100/50000 (20%)]  \tLoss:   92.308884\trec:   64.250221\tkl:   28.058664\n",
      "Epoch: 409 [20100/50000 (40%)]  \tLoss:   91.422470\trec:   63.666794\tkl:   27.755672\n",
      "Epoch: 409 [30100/50000 (60%)]  \tLoss:   91.788185\trec:   63.971771\tkl:   27.816406\n",
      "Epoch: 409 [40100/50000 (80%)]  \tLoss:   87.233009\trec:   60.302616\tkl:   26.930395\n",
      "====> Epoch: 409 Average train loss: 90.4802\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7424\n",
      "Epoch: 410 [  100/50000 ( 0%)]  \tLoss:   85.358276\trec:   58.553158\tkl:   26.805122\n",
      "Epoch: 410 [10100/50000 (20%)]  \tLoss:   92.441833\trec:   64.244316\tkl:   28.197521\n",
      "Epoch: 410 [20100/50000 (40%)]  \tLoss:   86.342628\trec:   60.490845\tkl:   25.851784\n",
      "Epoch: 410 [30100/50000 (60%)]  \tLoss:   92.555229\trec:   64.288849\tkl:   28.266380\n",
      "Epoch: 410 [40100/50000 (80%)]  \tLoss:   92.245331\trec:   65.060402\tkl:   27.184933\n",
      "====> Epoch: 410 Average train loss: 90.4781\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5930\n",
      "Epoch: 411 [  100/50000 ( 0%)]  \tLoss:   91.003220\trec:   62.201580\tkl:   28.801643\n",
      "Epoch: 411 [10100/50000 (20%)]  \tLoss:   89.164078\trec:   61.540863\tkl:   27.623220\n",
      "Epoch: 411 [20100/50000 (40%)]  \tLoss:   92.141396\trec:   63.568661\tkl:   28.572733\n",
      "Epoch: 411 [30100/50000 (60%)]  \tLoss:   91.546631\trec:   63.721172\tkl:   27.825459\n",
      "Epoch: 411 [40100/50000 (80%)]  \tLoss:   93.367310\trec:   65.120575\tkl:   28.246740\n",
      "====> Epoch: 411 Average train loss: 90.4842\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6901\n",
      "Epoch: 412 [  100/50000 ( 0%)]  \tLoss:   85.548660\trec:   58.605312\tkl:   26.943350\n",
      "Epoch: 412 [10100/50000 (20%)]  \tLoss:   87.930954\trec:   60.410156\tkl:   27.520803\n",
      "Epoch: 412 [20100/50000 (40%)]  \tLoss:   86.915504\trec:   59.943413\tkl:   26.972094\n",
      "Epoch: 412 [30100/50000 (60%)]  \tLoss:   88.721695\trec:   61.630356\tkl:   27.091347\n",
      "Epoch: 412 [40100/50000 (80%)]  \tLoss:   90.390327\trec:   62.916103\tkl:   27.474230\n",
      "====> Epoch: 412 Average train loss: 90.4593\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6863\n",
      "Epoch: 413 [  100/50000 ( 0%)]  \tLoss:   88.184830\trec:   61.310081\tkl:   26.874752\n",
      "Epoch: 413 [10100/50000 (20%)]  \tLoss:   89.918785\trec:   62.065590\tkl:   27.853193\n",
      "Epoch: 413 [20100/50000 (40%)]  \tLoss:   91.671501\trec:   63.242855\tkl:   28.428642\n",
      "Epoch: 413 [30100/50000 (60%)]  \tLoss:   85.859825\trec:   58.367744\tkl:   27.492083\n",
      "Epoch: 413 [40100/50000 (80%)]  \tLoss:   91.440758\trec:   63.530254\tkl:   27.910513\n",
      "====> Epoch: 413 Average train loss: 90.4471\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6137\n",
      "Epoch: 414 [  100/50000 ( 0%)]  \tLoss:   92.589180\trec:   63.452473\tkl:   29.136705\n",
      "Epoch: 414 [10100/50000 (20%)]  \tLoss:   90.966743\trec:   62.701504\tkl:   28.265245\n",
      "Epoch: 414 [20100/50000 (40%)]  \tLoss:   92.157204\trec:   64.148575\tkl:   28.008633\n",
      "Epoch: 414 [30100/50000 (60%)]  \tLoss:   88.519806\trec:   61.168991\tkl:   27.350815\n",
      "Epoch: 414 [40100/50000 (80%)]  \tLoss:   92.565331\trec:   64.452934\tkl:   28.112396\n",
      "====> Epoch: 414 Average train loss: 90.4479\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6496\n",
      "Epoch: 415 [  100/50000 ( 0%)]  \tLoss:   89.920036\trec:   61.425426\tkl:   28.494608\n",
      "Epoch: 415 [10100/50000 (20%)]  \tLoss:   88.958374\trec:   60.977787\tkl:   27.980587\n",
      "Epoch: 415 [20100/50000 (40%)]  \tLoss:   88.949814\trec:   61.157631\tkl:   27.792183\n",
      "Epoch: 415 [30100/50000 (60%)]  \tLoss:   88.311958\trec:   61.511597\tkl:   26.800367\n",
      "Epoch: 415 [40100/50000 (80%)]  \tLoss:   87.696686\trec:   61.265461\tkl:   26.431225\n",
      "====> Epoch: 415 Average train loss: 90.4423\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6847\n",
      "Epoch: 416 [  100/50000 ( 0%)]  \tLoss:   90.219727\trec:   62.139656\tkl:   28.080065\n",
      "Epoch: 416 [10100/50000 (20%)]  \tLoss:   90.267677\trec:   62.852577\tkl:   27.415096\n",
      "Epoch: 416 [20100/50000 (40%)]  \tLoss:   87.460083\trec:   60.230602\tkl:   27.229479\n",
      "Epoch: 416 [30100/50000 (60%)]  \tLoss:   92.950401\trec:   64.952995\tkl:   27.997406\n",
      "Epoch: 416 [40100/50000 (80%)]  \tLoss:   91.165367\trec:   62.563869\tkl:   28.601498\n",
      "====> Epoch: 416 Average train loss: 90.4364\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6259\n",
      "Epoch: 417 [  100/50000 ( 0%)]  \tLoss:   90.080650\trec:   62.277382\tkl:   27.803274\n",
      "Epoch: 417 [10100/50000 (20%)]  \tLoss:   91.482681\trec:   63.623779\tkl:   27.858904\n",
      "Epoch: 417 [20100/50000 (40%)]  \tLoss:   91.428200\trec:   63.737663\tkl:   27.690535\n",
      "Epoch: 417 [30100/50000 (60%)]  \tLoss:   92.284958\trec:   63.443928\tkl:   28.841030\n",
      "Epoch: 417 [40100/50000 (80%)]  \tLoss:   90.116287\trec:   63.072498\tkl:   27.043783\n",
      "====> Epoch: 417 Average train loss: 90.4428\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5989\n",
      "Epoch: 418 [  100/50000 ( 0%)]  \tLoss:   90.291191\trec:   62.747093\tkl:   27.544102\n",
      "Epoch: 418 [10100/50000 (20%)]  \tLoss:   92.212967\trec:   64.131538\tkl:   28.081430\n",
      "Epoch: 418 [20100/50000 (40%)]  \tLoss:   90.625809\trec:   62.652992\tkl:   27.972816\n",
      "Epoch: 418 [30100/50000 (60%)]  \tLoss:   88.801933\trec:   61.310078\tkl:   27.491858\n",
      "Epoch: 418 [40100/50000 (80%)]  \tLoss:   94.918884\trec:   66.200676\tkl:   28.718208\n",
      "====> Epoch: 418 Average train loss: 90.4072\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5893\n",
      "Epoch: 419 [  100/50000 ( 0%)]  \tLoss:   89.015259\trec:   60.244492\tkl:   28.770771\n",
      "Epoch: 419 [10100/50000 (20%)]  \tLoss:   88.513062\trec:   60.404755\tkl:   28.108311\n",
      "Epoch: 419 [20100/50000 (40%)]  \tLoss:   87.768784\trec:   60.104668\tkl:   27.664116\n",
      "Epoch: 419 [30100/50000 (60%)]  \tLoss:   90.363960\trec:   62.899414\tkl:   27.464556\n",
      "Epoch: 419 [40100/50000 (80%)]  \tLoss:   85.862831\trec:   60.211826\tkl:   25.651001\n",
      "====> Epoch: 419 Average train loss: 90.4452\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7109\n",
      "Epoch: 420 [  100/50000 ( 0%)]  \tLoss:   91.273178\trec:   63.274921\tkl:   27.998260\n",
      "Epoch: 420 [10100/50000 (20%)]  \tLoss:   91.773964\trec:   64.511559\tkl:   27.262407\n",
      "Epoch: 420 [20100/50000 (40%)]  \tLoss:   87.810425\trec:   60.446453\tkl:   27.363974\n",
      "Epoch: 420 [30100/50000 (60%)]  \tLoss:   89.373047\trec:   62.105068\tkl:   27.267982\n",
      "Epoch: 420 [40100/50000 (80%)]  \tLoss:   91.607399\trec:   63.949631\tkl:   27.657768\n",
      "====> Epoch: 420 Average train loss: 90.4199\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6185\n",
      "Epoch: 421 [  100/50000 ( 0%)]  \tLoss:   88.548828\trec:   60.750496\tkl:   27.798325\n",
      "Epoch: 421 [10100/50000 (20%)]  \tLoss:   88.423416\trec:   61.324734\tkl:   27.098686\n",
      "Epoch: 421 [20100/50000 (40%)]  \tLoss:   90.934532\trec:   63.193993\tkl:   27.740536\n",
      "Epoch: 421 [30100/50000 (60%)]  \tLoss:   87.547462\trec:   59.533161\tkl:   28.014301\n",
      "Epoch: 421 [40100/50000 (80%)]  \tLoss:   88.891670\trec:   61.383614\tkl:   27.508057\n",
      "====> Epoch: 421 Average train loss: 90.4055\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7007\n",
      "Epoch: 422 [  100/50000 ( 0%)]  \tLoss:   89.802734\trec:   62.364021\tkl:   27.438711\n",
      "Epoch: 422 [10100/50000 (20%)]  \tLoss:   88.063194\trec:   60.777924\tkl:   27.285269\n",
      "Epoch: 422 [20100/50000 (40%)]  \tLoss:   94.038307\trec:   65.749939\tkl:   28.288368\n",
      "Epoch: 422 [30100/50000 (60%)]  \tLoss:   88.991394\trec:   61.472221\tkl:   27.519176\n",
      "Epoch: 422 [40100/50000 (80%)]  \tLoss:   89.517906\trec:   62.820248\tkl:   26.697660\n",
      "====> Epoch: 422 Average train loss: 90.3943\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5524\n",
      "Epoch: 423 [  100/50000 ( 0%)]  \tLoss:   87.778290\trec:   60.447300\tkl:   27.330994\n",
      "Epoch: 423 [10100/50000 (20%)]  \tLoss:   87.008377\trec:   60.614052\tkl:   26.394321\n",
      "Epoch: 423 [20100/50000 (40%)]  \tLoss:   86.806969\trec:   60.129101\tkl:   26.677868\n",
      "Epoch: 423 [30100/50000 (60%)]  \tLoss:   88.744125\trec:   61.471455\tkl:   27.272675\n",
      "Epoch: 423 [40100/50000 (80%)]  \tLoss:   90.189430\trec:   62.716034\tkl:   27.473402\n",
      "====> Epoch: 423 Average train loss: 90.3846\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6566\n",
      "Epoch: 424 [  100/50000 ( 0%)]  \tLoss:   87.584236\trec:   59.639439\tkl:   27.944794\n",
      "Epoch: 424 [10100/50000 (20%)]  \tLoss:   88.284744\trec:   60.961590\tkl:   27.323151\n",
      "Epoch: 424 [20100/50000 (40%)]  \tLoss:   93.046974\trec:   64.324020\tkl:   28.722958\n",
      "Epoch: 424 [30100/50000 (60%)]  \tLoss:   91.300255\trec:   64.385490\tkl:   26.914762\n",
      "Epoch: 424 [40100/50000 (80%)]  \tLoss:   92.918510\trec:   65.172440\tkl:   27.746073\n",
      "====> Epoch: 424 Average train loss: 90.3883\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6745\n",
      "Epoch: 425 [  100/50000 ( 0%)]  \tLoss:   91.321968\trec:   63.431484\tkl:   27.890486\n",
      "Epoch: 425 [10100/50000 (20%)]  \tLoss:   89.554985\trec:   61.808132\tkl:   27.746855\n",
      "Epoch: 425 [20100/50000 (40%)]  \tLoss:   92.641090\trec:   64.841850\tkl:   27.799242\n",
      "Epoch: 425 [30100/50000 (60%)]  \tLoss:   90.629234\trec:   62.401882\tkl:   28.227348\n",
      "Epoch: 425 [40100/50000 (80%)]  \tLoss:   90.450562\trec:   62.531563\tkl:   27.919003\n",
      "====> Epoch: 425 Average train loss: 90.3916\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.7239\n",
      "Epoch: 426 [  100/50000 ( 0%)]  \tLoss:   90.504822\trec:   62.914330\tkl:   27.590494\n",
      "Epoch: 426 [10100/50000 (20%)]  \tLoss:   90.951248\trec:   62.034870\tkl:   28.916372\n",
      "Epoch: 426 [20100/50000 (40%)]  \tLoss:   91.557846\trec:   63.687786\tkl:   27.870058\n",
      "Epoch: 426 [30100/50000 (60%)]  \tLoss:   89.867485\trec:   62.155376\tkl:   27.712114\n",
      "Epoch: 426 [40100/50000 (80%)]  \tLoss:   87.691635\trec:   60.800720\tkl:   26.890921\n",
      "====> Epoch: 426 Average train loss: 90.3913\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6446\n",
      "Epoch: 427 [  100/50000 ( 0%)]  \tLoss:   89.916382\trec:   62.817383\tkl:   27.098999\n",
      "Epoch: 427 [10100/50000 (20%)]  \tLoss:   89.057671\trec:   61.045124\tkl:   28.012550\n",
      "Epoch: 427 [20100/50000 (40%)]  \tLoss:   92.486473\trec:   63.757771\tkl:   28.728701\n",
      "Epoch: 427 [30100/50000 (60%)]  \tLoss:   96.145607\trec:   66.174522\tkl:   29.971081\n",
      "Epoch: 427 [40100/50000 (80%)]  \tLoss:   88.120964\trec:   60.760399\tkl:   27.360569\n",
      "====> Epoch: 427 Average train loss: 90.3873\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6526\n",
      "Epoch: 428 [  100/50000 ( 0%)]  \tLoss:   87.127365\trec:   60.132481\tkl:   26.994886\n",
      "Epoch: 428 [10100/50000 (20%)]  \tLoss:   90.505043\trec:   62.730789\tkl:   27.774258\n",
      "Epoch: 428 [20100/50000 (40%)]  \tLoss:   87.084900\trec:   59.770027\tkl:   27.314873\n",
      "Epoch: 428 [30100/50000 (60%)]  \tLoss:   89.209335\trec:   61.872204\tkl:   27.337133\n",
      "Epoch: 428 [40100/50000 (80%)]  \tLoss:   90.665527\trec:   63.047546\tkl:   27.617983\n",
      "====> Epoch: 428 Average train loss: 90.3809\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6465\n",
      "Epoch: 429 [  100/50000 ( 0%)]  \tLoss:   91.254868\trec:   63.133896\tkl:   28.120975\n",
      "Epoch: 429 [10100/50000 (20%)]  \tLoss:   88.963516\trec:   60.847416\tkl:   28.116098\n",
      "Epoch: 429 [20100/50000 (40%)]  \tLoss:   89.239136\trec:   63.075985\tkl:   26.163158\n",
      "Epoch: 429 [30100/50000 (60%)]  \tLoss:   96.555077\trec:   68.391022\tkl:   28.164062\n",
      "Epoch: 429 [40100/50000 (80%)]  \tLoss:   92.129379\trec:   64.608200\tkl:   27.521181\n",
      "====> Epoch: 429 Average train loss: 90.3677\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5981\n",
      "Epoch: 430 [  100/50000 ( 0%)]  \tLoss:   92.100456\trec:   64.606194\tkl:   27.494263\n",
      "Epoch: 430 [10100/50000 (20%)]  \tLoss:   90.423622\trec:   62.032391\tkl:   28.391230\n",
      "Epoch: 430 [20100/50000 (40%)]  \tLoss:   92.383423\trec:   64.435699\tkl:   27.947727\n",
      "Epoch: 430 [30100/50000 (60%)]  \tLoss:   89.528885\trec:   61.434662\tkl:   28.094217\n",
      "Epoch: 430 [40100/50000 (80%)]  \tLoss:   89.997673\trec:   62.596401\tkl:   27.401276\n",
      "====> Epoch: 430 Average train loss: 90.3552\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6264\n",
      "Epoch: 431 [  100/50000 ( 0%)]  \tLoss:   88.757797\trec:   61.625801\tkl:   27.132004\n",
      "Epoch: 431 [10100/50000 (20%)]  \tLoss:   93.817253\trec:   65.584488\tkl:   28.232765\n",
      "Epoch: 431 [20100/50000 (40%)]  \tLoss:   88.416130\trec:   61.355560\tkl:   27.060575\n",
      "Epoch: 431 [30100/50000 (60%)]  \tLoss:   90.690147\trec:   63.033211\tkl:   27.656933\n",
      "Epoch: 431 [40100/50000 (80%)]  \tLoss:   90.649422\trec:   62.893017\tkl:   27.756405\n",
      "====> Epoch: 431 Average train loss: 90.3382\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6825\n",
      "Epoch: 432 [  100/50000 ( 0%)]  \tLoss:   94.962517\trec:   66.275871\tkl:   28.686655\n",
      "Epoch: 432 [10100/50000 (20%)]  \tLoss:   89.512222\trec:   62.121765\tkl:   27.390463\n",
      "Epoch: 432 [20100/50000 (40%)]  \tLoss:   90.948669\trec:   62.355930\tkl:   28.592733\n",
      "Epoch: 432 [30100/50000 (60%)]  \tLoss:   87.357712\trec:   59.517666\tkl:   27.840048\n",
      "Epoch: 432 [40100/50000 (80%)]  \tLoss:   91.834587\trec:   63.082069\tkl:   28.752514\n",
      "====> Epoch: 432 Average train loss: 90.3579\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5310\n",
      "Epoch: 433 [  100/50000 ( 0%)]  \tLoss:   93.588989\trec:   64.197472\tkl:   29.391518\n",
      "Epoch: 433 [10100/50000 (20%)]  \tLoss:   89.953087\trec:   61.645718\tkl:   28.307373\n",
      "Epoch: 433 [20100/50000 (40%)]  \tLoss:   90.469147\trec:   62.552998\tkl:   27.916149\n",
      "Epoch: 433 [30100/50000 (60%)]  \tLoss:   86.446884\trec:   59.278454\tkl:   27.168427\n",
      "Epoch: 433 [40100/50000 (80%)]  \tLoss:   89.983429\trec:   61.534924\tkl:   28.448500\n",
      "====> Epoch: 433 Average train loss: 90.3639\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6063\n",
      "Epoch: 434 [  100/50000 ( 0%)]  \tLoss:   95.553337\trec:   65.751503\tkl:   29.801830\n",
      "Epoch: 434 [10100/50000 (20%)]  \tLoss:   93.351578\trec:   66.089546\tkl:   27.262033\n",
      "Epoch: 434 [20100/50000 (40%)]  \tLoss:   91.107750\trec:   62.917194\tkl:   28.190552\n",
      "Epoch: 434 [30100/50000 (60%)]  \tLoss:   87.996796\trec:   60.555790\tkl:   27.441000\n",
      "Epoch: 434 [40100/50000 (80%)]  \tLoss:   90.740074\trec:   63.141777\tkl:   27.598305\n",
      "====> Epoch: 434 Average train loss: 90.3203\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6577\n",
      "Epoch: 435 [  100/50000 ( 0%)]  \tLoss:   95.291267\trec:   66.688690\tkl:   28.602579\n",
      "Epoch: 435 [10100/50000 (20%)]  \tLoss:   87.992264\trec:   61.127590\tkl:   26.864676\n",
      "Epoch: 435 [20100/50000 (40%)]  \tLoss:   92.485962\trec:   64.544243\tkl:   27.941721\n",
      "Epoch: 435 [30100/50000 (60%)]  \tLoss:   89.020660\trec:   62.103729\tkl:   26.916933\n",
      "Epoch: 435 [40100/50000 (80%)]  \tLoss:   89.940529\trec:   61.783337\tkl:   28.157187\n",
      "====> Epoch: 435 Average train loss: 90.3093\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5824\n",
      "Epoch: 436 [  100/50000 ( 0%)]  \tLoss:   92.909142\trec:   64.001518\tkl:   28.907621\n",
      "Epoch: 436 [10100/50000 (20%)]  \tLoss:   89.794403\trec:   61.685902\tkl:   28.108500\n",
      "Epoch: 436 [20100/50000 (40%)]  \tLoss:   94.073532\trec:   65.217979\tkl:   28.855547\n",
      "Epoch: 436 [30100/50000 (60%)]  \tLoss:   89.967705\trec:   62.222397\tkl:   27.745304\n",
      "Epoch: 436 [40100/50000 (80%)]  \tLoss:   93.210114\trec:   65.325615\tkl:   27.884504\n",
      "====> Epoch: 436 Average train loss: 90.3190\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6412\n",
      "Epoch: 437 [  100/50000 ( 0%)]  \tLoss:   90.850876\trec:   62.674717\tkl:   28.176157\n",
      "Epoch: 437 [10100/50000 (20%)]  \tLoss:   86.955925\trec:   59.429882\tkl:   27.526047\n",
      "Epoch: 437 [20100/50000 (40%)]  \tLoss:   91.108444\trec:   62.596664\tkl:   28.511784\n",
      "Epoch: 437 [30100/50000 (60%)]  \tLoss:   88.065529\trec:   60.933807\tkl:   27.131718\n",
      "Epoch: 437 [40100/50000 (80%)]  \tLoss:   86.307205\trec:   59.491352\tkl:   26.815859\n",
      "====> Epoch: 437 Average train loss: 90.3081\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5660\n",
      "Epoch: 438 [  100/50000 ( 0%)]  \tLoss:   90.352859\trec:   61.457565\tkl:   28.895294\n",
      "Epoch: 438 [10100/50000 (20%)]  \tLoss:   87.880211\trec:   60.599567\tkl:   27.280640\n",
      "Epoch: 438 [20100/50000 (40%)]  \tLoss:   93.431229\trec:   65.206558\tkl:   28.224663\n",
      "Epoch: 438 [30100/50000 (60%)]  \tLoss:   89.962891\trec:   63.230980\tkl:   26.731903\n",
      "Epoch: 438 [40100/50000 (80%)]  \tLoss:   92.937752\trec:   64.337372\tkl:   28.600380\n",
      "====> Epoch: 438 Average train loss: 90.2858\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6744\n",
      "Epoch: 439 [  100/50000 ( 0%)]  \tLoss:   90.710701\trec:   62.467289\tkl:   28.243412\n",
      "Epoch: 439 [10100/50000 (20%)]  \tLoss:   88.859039\trec:   61.364124\tkl:   27.494921\n",
      "Epoch: 439 [20100/50000 (40%)]  \tLoss:   89.860367\trec:   63.075508\tkl:   26.784863\n",
      "Epoch: 439 [30100/50000 (60%)]  \tLoss:   91.696609\trec:   63.637287\tkl:   28.059320\n",
      "Epoch: 439 [40100/50000 (80%)]  \tLoss:   90.882820\trec:   62.232098\tkl:   28.650723\n",
      "====> Epoch: 439 Average train loss: 90.2716\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5533\n",
      "Epoch: 440 [  100/50000 ( 0%)]  \tLoss:   93.508179\trec:   64.652840\tkl:   28.855337\n",
      "Epoch: 440 [10100/50000 (20%)]  \tLoss:   92.108101\trec:   64.625671\tkl:   27.482433\n",
      "Epoch: 440 [20100/50000 (40%)]  \tLoss:   93.816978\trec:   65.802956\tkl:   28.014021\n",
      "Epoch: 440 [30100/50000 (60%)]  \tLoss:   86.375450\trec:   59.759312\tkl:   26.616142\n",
      "Epoch: 440 [40100/50000 (80%)]  \tLoss:   87.873825\trec:   61.469616\tkl:   26.404203\n",
      "====> Epoch: 440 Average train loss: 90.2896\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5850\n",
      "Epoch: 441 [  100/50000 ( 0%)]  \tLoss:   92.339012\trec:   63.595066\tkl:   28.743942\n",
      "Epoch: 441 [10100/50000 (20%)]  \tLoss:   91.187706\trec:   62.944347\tkl:   28.243355\n",
      "Epoch: 441 [20100/50000 (40%)]  \tLoss:   91.857788\trec:   63.749798\tkl:   28.107998\n",
      "Epoch: 441 [30100/50000 (60%)]  \tLoss:   89.836151\trec:   61.983730\tkl:   27.852421\n",
      "Epoch: 441 [40100/50000 (80%)]  \tLoss:   89.866776\trec:   62.446953\tkl:   27.419828\n",
      "====> Epoch: 441 Average train loss: 90.2696\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6524\n",
      "Epoch: 442 [  100/50000 ( 0%)]  \tLoss:   89.655663\trec:   62.340096\tkl:   27.315565\n",
      "Epoch: 442 [10100/50000 (20%)]  \tLoss:   88.537285\trec:   59.953865\tkl:   28.583418\n",
      "Epoch: 442 [20100/50000 (40%)]  \tLoss:   91.197319\trec:   63.403812\tkl:   27.793505\n",
      "Epoch: 442 [30100/50000 (60%)]  \tLoss:   89.824287\trec:   62.121929\tkl:   27.702360\n",
      "Epoch: 442 [40100/50000 (80%)]  \tLoss:   92.845436\trec:   65.206299\tkl:   27.639139\n",
      "====> Epoch: 442 Average train loss: 90.2826\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5888\n",
      "Epoch: 443 [  100/50000 ( 0%)]  \tLoss:   92.679962\trec:   65.124718\tkl:   27.555237\n",
      "Epoch: 443 [10100/50000 (20%)]  \tLoss:   92.988472\trec:   64.664215\tkl:   28.324253\n",
      "Epoch: 443 [20100/50000 (40%)]  \tLoss:   92.533524\trec:   63.891289\tkl:   28.642235\n",
      "Epoch: 443 [30100/50000 (60%)]  \tLoss:   89.533768\trec:   61.874443\tkl:   27.659330\n",
      "Epoch: 443 [40100/50000 (80%)]  \tLoss:   87.773140\trec:   61.393391\tkl:   26.379745\n",
      "====> Epoch: 443 Average train loss: 90.2791\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6160\n",
      "Epoch: 444 [  100/50000 ( 0%)]  \tLoss:   91.951714\trec:   64.536652\tkl:   27.415066\n",
      "Epoch: 444 [10100/50000 (20%)]  \tLoss:   88.485031\trec:   60.873348\tkl:   27.611677\n",
      "Epoch: 444 [20100/50000 (40%)]  \tLoss:   90.137733\trec:   62.256195\tkl:   27.881533\n",
      "Epoch: 444 [30100/50000 (60%)]  \tLoss:   90.642319\trec:   62.624481\tkl:   28.017845\n",
      "Epoch: 444 [40100/50000 (80%)]  \tLoss:   91.954575\trec:   63.195976\tkl:   28.758602\n",
      "====> Epoch: 444 Average train loss: 90.2499\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6075\n",
      "Epoch: 445 [  100/50000 ( 0%)]  \tLoss:   91.189003\trec:   63.799583\tkl:   27.389416\n",
      "Epoch: 445 [10100/50000 (20%)]  \tLoss:   89.224480\trec:   60.517200\tkl:   28.707279\n",
      "Epoch: 445 [20100/50000 (40%)]  \tLoss:   86.455322\trec:   59.715740\tkl:   26.739582\n",
      "Epoch: 445 [30100/50000 (60%)]  \tLoss:   92.035172\trec:   63.778973\tkl:   28.256205\n",
      "Epoch: 445 [40100/50000 (80%)]  \tLoss:   93.215996\trec:   65.395813\tkl:   27.820175\n",
      "====> Epoch: 445 Average train loss: 90.2767\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5282\n",
      "Epoch: 446 [  100/50000 ( 0%)]  \tLoss:   89.593651\trec:   62.103374\tkl:   27.490278\n",
      "Epoch: 446 [10100/50000 (20%)]  \tLoss:   88.855171\trec:   61.918427\tkl:   26.936750\n",
      "Epoch: 446 [20100/50000 (40%)]  \tLoss:   89.718208\trec:   62.727947\tkl:   26.990263\n",
      "Epoch: 446 [30100/50000 (60%)]  \tLoss:   90.538963\trec:   62.645985\tkl:   27.892979\n",
      "Epoch: 446 [40100/50000 (80%)]  \tLoss:   90.214615\trec:   62.697285\tkl:   27.517334\n",
      "====> Epoch: 446 Average train loss: 90.2772\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5735\n",
      "Epoch: 447 [  100/50000 ( 0%)]  \tLoss:   90.563141\trec:   61.925930\tkl:   28.637217\n",
      "Epoch: 447 [10100/50000 (20%)]  \tLoss:   90.480225\trec:   62.476757\tkl:   28.003466\n",
      "Epoch: 447 [20100/50000 (40%)]  \tLoss:   86.209801\trec:   60.375595\tkl:   25.834204\n",
      "Epoch: 447 [30100/50000 (60%)]  \tLoss:   90.174606\trec:   62.522694\tkl:   27.651918\n",
      "Epoch: 447 [40100/50000 (80%)]  \tLoss:   89.699867\trec:   61.819775\tkl:   27.880096\n",
      "====> Epoch: 447 Average train loss: 90.2447\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4908\n",
      "Epoch: 448 [  100/50000 ( 0%)]  \tLoss:   87.809799\trec:   60.176697\tkl:   27.633104\n",
      "Epoch: 448 [10100/50000 (20%)]  \tLoss:   87.728271\trec:   60.227364\tkl:   27.500906\n",
      "Epoch: 448 [20100/50000 (40%)]  \tLoss:   87.018028\trec:   59.886211\tkl:   27.131821\n",
      "Epoch: 448 [30100/50000 (60%)]  \tLoss:   91.262939\trec:   63.622280\tkl:   27.640661\n",
      "Epoch: 448 [40100/50000 (80%)]  \tLoss:   90.099434\trec:   61.981659\tkl:   28.117773\n",
      "====> Epoch: 448 Average train loss: 90.2480\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5363\n",
      "Epoch: 449 [  100/50000 ( 0%)]  \tLoss:   83.296318\trec:   56.474262\tkl:   26.822056\n",
      "Epoch: 449 [10100/50000 (20%)]  \tLoss:   92.316422\trec:   63.819759\tkl:   28.496664\n",
      "Epoch: 449 [20100/50000 (40%)]  \tLoss:   88.878387\trec:   61.320473\tkl:   27.557915\n",
      "Epoch: 449 [30100/50000 (60%)]  \tLoss:   91.658524\trec:   62.844784\tkl:   28.813738\n",
      "Epoch: 449 [40100/50000 (80%)]  \tLoss:   90.191124\trec:   62.345535\tkl:   27.845585\n",
      "====> Epoch: 449 Average train loss: 90.2451\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5077\n",
      "Epoch: 450 [  100/50000 ( 0%)]  \tLoss:   85.916443\trec:   58.971050\tkl:   26.945400\n",
      "Epoch: 450 [10100/50000 (20%)]  \tLoss:   91.988632\trec:   63.761391\tkl:   28.227245\n",
      "Epoch: 450 [20100/50000 (40%)]  \tLoss:   90.768806\trec:   62.804409\tkl:   27.964399\n",
      "Epoch: 450 [30100/50000 (60%)]  \tLoss:   88.978203\trec:   61.727917\tkl:   27.250278\n",
      "Epoch: 450 [40100/50000 (80%)]  \tLoss:   92.705307\trec:   64.028107\tkl:   28.677206\n",
      "====> Epoch: 450 Average train loss: 90.2388\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4662\n",
      "Epoch: 451 [  100/50000 ( 0%)]  \tLoss:   90.158463\trec:   62.558319\tkl:   27.600145\n",
      "Epoch: 451 [10100/50000 (20%)]  \tLoss:   90.704773\trec:   63.149101\tkl:   27.555674\n",
      "Epoch: 451 [20100/50000 (40%)]  \tLoss:   89.195137\trec:   61.372391\tkl:   27.822744\n",
      "Epoch: 451 [30100/50000 (60%)]  \tLoss:   90.633141\trec:   62.948090\tkl:   27.685053\n",
      "Epoch: 451 [40100/50000 (80%)]  \tLoss:   90.733139\trec:   63.869209\tkl:   26.863930\n",
      "====> Epoch: 451 Average train loss: 90.2283\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5093\n",
      "Epoch: 452 [  100/50000 ( 0%)]  \tLoss:   93.508263\trec:   65.056786\tkl:   28.451479\n",
      "Epoch: 452 [10100/50000 (20%)]  \tLoss:   91.327126\trec:   64.258629\tkl:   27.068495\n",
      "Epoch: 452 [20100/50000 (40%)]  \tLoss:   84.301910\trec:   58.309696\tkl:   25.992210\n",
      "Epoch: 452 [30100/50000 (60%)]  \tLoss:   92.039024\trec:   64.003189\tkl:   28.035831\n",
      "Epoch: 452 [40100/50000 (80%)]  \tLoss:   92.718872\trec:   63.646404\tkl:   29.072470\n",
      "====> Epoch: 452 Average train loss: 90.2256\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4602\n",
      "Epoch: 453 [  100/50000 ( 0%)]  \tLoss:   88.546944\trec:   61.261345\tkl:   27.285595\n",
      "Epoch: 453 [10100/50000 (20%)]  \tLoss:   90.763184\trec:   62.682919\tkl:   28.080265\n",
      "Epoch: 453 [20100/50000 (40%)]  \tLoss:   90.301987\trec:   63.689312\tkl:   26.612680\n",
      "Epoch: 453 [30100/50000 (60%)]  \tLoss:   91.505585\trec:   63.283222\tkl:   28.222368\n",
      "Epoch: 453 [40100/50000 (80%)]  \tLoss:   94.249336\trec:   65.758469\tkl:   28.490873\n",
      "====> Epoch: 453 Average train loss: 90.2227\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5554\n",
      "Epoch: 454 [  100/50000 ( 0%)]  \tLoss:   93.211365\trec:   64.738762\tkl:   28.472607\n",
      "Epoch: 454 [10100/50000 (20%)]  \tLoss:   91.289291\trec:   63.077751\tkl:   28.211540\n",
      "Epoch: 454 [20100/50000 (40%)]  \tLoss:   90.875351\trec:   63.010166\tkl:   27.865191\n",
      "Epoch: 454 [30100/50000 (60%)]  \tLoss:   91.153061\trec:   63.037022\tkl:   28.116049\n",
      "Epoch: 454 [40100/50000 (80%)]  \tLoss:   90.133308\trec:   62.230373\tkl:   27.902937\n",
      "====> Epoch: 454 Average train loss: 90.2156\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5227\n",
      "Epoch: 455 [  100/50000 ( 0%)]  \tLoss:   90.508705\trec:   63.166775\tkl:   27.341932\n",
      "Epoch: 455 [10100/50000 (20%)]  \tLoss:   86.678261\trec:   59.100761\tkl:   27.577497\n",
      "Epoch: 455 [20100/50000 (40%)]  \tLoss:   92.861763\trec:   64.210869\tkl:   28.650892\n",
      "Epoch: 455 [30100/50000 (60%)]  \tLoss:   86.221535\trec:   59.652634\tkl:   26.568895\n",
      "Epoch: 455 [40100/50000 (80%)]  \tLoss:   88.225426\trec:   61.127831\tkl:   27.097597\n",
      "====> Epoch: 455 Average train loss: 90.2209\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5034\n",
      "Epoch: 456 [  100/50000 ( 0%)]  \tLoss:   90.147423\trec:   63.296463\tkl:   26.850952\n",
      "Epoch: 456 [10100/50000 (20%)]  \tLoss:   87.192711\trec:   60.580479\tkl:   26.612240\n",
      "Epoch: 456 [20100/50000 (40%)]  \tLoss:   87.335396\trec:   60.245438\tkl:   27.089958\n",
      "Epoch: 456 [30100/50000 (60%)]  \tLoss:   85.372414\trec:   58.356335\tkl:   27.016073\n",
      "Epoch: 456 [40100/50000 (80%)]  \tLoss:   91.654259\trec:   63.172909\tkl:   28.481350\n",
      "====> Epoch: 456 Average train loss: 90.1932\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5525\n",
      "Epoch: 457 [  100/50000 ( 0%)]  \tLoss:   94.571144\trec:   66.572823\tkl:   27.998320\n",
      "Epoch: 457 [10100/50000 (20%)]  \tLoss:   88.508751\trec:   60.429539\tkl:   28.079203\n",
      "Epoch: 457 [20100/50000 (40%)]  \tLoss:   89.092438\trec:   61.408447\tkl:   27.683994\n",
      "Epoch: 457 [30100/50000 (60%)]  \tLoss:   90.155434\trec:   62.459511\tkl:   27.695927\n",
      "Epoch: 457 [40100/50000 (80%)]  \tLoss:   91.289337\trec:   63.385555\tkl:   27.903784\n",
      "====> Epoch: 457 Average train loss: 90.2065\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4849\n",
      "Epoch: 458 [  100/50000 ( 0%)]  \tLoss:   87.394745\trec:   60.456688\tkl:   26.938057\n",
      "Epoch: 458 [10100/50000 (20%)]  \tLoss:   88.661919\trec:   61.514492\tkl:   27.147430\n",
      "Epoch: 458 [20100/50000 (40%)]  \tLoss:   89.304062\trec:   63.228455\tkl:   26.075607\n",
      "Epoch: 458 [30100/50000 (60%)]  \tLoss:   88.158379\trec:   60.676224\tkl:   27.482155\n",
      "Epoch: 458 [40100/50000 (80%)]  \tLoss:   92.597046\trec:   63.946423\tkl:   28.650620\n",
      "====> Epoch: 458 Average train loss: 90.1984\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5806\n",
      "Epoch: 459 [  100/50000 ( 0%)]  \tLoss:   90.340385\trec:   62.633457\tkl:   27.706932\n",
      "Epoch: 459 [10100/50000 (20%)]  \tLoss:   90.798836\trec:   62.505917\tkl:   28.292919\n",
      "Epoch: 459 [20100/50000 (40%)]  \tLoss:   89.691002\trec:   62.607582\tkl:   27.083422\n",
      "Epoch: 459 [30100/50000 (60%)]  \tLoss:   91.265564\trec:   62.684502\tkl:   28.581068\n",
      "Epoch: 459 [40100/50000 (80%)]  \tLoss:   90.311890\trec:   62.985340\tkl:   27.326551\n",
      "====> Epoch: 459 Average train loss: 90.1744\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5501\n",
      "Epoch: 460 [  100/50000 ( 0%)]  \tLoss:   87.540054\trec:   60.545593\tkl:   26.994459\n",
      "Epoch: 460 [10100/50000 (20%)]  \tLoss:   87.212570\trec:   59.511356\tkl:   27.701210\n",
      "Epoch: 460 [20100/50000 (40%)]  \tLoss:   88.945770\trec:   61.182026\tkl:   27.763742\n",
      "Epoch: 460 [30100/50000 (60%)]  \tLoss:   89.594467\trec:   62.082947\tkl:   27.511522\n",
      "Epoch: 460 [40100/50000 (80%)]  \tLoss:   86.794029\trec:   59.067410\tkl:   27.726620\n",
      "====> Epoch: 460 Average train loss: 90.1743\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5622\n",
      "Epoch: 461 [  100/50000 ( 0%)]  \tLoss:   91.301598\trec:   63.247303\tkl:   28.054291\n",
      "Epoch: 461 [10100/50000 (20%)]  \tLoss:   84.704460\trec:   57.364483\tkl:   27.339979\n",
      "Epoch: 461 [20100/50000 (40%)]  \tLoss:   89.764336\trec:   62.072548\tkl:   27.691782\n",
      "Epoch: 461 [30100/50000 (60%)]  \tLoss:   90.846832\trec:   62.674690\tkl:   28.172142\n",
      "Epoch: 461 [40100/50000 (80%)]  \tLoss:   93.770912\trec:   65.263588\tkl:   28.507324\n",
      "====> Epoch: 461 Average train loss: 90.1659\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4684\n",
      "Epoch: 462 [  100/50000 ( 0%)]  \tLoss:   90.662437\trec:   62.526192\tkl:   28.136244\n",
      "Epoch: 462 [10100/50000 (20%)]  \tLoss:   91.062012\trec:   62.426167\tkl:   28.635839\n",
      "Epoch: 462 [20100/50000 (40%)]  \tLoss:   87.716652\trec:   60.645203\tkl:   27.071447\n",
      "Epoch: 462 [30100/50000 (60%)]  \tLoss:   87.256287\trec:   60.086884\tkl:   27.169401\n",
      "Epoch: 462 [40100/50000 (80%)]  \tLoss:   91.837578\trec:   64.088257\tkl:   27.749323\n",
      "====> Epoch: 462 Average train loss: 90.1742\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6323\n",
      "Epoch: 463 [  100/50000 ( 0%)]  \tLoss:   88.298355\trec:   61.053818\tkl:   27.244543\n",
      "Epoch: 463 [10100/50000 (20%)]  \tLoss:   86.971695\trec:   59.314987\tkl:   27.656708\n",
      "Epoch: 463 [20100/50000 (40%)]  \tLoss:   91.667984\trec:   63.137241\tkl:   28.530745\n",
      "Epoch: 463 [30100/50000 (60%)]  \tLoss:   89.414673\trec:   62.119881\tkl:   27.294794\n",
      "Epoch: 463 [40100/50000 (80%)]  \tLoss:   91.728172\trec:   63.238571\tkl:   28.489601\n",
      "====> Epoch: 463 Average train loss: 90.1662\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5915\n",
      "Epoch: 464 [  100/50000 ( 0%)]  \tLoss:   90.893532\trec:   63.247932\tkl:   27.645599\n",
      "Epoch: 464 [10100/50000 (20%)]  \tLoss:   89.783630\trec:   61.929222\tkl:   27.854408\n",
      "Epoch: 464 [20100/50000 (40%)]  \tLoss:   91.213951\trec:   63.919861\tkl:   27.294090\n",
      "Epoch: 464 [30100/50000 (60%)]  \tLoss:   88.786583\trec:   60.731785\tkl:   28.054796\n",
      "Epoch: 464 [40100/50000 (80%)]  \tLoss:   92.976250\trec:   65.161026\tkl:   27.815222\n",
      "====> Epoch: 464 Average train loss: 90.1511\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5189\n",
      "Epoch: 465 [  100/50000 ( 0%)]  \tLoss:   90.971191\trec:   63.049129\tkl:   27.922058\n",
      "Epoch: 465 [10100/50000 (20%)]  \tLoss:   95.139824\trec:   66.371521\tkl:   28.768295\n",
      "Epoch: 465 [20100/50000 (40%)]  \tLoss:   92.531654\trec:   63.770214\tkl:   28.761442\n",
      "Epoch: 465 [30100/50000 (60%)]  \tLoss:   93.357422\trec:   65.487511\tkl:   27.869913\n",
      "Epoch: 465 [40100/50000 (80%)]  \tLoss:   90.010948\trec:   62.450512\tkl:   27.560436\n",
      "====> Epoch: 465 Average train loss: 90.1606\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4543\n",
      "Epoch: 466 [  100/50000 ( 0%)]  \tLoss:   89.515594\trec:   62.223759\tkl:   27.291836\n",
      "Epoch: 466 [10100/50000 (20%)]  \tLoss:   93.062653\trec:   65.228462\tkl:   27.834188\n",
      "Epoch: 466 [20100/50000 (40%)]  \tLoss:   89.108650\trec:   61.343262\tkl:   27.765392\n",
      "Epoch: 466 [30100/50000 (60%)]  \tLoss:   93.560440\trec:   65.293785\tkl:   28.266649\n",
      "Epoch: 466 [40100/50000 (80%)]  \tLoss:   91.348846\trec:   63.579941\tkl:   27.768908\n",
      "====> Epoch: 466 Average train loss: 90.1423\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5798\n",
      "Epoch: 467 [  100/50000 ( 0%)]  \tLoss:   92.703789\trec:   64.513023\tkl:   28.190762\n",
      "Epoch: 467 [10100/50000 (20%)]  \tLoss:   88.416016\trec:   62.169399\tkl:   26.246611\n",
      "Epoch: 467 [20100/50000 (40%)]  \tLoss:   89.588608\trec:   62.107109\tkl:   27.481503\n",
      "Epoch: 467 [30100/50000 (60%)]  \tLoss:   87.153564\trec:   59.608006\tkl:   27.545555\n",
      "Epoch: 467 [40100/50000 (80%)]  \tLoss:   87.443642\trec:   60.050739\tkl:   27.392900\n",
      "====> Epoch: 467 Average train loss: 90.1438\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5938\n",
      "Epoch: 468 [  100/50000 ( 0%)]  \tLoss:   90.591248\trec:   62.774422\tkl:   27.816820\n",
      "Epoch: 468 [10100/50000 (20%)]  \tLoss:   88.911728\trec:   61.492302\tkl:   27.419420\n",
      "Epoch: 468 [20100/50000 (40%)]  \tLoss:   87.648491\trec:   60.258373\tkl:   27.390127\n",
      "Epoch: 468 [30100/50000 (60%)]  \tLoss:   87.413803\trec:   61.179211\tkl:   26.234592\n",
      "Epoch: 468 [40100/50000 (80%)]  \tLoss:   89.326797\trec:   61.786606\tkl:   27.540184\n",
      "====> Epoch: 468 Average train loss: 90.1185\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4550\n",
      "Epoch: 469 [  100/50000 ( 0%)]  \tLoss:   88.958771\trec:   61.212597\tkl:   27.746176\n",
      "Epoch: 469 [10100/50000 (20%)]  \tLoss:   88.204292\trec:   60.722702\tkl:   27.481594\n",
      "Epoch: 469 [20100/50000 (40%)]  \tLoss:   90.785187\trec:   63.600368\tkl:   27.184816\n",
      "Epoch: 469 [30100/50000 (60%)]  \tLoss:   87.164474\trec:   60.046726\tkl:   27.117746\n",
      "Epoch: 469 [40100/50000 (80%)]  \tLoss:   86.898857\trec:   59.815491\tkl:   27.083361\n",
      "====> Epoch: 469 Average train loss: 90.1212\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.6650\n",
      "Epoch: 470 [  100/50000 ( 0%)]  \tLoss:   93.015274\trec:   64.703865\tkl:   28.311411\n",
      "Epoch: 470 [10100/50000 (20%)]  \tLoss:   90.439903\trec:   62.154823\tkl:   28.285082\n",
      "Epoch: 470 [20100/50000 (40%)]  \tLoss:   89.574112\trec:   61.386864\tkl:   28.187248\n",
      "Epoch: 470 [30100/50000 (60%)]  \tLoss:   87.121307\trec:   59.791855\tkl:   27.329447\n",
      "Epoch: 470 [40100/50000 (80%)]  \tLoss:   87.490738\trec:   60.388733\tkl:   27.102001\n",
      "====> Epoch: 470 Average train loss: 90.0968\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4720\n",
      "Epoch: 471 [  100/50000 ( 0%)]  \tLoss:   89.199738\trec:   61.066418\tkl:   28.133318\n",
      "Epoch: 471 [10100/50000 (20%)]  \tLoss:   89.730820\trec:   61.267284\tkl:   28.463535\n",
      "Epoch: 471 [20100/50000 (40%)]  \tLoss:   89.061226\trec:   61.156059\tkl:   27.905170\n",
      "Epoch: 471 [30100/50000 (60%)]  \tLoss:   89.852928\trec:   62.036713\tkl:   27.816216\n",
      "Epoch: 471 [40100/50000 (80%)]  \tLoss:   89.859894\trec:   61.285847\tkl:   28.574045\n",
      "====> Epoch: 471 Average train loss: 90.0969\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5567\n",
      "Epoch: 472 [  100/50000 ( 0%)]  \tLoss:   91.488487\trec:   64.190193\tkl:   27.298292\n",
      "Epoch: 472 [10100/50000 (20%)]  \tLoss:   88.712440\trec:   60.296150\tkl:   28.416283\n",
      "Epoch: 472 [20100/50000 (40%)]  \tLoss:   86.164795\trec:   58.669724\tkl:   27.495068\n",
      "Epoch: 472 [30100/50000 (60%)]  \tLoss:   91.511658\trec:   63.476986\tkl:   28.034678\n",
      "Epoch: 472 [40100/50000 (80%)]  \tLoss:   86.231171\trec:   59.464024\tkl:   26.767153\n",
      "====> Epoch: 472 Average train loss: 90.1130\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4235\n",
      "Epoch: 473 [  100/50000 ( 0%)]  \tLoss:   87.209892\trec:   59.794785\tkl:   27.415108\n",
      "Epoch: 473 [10100/50000 (20%)]  \tLoss:   89.930702\trec:   61.876698\tkl:   28.054008\n",
      "Epoch: 473 [20100/50000 (40%)]  \tLoss:   89.279442\trec:   61.161064\tkl:   28.118382\n",
      "Epoch: 473 [30100/50000 (60%)]  \tLoss:   86.289345\trec:   59.903358\tkl:   26.385986\n",
      "Epoch: 473 [40100/50000 (80%)]  \tLoss:   88.090889\trec:   61.433701\tkl:   26.657187\n",
      "====> Epoch: 473 Average train loss: 90.1261\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4794\n",
      "Epoch: 474 [  100/50000 ( 0%)]  \tLoss:   92.586006\trec:   64.424332\tkl:   28.161667\n",
      "Epoch: 474 [10100/50000 (20%)]  \tLoss:   88.536987\trec:   60.562553\tkl:   27.974443\n",
      "Epoch: 474 [20100/50000 (40%)]  \tLoss:   89.265099\trec:   61.518074\tkl:   27.747021\n",
      "Epoch: 474 [30100/50000 (60%)]  \tLoss:   88.573586\trec:   60.861923\tkl:   27.711662\n",
      "Epoch: 474 [40100/50000 (80%)]  \tLoss:   90.539200\trec:   61.752449\tkl:   28.786743\n",
      "====> Epoch: 474 Average train loss: 90.1123\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4389\n",
      "Epoch: 475 [  100/50000 ( 0%)]  \tLoss:   86.790474\trec:   59.741459\tkl:   27.049019\n",
      "Epoch: 475 [10100/50000 (20%)]  \tLoss:   90.797127\trec:   62.040154\tkl:   28.756971\n",
      "Epoch: 475 [20100/50000 (40%)]  \tLoss:   86.626343\trec:   58.973587\tkl:   27.652763\n",
      "Epoch: 475 [30100/50000 (60%)]  \tLoss:   89.702637\trec:   61.785942\tkl:   27.916691\n",
      "Epoch: 475 [40100/50000 (80%)]  \tLoss:   92.433540\trec:   64.170624\tkl:   28.262918\n",
      "====> Epoch: 475 Average train loss: 90.0835\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4496\n",
      "Epoch: 476 [  100/50000 ( 0%)]  \tLoss:   89.166656\trec:   60.767685\tkl:   28.398970\n",
      "Epoch: 476 [10100/50000 (20%)]  \tLoss:   91.870094\trec:   63.652348\tkl:   28.217743\n",
      "Epoch: 476 [20100/50000 (40%)]  \tLoss:   88.953224\trec:   61.752235\tkl:   27.200981\n",
      "Epoch: 476 [30100/50000 (60%)]  \tLoss:   90.565567\trec:   63.718193\tkl:   26.847378\n",
      "Epoch: 476 [40100/50000 (80%)]  \tLoss:   90.291443\trec:   63.062431\tkl:   27.229017\n",
      "====> Epoch: 476 Average train loss: 90.1014\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5139\n",
      "Epoch: 477 [  100/50000 ( 0%)]  \tLoss:   87.665977\trec:   59.570316\tkl:   28.095659\n",
      "Epoch: 477 [10100/50000 (20%)]  \tLoss:   86.830505\trec:   59.729950\tkl:   27.100552\n",
      "Epoch: 477 [20100/50000 (40%)]  \tLoss:   87.883575\trec:   61.017448\tkl:   26.866117\n",
      "Epoch: 477 [30100/50000 (60%)]  \tLoss:   90.833092\trec:   62.834873\tkl:   27.998220\n",
      "Epoch: 477 [40100/50000 (80%)]  \tLoss:   90.448532\trec:   61.524059\tkl:   28.924469\n",
      "====> Epoch: 477 Average train loss: 90.0874\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4946\n",
      "Epoch: 478 [  100/50000 ( 0%)]  \tLoss:   90.558319\trec:   63.447033\tkl:   27.111286\n",
      "Epoch: 478 [10100/50000 (20%)]  \tLoss:   92.730385\trec:   63.886131\tkl:   28.844257\n",
      "Epoch: 478 [20100/50000 (40%)]  \tLoss:   91.143555\trec:   62.880627\tkl:   28.262918\n",
      "Epoch: 478 [30100/50000 (60%)]  \tLoss:   90.737648\trec:   62.304668\tkl:   28.432978\n",
      "Epoch: 478 [40100/50000 (80%)]  \tLoss:   88.273628\trec:   60.394978\tkl:   27.878649\n",
      "====> Epoch: 478 Average train loss: 90.0751\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4686\n",
      "Epoch: 479 [  100/50000 ( 0%)]  \tLoss:   94.902641\trec:   66.837746\tkl:   28.064901\n",
      "Epoch: 479 [10100/50000 (20%)]  \tLoss:   93.810234\trec:   64.704109\tkl:   29.106119\n",
      "Epoch: 479 [20100/50000 (40%)]  \tLoss:   87.333427\trec:   59.281933\tkl:   28.051493\n",
      "Epoch: 479 [30100/50000 (60%)]  \tLoss:   90.825089\trec:   63.810917\tkl:   27.014168\n",
      "Epoch: 479 [40100/50000 (80%)]  \tLoss:   90.909767\trec:   63.489159\tkl:   27.420605\n",
      "====> Epoch: 479 Average train loss: 90.1026\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3946\n",
      "Epoch: 480 [  100/50000 ( 0%)]  \tLoss:   88.095932\trec:   61.807102\tkl:   26.288828\n",
      "Epoch: 480 [10100/50000 (20%)]  \tLoss:   89.656326\trec:   61.490086\tkl:   28.166235\n",
      "Epoch: 480 [20100/50000 (40%)]  \tLoss:   90.749657\trec:   61.814243\tkl:   28.935415\n",
      "Epoch: 480 [30100/50000 (60%)]  \tLoss:   89.369415\trec:   61.849056\tkl:   27.520351\n",
      "Epoch: 480 [40100/50000 (80%)]  \tLoss:   92.057770\trec:   63.456112\tkl:   28.601664\n",
      "====> Epoch: 480 Average train loss: 90.0561\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3995\n",
      "Epoch: 481 [  100/50000 ( 0%)]  \tLoss:   91.632126\trec:   63.837307\tkl:   27.794823\n",
      "Epoch: 481 [10100/50000 (20%)]  \tLoss:   87.986046\trec:   61.035740\tkl:   26.950302\n",
      "Epoch: 481 [20100/50000 (40%)]  \tLoss:   92.239487\trec:   64.764740\tkl:   27.474749\n",
      "Epoch: 481 [30100/50000 (60%)]  \tLoss:   89.373535\trec:   61.620098\tkl:   27.753441\n",
      "Epoch: 481 [40100/50000 (80%)]  \tLoss:   90.284355\trec:   62.666546\tkl:   27.617802\n",
      "====> Epoch: 481 Average train loss: 90.0499\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3941\n",
      "Epoch: 482 [  100/50000 ( 0%)]  \tLoss:   92.756226\trec:   64.062088\tkl:   28.694139\n",
      "Epoch: 482 [10100/50000 (20%)]  \tLoss:   95.485817\trec:   66.251884\tkl:   29.233931\n",
      "Epoch: 482 [20100/50000 (40%)]  \tLoss:   85.971291\trec:   59.950718\tkl:   26.020565\n",
      "Epoch: 482 [30100/50000 (60%)]  \tLoss:   92.417282\trec:   64.624489\tkl:   27.792793\n",
      "Epoch: 482 [40100/50000 (80%)]  \tLoss:   92.073456\trec:   63.912441\tkl:   28.161013\n",
      "====> Epoch: 482 Average train loss: 90.0553\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4454\n",
      "Epoch: 483 [  100/50000 ( 0%)]  \tLoss:   93.640854\trec:   65.605515\tkl:   28.035347\n",
      "Epoch: 483 [10100/50000 (20%)]  \tLoss:   91.065865\trec:   62.424599\tkl:   28.641270\n",
      "Epoch: 483 [20100/50000 (40%)]  \tLoss:   92.685135\trec:   63.967819\tkl:   28.717319\n",
      "Epoch: 483 [30100/50000 (60%)]  \tLoss:   87.789963\trec:   59.372986\tkl:   28.416977\n",
      "Epoch: 483 [40100/50000 (80%)]  \tLoss:   91.330795\trec:   62.885178\tkl:   28.445625\n",
      "====> Epoch: 483 Average train loss: 90.0464\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4184\n",
      "Epoch: 484 [  100/50000 ( 0%)]  \tLoss:   85.339630\trec:   58.498676\tkl:   26.840956\n",
      "Epoch: 484 [10100/50000 (20%)]  \tLoss:   89.040108\trec:   61.335526\tkl:   27.704580\n",
      "Epoch: 484 [20100/50000 (40%)]  \tLoss:   87.447090\trec:   60.279404\tkl:   27.167681\n",
      "Epoch: 484 [30100/50000 (60%)]  \tLoss:   88.949318\trec:   61.565456\tkl:   27.383858\n",
      "Epoch: 484 [40100/50000 (80%)]  \tLoss:   91.938492\trec:   63.541355\tkl:   28.397142\n",
      "====> Epoch: 484 Average train loss: 90.0410\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4425\n",
      "Epoch: 485 [  100/50000 ( 0%)]  \tLoss:   88.682755\trec:   60.746819\tkl:   27.935936\n",
      "Epoch: 485 [10100/50000 (20%)]  \tLoss:   89.230232\trec:   61.489834\tkl:   27.740404\n",
      "Epoch: 485 [20100/50000 (40%)]  \tLoss:   91.770157\trec:   63.827831\tkl:   27.942318\n",
      "Epoch: 485 [30100/50000 (60%)]  \tLoss:   83.565155\trec:   57.656532\tkl:   25.908621\n",
      "Epoch: 485 [40100/50000 (80%)]  \tLoss:   93.284294\trec:   64.737793\tkl:   28.546501\n",
      "====> Epoch: 485 Average train loss: 90.0494\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4340\n",
      "Epoch: 486 [  100/50000 ( 0%)]  \tLoss:   91.357155\trec:   63.176357\tkl:   28.180803\n",
      "Epoch: 486 [10100/50000 (20%)]  \tLoss:   85.143555\trec:   59.300121\tkl:   25.843430\n",
      "Epoch: 486 [20100/50000 (40%)]  \tLoss:   87.451492\trec:   59.757294\tkl:   27.694199\n",
      "Epoch: 486 [30100/50000 (60%)]  \tLoss:   87.060936\trec:   59.551922\tkl:   27.509012\n",
      "Epoch: 486 [40100/50000 (80%)]  \tLoss:   93.794403\trec:   64.953934\tkl:   28.840471\n",
      "====> Epoch: 486 Average train loss: 90.0187\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4219\n",
      "Epoch: 487 [  100/50000 ( 0%)]  \tLoss:   91.849724\trec:   63.961449\tkl:   27.888271\n",
      "Epoch: 487 [10100/50000 (20%)]  \tLoss:   91.554649\trec:   63.638531\tkl:   27.916117\n",
      "Epoch: 487 [20100/50000 (40%)]  \tLoss:   88.069649\trec:   60.608501\tkl:   27.461142\n",
      "Epoch: 487 [30100/50000 (60%)]  \tLoss:   88.516579\trec:   61.021862\tkl:   27.494719\n",
      "Epoch: 487 [40100/50000 (80%)]  \tLoss:   90.716309\trec:   62.861462\tkl:   27.854849\n",
      "====> Epoch: 487 Average train loss: 90.0314\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3851\n",
      "Epoch: 488 [  100/50000 ( 0%)]  \tLoss:   90.084900\trec:   62.567329\tkl:   27.517578\n",
      "Epoch: 488 [10100/50000 (20%)]  \tLoss:   89.519020\trec:   61.734699\tkl:   27.784319\n",
      "Epoch: 488 [20100/50000 (40%)]  \tLoss:   92.011070\trec:   64.285500\tkl:   27.725573\n",
      "Epoch: 488 [30100/50000 (60%)]  \tLoss:   94.502419\trec:   65.744194\tkl:   28.758224\n",
      "Epoch: 488 [40100/50000 (80%)]  \tLoss:   89.818748\trec:   62.262890\tkl:   27.555855\n",
      "====> Epoch: 488 Average train loss: 90.0226\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3872\n",
      "Epoch: 489 [  100/50000 ( 0%)]  \tLoss:   92.919342\trec:   64.515045\tkl:   28.404305\n",
      "Epoch: 489 [10100/50000 (20%)]  \tLoss:   89.044724\trec:   61.660412\tkl:   27.384312\n",
      "Epoch: 489 [20100/50000 (40%)]  \tLoss:   90.454102\trec:   62.499203\tkl:   27.954895\n",
      "Epoch: 489 [30100/50000 (60%)]  \tLoss:   89.630836\trec:   61.697830\tkl:   27.933010\n",
      "Epoch: 489 [40100/50000 (80%)]  \tLoss:   91.084053\trec:   63.275017\tkl:   27.809034\n",
      "====> Epoch: 489 Average train loss: 90.0280\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4493\n",
      "Epoch: 490 [  100/50000 ( 0%)]  \tLoss:   93.654327\trec:   65.051689\tkl:   28.602634\n",
      "Epoch: 490 [10100/50000 (20%)]  \tLoss:   89.575645\trec:   61.316257\tkl:   28.259378\n",
      "Epoch: 490 [20100/50000 (40%)]  \tLoss:   91.382332\trec:   63.132069\tkl:   28.250261\n",
      "Epoch: 490 [30100/50000 (60%)]  \tLoss:   88.846611\trec:   61.369308\tkl:   27.477299\n",
      "Epoch: 490 [40100/50000 (80%)]  \tLoss:   90.294449\trec:   62.338875\tkl:   27.955570\n",
      "====> Epoch: 490 Average train loss: 90.0063\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5158\n",
      "Epoch: 491 [  100/50000 ( 0%)]  \tLoss:   88.880585\trec:   61.986931\tkl:   26.893652\n",
      "Epoch: 491 [10100/50000 (20%)]  \tLoss:   89.495682\trec:   60.930561\tkl:   28.565125\n",
      "Epoch: 491 [20100/50000 (40%)]  \tLoss:   89.588387\trec:   62.202011\tkl:   27.386375\n",
      "Epoch: 491 [30100/50000 (60%)]  \tLoss:   90.924255\trec:   62.386242\tkl:   28.538017\n",
      "Epoch: 491 [40100/50000 (80%)]  \tLoss:   92.828110\trec:   64.748932\tkl:   28.079180\n",
      "====> Epoch: 491 Average train loss: 90.0101\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3809\n",
      "Epoch: 492 [  100/50000 ( 0%)]  \tLoss:   89.164879\trec:   61.255478\tkl:   27.909403\n",
      "Epoch: 492 [10100/50000 (20%)]  \tLoss:   93.406052\trec:   64.500107\tkl:   28.905947\n",
      "Epoch: 492 [20100/50000 (40%)]  \tLoss:   90.426659\trec:   61.994461\tkl:   28.432199\n",
      "Epoch: 492 [30100/50000 (60%)]  \tLoss:   89.688950\trec:   63.538662\tkl:   26.150295\n",
      "Epoch: 492 [40100/50000 (80%)]  \tLoss:   91.234375\trec:   62.351131\tkl:   28.883247\n",
      "====> Epoch: 492 Average train loss: 89.9933\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4988\n",
      "Epoch: 493 [  100/50000 ( 0%)]  \tLoss:   89.780533\trec:   61.392334\tkl:   28.388203\n",
      "Epoch: 493 [10100/50000 (20%)]  \tLoss:   93.274460\trec:   64.639931\tkl:   28.634529\n",
      "Epoch: 493 [20100/50000 (40%)]  \tLoss:   90.478287\trec:   62.286667\tkl:   28.191618\n",
      "Epoch: 493 [30100/50000 (60%)]  \tLoss:   86.686729\trec:   59.450134\tkl:   27.236589\n",
      "Epoch: 493 [40100/50000 (80%)]  \tLoss:   90.740662\trec:   62.755730\tkl:   27.984928\n",
      "====> Epoch: 493 Average train loss: 89.9986\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5239\n",
      "Epoch: 494 [  100/50000 ( 0%)]  \tLoss:   88.360451\trec:   61.306156\tkl:   27.054295\n",
      "Epoch: 494 [10100/50000 (20%)]  \tLoss:   87.455353\trec:   60.818630\tkl:   26.636713\n",
      "Epoch: 494 [20100/50000 (40%)]  \tLoss:   89.901337\trec:   62.170467\tkl:   27.730869\n",
      "Epoch: 494 [30100/50000 (60%)]  \tLoss:   90.511375\trec:   61.785019\tkl:   28.726357\n",
      "Epoch: 494 [40100/50000 (80%)]  \tLoss:   87.969101\trec:   60.362938\tkl:   27.606157\n",
      "====> Epoch: 494 Average train loss: 89.9892\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3589\n",
      "Epoch: 495 [  100/50000 ( 0%)]  \tLoss:   92.366638\trec:   63.173542\tkl:   29.193090\n",
      "Epoch: 495 [10100/50000 (20%)]  \tLoss:   90.665298\trec:   62.285252\tkl:   28.380049\n",
      "Epoch: 495 [20100/50000 (40%)]  \tLoss:   86.341812\trec:   59.171833\tkl:   27.169979\n",
      "Epoch: 495 [30100/50000 (60%)]  \tLoss:   93.901421\trec:   65.576431\tkl:   28.324991\n",
      "Epoch: 495 [40100/50000 (80%)]  \tLoss:   95.618790\trec:   66.494591\tkl:   29.124199\n",
      "====> Epoch: 495 Average train loss: 90.0063\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3723\n",
      "Epoch: 496 [  100/50000 ( 0%)]  \tLoss:   92.100937\trec:   63.440483\tkl:   28.660458\n",
      "Epoch: 496 [10100/50000 (20%)]  \tLoss:   89.767868\trec:   61.691406\tkl:   28.076469\n",
      "Epoch: 496 [20100/50000 (40%)]  \tLoss:   91.475632\trec:   62.828003\tkl:   28.647629\n",
      "Epoch: 496 [30100/50000 (60%)]  \tLoss:   90.484047\trec:   62.746639\tkl:   27.737411\n",
      "Epoch: 496 [40100/50000 (80%)]  \tLoss:   89.767731\trec:   62.512802\tkl:   27.254930\n",
      "====> Epoch: 496 Average train loss: 89.9747\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3870\n",
      "Epoch: 497 [  100/50000 ( 0%)]  \tLoss:   88.742538\trec:   60.929096\tkl:   27.813442\n",
      "Epoch: 497 [10100/50000 (20%)]  \tLoss:   89.871521\trec:   61.838348\tkl:   28.033178\n",
      "Epoch: 497 [20100/50000 (40%)]  \tLoss:   95.940887\trec:   66.350258\tkl:   29.590628\n",
      "Epoch: 497 [30100/50000 (60%)]  \tLoss:   84.893051\trec:   57.509880\tkl:   27.383173\n",
      "Epoch: 497 [40100/50000 (80%)]  \tLoss:   88.740662\trec:   61.142437\tkl:   27.598232\n",
      "====> Epoch: 497 Average train loss: 89.9991\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2899\n",
      "Epoch: 498 [  100/50000 ( 0%)]  \tLoss:   88.944122\trec:   61.243523\tkl:   27.700592\n",
      "Epoch: 498 [10100/50000 (20%)]  \tLoss:   92.282761\trec:   64.273720\tkl:   28.009043\n",
      "Epoch: 498 [20100/50000 (40%)]  \tLoss:   89.897087\trec:   62.018894\tkl:   27.878197\n",
      "Epoch: 498 [30100/50000 (60%)]  \tLoss:   91.396660\trec:   63.047371\tkl:   28.349283\n",
      "Epoch: 498 [40100/50000 (80%)]  \tLoss:   91.937355\trec:   62.996395\tkl:   28.940956\n",
      "====> Epoch: 498 Average train loss: 89.9607\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3971\n",
      "Epoch: 499 [  100/50000 ( 0%)]  \tLoss:   83.720490\trec:   56.743690\tkl:   26.976797\n",
      "Epoch: 499 [10100/50000 (20%)]  \tLoss:   89.366714\trec:   63.040760\tkl:   26.325954\n",
      "Epoch: 499 [20100/50000 (40%)]  \tLoss:   92.377831\trec:   63.491901\tkl:   28.885921\n",
      "Epoch: 499 [30100/50000 (60%)]  \tLoss:   92.758705\trec:   64.199989\tkl:   28.558720\n",
      "Epoch: 499 [40100/50000 (80%)]  \tLoss:   90.094391\trec:   61.976532\tkl:   28.117855\n",
      "====> Epoch: 499 Average train loss: 89.9705\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3687\n",
      "Epoch: 500 [  100/50000 ( 0%)]  \tLoss:   90.383347\trec:   61.923183\tkl:   28.460167\n",
      "Epoch: 500 [10100/50000 (20%)]  \tLoss:   88.582504\trec:   61.158886\tkl:   27.423620\n",
      "Epoch: 500 [20100/50000 (40%)]  \tLoss:   90.585564\trec:   63.792393\tkl:   26.793179\n",
      "Epoch: 500 [30100/50000 (60%)]  \tLoss:   91.893082\trec:   64.199333\tkl:   27.693754\n",
      "Epoch: 500 [40100/50000 (80%)]  \tLoss:   86.075310\trec:   58.738045\tkl:   27.337265\n",
      "====> Epoch: 500 Average train loss: 89.9462\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3707\n",
      "Epoch: 501 [  100/50000 ( 0%)]  \tLoss:   89.410782\trec:   61.956257\tkl:   27.454515\n",
      "Epoch: 501 [10100/50000 (20%)]  \tLoss:   86.634377\trec:   59.736286\tkl:   26.898090\n",
      "Epoch: 501 [20100/50000 (40%)]  \tLoss:   90.816170\trec:   62.410023\tkl:   28.406151\n",
      "Epoch: 501 [30100/50000 (60%)]  \tLoss:   85.076607\trec:   58.763535\tkl:   26.313076\n",
      "Epoch: 501 [40100/50000 (80%)]  \tLoss:   92.157578\trec:   63.596706\tkl:   28.560865\n",
      "====> Epoch: 501 Average train loss: 89.9797\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4330\n",
      "Epoch: 502 [  100/50000 ( 0%)]  \tLoss:   87.780327\trec:   59.617352\tkl:   28.162975\n",
      "Epoch: 502 [10100/50000 (20%)]  \tLoss:   90.855713\trec:   62.860058\tkl:   27.995653\n",
      "Epoch: 502 [20100/50000 (40%)]  \tLoss:   91.190170\trec:   63.446453\tkl:   27.743717\n",
      "Epoch: 502 [30100/50000 (60%)]  \tLoss:   93.985291\trec:   65.556679\tkl:   28.428612\n",
      "Epoch: 502 [40100/50000 (80%)]  \tLoss:   89.258522\trec:   61.969151\tkl:   27.289377\n",
      "====> Epoch: 502 Average train loss: 89.9553\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4386\n",
      "Epoch: 503 [  100/50000 ( 0%)]  \tLoss:   90.475632\trec:   61.604843\tkl:   28.870790\n",
      "Epoch: 503 [10100/50000 (20%)]  \tLoss:   91.816208\trec:   63.903942\tkl:   27.912262\n",
      "Epoch: 503 [20100/50000 (40%)]  \tLoss:   87.565353\trec:   60.065716\tkl:   27.499636\n",
      "Epoch: 503 [30100/50000 (60%)]  \tLoss:   91.546959\trec:   62.858654\tkl:   28.688305\n",
      "Epoch: 503 [40100/50000 (80%)]  \tLoss:   88.039452\trec:   60.718090\tkl:   27.321367\n",
      "====> Epoch: 503 Average train loss: 89.9522\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2444\n",
      "Epoch: 504 [  100/50000 ( 0%)]  \tLoss:   90.094322\trec:   62.502831\tkl:   27.591494\n",
      "Epoch: 504 [10100/50000 (20%)]  \tLoss:   90.334892\trec:   61.747250\tkl:   28.587639\n",
      "Epoch: 504 [20100/50000 (40%)]  \tLoss:   89.693359\trec:   62.151192\tkl:   27.542168\n",
      "Epoch: 504 [30100/50000 (60%)]  \tLoss:   89.338028\trec:   60.746864\tkl:   28.591156\n",
      "Epoch: 504 [40100/50000 (80%)]  \tLoss:   91.107521\trec:   61.823975\tkl:   29.283545\n",
      "====> Epoch: 504 Average train loss: 89.9569\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4365\n",
      "Epoch: 505 [  100/50000 ( 0%)]  \tLoss:   88.843979\trec:   61.947048\tkl:   26.896938\n",
      "Epoch: 505 [10100/50000 (20%)]  \tLoss:   90.296661\trec:   61.757359\tkl:   28.539299\n",
      "Epoch: 505 [20100/50000 (40%)]  \tLoss:   91.848053\trec:   63.584431\tkl:   28.263624\n",
      "Epoch: 505 [30100/50000 (60%)]  \tLoss:   88.781189\trec:   60.754242\tkl:   28.026953\n",
      "Epoch: 505 [40100/50000 (80%)]  \tLoss:   89.919533\trec:   61.448269\tkl:   28.471256\n",
      "====> Epoch: 505 Average train loss: 89.9717\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3612\n",
      "Epoch: 506 [  100/50000 ( 0%)]  \tLoss:   92.776108\trec:   64.243660\tkl:   28.532450\n",
      "Epoch: 506 [10100/50000 (20%)]  \tLoss:   90.927925\trec:   63.440441\tkl:   27.487480\n",
      "Epoch: 506 [20100/50000 (40%)]  \tLoss:   87.438042\trec:   60.789783\tkl:   26.648266\n",
      "Epoch: 506 [30100/50000 (60%)]  \tLoss:   92.173767\trec:   63.589756\tkl:   28.584015\n",
      "Epoch: 506 [40100/50000 (80%)]  \tLoss:   86.011665\trec:   59.912586\tkl:   26.099081\n",
      "====> Epoch: 506 Average train loss: 89.9243\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4127\n",
      "Epoch: 507 [  100/50000 ( 0%)]  \tLoss:   87.344040\trec:   59.439007\tkl:   27.905033\n",
      "Epoch: 507 [10100/50000 (20%)]  \tLoss:   89.640297\trec:   62.160007\tkl:   27.480295\n",
      "Epoch: 507 [20100/50000 (40%)]  \tLoss:   88.741562\trec:   60.760666\tkl:   27.980888\n",
      "Epoch: 507 [30100/50000 (60%)]  \tLoss:   92.709869\trec:   64.495255\tkl:   28.214613\n",
      "Epoch: 507 [40100/50000 (80%)]  \tLoss:   93.028824\trec:   64.674294\tkl:   28.354528\n",
      "====> Epoch: 507 Average train loss: 89.9075\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2773\n",
      "Epoch: 508 [  100/50000 ( 0%)]  \tLoss:   90.204529\trec:   62.654507\tkl:   27.550018\n",
      "Epoch: 508 [10100/50000 (20%)]  \tLoss:   88.334579\trec:   61.283611\tkl:   27.050964\n",
      "Epoch: 508 [20100/50000 (40%)]  \tLoss:   89.179008\trec:   61.754990\tkl:   27.424023\n",
      "Epoch: 508 [30100/50000 (60%)]  \tLoss:   89.242683\trec:   60.820007\tkl:   28.422672\n",
      "Epoch: 508 [40100/50000 (80%)]  \tLoss:   91.826050\trec:   63.402451\tkl:   28.423607\n",
      "====> Epoch: 508 Average train loss: 89.9361\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2740\n",
      "Epoch: 509 [  100/50000 ( 0%)]  \tLoss:   89.495857\trec:   61.147488\tkl:   28.348368\n",
      "Epoch: 509 [10100/50000 (20%)]  \tLoss:   92.983711\trec:   64.573921\tkl:   28.409786\n",
      "Epoch: 509 [20100/50000 (40%)]  \tLoss:   93.375275\trec:   65.085175\tkl:   28.290102\n",
      "Epoch: 509 [30100/50000 (60%)]  \tLoss:   92.871559\trec:   64.471489\tkl:   28.400078\n",
      "Epoch: 509 [40100/50000 (80%)]  \tLoss:   88.587387\trec:   60.917168\tkl:   27.670227\n",
      "====> Epoch: 509 Average train loss: 89.9116\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3841\n",
      "Epoch: 510 [  100/50000 ( 0%)]  \tLoss:   95.788849\trec:   66.974571\tkl:   28.814278\n",
      "Epoch: 510 [10100/50000 (20%)]  \tLoss:   90.361954\trec:   61.567566\tkl:   28.794390\n",
      "Epoch: 510 [20100/50000 (40%)]  \tLoss:   89.993317\trec:   62.523552\tkl:   27.469761\n",
      "Epoch: 510 [30100/50000 (60%)]  \tLoss:   88.597099\trec:   60.403885\tkl:   28.193214\n",
      "Epoch: 510 [40100/50000 (80%)]  \tLoss:   91.104820\trec:   62.995018\tkl:   28.109798\n",
      "====> Epoch: 510 Average train loss: 89.9196\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3514\n",
      "Epoch: 511 [  100/50000 ( 0%)]  \tLoss:   89.068260\trec:   60.314053\tkl:   28.754213\n",
      "Epoch: 511 [10100/50000 (20%)]  \tLoss:   87.836853\trec:   60.342659\tkl:   27.494198\n",
      "Epoch: 511 [20100/50000 (40%)]  \tLoss:   91.307518\trec:   64.759346\tkl:   26.548178\n",
      "Epoch: 511 [30100/50000 (60%)]  \tLoss:   89.818787\trec:   62.849033\tkl:   26.969755\n",
      "Epoch: 511 [40100/50000 (80%)]  \tLoss:   86.041695\trec:   59.129761\tkl:   26.911938\n",
      "====> Epoch: 511 Average train loss: 89.8978\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4137\n",
      "Epoch: 512 [  100/50000 ( 0%)]  \tLoss:   91.741463\trec:   63.138332\tkl:   28.603132\n",
      "Epoch: 512 [10100/50000 (20%)]  \tLoss:   92.558052\trec:   63.982811\tkl:   28.575247\n",
      "Epoch: 512 [20100/50000 (40%)]  \tLoss:   88.198341\trec:   59.957020\tkl:   28.241318\n",
      "Epoch: 512 [30100/50000 (60%)]  \tLoss:   87.058113\trec:   59.337204\tkl:   27.720905\n",
      "Epoch: 512 [40100/50000 (80%)]  \tLoss:   89.326759\trec:   62.113392\tkl:   27.213358\n",
      "====> Epoch: 512 Average train loss: 89.8950\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3764\n",
      "Epoch: 513 [  100/50000 ( 0%)]  \tLoss:   91.536934\trec:   63.413406\tkl:   28.123526\n",
      "Epoch: 513 [10100/50000 (20%)]  \tLoss:   91.154922\trec:   62.422653\tkl:   28.732265\n",
      "Epoch: 513 [20100/50000 (40%)]  \tLoss:   86.237839\trec:   58.907703\tkl:   27.330135\n",
      "Epoch: 513 [30100/50000 (60%)]  \tLoss:   88.936562\trec:   61.407635\tkl:   27.528931\n",
      "Epoch: 513 [40100/50000 (80%)]  \tLoss:   88.481483\trec:   60.878384\tkl:   27.603102\n",
      "====> Epoch: 513 Average train loss: 89.8976\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3244\n",
      "Epoch: 514 [  100/50000 ( 0%)]  \tLoss:   89.709473\trec:   61.981190\tkl:   27.728285\n",
      "Epoch: 514 [10100/50000 (20%)]  \tLoss:   91.775688\trec:   63.320347\tkl:   28.455345\n",
      "Epoch: 514 [20100/50000 (40%)]  \tLoss:   94.472176\trec:   66.550507\tkl:   27.921667\n",
      "Epoch: 514 [30100/50000 (60%)]  \tLoss:   88.436905\trec:   60.994404\tkl:   27.442501\n",
      "Epoch: 514 [40100/50000 (80%)]  \tLoss:   88.442596\trec:   61.023701\tkl:   27.418894\n",
      "====> Epoch: 514 Average train loss: 89.9017\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3241\n",
      "Epoch: 515 [  100/50000 ( 0%)]  \tLoss:   93.005386\trec:   64.667496\tkl:   28.337894\n",
      "Epoch: 515 [10100/50000 (20%)]  \tLoss:   89.247696\trec:   61.193306\tkl:   28.054386\n",
      "Epoch: 515 [20100/50000 (40%)]  \tLoss:   89.647179\trec:   61.498589\tkl:   28.148588\n",
      "Epoch: 515 [30100/50000 (60%)]  \tLoss:   87.647919\trec:   59.318596\tkl:   28.329319\n",
      "Epoch: 515 [40100/50000 (80%)]  \tLoss:   86.111359\trec:   58.834667\tkl:   27.276691\n",
      "====> Epoch: 515 Average train loss: 89.9022\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.5350\n",
      "Epoch: 516 [  100/50000 ( 0%)]  \tLoss:   88.517967\trec:   60.674053\tkl:   27.843916\n",
      "Epoch: 516 [10100/50000 (20%)]  \tLoss:   93.281502\trec:   65.278786\tkl:   28.002714\n",
      "Epoch: 516 [20100/50000 (40%)]  \tLoss:   88.080040\trec:   60.722107\tkl:   27.357933\n",
      "Epoch: 516 [30100/50000 (60%)]  \tLoss:   88.766830\trec:   61.073826\tkl:   27.693010\n",
      "Epoch: 516 [40100/50000 (80%)]  \tLoss:   90.464775\trec:   62.352367\tkl:   28.112404\n",
      "====> Epoch: 516 Average train loss: 89.8706\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3785\n",
      "Epoch: 517 [  100/50000 ( 0%)]  \tLoss:   88.939194\trec:   60.712322\tkl:   28.226870\n",
      "Epoch: 517 [10100/50000 (20%)]  \tLoss:   89.233231\trec:   60.438686\tkl:   28.794542\n",
      "Epoch: 517 [20100/50000 (40%)]  \tLoss:   89.152130\trec:   61.248611\tkl:   27.903517\n",
      "Epoch: 517 [30100/50000 (60%)]  \tLoss:   91.329559\trec:   63.709759\tkl:   27.619797\n",
      "Epoch: 517 [40100/50000 (80%)]  \tLoss:   91.144371\trec:   62.844528\tkl:   28.299849\n",
      "====> Epoch: 517 Average train loss: 89.8886\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3007\n",
      "Epoch: 518 [  100/50000 ( 0%)]  \tLoss:   92.376060\trec:   64.827255\tkl:   27.548807\n",
      "Epoch: 518 [10100/50000 (20%)]  \tLoss:   86.066658\trec:   60.137039\tkl:   25.929617\n",
      "Epoch: 518 [20100/50000 (40%)]  \tLoss:   90.220222\trec:   62.212578\tkl:   28.007643\n",
      "Epoch: 518 [30100/50000 (60%)]  \tLoss:   87.604851\trec:   60.802177\tkl:   26.802675\n",
      "Epoch: 518 [40100/50000 (80%)]  \tLoss:   89.825424\trec:   61.973679\tkl:   27.851748\n",
      "====> Epoch: 518 Average train loss: 89.8617\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3856\n",
      "Epoch: 519 [  100/50000 ( 0%)]  \tLoss:   85.926498\trec:   59.068455\tkl:   26.858042\n",
      "Epoch: 519 [10100/50000 (20%)]  \tLoss:   91.835556\trec:   62.655083\tkl:   29.180473\n",
      "Epoch: 519 [20100/50000 (40%)]  \tLoss:   89.196709\trec:   61.510033\tkl:   27.686676\n",
      "Epoch: 519 [30100/50000 (60%)]  \tLoss:   92.419342\trec:   63.417412\tkl:   29.001936\n",
      "Epoch: 519 [40100/50000 (80%)]  \tLoss:   89.522476\trec:   61.438007\tkl:   28.084469\n",
      "====> Epoch: 519 Average train loss: 89.8696\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3064\n",
      "Epoch: 520 [  100/50000 ( 0%)]  \tLoss:   91.172691\trec:   63.377224\tkl:   27.795467\n",
      "Epoch: 520 [10100/50000 (20%)]  \tLoss:   92.962204\trec:   64.376663\tkl:   28.585541\n",
      "Epoch: 520 [20100/50000 (40%)]  \tLoss:   88.632912\trec:   61.197102\tkl:   27.435801\n",
      "Epoch: 520 [30100/50000 (60%)]  \tLoss:   90.156303\trec:   61.999714\tkl:   28.156595\n",
      "Epoch: 520 [40100/50000 (80%)]  \tLoss:   92.952812\trec:   63.484753\tkl:   29.468061\n",
      "====> Epoch: 520 Average train loss: 89.8615\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3349\n",
      "Epoch: 521 [  100/50000 ( 0%)]  \tLoss:   88.791046\trec:   59.851810\tkl:   28.939232\n",
      "Epoch: 521 [10100/50000 (20%)]  \tLoss:   90.914352\trec:   62.442383\tkl:   28.471975\n",
      "Epoch: 521 [20100/50000 (40%)]  \tLoss:   92.257668\trec:   63.414032\tkl:   28.843636\n",
      "Epoch: 521 [30100/50000 (60%)]  \tLoss:   89.886330\trec:   62.142910\tkl:   27.743412\n",
      "Epoch: 521 [40100/50000 (80%)]  \tLoss:   88.269897\trec:   60.364643\tkl:   27.905264\n",
      "====> Epoch: 521 Average train loss: 89.8496\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3698\n",
      "Epoch: 522 [  100/50000 ( 0%)]  \tLoss:   84.999001\trec:   57.112816\tkl:   27.886190\n",
      "Epoch: 522 [10100/50000 (20%)]  \tLoss:   86.489296\trec:   58.818279\tkl:   27.671013\n",
      "Epoch: 522 [20100/50000 (40%)]  \tLoss:   86.569878\trec:   59.962238\tkl:   26.607635\n",
      "Epoch: 522 [30100/50000 (60%)]  \tLoss:   90.908279\trec:   62.312004\tkl:   28.596279\n",
      "Epoch: 522 [40100/50000 (80%)]  \tLoss:   88.237495\trec:   60.116867\tkl:   28.120625\n",
      "====> Epoch: 522 Average train loss: 89.8371\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3063\n",
      "Epoch: 523 [  100/50000 ( 0%)]  \tLoss:   93.609322\trec:   64.267647\tkl:   29.341677\n",
      "Epoch: 523 [10100/50000 (20%)]  \tLoss:   91.505005\trec:   63.305820\tkl:   28.199188\n",
      "Epoch: 523 [20100/50000 (40%)]  \tLoss:   89.959244\trec:   62.030994\tkl:   27.928251\n",
      "Epoch: 523 [30100/50000 (60%)]  \tLoss:   87.836090\trec:   60.077480\tkl:   27.758610\n",
      "Epoch: 523 [40100/50000 (80%)]  \tLoss:   88.101318\trec:   60.198555\tkl:   27.902765\n",
      "====> Epoch: 523 Average train loss: 89.8403\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3189\n",
      "Epoch: 524 [  100/50000 ( 0%)]  \tLoss:   90.351913\trec:   61.754284\tkl:   28.597626\n",
      "Epoch: 524 [10100/50000 (20%)]  \tLoss:   89.051865\trec:   60.592079\tkl:   28.459782\n",
      "Epoch: 524 [20100/50000 (40%)]  \tLoss:   86.079323\trec:   58.700966\tkl:   27.378359\n",
      "Epoch: 524 [30100/50000 (60%)]  \tLoss:   90.244118\trec:   62.065742\tkl:   28.178375\n",
      "Epoch: 524 [40100/50000 (80%)]  \tLoss:   90.616638\trec:   62.914898\tkl:   27.701742\n",
      "====> Epoch: 524 Average train loss: 89.8500\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3158\n",
      "Epoch: 525 [  100/50000 ( 0%)]  \tLoss:   91.685524\trec:   63.461658\tkl:   28.223862\n",
      "Epoch: 525 [10100/50000 (20%)]  \tLoss:   82.763893\trec:   55.697498\tkl:   27.066397\n",
      "Epoch: 525 [20100/50000 (40%)]  \tLoss:   90.498322\trec:   62.578804\tkl:   27.919521\n",
      "Epoch: 525 [30100/50000 (60%)]  \tLoss:   89.901581\trec:   61.704647\tkl:   28.196928\n",
      "Epoch: 525 [40100/50000 (80%)]  \tLoss:   92.032455\trec:   63.789543\tkl:   28.242914\n",
      "====> Epoch: 525 Average train loss: 89.8229\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.4682\n",
      "Epoch: 526 [  100/50000 ( 0%)]  \tLoss:   84.024200\trec:   56.814922\tkl:   27.209282\n",
      "Epoch: 526 [10100/50000 (20%)]  \tLoss:   91.103584\trec:   63.204472\tkl:   27.899111\n",
      "Epoch: 526 [20100/50000 (40%)]  \tLoss:   92.981560\trec:   64.493095\tkl:   28.488468\n",
      "Epoch: 526 [30100/50000 (60%)]  \tLoss:   88.753456\trec:   61.423729\tkl:   27.329725\n",
      "Epoch: 526 [40100/50000 (80%)]  \tLoss:   88.329414\trec:   60.636013\tkl:   27.693398\n",
      "====> Epoch: 526 Average train loss: 89.8333\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3154\n",
      "Epoch: 527 [  100/50000 ( 0%)]  \tLoss:   83.644211\trec:   57.143311\tkl:   26.500895\n",
      "Epoch: 527 [10100/50000 (20%)]  \tLoss:   84.196877\trec:   57.403702\tkl:   26.793169\n",
      "Epoch: 527 [20100/50000 (40%)]  \tLoss:   91.262924\trec:   63.028866\tkl:   28.234060\n",
      "Epoch: 527 [30100/50000 (60%)]  \tLoss:   89.020721\trec:   60.570755\tkl:   28.449965\n",
      "Epoch: 527 [40100/50000 (80%)]  \tLoss:   93.801460\trec:   64.738289\tkl:   29.063171\n",
      "====> Epoch: 527 Average train loss: 89.8338\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2674\n",
      "Epoch: 528 [  100/50000 ( 0%)]  \tLoss:   88.079514\trec:   60.402081\tkl:   27.677433\n",
      "Epoch: 528 [10100/50000 (20%)]  \tLoss:   91.351631\trec:   63.675438\tkl:   27.676193\n",
      "Epoch: 528 [20100/50000 (40%)]  \tLoss:   87.072685\trec:   59.389812\tkl:   27.682871\n",
      "Epoch: 528 [30100/50000 (60%)]  \tLoss:   91.397598\trec:   63.002094\tkl:   28.395508\n",
      "Epoch: 528 [40100/50000 (80%)]  \tLoss:   88.084160\trec:   61.379745\tkl:   26.704414\n",
      "====> Epoch: 528 Average train loss: 89.8214\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2890\n",
      "Epoch: 529 [  100/50000 ( 0%)]  \tLoss:   90.744980\trec:   62.569180\tkl:   28.175800\n",
      "Epoch: 529 [10100/50000 (20%)]  \tLoss:   91.100151\trec:   62.198093\tkl:   28.902060\n",
      "Epoch: 529 [20100/50000 (40%)]  \tLoss:   93.307594\trec:   64.767326\tkl:   28.540270\n",
      "Epoch: 529 [30100/50000 (60%)]  \tLoss:   84.936562\trec:   58.147068\tkl:   26.789494\n",
      "Epoch: 529 [40100/50000 (80%)]  \tLoss:   90.140739\trec:   62.555290\tkl:   27.585443\n",
      "====> Epoch: 529 Average train loss: 89.8254\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2512\n",
      "Epoch: 530 [  100/50000 ( 0%)]  \tLoss:   84.274498\trec:   57.456081\tkl:   26.818420\n",
      "Epoch: 530 [10100/50000 (20%)]  \tLoss:   91.289268\trec:   62.939880\tkl:   28.349384\n",
      "Epoch: 530 [20100/50000 (40%)]  \tLoss:   91.738884\trec:   63.412975\tkl:   28.325912\n",
      "Epoch: 530 [30100/50000 (60%)]  \tLoss:   87.285767\trec:   59.467846\tkl:   27.817923\n",
      "Epoch: 530 [40100/50000 (80%)]  \tLoss:   91.354454\trec:   62.970619\tkl:   28.383833\n",
      "====> Epoch: 530 Average train loss: 89.8509\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3563\n",
      "Epoch: 531 [  100/50000 ( 0%)]  \tLoss:   92.325233\trec:   63.530750\tkl:   28.794477\n",
      "Epoch: 531 [10100/50000 (20%)]  \tLoss:   93.846313\trec:   64.928757\tkl:   28.917559\n",
      "Epoch: 531 [20100/50000 (40%)]  \tLoss:   89.057915\trec:   61.009880\tkl:   28.048037\n",
      "Epoch: 531 [30100/50000 (60%)]  \tLoss:   91.986931\trec:   63.406101\tkl:   28.580830\n",
      "Epoch: 531 [40100/50000 (80%)]  \tLoss:   93.321060\trec:   65.124763\tkl:   28.196299\n",
      "====> Epoch: 531 Average train loss: 89.8072\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2607\n",
      "Epoch: 532 [  100/50000 ( 0%)]  \tLoss:   87.772987\trec:   60.535439\tkl:   27.237549\n",
      "Epoch: 532 [10100/50000 (20%)]  \tLoss:   92.366158\trec:   63.807888\tkl:   28.558271\n",
      "Epoch: 532 [20100/50000 (40%)]  \tLoss:   86.342339\trec:   59.872925\tkl:   26.469418\n",
      "Epoch: 532 [30100/50000 (60%)]  \tLoss:   87.578514\trec:   59.534794\tkl:   28.043718\n",
      "Epoch: 532 [40100/50000 (80%)]  \tLoss:   85.125572\trec:   58.085938\tkl:   27.039639\n",
      "====> Epoch: 532 Average train loss: 89.8066\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3382\n",
      "Epoch: 533 [  100/50000 ( 0%)]  \tLoss:   88.102516\trec:   59.999451\tkl:   28.103065\n",
      "Epoch: 533 [10100/50000 (20%)]  \tLoss:   91.180679\trec:   62.380322\tkl:   28.800362\n",
      "Epoch: 533 [20100/50000 (40%)]  \tLoss:   93.711746\trec:   64.780800\tkl:   28.930948\n",
      "Epoch: 533 [30100/50000 (60%)]  \tLoss:   90.031288\trec:   61.841354\tkl:   28.189934\n",
      "Epoch: 533 [40100/50000 (80%)]  \tLoss:   86.378174\trec:   59.077766\tkl:   27.300406\n",
      "====> Epoch: 533 Average train loss: 89.8136\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3446\n",
      "Epoch: 534 [  100/50000 ( 0%)]  \tLoss:   88.270699\trec:   60.736675\tkl:   27.534027\n",
      "Epoch: 534 [10100/50000 (20%)]  \tLoss:   89.174957\trec:   60.951862\tkl:   28.223095\n",
      "Epoch: 534 [20100/50000 (40%)]  \tLoss:   88.644157\trec:   60.996922\tkl:   27.647236\n",
      "Epoch: 534 [30100/50000 (60%)]  \tLoss:   90.065331\trec:   62.580994\tkl:   27.484337\n",
      "Epoch: 534 [40100/50000 (80%)]  \tLoss:   90.660721\trec:   62.383934\tkl:   28.276787\n",
      "====> Epoch: 534 Average train loss: 89.7866\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3218\n",
      "Epoch: 535 [  100/50000 ( 0%)]  \tLoss:   91.899979\trec:   64.584373\tkl:   27.315607\n",
      "Epoch: 535 [10100/50000 (20%)]  \tLoss:   85.559982\trec:   58.358013\tkl:   27.201962\n",
      "Epoch: 535 [20100/50000 (40%)]  \tLoss:   93.649155\trec:   65.371719\tkl:   28.277445\n",
      "Epoch: 535 [30100/50000 (60%)]  \tLoss:   92.450996\trec:   63.189743\tkl:   29.261253\n",
      "Epoch: 535 [40100/50000 (80%)]  \tLoss:   91.210945\trec:   62.260509\tkl:   28.950434\n",
      "====> Epoch: 535 Average train loss: 89.8073\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2652\n",
      "Epoch: 536 [  100/50000 ( 0%)]  \tLoss:   91.461426\trec:   63.314911\tkl:   28.146507\n",
      "Epoch: 536 [10100/50000 (20%)]  \tLoss:   88.140633\trec:   60.775234\tkl:   27.365400\n",
      "Epoch: 536 [20100/50000 (40%)]  \tLoss:   86.590401\trec:   59.874657\tkl:   26.715742\n",
      "Epoch: 536 [30100/50000 (60%)]  \tLoss:   91.035683\trec:   62.026913\tkl:   29.008774\n",
      "Epoch: 536 [40100/50000 (80%)]  \tLoss:   90.895226\trec:   61.784081\tkl:   29.111145\n",
      "====> Epoch: 536 Average train loss: 89.8156\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3075\n",
      "Epoch: 537 [  100/50000 ( 0%)]  \tLoss:   89.094398\trec:   61.235516\tkl:   27.858889\n",
      "Epoch: 537 [10100/50000 (20%)]  \tLoss:   86.374229\trec:   59.691807\tkl:   26.682421\n",
      "Epoch: 537 [20100/50000 (40%)]  \tLoss:   88.592293\trec:   61.204670\tkl:   27.387621\n",
      "Epoch: 537 [30100/50000 (60%)]  \tLoss:   88.233444\trec:   59.500633\tkl:   28.732815\n",
      "Epoch: 537 [40100/50000 (80%)]  \tLoss:   92.415154\trec:   63.663242\tkl:   28.751913\n",
      "====> Epoch: 537 Average train loss: 89.7850\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3254\n",
      "Epoch: 538 [  100/50000 ( 0%)]  \tLoss:   89.510841\trec:   61.980419\tkl:   27.530415\n",
      "Epoch: 538 [10100/50000 (20%)]  \tLoss:   89.429512\trec:   61.280807\tkl:   28.148705\n",
      "Epoch: 538 [20100/50000 (40%)]  \tLoss:   91.850418\trec:   63.721718\tkl:   28.128700\n",
      "Epoch: 538 [30100/50000 (60%)]  \tLoss:   86.295563\trec:   59.330967\tkl:   26.964600\n",
      "Epoch: 538 [40100/50000 (80%)]  \tLoss:   88.820679\trec:   60.656784\tkl:   28.163897\n",
      "====> Epoch: 538 Average train loss: 89.7821\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3620\n",
      "Epoch: 539 [  100/50000 ( 0%)]  \tLoss:   89.117302\trec:   60.230820\tkl:   28.886482\n",
      "Epoch: 539 [10100/50000 (20%)]  \tLoss:   85.116638\trec:   58.246494\tkl:   26.870140\n",
      "Epoch: 539 [20100/50000 (40%)]  \tLoss:   89.045738\trec:   61.396278\tkl:   27.649460\n",
      "Epoch: 539 [30100/50000 (60%)]  \tLoss:   91.291481\trec:   63.139950\tkl:   28.151535\n",
      "Epoch: 539 [40100/50000 (80%)]  \tLoss:   87.509064\trec:   60.603821\tkl:   26.905243\n",
      "====> Epoch: 539 Average train loss: 89.7651\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2910\n",
      "Epoch: 540 [  100/50000 ( 0%)]  \tLoss:   89.341461\trec:   61.303036\tkl:   28.038431\n",
      "Epoch: 540 [10100/50000 (20%)]  \tLoss:   89.806953\trec:   61.714245\tkl:   28.092705\n",
      "Epoch: 540 [20100/50000 (40%)]  \tLoss:   90.380615\trec:   62.757195\tkl:   27.623415\n",
      "Epoch: 540 [30100/50000 (60%)]  \tLoss:   95.012306\trec:   65.848610\tkl:   29.163691\n",
      "Epoch: 540 [40100/50000 (80%)]  \tLoss:   92.904663\trec:   64.198341\tkl:   28.706329\n",
      "====> Epoch: 540 Average train loss: 89.7758\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2635\n",
      "Epoch: 541 [  100/50000 ( 0%)]  \tLoss:   89.077751\trec:   60.770027\tkl:   28.307730\n",
      "Epoch: 541 [10100/50000 (20%)]  \tLoss:   88.261124\trec:   60.984539\tkl:   27.276581\n",
      "Epoch: 541 [20100/50000 (40%)]  \tLoss:   91.396095\trec:   62.739422\tkl:   28.656672\n",
      "Epoch: 541 [30100/50000 (60%)]  \tLoss:   90.522438\trec:   62.859997\tkl:   27.662441\n",
      "Epoch: 541 [40100/50000 (80%)]  \tLoss:   92.040138\trec:   63.628284\tkl:   28.411856\n",
      "====> Epoch: 541 Average train loss: 89.7705\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2336\n",
      "Epoch: 542 [  100/50000 ( 0%)]  \tLoss:   89.882607\trec:   62.850956\tkl:   27.031647\n",
      "Epoch: 542 [10100/50000 (20%)]  \tLoss:   88.583984\trec:   61.561825\tkl:   27.022163\n",
      "Epoch: 542 [20100/50000 (40%)]  \tLoss:   91.182579\trec:   62.841999\tkl:   28.340580\n",
      "Epoch: 542 [30100/50000 (60%)]  \tLoss:   93.369438\trec:   64.989960\tkl:   28.379484\n",
      "Epoch: 542 [40100/50000 (80%)]  \tLoss:   90.357826\trec:   61.954723\tkl:   28.403111\n",
      "====> Epoch: 542 Average train loss: 89.7555\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1594\n",
      "Epoch: 543 [  100/50000 ( 0%)]  \tLoss:   87.620079\trec:   59.464844\tkl:   28.155239\n",
      "Epoch: 543 [10100/50000 (20%)]  \tLoss:   89.875694\trec:   62.323250\tkl:   27.552441\n",
      "Epoch: 543 [20100/50000 (40%)]  \tLoss:   90.239098\trec:   62.522320\tkl:   27.716776\n",
      "Epoch: 543 [30100/50000 (60%)]  \tLoss:   92.783745\trec:   65.115921\tkl:   27.667826\n",
      "Epoch: 543 [40100/50000 (80%)]  \tLoss:   88.812401\trec:   60.991257\tkl:   27.821142\n",
      "====> Epoch: 543 Average train loss: 89.7729\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3050\n",
      "Epoch: 544 [  100/50000 ( 0%)]  \tLoss:   89.433228\trec:   61.887085\tkl:   27.546146\n",
      "Epoch: 544 [10100/50000 (20%)]  \tLoss:   90.887436\trec:   63.134319\tkl:   27.753115\n",
      "Epoch: 544 [20100/50000 (40%)]  \tLoss:   91.971954\trec:   63.947998\tkl:   28.023954\n",
      "Epoch: 544 [30100/50000 (60%)]  \tLoss:   90.360847\trec:   62.122139\tkl:   28.238710\n",
      "Epoch: 544 [40100/50000 (80%)]  \tLoss:   89.659119\trec:   60.705643\tkl:   28.953478\n",
      "====> Epoch: 544 Average train loss: 89.7513\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3089\n",
      "Epoch: 545 [  100/50000 ( 0%)]  \tLoss:   91.172905\trec:   63.164230\tkl:   28.008680\n",
      "Epoch: 545 [10100/50000 (20%)]  \tLoss:   90.728638\trec:   62.291122\tkl:   28.437517\n",
      "Epoch: 545 [20100/50000 (40%)]  \tLoss:   89.619728\trec:   62.840324\tkl:   26.779398\n",
      "Epoch: 545 [30100/50000 (60%)]  \tLoss:   94.012642\trec:   65.373444\tkl:   28.639198\n",
      "Epoch: 545 [40100/50000 (80%)]  \tLoss:   90.981964\trec:   63.033154\tkl:   27.948809\n",
      "====> Epoch: 545 Average train loss: 89.7422\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2742\n",
      "Epoch: 546 [  100/50000 ( 0%)]  \tLoss:   92.087196\trec:   64.488007\tkl:   27.599190\n",
      "Epoch: 546 [10100/50000 (20%)]  \tLoss:   92.635597\trec:   63.041862\tkl:   29.593729\n",
      "Epoch: 546 [20100/50000 (40%)]  \tLoss:   84.908554\trec:   57.314404\tkl:   27.594151\n",
      "Epoch: 546 [30100/50000 (60%)]  \tLoss:   91.742294\trec:   64.289589\tkl:   27.452705\n",
      "Epoch: 546 [40100/50000 (80%)]  \tLoss:   87.729675\trec:   60.613106\tkl:   27.116571\n",
      "====> Epoch: 546 Average train loss: 89.7458\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2825\n",
      "Epoch: 547 [  100/50000 ( 0%)]  \tLoss:   82.195679\trec:   55.831684\tkl:   26.363998\n",
      "Epoch: 547 [10100/50000 (20%)]  \tLoss:   86.012062\trec:   58.038265\tkl:   27.973795\n",
      "Epoch: 547 [20100/50000 (40%)]  \tLoss:   90.520973\trec:   62.374226\tkl:   28.146749\n",
      "Epoch: 547 [30100/50000 (60%)]  \tLoss:   83.486404\trec:   56.455185\tkl:   27.031219\n",
      "Epoch: 547 [40100/50000 (80%)]  \tLoss:   90.047157\trec:   62.486103\tkl:   27.561056\n",
      "====> Epoch: 547 Average train loss: 89.7491\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3421\n",
      "Epoch: 548 [  100/50000 ( 0%)]  \tLoss:   92.171288\trec:   63.214825\tkl:   28.956465\n",
      "Epoch: 548 [10100/50000 (20%)]  \tLoss:   90.459763\trec:   62.015095\tkl:   28.444668\n",
      "Epoch: 548 [20100/50000 (40%)]  \tLoss:   87.641632\trec:   61.000019\tkl:   26.641611\n",
      "Epoch: 548 [30100/50000 (60%)]  \tLoss:   85.999908\trec:   58.855286\tkl:   27.144623\n",
      "Epoch: 548 [40100/50000 (80%)]  \tLoss:   86.042175\trec:   58.597240\tkl:   27.444937\n",
      "====> Epoch: 548 Average train loss: 89.7354\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1924\n",
      "Epoch: 549 [  100/50000 ( 0%)]  \tLoss:   89.232613\trec:   60.923935\tkl:   28.308681\n",
      "Epoch: 549 [10100/50000 (20%)]  \tLoss:   91.860718\trec:   63.322292\tkl:   28.538422\n",
      "Epoch: 549 [20100/50000 (40%)]  \tLoss:   91.852364\trec:   62.669189\tkl:   29.183172\n",
      "Epoch: 549 [30100/50000 (60%)]  \tLoss:   90.136215\trec:   61.754734\tkl:   28.381483\n",
      "Epoch: 549 [40100/50000 (80%)]  \tLoss:   89.449623\trec:   62.036610\tkl:   27.413017\n",
      "====> Epoch: 549 Average train loss: 89.7313\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2326\n",
      "Epoch: 550 [  100/50000 ( 0%)]  \tLoss:   92.964531\trec:   64.570015\tkl:   28.394510\n",
      "Epoch: 550 [10100/50000 (20%)]  \tLoss:   90.565895\trec:   61.393982\tkl:   29.171919\n",
      "Epoch: 550 [20100/50000 (40%)]  \tLoss:   90.201469\trec:   61.959373\tkl:   28.242096\n",
      "Epoch: 550 [30100/50000 (60%)]  \tLoss:   87.968712\trec:   60.288509\tkl:   27.680199\n",
      "Epoch: 550 [40100/50000 (80%)]  \tLoss:   90.785095\trec:   62.633427\tkl:   28.151669\n",
      "====> Epoch: 550 Average train loss: 89.7168\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2038\n",
      "Epoch: 551 [  100/50000 ( 0%)]  \tLoss:   87.323380\trec:   59.777431\tkl:   27.545946\n",
      "Epoch: 551 [10100/50000 (20%)]  \tLoss:   91.264076\trec:   63.502640\tkl:   27.761440\n",
      "Epoch: 551 [20100/50000 (40%)]  \tLoss:   89.091011\trec:   61.246967\tkl:   27.844042\n",
      "Epoch: 551 [30100/50000 (60%)]  \tLoss:   91.209450\trec:   64.126663\tkl:   27.082792\n",
      "Epoch: 551 [40100/50000 (80%)]  \tLoss:   85.950386\trec:   57.966633\tkl:   27.983751\n",
      "====> Epoch: 551 Average train loss: 89.7211\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1716\n",
      "Epoch: 552 [  100/50000 ( 0%)]  \tLoss:   90.742477\trec:   62.612568\tkl:   28.129913\n",
      "Epoch: 552 [10100/50000 (20%)]  \tLoss:   86.109932\trec:   58.942749\tkl:   27.167183\n",
      "Epoch: 552 [20100/50000 (40%)]  \tLoss:   89.071411\trec:   61.106289\tkl:   27.965124\n",
      "Epoch: 552 [30100/50000 (60%)]  \tLoss:   88.441147\trec:   61.148251\tkl:   27.292904\n",
      "Epoch: 552 [40100/50000 (80%)]  \tLoss:   89.315147\trec:   61.401455\tkl:   27.913691\n",
      "====> Epoch: 552 Average train loss: 89.7224\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2512\n",
      "Epoch: 553 [  100/50000 ( 0%)]  \tLoss:   92.868370\trec:   64.249321\tkl:   28.619045\n",
      "Epoch: 553 [10100/50000 (20%)]  \tLoss:   91.543930\trec:   62.166611\tkl:   29.377323\n",
      "Epoch: 553 [20100/50000 (40%)]  \tLoss:   92.720505\trec:   63.991657\tkl:   28.728842\n",
      "Epoch: 553 [30100/50000 (60%)]  \tLoss:   92.228256\trec:   64.457626\tkl:   27.770632\n",
      "Epoch: 553 [40100/50000 (80%)]  \tLoss:   92.071091\trec:   63.057365\tkl:   29.013720\n",
      "====> Epoch: 553 Average train loss: 89.7100\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3188\n",
      "Epoch: 554 [  100/50000 ( 0%)]  \tLoss:   90.306900\trec:   62.440681\tkl:   27.866220\n",
      "Epoch: 554 [10100/50000 (20%)]  \tLoss:   89.911522\trec:   62.872917\tkl:   27.038603\n",
      "Epoch: 554 [20100/50000 (40%)]  \tLoss:   89.247879\trec:   61.295757\tkl:   27.952126\n",
      "Epoch: 554 [30100/50000 (60%)]  \tLoss:   87.294685\trec:   60.109898\tkl:   27.184793\n",
      "Epoch: 554 [40100/50000 (80%)]  \tLoss:   84.601669\trec:   57.069572\tkl:   27.532095\n",
      "====> Epoch: 554 Average train loss: 89.7146\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1978\n",
      "Epoch: 555 [  100/50000 ( 0%)]  \tLoss:   87.353706\trec:   59.836758\tkl:   27.516958\n",
      "Epoch: 555 [10100/50000 (20%)]  \tLoss:   93.623116\trec:   65.023613\tkl:   28.599499\n",
      "Epoch: 555 [20100/50000 (40%)]  \tLoss:   89.155800\trec:   61.657402\tkl:   27.498392\n",
      "Epoch: 555 [30100/50000 (60%)]  \tLoss:   88.507912\trec:   60.519657\tkl:   27.988256\n",
      "Epoch: 555 [40100/50000 (80%)]  \tLoss:   87.122887\trec:   58.895935\tkl:   28.226952\n",
      "====> Epoch: 555 Average train loss: 89.6986\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2489\n",
      "Epoch: 556 [  100/50000 ( 0%)]  \tLoss:   91.348335\trec:   63.104961\tkl:   28.243383\n",
      "Epoch: 556 [10100/50000 (20%)]  \tLoss:   90.007431\trec:   61.936234\tkl:   28.071198\n",
      "Epoch: 556 [20100/50000 (40%)]  \tLoss:   89.071289\trec:   61.020351\tkl:   28.050941\n",
      "Epoch: 556 [30100/50000 (60%)]  \tLoss:   86.764862\trec:   59.297775\tkl:   27.467081\n",
      "Epoch: 556 [40100/50000 (80%)]  \tLoss:   90.733719\trec:   62.754574\tkl:   27.979143\n",
      "====> Epoch: 556 Average train loss: 89.7084\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2364\n",
      "Epoch: 557 [  100/50000 ( 0%)]  \tLoss:   88.095406\trec:   60.239822\tkl:   27.855591\n",
      "Epoch: 557 [10100/50000 (20%)]  \tLoss:   91.405777\trec:   61.905579\tkl:   29.500200\n",
      "Epoch: 557 [20100/50000 (40%)]  \tLoss:   92.483963\trec:   64.281601\tkl:   28.202362\n",
      "Epoch: 557 [30100/50000 (60%)]  \tLoss:   88.463692\trec:   61.354004\tkl:   27.109692\n",
      "Epoch: 557 [40100/50000 (80%)]  \tLoss:   87.437439\trec:   60.187119\tkl:   27.250322\n",
      "====> Epoch: 557 Average train loss: 89.7097\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2126\n",
      "Epoch: 558 [  100/50000 ( 0%)]  \tLoss:   93.792068\trec:   64.505943\tkl:   29.286123\n",
      "Epoch: 558 [10100/50000 (20%)]  \tLoss:   89.981133\trec:   61.443047\tkl:   28.538080\n",
      "Epoch: 558 [20100/50000 (40%)]  \tLoss:   87.935982\trec:   59.533386\tkl:   28.402599\n",
      "Epoch: 558 [30100/50000 (60%)]  \tLoss:   86.232140\trec:   58.923309\tkl:   27.308828\n",
      "Epoch: 558 [40100/50000 (80%)]  \tLoss:   88.043388\trec:   60.073509\tkl:   27.969875\n",
      "====> Epoch: 558 Average train loss: 89.6890\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2490\n",
      "Epoch: 559 [  100/50000 ( 0%)]  \tLoss:   86.516167\trec:   59.293884\tkl:   27.222290\n",
      "Epoch: 559 [10100/50000 (20%)]  \tLoss:   91.151955\trec:   62.957409\tkl:   28.194536\n",
      "Epoch: 559 [20100/50000 (40%)]  \tLoss:   90.627632\trec:   63.312576\tkl:   27.315058\n",
      "Epoch: 559 [30100/50000 (60%)]  \tLoss:   90.696678\trec:   62.289455\tkl:   28.407221\n",
      "Epoch: 559 [40100/50000 (80%)]  \tLoss:   92.688751\trec:   64.093155\tkl:   28.595589\n",
      "====> Epoch: 559 Average train loss: 89.6928\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1868\n",
      "Epoch: 560 [  100/50000 ( 0%)]  \tLoss:   90.548592\trec:   61.793755\tkl:   28.754843\n",
      "Epoch: 560 [10100/50000 (20%)]  \tLoss:   89.359421\trec:   62.225796\tkl:   27.133625\n",
      "Epoch: 560 [20100/50000 (40%)]  \tLoss:   88.889023\trec:   61.260311\tkl:   27.628716\n",
      "Epoch: 560 [30100/50000 (60%)]  \tLoss:   90.856850\trec:   62.247509\tkl:   28.609350\n",
      "Epoch: 560 [40100/50000 (80%)]  \tLoss:   88.098709\trec:   59.800632\tkl:   28.298080\n",
      "====> Epoch: 560 Average train loss: 89.6666\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2384\n",
      "Epoch: 561 [  100/50000 ( 0%)]  \tLoss:   93.736687\trec:   64.826874\tkl:   28.909811\n",
      "Epoch: 561 [10100/50000 (20%)]  \tLoss:   87.770432\trec:   60.363113\tkl:   27.407310\n",
      "Epoch: 561 [20100/50000 (40%)]  \tLoss:   86.263260\trec:   59.152798\tkl:   27.110458\n",
      "Epoch: 561 [30100/50000 (60%)]  \tLoss:   89.169472\trec:   61.058239\tkl:   28.111225\n",
      "Epoch: 561 [40100/50000 (80%)]  \tLoss:   86.968765\trec:   59.197353\tkl:   27.771416\n",
      "====> Epoch: 561 Average train loss: 89.6768\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2160\n",
      "Epoch: 562 [  100/50000 ( 0%)]  \tLoss:   88.593155\trec:   60.787354\tkl:   27.805799\n",
      "Epoch: 562 [10100/50000 (20%)]  \tLoss:   92.875816\trec:   63.894482\tkl:   28.981337\n",
      "Epoch: 562 [20100/50000 (40%)]  \tLoss:   90.594521\trec:   62.266869\tkl:   28.327648\n",
      "Epoch: 562 [30100/50000 (60%)]  \tLoss:   88.043106\trec:   60.709324\tkl:   27.333784\n",
      "Epoch: 562 [40100/50000 (80%)]  \tLoss:   92.237732\trec:   63.662586\tkl:   28.575146\n",
      "====> Epoch: 562 Average train loss: 89.6762\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2056\n",
      "Epoch: 563 [  100/50000 ( 0%)]  \tLoss:   86.916641\trec:   59.640652\tkl:   27.275986\n",
      "Epoch: 563 [10100/50000 (20%)]  \tLoss:   96.078857\trec:   66.938263\tkl:   29.140594\n",
      "Epoch: 563 [20100/50000 (40%)]  \tLoss:   90.483162\trec:   61.970638\tkl:   28.512529\n",
      "Epoch: 563 [30100/50000 (60%)]  \tLoss:   89.166206\trec:   61.308285\tkl:   27.857920\n",
      "Epoch: 563 [40100/50000 (80%)]  \tLoss:   90.116249\trec:   61.790424\tkl:   28.325827\n",
      "====> Epoch: 563 Average train loss: 89.6578\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1470\n",
      "Epoch: 564 [  100/50000 ( 0%)]  \tLoss:   88.911980\trec:   61.356220\tkl:   27.555763\n",
      "Epoch: 564 [10100/50000 (20%)]  \tLoss:   91.971169\trec:   63.328091\tkl:   28.643085\n",
      "Epoch: 564 [20100/50000 (40%)]  \tLoss:   86.975937\trec:   59.420418\tkl:   27.555521\n",
      "Epoch: 564 [30100/50000 (60%)]  \tLoss:   86.689774\trec:   60.135090\tkl:   26.554682\n",
      "Epoch: 564 [40100/50000 (80%)]  \tLoss:   89.809616\trec:   62.391182\tkl:   27.418436\n",
      "====> Epoch: 564 Average train loss: 89.6707\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2799\n",
      "Epoch: 565 [  100/50000 ( 0%)]  \tLoss:   89.674141\trec:   60.997395\tkl:   28.676748\n",
      "Epoch: 565 [10100/50000 (20%)]  \tLoss:   88.128868\trec:   60.904751\tkl:   27.224121\n",
      "Epoch: 565 [20100/50000 (40%)]  \tLoss:   86.259262\trec:   58.574734\tkl:   27.684530\n",
      "Epoch: 565 [30100/50000 (60%)]  \tLoss:   88.217110\trec:   60.136765\tkl:   28.080345\n",
      "Epoch: 565 [40100/50000 (80%)]  \tLoss:   89.217262\trec:   61.363743\tkl:   27.853516\n",
      "====> Epoch: 565 Average train loss: 89.6781\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3115\n",
      "Epoch: 566 [  100/50000 ( 0%)]  \tLoss:   88.502556\trec:   60.452694\tkl:   28.049866\n",
      "Epoch: 566 [10100/50000 (20%)]  \tLoss:   91.809052\trec:   64.236984\tkl:   27.572067\n",
      "Epoch: 566 [20100/50000 (40%)]  \tLoss:   87.903175\trec:   60.061657\tkl:   27.841513\n",
      "Epoch: 566 [30100/50000 (60%)]  \tLoss:   86.813911\trec:   59.439377\tkl:   27.374533\n",
      "Epoch: 566 [40100/50000 (80%)]  \tLoss:   84.922890\trec:   58.036648\tkl:   26.886238\n",
      "====> Epoch: 566 Average train loss: 89.6610\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2621\n",
      "Epoch: 567 [  100/50000 ( 0%)]  \tLoss:   86.446320\trec:   59.040882\tkl:   27.405434\n",
      "Epoch: 567 [10100/50000 (20%)]  \tLoss:   87.636238\trec:   59.936523\tkl:   27.699718\n",
      "Epoch: 567 [20100/50000 (40%)]  \tLoss:   90.513412\trec:   62.402927\tkl:   28.110493\n",
      "Epoch: 567 [30100/50000 (60%)]  \tLoss:   95.057076\trec:   66.341171\tkl:   28.715904\n",
      "Epoch: 567 [40100/50000 (80%)]  \tLoss:   86.407188\trec:   58.367657\tkl:   28.039526\n",
      "====> Epoch: 567 Average train loss: 89.6423\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1933\n",
      "Epoch: 568 [  100/50000 ( 0%)]  \tLoss:   88.029480\trec:   60.485691\tkl:   27.543789\n",
      "Epoch: 568 [10100/50000 (20%)]  \tLoss:   90.752808\trec:   61.428448\tkl:   29.324364\n",
      "Epoch: 568 [20100/50000 (40%)]  \tLoss:   92.178062\trec:   63.683495\tkl:   28.494576\n",
      "Epoch: 568 [30100/50000 (60%)]  \tLoss:   92.014366\trec:   63.927158\tkl:   28.087206\n",
      "Epoch: 568 [40100/50000 (80%)]  \tLoss:   92.507790\trec:   63.143665\tkl:   29.364126\n",
      "====> Epoch: 568 Average train loss: 89.6590\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2167\n",
      "Epoch: 569 [  100/50000 ( 0%)]  \tLoss:   90.391541\trec:   62.682205\tkl:   27.709341\n",
      "Epoch: 569 [10100/50000 (20%)]  \tLoss:   88.829178\trec:   61.254852\tkl:   27.574324\n",
      "Epoch: 569 [20100/50000 (40%)]  \tLoss:   93.042076\trec:   64.461266\tkl:   28.580811\n",
      "Epoch: 569 [30100/50000 (60%)]  \tLoss:   89.318672\trec:   61.697689\tkl:   27.620981\n",
      "Epoch: 569 [40100/50000 (80%)]  \tLoss:   88.420837\trec:   60.498451\tkl:   27.922392\n",
      "====> Epoch: 569 Average train loss: 89.6691\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1733\n",
      "Epoch: 570 [  100/50000 ( 0%)]  \tLoss:   90.898804\trec:   62.149216\tkl:   28.749584\n",
      "Epoch: 570 [10100/50000 (20%)]  \tLoss:   86.810020\trec:   59.760330\tkl:   27.049683\n",
      "Epoch: 570 [20100/50000 (40%)]  \tLoss:   87.335976\trec:   59.871872\tkl:   27.464104\n",
      "Epoch: 570 [30100/50000 (60%)]  \tLoss:   92.664688\trec:   65.302841\tkl:   27.361843\n",
      "Epoch: 570 [40100/50000 (80%)]  \tLoss:   89.018364\trec:   61.221088\tkl:   27.797279\n",
      "====> Epoch: 570 Average train loss: 89.6546\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1148\n",
      "Epoch: 571 [  100/50000 ( 0%)]  \tLoss:   92.197449\trec:   62.990219\tkl:   29.207233\n",
      "Epoch: 571 [10100/50000 (20%)]  \tLoss:   87.446251\trec:   60.414776\tkl:   27.031479\n",
      "Epoch: 571 [20100/50000 (40%)]  \tLoss:   89.397964\trec:   62.349705\tkl:   27.048265\n",
      "Epoch: 571 [30100/50000 (60%)]  \tLoss:   88.612137\trec:   60.804695\tkl:   27.807438\n",
      "Epoch: 571 [40100/50000 (80%)]  \tLoss:   91.887497\trec:   62.913197\tkl:   28.974300\n",
      "====> Epoch: 571 Average train loss: 89.6353\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3451\n",
      "Epoch: 572 [  100/50000 ( 0%)]  \tLoss:   88.806206\trec:   60.650330\tkl:   28.155874\n",
      "Epoch: 572 [10100/50000 (20%)]  \tLoss:   90.030624\trec:   62.020943\tkl:   28.009680\n",
      "Epoch: 572 [20100/50000 (40%)]  \tLoss:   91.122818\trec:   62.616482\tkl:   28.506334\n",
      "Epoch: 572 [30100/50000 (60%)]  \tLoss:   86.211464\trec:   59.388794\tkl:   26.822672\n",
      "Epoch: 572 [40100/50000 (80%)]  \tLoss:   89.690193\trec:   61.913704\tkl:   27.776484\n",
      "====> Epoch: 572 Average train loss: 89.6452\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2246\n",
      "Epoch: 573 [  100/50000 ( 0%)]  \tLoss:   90.166191\trec:   61.152870\tkl:   29.013325\n",
      "Epoch: 573 [10100/50000 (20%)]  \tLoss:   89.452911\trec:   61.530293\tkl:   27.922615\n",
      "Epoch: 573 [20100/50000 (40%)]  \tLoss:   86.893417\trec:   60.193726\tkl:   26.699694\n",
      "Epoch: 573 [30100/50000 (60%)]  \tLoss:   90.180328\trec:   61.011894\tkl:   29.168436\n",
      "Epoch: 573 [40100/50000 (80%)]  \tLoss:   82.424545\trec:   56.546719\tkl:   25.877827\n",
      "====> Epoch: 573 Average train loss: 89.6231\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1729\n",
      "Epoch: 574 [  100/50000 ( 0%)]  \tLoss:   85.477745\trec:   58.318439\tkl:   27.159302\n",
      "Epoch: 574 [10100/50000 (20%)]  \tLoss:   88.090233\trec:   60.622917\tkl:   27.467308\n",
      "Epoch: 574 [20100/50000 (40%)]  \tLoss:   89.025978\trec:   60.570847\tkl:   28.455122\n",
      "Epoch: 574 [30100/50000 (60%)]  \tLoss:   90.530434\trec:   62.498405\tkl:   28.032034\n",
      "Epoch: 574 [40100/50000 (80%)]  \tLoss:   88.462677\trec:   61.519325\tkl:   26.943344\n",
      "====> Epoch: 574 Average train loss: 89.6320\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2065\n",
      "Epoch: 575 [  100/50000 ( 0%)]  \tLoss:   89.841148\trec:   61.963142\tkl:   27.878002\n",
      "Epoch: 575 [10100/50000 (20%)]  \tLoss:   91.453003\trec:   63.444519\tkl:   28.008480\n",
      "Epoch: 575 [20100/50000 (40%)]  \tLoss:   91.773453\trec:   63.179855\tkl:   28.593597\n",
      "Epoch: 575 [30100/50000 (60%)]  \tLoss:   87.147057\trec:   59.105732\tkl:   28.041328\n",
      "Epoch: 575 [40100/50000 (80%)]  \tLoss:   88.948868\trec:   60.367580\tkl:   28.581278\n",
      "====> Epoch: 575 Average train loss: 89.6283\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3331\n",
      "Epoch: 576 [  100/50000 ( 0%)]  \tLoss:   91.992790\trec:   63.854393\tkl:   28.138393\n",
      "Epoch: 576 [10100/50000 (20%)]  \tLoss:   91.005806\trec:   63.171230\tkl:   27.834579\n",
      "Epoch: 576 [20100/50000 (40%)]  \tLoss:   86.327614\trec:   58.769028\tkl:   27.558590\n",
      "Epoch: 576 [30100/50000 (60%)]  \tLoss:   88.031845\trec:   60.733398\tkl:   27.298447\n",
      "Epoch: 576 [40100/50000 (80%)]  \tLoss:   86.448486\trec:   58.838562\tkl:   27.609921\n",
      "====> Epoch: 576 Average train loss: 89.6214\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.3269\n",
      "Epoch: 577 [  100/50000 ( 0%)]  \tLoss:   90.597969\trec:   62.372631\tkl:   28.225336\n",
      "Epoch: 577 [10100/50000 (20%)]  \tLoss:   88.665314\trec:   60.981941\tkl:   27.683371\n",
      "Epoch: 577 [20100/50000 (40%)]  \tLoss:   90.713425\trec:   61.670055\tkl:   29.043369\n",
      "Epoch: 577 [30100/50000 (60%)]  \tLoss:   89.411697\trec:   62.320580\tkl:   27.091122\n",
      "Epoch: 577 [40100/50000 (80%)]  \tLoss:   89.029999\trec:   60.643406\tkl:   28.386597\n",
      "====> Epoch: 577 Average train loss: 89.6105\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2759\n",
      "Epoch: 578 [  100/50000 ( 0%)]  \tLoss:   87.773651\trec:   59.956795\tkl:   27.816854\n",
      "Epoch: 578 [10100/50000 (20%)]  \tLoss:   87.518982\trec:   59.746220\tkl:   27.772766\n",
      "Epoch: 578 [20100/50000 (40%)]  \tLoss:   90.381012\trec:   62.684971\tkl:   27.696049\n",
      "Epoch: 578 [30100/50000 (60%)]  \tLoss:   90.830132\trec:   63.807774\tkl:   27.022362\n",
      "Epoch: 578 [40100/50000 (80%)]  \tLoss:   87.771759\trec:   60.917690\tkl:   26.854061\n",
      "====> Epoch: 578 Average train loss: 89.6012\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2167\n",
      "Epoch: 579 [  100/50000 ( 0%)]  \tLoss:   93.674980\trec:   63.311348\tkl:   30.363630\n",
      "Epoch: 579 [10100/50000 (20%)]  \tLoss:   92.991798\trec:   65.614075\tkl:   27.377714\n",
      "Epoch: 579 [20100/50000 (40%)]  \tLoss:   90.140526\trec:   62.020184\tkl:   28.120346\n",
      "Epoch: 579 [30100/50000 (60%)]  \tLoss:   87.302399\trec:   60.198601\tkl:   27.103794\n",
      "Epoch: 579 [40100/50000 (80%)]  \tLoss:   84.553452\trec:   57.582748\tkl:   26.970711\n",
      "====> Epoch: 579 Average train loss: 89.6021\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1425\n",
      "Epoch: 580 [  100/50000 ( 0%)]  \tLoss:   87.293678\trec:   59.705154\tkl:   27.588528\n",
      "Epoch: 580 [10100/50000 (20%)]  \tLoss:   92.510399\trec:   65.130417\tkl:   27.379980\n",
      "Epoch: 580 [20100/50000 (40%)]  \tLoss:   88.885078\trec:   61.209942\tkl:   27.675137\n",
      "Epoch: 580 [30100/50000 (60%)]  \tLoss:   89.053436\trec:   61.434315\tkl:   27.619120\n",
      "Epoch: 580 [40100/50000 (80%)]  \tLoss:   87.467224\trec:   59.579807\tkl:   27.887417\n",
      "====> Epoch: 580 Average train loss: 89.6098\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2110\n",
      "Epoch: 581 [  100/50000 ( 0%)]  \tLoss:   86.466492\trec:   59.565971\tkl:   26.900520\n",
      "Epoch: 581 [10100/50000 (20%)]  \tLoss:   90.823318\trec:   62.030331\tkl:   28.792982\n",
      "Epoch: 581 [20100/50000 (40%)]  \tLoss:   87.638496\trec:   60.041931\tkl:   27.596558\n",
      "Epoch: 581 [30100/50000 (60%)]  \tLoss:   92.692810\trec:   64.324028\tkl:   28.368778\n",
      "Epoch: 581 [40100/50000 (80%)]  \tLoss:   88.021347\trec:   60.368820\tkl:   27.652529\n",
      "====> Epoch: 581 Average train loss: 89.6074\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1591\n",
      "Epoch: 582 [  100/50000 ( 0%)]  \tLoss:   93.348358\trec:   63.985458\tkl:   29.362900\n",
      "Epoch: 582 [10100/50000 (20%)]  \tLoss:   89.670601\trec:   61.605686\tkl:   28.064915\n",
      "Epoch: 582 [20100/50000 (40%)]  \tLoss:   88.289276\trec:   59.724579\tkl:   28.564701\n",
      "Epoch: 582 [30100/50000 (60%)]  \tLoss:   90.789932\trec:   62.235710\tkl:   28.554218\n",
      "Epoch: 582 [40100/50000 (80%)]  \tLoss:   86.296593\trec:   59.293049\tkl:   27.003540\n",
      "====> Epoch: 582 Average train loss: 89.6110\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2063\n",
      "Epoch: 583 [  100/50000 ( 0%)]  \tLoss:   88.186752\trec:   60.403030\tkl:   27.783726\n",
      "Epoch: 583 [10100/50000 (20%)]  \tLoss:   90.787010\trec:   62.662666\tkl:   28.124340\n",
      "Epoch: 583 [20100/50000 (40%)]  \tLoss:   86.766006\trec:   59.542236\tkl:   27.223768\n",
      "Epoch: 583 [30100/50000 (60%)]  \tLoss:   88.507324\trec:   60.477341\tkl:   28.029982\n",
      "Epoch: 583 [40100/50000 (80%)]  \tLoss:   92.121559\trec:   63.880272\tkl:   28.241285\n",
      "====> Epoch: 583 Average train loss: 89.5805\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1604\n",
      "Epoch: 584 [  100/50000 ( 0%)]  \tLoss:   85.189224\trec:   58.815559\tkl:   26.373665\n",
      "Epoch: 584 [10100/50000 (20%)]  \tLoss:   88.728317\trec:   60.472607\tkl:   28.255714\n",
      "Epoch: 584 [20100/50000 (40%)]  \tLoss:   90.103935\trec:   63.240124\tkl:   26.863810\n",
      "Epoch: 584 [30100/50000 (60%)]  \tLoss:   92.129082\trec:   63.718212\tkl:   28.410864\n",
      "Epoch: 584 [40100/50000 (80%)]  \tLoss:   90.638672\trec:   62.316765\tkl:   28.321909\n",
      "====> Epoch: 584 Average train loss: 89.5829\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1555\n",
      "Epoch: 585 [  100/50000 ( 0%)]  \tLoss:   87.227097\trec:   60.144543\tkl:   27.082550\n",
      "Epoch: 585 [10100/50000 (20%)]  \tLoss:   86.585968\trec:   60.653236\tkl:   25.932730\n",
      "Epoch: 585 [20100/50000 (40%)]  \tLoss:   91.327179\trec:   62.526291\tkl:   28.800886\n",
      "Epoch: 585 [30100/50000 (60%)]  \tLoss:   86.886307\trec:   59.358025\tkl:   27.528280\n",
      "Epoch: 585 [40100/50000 (80%)]  \tLoss:   90.281128\trec:   62.376656\tkl:   27.904482\n",
      "====> Epoch: 585 Average train loss: 89.5767\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2077\n",
      "Epoch: 586 [  100/50000 ( 0%)]  \tLoss:   86.425323\trec:   58.157654\tkl:   28.267666\n",
      "Epoch: 586 [10100/50000 (20%)]  \tLoss:   91.408417\trec:   63.287441\tkl:   28.120981\n",
      "Epoch: 586 [20100/50000 (40%)]  \tLoss:   87.351540\trec:   59.013786\tkl:   28.337753\n",
      "Epoch: 586 [30100/50000 (60%)]  \tLoss:   91.580688\trec:   62.404072\tkl:   29.176622\n",
      "Epoch: 586 [40100/50000 (80%)]  \tLoss:   89.256737\trec:   61.147774\tkl:   28.108965\n",
      "====> Epoch: 586 Average train loss: 89.5747\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1676\n",
      "Epoch: 587 [  100/50000 ( 0%)]  \tLoss:   86.269547\trec:   58.155254\tkl:   28.114296\n",
      "Epoch: 587 [10100/50000 (20%)]  \tLoss:   91.122887\trec:   63.161446\tkl:   27.961445\n",
      "Epoch: 587 [20100/50000 (40%)]  \tLoss:   85.574196\trec:   58.957409\tkl:   26.616781\n",
      "Epoch: 587 [30100/50000 (60%)]  \tLoss:   89.509682\trec:   62.147915\tkl:   27.361769\n",
      "Epoch: 587 [40100/50000 (80%)]  \tLoss:   91.166641\trec:   62.600796\tkl:   28.565847\n",
      "====> Epoch: 587 Average train loss: 89.5796\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1139\n",
      "Epoch: 588 [  100/50000 ( 0%)]  \tLoss:   90.229225\trec:   61.552685\tkl:   28.676542\n",
      "Epoch: 588 [10100/50000 (20%)]  \tLoss:   89.997635\trec:   61.554489\tkl:   28.443150\n",
      "Epoch: 588 [20100/50000 (40%)]  \tLoss:   92.347549\trec:   63.188885\tkl:   29.158659\n",
      "Epoch: 588 [30100/50000 (60%)]  \tLoss:   84.185036\trec:   57.131718\tkl:   27.053314\n",
      "Epoch: 588 [40100/50000 (80%)]  \tLoss:   94.211372\trec:   65.172546\tkl:   29.038830\n",
      "====> Epoch: 588 Average train loss: 89.6061\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1849\n",
      "Epoch: 589 [  100/50000 ( 0%)]  \tLoss:   93.475769\trec:   65.073502\tkl:   28.402266\n",
      "Epoch: 589 [10100/50000 (20%)]  \tLoss:   91.343803\trec:   63.387154\tkl:   27.956650\n",
      "Epoch: 589 [20100/50000 (40%)]  \tLoss:   87.065819\trec:   60.193504\tkl:   26.872318\n",
      "Epoch: 589 [30100/50000 (60%)]  \tLoss:   90.748741\trec:   61.458305\tkl:   29.290436\n",
      "Epoch: 589 [40100/50000 (80%)]  \tLoss:   86.686989\trec:   59.245415\tkl:   27.441580\n",
      "====> Epoch: 589 Average train loss: 89.5558\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1632\n",
      "Epoch: 590 [  100/50000 ( 0%)]  \tLoss:   88.538452\trec:   61.360371\tkl:   27.178083\n",
      "Epoch: 590 [10100/50000 (20%)]  \tLoss:   92.969284\trec:   63.741459\tkl:   29.227829\n",
      "Epoch: 590 [20100/50000 (40%)]  \tLoss:   90.087051\trec:   62.238571\tkl:   27.848471\n",
      "Epoch: 590 [30100/50000 (60%)]  \tLoss:   90.961113\trec:   63.107693\tkl:   27.853422\n",
      "Epoch: 590 [40100/50000 (80%)]  \tLoss:   89.445717\trec:   62.412224\tkl:   27.033495\n",
      "====> Epoch: 590 Average train loss: 89.5466\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1257\n",
      "Epoch: 591 [  100/50000 ( 0%)]  \tLoss:   87.622490\trec:   60.087116\tkl:   27.535374\n",
      "Epoch: 591 [10100/50000 (20%)]  \tLoss:   88.832031\trec:   61.384422\tkl:   27.447611\n",
      "Epoch: 591 [20100/50000 (40%)]  \tLoss:   89.875595\trec:   62.853046\tkl:   27.022551\n",
      "Epoch: 591 [30100/50000 (60%)]  \tLoss:   91.724800\trec:   62.856091\tkl:   28.868708\n",
      "Epoch: 591 [40100/50000 (80%)]  \tLoss:   88.012291\trec:   61.577919\tkl:   26.434378\n",
      "====> Epoch: 591 Average train loss: 89.5545\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0797\n",
      "Epoch: 592 [  100/50000 ( 0%)]  \tLoss:   85.817307\trec:   58.076946\tkl:   27.740351\n",
      "Epoch: 592 [10100/50000 (20%)]  \tLoss:   89.499962\trec:   60.905029\tkl:   28.594927\n",
      "Epoch: 592 [20100/50000 (40%)]  \tLoss:   89.108414\trec:   61.409138\tkl:   27.699272\n",
      "Epoch: 592 [30100/50000 (60%)]  \tLoss:   90.729492\trec:   62.079384\tkl:   28.650103\n",
      "Epoch: 592 [40100/50000 (80%)]  \tLoss:   89.866188\trec:   62.764111\tkl:   27.102079\n",
      "====> Epoch: 592 Average train loss: 89.5521\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0712\n",
      "Epoch: 593 [  100/50000 ( 0%)]  \tLoss:   89.840431\trec:   61.405518\tkl:   28.434916\n",
      "Epoch: 593 [10100/50000 (20%)]  \tLoss:   90.285927\trec:   61.763191\tkl:   28.522732\n",
      "Epoch: 593 [20100/50000 (40%)]  \tLoss:   89.007812\trec:   61.164806\tkl:   27.843008\n",
      "Epoch: 593 [30100/50000 (60%)]  \tLoss:   89.954018\trec:   62.302635\tkl:   27.651381\n",
      "Epoch: 593 [40100/50000 (80%)]  \tLoss:   88.141861\trec:   60.379890\tkl:   27.761972\n",
      "====> Epoch: 593 Average train loss: 89.5523\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1203\n",
      "Epoch: 594 [  100/50000 ( 0%)]  \tLoss:   91.677750\trec:   63.105801\tkl:   28.571953\n",
      "Epoch: 594 [10100/50000 (20%)]  \tLoss:   87.171852\trec:   59.440655\tkl:   27.731203\n",
      "Epoch: 594 [20100/50000 (40%)]  \tLoss:   88.680504\trec:   60.704529\tkl:   27.975975\n",
      "Epoch: 594 [30100/50000 (60%)]  \tLoss:   93.921913\trec:   64.309616\tkl:   29.612289\n",
      "Epoch: 594 [40100/50000 (80%)]  \tLoss:   86.892792\trec:   58.988548\tkl:   27.904247\n",
      "====> Epoch: 594 Average train loss: 89.5399\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1158\n",
      "Epoch: 595 [  100/50000 ( 0%)]  \tLoss:   88.556465\trec:   60.803978\tkl:   27.752480\n",
      "Epoch: 595 [10100/50000 (20%)]  \tLoss:   89.457848\trec:   61.325672\tkl:   28.132177\n",
      "Epoch: 595 [20100/50000 (40%)]  \tLoss:   82.236916\trec:   55.858730\tkl:   26.378183\n",
      "Epoch: 595 [30100/50000 (60%)]  \tLoss:   86.929237\trec:   59.167316\tkl:   27.761923\n",
      "Epoch: 595 [40100/50000 (80%)]  \tLoss:   88.705688\trec:   61.089237\tkl:   27.616455\n",
      "====> Epoch: 595 Average train loss: 89.5370\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2182\n",
      "Epoch: 596 [  100/50000 ( 0%)]  \tLoss:   90.733376\trec:   62.373848\tkl:   28.359526\n",
      "Epoch: 596 [10100/50000 (20%)]  \tLoss:   92.556053\trec:   64.218750\tkl:   28.337301\n",
      "Epoch: 596 [20100/50000 (40%)]  \tLoss:   90.791336\trec:   62.794930\tkl:   27.996408\n",
      "Epoch: 596 [30100/50000 (60%)]  \tLoss:   86.314987\trec:   59.384304\tkl:   26.930685\n",
      "Epoch: 596 [40100/50000 (80%)]  \tLoss:   89.415443\trec:   60.792568\tkl:   28.622883\n",
      "====> Epoch: 596 Average train loss: 89.5505\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1667\n",
      "Epoch: 597 [  100/50000 ( 0%)]  \tLoss:   91.078064\trec:   62.272392\tkl:   28.805676\n",
      "Epoch: 597 [10100/50000 (20%)]  \tLoss:   86.180290\trec:   58.352184\tkl:   27.828100\n",
      "Epoch: 597 [20100/50000 (40%)]  \tLoss:   88.477272\trec:   60.377342\tkl:   28.099932\n",
      "Epoch: 597 [30100/50000 (60%)]  \tLoss:   90.208603\trec:   62.378616\tkl:   27.829985\n",
      "Epoch: 597 [40100/50000 (80%)]  \tLoss:   89.077995\trec:   61.194443\tkl:   27.883554\n",
      "====> Epoch: 597 Average train loss: 89.5285\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1984\n",
      "Epoch: 598 [  100/50000 ( 0%)]  \tLoss:   91.104630\trec:   63.137196\tkl:   27.967430\n",
      "Epoch: 598 [10100/50000 (20%)]  \tLoss:   90.874229\trec:   62.172199\tkl:   28.702023\n",
      "Epoch: 598 [20100/50000 (40%)]  \tLoss:   91.188454\trec:   63.595711\tkl:   27.592743\n",
      "Epoch: 598 [30100/50000 (60%)]  \tLoss:   89.019409\trec:   61.254978\tkl:   27.764427\n",
      "Epoch: 598 [40100/50000 (80%)]  \tLoss:   90.528336\trec:   62.624218\tkl:   27.904116\n",
      "====> Epoch: 598 Average train loss: 89.5177\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1989\n",
      "Epoch: 599 [  100/50000 ( 0%)]  \tLoss:   90.817596\trec:   61.388531\tkl:   29.429071\n",
      "Epoch: 599 [10100/50000 (20%)]  \tLoss:   86.502419\trec:   59.225933\tkl:   27.276489\n",
      "Epoch: 599 [20100/50000 (40%)]  \tLoss:   88.399414\trec:   60.096302\tkl:   28.303114\n",
      "Epoch: 599 [30100/50000 (60%)]  \tLoss:   91.887321\trec:   63.041843\tkl:   28.845480\n",
      "Epoch: 599 [40100/50000 (80%)]  \tLoss:   87.754852\trec:   60.463280\tkl:   27.291573\n",
      "====> Epoch: 599 Average train loss: 89.5083\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1248\n",
      "Epoch: 600 [  100/50000 ( 0%)]  \tLoss:   88.108963\trec:   60.298088\tkl:   27.810871\n",
      "Epoch: 600 [10100/50000 (20%)]  \tLoss:   94.229179\trec:   66.116814\tkl:   28.112360\n",
      "Epoch: 600 [20100/50000 (40%)]  \tLoss:   92.925369\trec:   64.137840\tkl:   28.787533\n",
      "Epoch: 600 [30100/50000 (60%)]  \tLoss:   89.594688\trec:   61.961151\tkl:   27.633539\n",
      "Epoch: 600 [40100/50000 (80%)]  \tLoss:   83.985367\trec:   57.714981\tkl:   26.270386\n",
      "====> Epoch: 600 Average train loss: 89.5274\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1136\n",
      "Epoch: 601 [  100/50000 ( 0%)]  \tLoss:   89.300880\trec:   61.448994\tkl:   27.851885\n",
      "Epoch: 601 [10100/50000 (20%)]  \tLoss:   87.133438\trec:   59.448250\tkl:   27.685188\n",
      "Epoch: 601 [20100/50000 (40%)]  \tLoss:   87.572380\trec:   60.799030\tkl:   26.773352\n",
      "Epoch: 601 [30100/50000 (60%)]  \tLoss:   89.337555\trec:   60.720165\tkl:   28.617392\n",
      "Epoch: 601 [40100/50000 (80%)]  \tLoss:   90.460899\trec:   62.636974\tkl:   27.823923\n",
      "====> Epoch: 601 Average train loss: 89.5002\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0907\n",
      "Epoch: 602 [  100/50000 ( 0%)]  \tLoss:   88.989464\trec:   61.225597\tkl:   27.763863\n",
      "Epoch: 602 [10100/50000 (20%)]  \tLoss:   93.577812\trec:   64.417618\tkl:   29.160191\n",
      "Epoch: 602 [20100/50000 (40%)]  \tLoss:   92.282524\trec:   63.156376\tkl:   29.126154\n",
      "Epoch: 602 [30100/50000 (60%)]  \tLoss:   91.234688\trec:   62.675995\tkl:   28.558691\n",
      "Epoch: 602 [40100/50000 (80%)]  \tLoss:   89.835838\trec:   61.781330\tkl:   28.054504\n",
      "====> Epoch: 602 Average train loss: 89.5188\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1263\n",
      "Epoch: 603 [  100/50000 ( 0%)]  \tLoss:   87.830475\trec:   59.956856\tkl:   27.873623\n",
      "Epoch: 603 [10100/50000 (20%)]  \tLoss:   90.737358\trec:   62.528374\tkl:   28.208992\n",
      "Epoch: 603 [20100/50000 (40%)]  \tLoss:   90.737808\trec:   62.063103\tkl:   28.674706\n",
      "Epoch: 603 [30100/50000 (60%)]  \tLoss:   87.416412\trec:   59.686508\tkl:   27.729906\n",
      "Epoch: 603 [40100/50000 (80%)]  \tLoss:   93.319557\trec:   63.159824\tkl:   30.159739\n",
      "====> Epoch: 603 Average train loss: 89.4971\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1248\n",
      "Epoch: 604 [  100/50000 ( 0%)]  \tLoss:   88.567284\trec:   60.858154\tkl:   27.709126\n",
      "Epoch: 604 [10100/50000 (20%)]  \tLoss:   90.523308\trec:   62.220974\tkl:   28.302336\n",
      "Epoch: 604 [20100/50000 (40%)]  \tLoss:   92.071190\trec:   63.510471\tkl:   28.560720\n",
      "Epoch: 604 [30100/50000 (60%)]  \tLoss:   92.342010\trec:   62.661903\tkl:   29.680107\n",
      "Epoch: 604 [40100/50000 (80%)]  \tLoss:   91.345444\trec:   63.614223\tkl:   27.731222\n",
      "====> Epoch: 604 Average train loss: 89.5009\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1900\n",
      "Epoch: 605 [  100/50000 ( 0%)]  \tLoss:   89.730865\trec:   62.683865\tkl:   27.047001\n",
      "Epoch: 605 [10100/50000 (20%)]  \tLoss:   91.168144\trec:   63.839886\tkl:   27.328255\n",
      "Epoch: 605 [20100/50000 (40%)]  \tLoss:   89.283607\trec:   61.325813\tkl:   27.957800\n",
      "Epoch: 605 [30100/50000 (60%)]  \tLoss:   89.270309\trec:   61.083847\tkl:   28.186470\n",
      "Epoch: 605 [40100/50000 (80%)]  \tLoss:   93.702652\trec:   63.650375\tkl:   30.052284\n",
      "====> Epoch: 605 Average train loss: 89.5077\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1131\n",
      "Epoch: 606 [  100/50000 ( 0%)]  \tLoss:   92.376114\trec:   63.261009\tkl:   29.115103\n",
      "Epoch: 606 [10100/50000 (20%)]  \tLoss:   86.651474\trec:   59.702812\tkl:   26.948662\n",
      "Epoch: 606 [20100/50000 (40%)]  \tLoss:   91.630547\trec:   63.278515\tkl:   28.352030\n",
      "Epoch: 606 [30100/50000 (60%)]  \tLoss:   92.007515\trec:   63.337666\tkl:   28.669847\n",
      "Epoch: 606 [40100/50000 (80%)]  \tLoss:   90.076042\trec:   61.790497\tkl:   28.285545\n",
      "====> Epoch: 606 Average train loss: 89.4975\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2148\n",
      "Epoch: 607 [  100/50000 ( 0%)]  \tLoss:   91.043015\trec:   63.191643\tkl:   27.851374\n",
      "Epoch: 607 [10100/50000 (20%)]  \tLoss:   88.976547\trec:   61.472683\tkl:   27.503864\n",
      "Epoch: 607 [20100/50000 (40%)]  \tLoss:   88.958336\trec:   60.871639\tkl:   28.086699\n",
      "Epoch: 607 [30100/50000 (60%)]  \tLoss:   90.138878\trec:   61.485352\tkl:   28.653524\n",
      "Epoch: 607 [40100/50000 (80%)]  \tLoss:   85.987877\trec:   59.035164\tkl:   26.952711\n",
      "====> Epoch: 607 Average train loss: 89.4970\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0466\n",
      "Epoch: 608 [  100/50000 ( 0%)]  \tLoss:   87.233658\trec:   59.874580\tkl:   27.359081\n",
      "Epoch: 608 [10100/50000 (20%)]  \tLoss:   89.233032\trec:   61.194702\tkl:   28.038334\n",
      "Epoch: 608 [20100/50000 (40%)]  \tLoss:   89.859665\trec:   61.682636\tkl:   28.177036\n",
      "Epoch: 608 [30100/50000 (60%)]  \tLoss:   89.324371\trec:   62.063068\tkl:   27.261301\n",
      "Epoch: 608 [40100/50000 (80%)]  \tLoss:   90.399292\trec:   62.019638\tkl:   28.379662\n",
      "====> Epoch: 608 Average train loss: 89.5054\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1496\n",
      "Epoch: 609 [  100/50000 ( 0%)]  \tLoss:   87.085670\trec:   59.983963\tkl:   27.101711\n",
      "Epoch: 609 [10100/50000 (20%)]  \tLoss:   90.448395\trec:   61.839149\tkl:   28.609253\n",
      "Epoch: 609 [20100/50000 (40%)]  \tLoss:   86.335808\trec:   58.562626\tkl:   27.773184\n",
      "Epoch: 609 [30100/50000 (60%)]  \tLoss:   91.569275\trec:   63.214188\tkl:   28.355082\n",
      "Epoch: 609 [40100/50000 (80%)]  \tLoss:   89.820595\trec:   61.934944\tkl:   27.885647\n",
      "====> Epoch: 609 Average train loss: 89.4787\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1218\n",
      "Epoch: 610 [  100/50000 ( 0%)]  \tLoss:   88.007263\trec:   61.438641\tkl:   26.568621\n",
      "Epoch: 610 [10100/50000 (20%)]  \tLoss:   90.401688\trec:   61.560703\tkl:   28.840986\n",
      "Epoch: 610 [20100/50000 (40%)]  \tLoss:   89.247917\trec:   61.827137\tkl:   27.420780\n",
      "Epoch: 610 [30100/50000 (60%)]  \tLoss:   89.347427\trec:   61.488781\tkl:   27.858646\n",
      "Epoch: 610 [40100/50000 (80%)]  \tLoss:   93.044411\trec:   63.987530\tkl:   29.056885\n",
      "====> Epoch: 610 Average train loss: 89.4856\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0236\n",
      "Epoch: 611 [  100/50000 ( 0%)]  \tLoss:   88.145134\trec:   60.838974\tkl:   27.306156\n",
      "Epoch: 611 [10100/50000 (20%)]  \tLoss:   90.404350\trec:   62.368870\tkl:   28.035482\n",
      "Epoch: 611 [20100/50000 (40%)]  \tLoss:   92.185997\trec:   62.774628\tkl:   29.411364\n",
      "Epoch: 611 [30100/50000 (60%)]  \tLoss:   88.907127\trec:   61.392597\tkl:   27.514530\n",
      "Epoch: 611 [40100/50000 (80%)]  \tLoss:   92.741577\trec:   64.164345\tkl:   28.577238\n",
      "====> Epoch: 611 Average train loss: 89.5002\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1271\n",
      "Epoch: 612 [  100/50000 ( 0%)]  \tLoss:   91.255623\trec:   63.470375\tkl:   27.785248\n",
      "Epoch: 612 [10100/50000 (20%)]  \tLoss:   91.481560\trec:   63.012424\tkl:   28.469139\n",
      "Epoch: 612 [20100/50000 (40%)]  \tLoss:   88.777596\trec:   61.603172\tkl:   27.174429\n",
      "Epoch: 612 [30100/50000 (60%)]  \tLoss:   87.443748\trec:   59.266949\tkl:   28.176804\n",
      "Epoch: 612 [40100/50000 (80%)]  \tLoss:   87.906593\trec:   59.785603\tkl:   28.120985\n",
      "====> Epoch: 612 Average train loss: 89.4765\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2024\n",
      "Epoch: 613 [  100/50000 ( 0%)]  \tLoss:   91.340416\trec:   62.959705\tkl:   28.380711\n",
      "Epoch: 613 [10100/50000 (20%)]  \tLoss:   88.459122\trec:   60.365810\tkl:   28.093309\n",
      "Epoch: 613 [20100/50000 (40%)]  \tLoss:   87.480995\trec:   59.966755\tkl:   27.514235\n",
      "Epoch: 613 [30100/50000 (60%)]  \tLoss:   92.327019\trec:   63.801670\tkl:   28.525351\n",
      "Epoch: 613 [40100/50000 (80%)]  \tLoss:   86.677597\trec:   58.744377\tkl:   27.933218\n",
      "====> Epoch: 613 Average train loss: 89.4931\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0855\n",
      "Epoch: 614 [  100/50000 ( 0%)]  \tLoss:   85.518867\trec:   58.294201\tkl:   27.224661\n",
      "Epoch: 614 [10100/50000 (20%)]  \tLoss:   85.768692\trec:   59.201538\tkl:   26.567148\n",
      "Epoch: 614 [20100/50000 (40%)]  \tLoss:   90.959900\trec:   62.940281\tkl:   28.019623\n",
      "Epoch: 614 [30100/50000 (60%)]  \tLoss:   91.212944\trec:   62.890469\tkl:   28.322479\n",
      "Epoch: 614 [40100/50000 (80%)]  \tLoss:   90.214211\trec:   63.341225\tkl:   26.872986\n",
      "====> Epoch: 614 Average train loss: 89.4565\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1382\n",
      "Epoch: 615 [  100/50000 ( 0%)]  \tLoss:   83.552048\trec:   57.949509\tkl:   25.602543\n",
      "Epoch: 615 [10100/50000 (20%)]  \tLoss:   86.526543\trec:   58.948860\tkl:   27.577681\n",
      "Epoch: 615 [20100/50000 (40%)]  \tLoss:   91.392235\trec:   62.540550\tkl:   28.851685\n",
      "Epoch: 615 [30100/50000 (60%)]  \tLoss:   89.282539\trec:   61.329399\tkl:   27.953144\n",
      "Epoch: 615 [40100/50000 (80%)]  \tLoss:   89.899048\trec:   62.219646\tkl:   27.679401\n",
      "====> Epoch: 615 Average train loss: 89.4721\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1669\n",
      "Epoch: 616 [  100/50000 ( 0%)]  \tLoss:   88.593689\trec:   60.576450\tkl:   28.017246\n",
      "Epoch: 616 [10100/50000 (20%)]  \tLoss:   89.110947\trec:   61.474285\tkl:   27.636658\n",
      "Epoch: 616 [20100/50000 (40%)]  \tLoss:   90.120987\trec:   62.065960\tkl:   28.055021\n",
      "Epoch: 616 [30100/50000 (60%)]  \tLoss:   88.645729\trec:   60.870136\tkl:   27.775593\n",
      "Epoch: 616 [40100/50000 (80%)]  \tLoss:   92.753143\trec:   63.479160\tkl:   29.273987\n",
      "====> Epoch: 616 Average train loss: 89.4498\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0764\n",
      "Epoch: 617 [  100/50000 ( 0%)]  \tLoss:   88.438217\trec:   60.293427\tkl:   28.144796\n",
      "Epoch: 617 [10100/50000 (20%)]  \tLoss:   91.895996\trec:   63.016735\tkl:   28.879263\n",
      "Epoch: 617 [20100/50000 (40%)]  \tLoss:   91.620079\trec:   63.112782\tkl:   28.507299\n",
      "Epoch: 617 [30100/50000 (60%)]  \tLoss:   92.168358\trec:   64.037628\tkl:   28.130737\n",
      "Epoch: 617 [40100/50000 (80%)]  \tLoss:   90.272293\trec:   62.585335\tkl:   27.686960\n",
      "====> Epoch: 617 Average train loss: 89.4558\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0979\n",
      "Epoch: 618 [  100/50000 ( 0%)]  \tLoss:   87.003273\trec:   59.103603\tkl:   27.899668\n",
      "Epoch: 618 [10100/50000 (20%)]  \tLoss:   89.939423\trec:   62.355946\tkl:   27.583473\n",
      "Epoch: 618 [20100/50000 (40%)]  \tLoss:   86.376953\trec:   58.136311\tkl:   28.240639\n",
      "Epoch: 618 [30100/50000 (60%)]  \tLoss:   93.247208\trec:   64.257164\tkl:   28.990044\n",
      "Epoch: 618 [40100/50000 (80%)]  \tLoss:   88.728302\trec:   60.498913\tkl:   28.229389\n",
      "====> Epoch: 618 Average train loss: 89.4631\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0936\n",
      "Epoch: 619 [  100/50000 ( 0%)]  \tLoss:   90.910469\trec:   62.752178\tkl:   28.158285\n",
      "Epoch: 619 [10100/50000 (20%)]  \tLoss:   89.830391\trec:   61.869617\tkl:   27.960772\n",
      "Epoch: 619 [20100/50000 (40%)]  \tLoss:   84.272438\trec:   57.613544\tkl:   26.658894\n",
      "Epoch: 619 [30100/50000 (60%)]  \tLoss:   88.685089\trec:   61.457035\tkl:   27.228050\n",
      "Epoch: 619 [40100/50000 (80%)]  \tLoss:   87.862267\trec:   60.600498\tkl:   27.261763\n",
      "====> Epoch: 619 Average train loss: 89.4463\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1519\n",
      "Epoch: 620 [  100/50000 ( 0%)]  \tLoss:   88.519333\trec:   60.755867\tkl:   27.763468\n",
      "Epoch: 620 [10100/50000 (20%)]  \tLoss:   86.582634\trec:   59.044106\tkl:   27.538525\n",
      "Epoch: 620 [20100/50000 (40%)]  \tLoss:   91.176529\trec:   62.292385\tkl:   28.884148\n",
      "Epoch: 620 [30100/50000 (60%)]  \tLoss:   91.072342\trec:   62.659649\tkl:   28.412689\n",
      "Epoch: 620 [40100/50000 (80%)]  \tLoss:   88.794800\trec:   60.549229\tkl:   28.245575\n",
      "====> Epoch: 620 Average train loss: 89.4469\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0112\n",
      "Epoch: 621 [  100/50000 ( 0%)]  \tLoss:   92.172539\trec:   63.613968\tkl:   28.558567\n",
      "Epoch: 621 [10100/50000 (20%)]  \tLoss:   89.889091\trec:   62.068584\tkl:   27.820507\n",
      "Epoch: 621 [20100/50000 (40%)]  \tLoss:   86.545181\trec:   58.971924\tkl:   27.573261\n",
      "Epoch: 621 [30100/50000 (60%)]  \tLoss:   89.960587\trec:   61.455975\tkl:   28.504608\n",
      "Epoch: 621 [40100/50000 (80%)]  \tLoss:   92.586998\trec:   64.175133\tkl:   28.411873\n",
      "====> Epoch: 621 Average train loss: 89.4218\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1455\n",
      "Epoch: 622 [  100/50000 ( 0%)]  \tLoss:   91.519127\trec:   62.157459\tkl:   29.361671\n",
      "Epoch: 622 [10100/50000 (20%)]  \tLoss:   89.927673\trec:   61.612122\tkl:   28.315556\n",
      "Epoch: 622 [20100/50000 (40%)]  \tLoss:   89.945328\trec:   62.227318\tkl:   27.718018\n",
      "Epoch: 622 [30100/50000 (60%)]  \tLoss:   88.714859\trec:   61.210850\tkl:   27.504015\n",
      "Epoch: 622 [40100/50000 (80%)]  \tLoss:   92.501152\trec:   64.227516\tkl:   28.273642\n",
      "====> Epoch: 622 Average train loss: 89.4354\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1307\n",
      "Epoch: 623 [  100/50000 ( 0%)]  \tLoss:   88.446884\trec:   61.289970\tkl:   27.156916\n",
      "Epoch: 623 [10100/50000 (20%)]  \tLoss:   91.072411\trec:   62.007420\tkl:   29.064989\n",
      "Epoch: 623 [20100/50000 (40%)]  \tLoss:   86.920776\trec:   59.332771\tkl:   27.588003\n",
      "Epoch: 623 [30100/50000 (60%)]  \tLoss:   87.262947\trec:   60.376560\tkl:   26.886389\n",
      "Epoch: 623 [40100/50000 (80%)]  \tLoss:   91.304131\trec:   64.030823\tkl:   27.273306\n",
      "====> Epoch: 623 Average train loss: 89.4472\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.2534\n",
      "Epoch: 624 [  100/50000 ( 0%)]  \tLoss:   88.931061\trec:   61.086365\tkl:   27.844694\n",
      "Epoch: 624 [10100/50000 (20%)]  \tLoss:   91.597267\trec:   63.388573\tkl:   28.208691\n",
      "Epoch: 624 [20100/50000 (40%)]  \tLoss:   87.741730\trec:   59.365086\tkl:   28.376642\n",
      "Epoch: 624 [30100/50000 (60%)]  \tLoss:   90.247841\trec:   61.361362\tkl:   28.886478\n",
      "Epoch: 624 [40100/50000 (80%)]  \tLoss:   88.264763\trec:   61.237503\tkl:   27.027260\n",
      "====> Epoch: 624 Average train loss: 89.4325\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0931\n",
      "Epoch: 625 [  100/50000 ( 0%)]  \tLoss:   91.685379\trec:   63.191483\tkl:   28.493896\n",
      "Epoch: 625 [10100/50000 (20%)]  \tLoss:   90.662430\trec:   63.558685\tkl:   27.103743\n",
      "Epoch: 625 [20100/50000 (40%)]  \tLoss:   91.436295\trec:   63.075169\tkl:   28.361128\n",
      "Epoch: 625 [30100/50000 (60%)]  \tLoss:   89.949898\trec:   61.758503\tkl:   28.191397\n",
      "Epoch: 625 [40100/50000 (80%)]  \tLoss:   90.750031\trec:   62.053570\tkl:   28.696459\n",
      "====> Epoch: 625 Average train loss: 89.4089\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0684\n",
      "Epoch: 626 [  100/50000 ( 0%)]  \tLoss:   88.174911\trec:   60.449558\tkl:   27.725351\n",
      "Epoch: 626 [10100/50000 (20%)]  \tLoss:   86.565826\trec:   58.864433\tkl:   27.701395\n",
      "Epoch: 626 [20100/50000 (40%)]  \tLoss:   88.359474\trec:   60.051315\tkl:   28.308151\n",
      "Epoch: 626 [30100/50000 (60%)]  \tLoss:   91.823425\trec:   64.212051\tkl:   27.611380\n",
      "Epoch: 626 [40100/50000 (80%)]  \tLoss:   87.113159\trec:   59.974911\tkl:   27.138248\n",
      "====> Epoch: 626 Average train loss: 89.4036\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1308\n",
      "Epoch: 627 [  100/50000 ( 0%)]  \tLoss:   90.953529\trec:   62.934406\tkl:   28.019127\n",
      "Epoch: 627 [10100/50000 (20%)]  \tLoss:   88.253502\trec:   60.003456\tkl:   28.250048\n",
      "Epoch: 627 [20100/50000 (40%)]  \tLoss:   87.799034\trec:   59.982635\tkl:   27.816393\n",
      "Epoch: 627 [30100/50000 (60%)]  \tLoss:   85.866989\trec:   58.208065\tkl:   27.658920\n",
      "Epoch: 627 [40100/50000 (80%)]  \tLoss:   89.521713\trec:   61.305088\tkl:   28.216631\n",
      "====> Epoch: 627 Average train loss: 89.4267\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0969\n",
      "Epoch: 628 [  100/50000 ( 0%)]  \tLoss:   88.110771\trec:   60.573574\tkl:   27.537197\n",
      "Epoch: 628 [10100/50000 (20%)]  \tLoss:   87.617264\trec:   59.775940\tkl:   27.841318\n",
      "Epoch: 628 [20100/50000 (40%)]  \tLoss:   89.652534\trec:   61.062630\tkl:   28.589901\n",
      "Epoch: 628 [30100/50000 (60%)]  \tLoss:   91.312187\trec:   62.816784\tkl:   28.495399\n",
      "Epoch: 628 [40100/50000 (80%)]  \tLoss:   91.518318\trec:   63.077393\tkl:   28.440928\n",
      "====> Epoch: 628 Average train loss: 89.4219\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0560\n",
      "Epoch: 629 [  100/50000 ( 0%)]  \tLoss:   90.280792\trec:   61.231541\tkl:   29.049246\n",
      "Epoch: 629 [10100/50000 (20%)]  \tLoss:   86.839493\trec:   59.627434\tkl:   27.212059\n",
      "Epoch: 629 [20100/50000 (40%)]  \tLoss:   90.954422\trec:   62.618275\tkl:   28.336147\n",
      "Epoch: 629 [30100/50000 (60%)]  \tLoss:   94.871109\trec:   65.697159\tkl:   29.173944\n",
      "Epoch: 629 [40100/50000 (80%)]  \tLoss:   86.416924\trec:   59.177856\tkl:   27.239065\n",
      "====> Epoch: 629 Average train loss: 89.4317\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0970\n",
      "Epoch: 630 [  100/50000 ( 0%)]  \tLoss:   83.684937\trec:   57.268017\tkl:   26.416922\n",
      "Epoch: 630 [10100/50000 (20%)]  \tLoss:   89.519234\trec:   61.142334\tkl:   28.376905\n",
      "Epoch: 630 [20100/50000 (40%)]  \tLoss:   90.763496\trec:   62.500744\tkl:   28.262749\n",
      "Epoch: 630 [30100/50000 (60%)]  \tLoss:   85.278671\trec:   58.614536\tkl:   26.664133\n",
      "Epoch: 630 [40100/50000 (80%)]  \tLoss:   93.767540\trec:   65.163864\tkl:   28.603676\n",
      "====> Epoch: 630 Average train loss: 89.4087\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0672\n",
      "Epoch: 631 [  100/50000 ( 0%)]  \tLoss:   89.927330\trec:   61.427536\tkl:   28.499796\n",
      "Epoch: 631 [10100/50000 (20%)]  \tLoss:   88.425346\trec:   60.443516\tkl:   27.981840\n",
      "Epoch: 631 [20100/50000 (40%)]  \tLoss:   91.128853\trec:   62.193439\tkl:   28.935415\n",
      "Epoch: 631 [30100/50000 (60%)]  \tLoss:   91.856796\trec:   62.649395\tkl:   29.207407\n",
      "Epoch: 631 [40100/50000 (80%)]  \tLoss:   91.873474\trec:   62.904724\tkl:   28.968746\n",
      "====> Epoch: 631 Average train loss: 89.4003\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0070\n",
      "Epoch: 632 [  100/50000 ( 0%)]  \tLoss:   86.330048\trec:   58.615955\tkl:   27.714088\n",
      "Epoch: 632 [10100/50000 (20%)]  \tLoss:   88.533501\trec:   60.454578\tkl:   28.078922\n",
      "Epoch: 632 [20100/50000 (40%)]  \tLoss:   89.371071\trec:   62.070419\tkl:   27.300659\n",
      "Epoch: 632 [30100/50000 (60%)]  \tLoss:   91.866592\trec:   63.570610\tkl:   28.295980\n",
      "Epoch: 632 [40100/50000 (80%)]  \tLoss:   90.482056\trec:   61.881062\tkl:   28.600998\n",
      "====> Epoch: 632 Average train loss: 89.4029\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0655\n",
      "Epoch: 633 [  100/50000 ( 0%)]  \tLoss:   86.294136\trec:   58.820179\tkl:   27.473961\n",
      "Epoch: 633 [10100/50000 (20%)]  \tLoss:   86.459991\trec:   58.753063\tkl:   27.706926\n",
      "Epoch: 633 [20100/50000 (40%)]  \tLoss:   89.919289\trec:   61.170204\tkl:   28.749081\n",
      "Epoch: 633 [30100/50000 (60%)]  \tLoss:   90.321915\trec:   62.268417\tkl:   28.053490\n",
      "Epoch: 633 [40100/50000 (80%)]  \tLoss:   87.983185\trec:   60.381496\tkl:   27.601679\n",
      "====> Epoch: 633 Average train loss: 89.3973\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0730\n",
      "Epoch: 634 [  100/50000 ( 0%)]  \tLoss:   91.041931\trec:   63.178963\tkl:   27.862972\n",
      "Epoch: 634 [10100/50000 (20%)]  \tLoss:   88.441597\trec:   60.955124\tkl:   27.486471\n",
      "Epoch: 634 [20100/50000 (40%)]  \tLoss:   90.833458\trec:   62.829746\tkl:   28.003714\n",
      "Epoch: 634 [30100/50000 (60%)]  \tLoss:   88.507370\trec:   60.965775\tkl:   27.541599\n",
      "Epoch: 634 [40100/50000 (80%)]  \tLoss:   87.437401\trec:   59.878071\tkl:   27.559334\n",
      "====> Epoch: 634 Average train loss: 89.3902\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0517\n",
      "Epoch: 635 [  100/50000 ( 0%)]  \tLoss:   89.155396\trec:   61.601677\tkl:   27.553720\n",
      "Epoch: 635 [10100/50000 (20%)]  \tLoss:   84.802498\trec:   57.992802\tkl:   26.809692\n",
      "Epoch: 635 [20100/50000 (40%)]  \tLoss:   88.821007\trec:   61.386929\tkl:   27.434078\n",
      "Epoch: 635 [30100/50000 (60%)]  \tLoss:   88.207863\trec:   60.624744\tkl:   27.583115\n",
      "Epoch: 635 [40100/50000 (80%)]  \tLoss:   88.273048\trec:   60.213978\tkl:   28.059067\n",
      "====> Epoch: 635 Average train loss: 89.4019\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0967\n",
      "Epoch: 636 [  100/50000 ( 0%)]  \tLoss:   88.409294\trec:   60.905918\tkl:   27.503384\n",
      "Epoch: 636 [10100/50000 (20%)]  \tLoss:   89.963768\trec:   62.100033\tkl:   27.863735\n",
      "Epoch: 636 [20100/50000 (40%)]  \tLoss:   92.567413\trec:   63.282024\tkl:   29.285383\n",
      "Epoch: 636 [30100/50000 (60%)]  \tLoss:   94.181778\trec:   65.447609\tkl:   28.734169\n",
      "Epoch: 636 [40100/50000 (80%)]  \tLoss:   86.960556\trec:   59.026352\tkl:   27.934204\n",
      "====> Epoch: 636 Average train loss: 89.3806\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9854\n",
      "Epoch: 637 [  100/50000 ( 0%)]  \tLoss:   87.363876\trec:   59.780308\tkl:   27.583569\n",
      "Epoch: 637 [10100/50000 (20%)]  \tLoss:   87.611832\trec:   59.571609\tkl:   28.040224\n",
      "Epoch: 637 [20100/50000 (40%)]  \tLoss:   87.793320\trec:   60.070217\tkl:   27.723095\n",
      "Epoch: 637 [30100/50000 (60%)]  \tLoss:   89.272461\trec:   61.016171\tkl:   28.256294\n",
      "Epoch: 637 [40100/50000 (80%)]  \tLoss:   91.626228\trec:   62.756538\tkl:   28.869692\n",
      "====> Epoch: 637 Average train loss: 89.3786\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0083\n",
      "Epoch: 638 [  100/50000 ( 0%)]  \tLoss:   89.745743\trec:   61.254276\tkl:   28.491467\n",
      "Epoch: 638 [10100/50000 (20%)]  \tLoss:   86.988388\trec:   59.603191\tkl:   27.385193\n",
      "Epoch: 638 [20100/50000 (40%)]  \tLoss:   88.951347\trec:   61.456882\tkl:   27.494465\n",
      "Epoch: 638 [30100/50000 (60%)]  \tLoss:   89.504066\trec:   61.131577\tkl:   28.372492\n",
      "Epoch: 638 [40100/50000 (80%)]  \tLoss:   89.040779\trec:   61.783539\tkl:   27.257236\n",
      "====> Epoch: 638 Average train loss: 89.3796\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0646\n",
      "Epoch: 639 [  100/50000 ( 0%)]  \tLoss:   91.218132\trec:   62.112148\tkl:   29.105988\n",
      "Epoch: 639 [10100/50000 (20%)]  \tLoss:   90.125801\trec:   62.107094\tkl:   28.018705\n",
      "Epoch: 639 [20100/50000 (40%)]  \tLoss:   88.255806\trec:   60.127586\tkl:   28.128222\n",
      "Epoch: 639 [30100/50000 (60%)]  \tLoss:   88.397072\trec:   61.617752\tkl:   26.779314\n",
      "Epoch: 639 [40100/50000 (80%)]  \tLoss:   90.112221\trec:   61.287830\tkl:   28.824394\n",
      "====> Epoch: 639 Average train loss: 89.3700\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0621\n",
      "Epoch: 640 [  100/50000 ( 0%)]  \tLoss:   90.573692\trec:   62.542034\tkl:   28.031651\n",
      "Epoch: 640 [10100/50000 (20%)]  \tLoss:   89.790993\trec:   61.479919\tkl:   28.311069\n",
      "Epoch: 640 [20100/50000 (40%)]  \tLoss:   87.719284\trec:   59.635143\tkl:   28.084141\n",
      "Epoch: 640 [30100/50000 (60%)]  \tLoss:   90.569618\trec:   62.188622\tkl:   28.380993\n",
      "Epoch: 640 [40100/50000 (80%)]  \tLoss:   88.345932\trec:   60.550682\tkl:   27.795258\n",
      "====> Epoch: 640 Average train loss: 89.3543\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0165\n",
      "Epoch: 641 [  100/50000 ( 0%)]  \tLoss:   87.163124\trec:   60.037224\tkl:   27.125898\n",
      "Epoch: 641 [10100/50000 (20%)]  \tLoss:   89.927429\trec:   61.843987\tkl:   28.083443\n",
      "Epoch: 641 [20100/50000 (40%)]  \tLoss:   88.479324\trec:   60.637276\tkl:   27.842051\n",
      "Epoch: 641 [30100/50000 (60%)]  \tLoss:   85.460464\trec:   58.193623\tkl:   27.266840\n",
      "Epoch: 641 [40100/50000 (80%)]  \tLoss:   89.324059\trec:   60.775799\tkl:   28.548256\n",
      "====> Epoch: 641 Average train loss: 89.3758\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0187\n",
      "Epoch: 642 [  100/50000 ( 0%)]  \tLoss:   91.343781\trec:   63.145042\tkl:   28.198732\n",
      "Epoch: 642 [10100/50000 (20%)]  \tLoss:   87.737091\trec:   60.891838\tkl:   26.845243\n",
      "Epoch: 642 [20100/50000 (40%)]  \tLoss:   94.198631\trec:   65.552330\tkl:   28.646297\n",
      "Epoch: 642 [30100/50000 (60%)]  \tLoss:   84.583389\trec:   57.624119\tkl:   26.959265\n",
      "Epoch: 642 [40100/50000 (80%)]  \tLoss:   91.224510\trec:   63.521797\tkl:   27.702719\n",
      "====> Epoch: 642 Average train loss: 89.3893\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0921\n",
      "Epoch: 643 [  100/50000 ( 0%)]  \tLoss:   90.144020\trec:   61.668335\tkl:   28.475687\n",
      "Epoch: 643 [10100/50000 (20%)]  \tLoss:   87.552048\trec:   60.690037\tkl:   26.862007\n",
      "Epoch: 643 [20100/50000 (40%)]  \tLoss:   86.005875\trec:   58.961220\tkl:   27.044662\n",
      "Epoch: 643 [30100/50000 (60%)]  \tLoss:   91.104103\trec:   62.667194\tkl:   28.436899\n",
      "Epoch: 643 [40100/50000 (80%)]  \tLoss:   91.322945\trec:   63.353386\tkl:   27.969563\n",
      "====> Epoch: 643 Average train loss: 89.3585\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0703\n",
      "Epoch: 644 [  100/50000 ( 0%)]  \tLoss:   89.661095\trec:   61.506176\tkl:   28.154919\n",
      "Epoch: 644 [10100/50000 (20%)]  \tLoss:   90.326660\trec:   62.055603\tkl:   28.271053\n",
      "Epoch: 644 [20100/50000 (40%)]  \tLoss:   86.354607\trec:   60.276630\tkl:   26.077976\n",
      "Epoch: 644 [30100/50000 (60%)]  \tLoss:   90.218849\trec:   61.684658\tkl:   28.534191\n",
      "Epoch: 644 [40100/50000 (80%)]  \tLoss:   92.384926\trec:   63.753929\tkl:   28.630999\n",
      "====> Epoch: 644 Average train loss: 89.3789\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1572\n",
      "Epoch: 645 [  100/50000 ( 0%)]  \tLoss:   89.281410\trec:   61.132450\tkl:   28.148966\n",
      "Epoch: 645 [10100/50000 (20%)]  \tLoss:   93.762466\trec:   64.602890\tkl:   29.159576\n",
      "Epoch: 645 [20100/50000 (40%)]  \tLoss:   89.187607\trec:   61.317162\tkl:   27.870445\n",
      "Epoch: 645 [30100/50000 (60%)]  \tLoss:   88.471275\trec:   60.370247\tkl:   28.101027\n",
      "Epoch: 645 [40100/50000 (80%)]  \tLoss:   90.697578\trec:   62.936054\tkl:   27.761522\n",
      "====> Epoch: 645 Average train loss: 89.3465\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9481\n",
      "Epoch: 646 [  100/50000 ( 0%)]  \tLoss:   89.832085\trec:   61.300694\tkl:   28.531401\n",
      "Epoch: 646 [10100/50000 (20%)]  \tLoss:   84.038498\trec:   56.351200\tkl:   27.687300\n",
      "Epoch: 646 [20100/50000 (40%)]  \tLoss:   87.095192\trec:   60.757114\tkl:   26.338076\n",
      "Epoch: 646 [30100/50000 (60%)]  \tLoss:   89.371445\trec:   61.396732\tkl:   27.974707\n",
      "Epoch: 646 [40100/50000 (80%)]  \tLoss:   88.827927\trec:   61.716030\tkl:   27.111904\n",
      "====> Epoch: 646 Average train loss: 89.3646\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1275\n",
      "Epoch: 647 [  100/50000 ( 0%)]  \tLoss:   86.774048\trec:   58.999058\tkl:   27.774994\n",
      "Epoch: 647 [10100/50000 (20%)]  \tLoss:   89.660057\trec:   61.307869\tkl:   28.352190\n",
      "Epoch: 647 [20100/50000 (40%)]  \tLoss:   89.702850\trec:   61.575722\tkl:   28.127123\n",
      "Epoch: 647 [30100/50000 (60%)]  \tLoss:   91.880699\trec:   63.201183\tkl:   28.679522\n",
      "Epoch: 647 [40100/50000 (80%)]  \tLoss:   86.930328\trec:   60.190086\tkl:   26.740244\n",
      "====> Epoch: 647 Average train loss: 89.3419\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1068\n",
      "Epoch: 648 [  100/50000 ( 0%)]  \tLoss:   88.919800\trec:   60.706512\tkl:   28.213291\n",
      "Epoch: 648 [10100/50000 (20%)]  \tLoss:   83.430946\trec:   57.024307\tkl:   26.406639\n",
      "Epoch: 648 [20100/50000 (40%)]  \tLoss:   82.435806\trec:   55.459030\tkl:   26.976776\n",
      "Epoch: 648 [30100/50000 (60%)]  \tLoss:   96.184082\trec:   67.196060\tkl:   28.988028\n",
      "Epoch: 648 [40100/50000 (80%)]  \tLoss:   91.155945\trec:   63.168915\tkl:   27.987034\n",
      "====> Epoch: 648 Average train loss: 89.3392\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0378\n",
      "Epoch: 649 [  100/50000 ( 0%)]  \tLoss:   87.779228\trec:   59.653847\tkl:   28.125380\n",
      "Epoch: 649 [10100/50000 (20%)]  \tLoss:   91.787544\trec:   63.839199\tkl:   27.948349\n",
      "Epoch: 649 [20100/50000 (40%)]  \tLoss:   86.452843\trec:   60.109802\tkl:   26.343039\n",
      "Epoch: 649 [30100/50000 (60%)]  \tLoss:   90.938637\trec:   62.865261\tkl:   28.073381\n",
      "Epoch: 649 [40100/50000 (80%)]  \tLoss:   89.939796\trec:   62.082500\tkl:   27.857294\n",
      "====> Epoch: 649 Average train loss: 89.3188\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1427\n",
      "Epoch: 650 [  100/50000 ( 0%)]  \tLoss:   88.234055\trec:   61.197395\tkl:   27.036657\n",
      "Epoch: 650 [10100/50000 (20%)]  \tLoss:   88.252480\trec:   60.408676\tkl:   27.843801\n",
      "Epoch: 650 [20100/50000 (40%)]  \tLoss:   92.483070\trec:   63.462070\tkl:   29.021008\n",
      "Epoch: 650 [30100/50000 (60%)]  \tLoss:   85.205795\trec:   58.026024\tkl:   27.179775\n",
      "Epoch: 650 [40100/50000 (80%)]  \tLoss:   86.953789\trec:   59.611618\tkl:   27.342163\n",
      "====> Epoch: 650 Average train loss: 89.3588\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9938\n",
      "Epoch: 651 [  100/50000 ( 0%)]  \tLoss:   87.619308\trec:   59.379608\tkl:   28.239697\n",
      "Epoch: 651 [10100/50000 (20%)]  \tLoss:   87.604454\trec:   59.890686\tkl:   27.713762\n",
      "Epoch: 651 [20100/50000 (40%)]  \tLoss:   91.395912\trec:   63.033226\tkl:   28.362690\n",
      "Epoch: 651 [30100/50000 (60%)]  \tLoss:   86.744858\trec:   59.121834\tkl:   27.623026\n",
      "Epoch: 651 [40100/50000 (80%)]  \tLoss:   90.566505\trec:   62.060749\tkl:   28.505747\n",
      "====> Epoch: 651 Average train loss: 89.3240\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1033\n",
      "Epoch: 652 [  100/50000 ( 0%)]  \tLoss:   88.409081\trec:   60.489868\tkl:   27.919212\n",
      "Epoch: 652 [10100/50000 (20%)]  \tLoss:   89.477722\trec:   60.939888\tkl:   28.537836\n",
      "Epoch: 652 [20100/50000 (40%)]  \tLoss:   90.771500\trec:   62.254158\tkl:   28.517347\n",
      "Epoch: 652 [30100/50000 (60%)]  \tLoss:   91.892380\trec:   63.176254\tkl:   28.716122\n",
      "Epoch: 652 [40100/50000 (80%)]  \tLoss:   89.031471\trec:   60.974373\tkl:   28.057098\n",
      "====> Epoch: 652 Average train loss: 89.3137\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0465\n",
      "Epoch: 653 [  100/50000 ( 0%)]  \tLoss:   94.085869\trec:   65.107552\tkl:   28.978315\n",
      "Epoch: 653 [10100/50000 (20%)]  \tLoss:   92.595291\trec:   63.747478\tkl:   28.847816\n",
      "Epoch: 653 [20100/50000 (40%)]  \tLoss:   92.754005\trec:   63.717258\tkl:   29.036737\n",
      "Epoch: 653 [30100/50000 (60%)]  \tLoss:   90.791306\trec:   62.665752\tkl:   28.125557\n",
      "Epoch: 653 [40100/50000 (80%)]  \tLoss:   89.913887\trec:   62.351276\tkl:   27.562613\n",
      "====> Epoch: 653 Average train loss: 89.3161\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0058\n",
      "Epoch: 654 [  100/50000 ( 0%)]  \tLoss:   89.937782\trec:   61.099159\tkl:   28.838623\n",
      "Epoch: 654 [10100/50000 (20%)]  \tLoss:   82.968712\trec:   55.954697\tkl:   27.014017\n",
      "Epoch: 654 [20100/50000 (40%)]  \tLoss:   88.245407\trec:   60.562420\tkl:   27.682987\n",
      "Epoch: 654 [30100/50000 (60%)]  \tLoss:   88.840759\trec:   60.572372\tkl:   28.268389\n",
      "Epoch: 654 [40100/50000 (80%)]  \tLoss:   89.772133\trec:   60.927513\tkl:   28.844624\n",
      "====> Epoch: 654 Average train loss: 89.3283\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1278\n",
      "Epoch: 655 [  100/50000 ( 0%)]  \tLoss:   88.249451\trec:   61.099442\tkl:   27.150007\n",
      "Epoch: 655 [10100/50000 (20%)]  \tLoss:   86.036812\trec:   58.762707\tkl:   27.274105\n",
      "Epoch: 655 [20100/50000 (40%)]  \tLoss:   92.741768\trec:   63.547035\tkl:   29.194729\n",
      "Epoch: 655 [30100/50000 (60%)]  \tLoss:   92.330193\trec:   64.140732\tkl:   28.189466\n",
      "Epoch: 655 [40100/50000 (80%)]  \tLoss:   86.844513\trec:   59.200058\tkl:   27.644453\n",
      "====> Epoch: 655 Average train loss: 89.3072\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0395\n",
      "Epoch: 656 [  100/50000 ( 0%)]  \tLoss:   90.499245\trec:   62.159706\tkl:   28.339539\n",
      "Epoch: 656 [10100/50000 (20%)]  \tLoss:   87.126396\trec:   59.313816\tkl:   27.812578\n",
      "Epoch: 656 [20100/50000 (40%)]  \tLoss:   91.340187\trec:   62.718742\tkl:   28.621439\n",
      "Epoch: 656 [30100/50000 (60%)]  \tLoss:   89.941597\trec:   60.967243\tkl:   28.974356\n",
      "Epoch: 656 [40100/50000 (80%)]  \tLoss:   91.143227\trec:   64.461090\tkl:   26.682146\n",
      "====> Epoch: 656 Average train loss: 89.3232\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1139\n",
      "Epoch: 657 [  100/50000 ( 0%)]  \tLoss:   82.615112\trec:   56.300945\tkl:   26.314171\n",
      "Epoch: 657 [10100/50000 (20%)]  \tLoss:   94.076561\trec:   64.722588\tkl:   29.353964\n",
      "Epoch: 657 [20100/50000 (40%)]  \tLoss:   86.928360\trec:   59.195236\tkl:   27.733114\n",
      "Epoch: 657 [30100/50000 (60%)]  \tLoss:   86.719902\trec:   59.092777\tkl:   27.627129\n",
      "Epoch: 657 [40100/50000 (80%)]  \tLoss:   87.254997\trec:   60.138210\tkl:   27.116781\n",
      "====> Epoch: 657 Average train loss: 89.3155\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0323\n",
      "Epoch: 658 [  100/50000 ( 0%)]  \tLoss:   88.138161\trec:   59.603828\tkl:   28.534332\n",
      "Epoch: 658 [10100/50000 (20%)]  \tLoss:   89.005524\trec:   60.523750\tkl:   28.481779\n",
      "Epoch: 658 [20100/50000 (40%)]  \tLoss:   87.475937\trec:   59.935947\tkl:   27.539995\n",
      "Epoch: 658 [30100/50000 (60%)]  \tLoss:   89.523094\trec:   60.491512\tkl:   29.031582\n",
      "Epoch: 658 [40100/50000 (80%)]  \tLoss:   87.484917\trec:   59.752083\tkl:   27.732841\n",
      "====> Epoch: 658 Average train loss: 89.3214\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0172\n",
      "Epoch: 659 [  100/50000 ( 0%)]  \tLoss:   87.966408\trec:   60.091595\tkl:   27.874807\n",
      "Epoch: 659 [10100/50000 (20%)]  \tLoss:   92.307732\trec:   63.942928\tkl:   28.364809\n",
      "Epoch: 659 [20100/50000 (40%)]  \tLoss:   85.935143\trec:   58.932346\tkl:   27.002798\n",
      "Epoch: 659 [30100/50000 (60%)]  \tLoss:   87.781349\trec:   60.642792\tkl:   27.138550\n",
      "Epoch: 659 [40100/50000 (80%)]  \tLoss:   87.630959\trec:   60.207939\tkl:   27.423019\n",
      "====> Epoch: 659 Average train loss: 89.3123\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0076\n",
      "Epoch: 660 [  100/50000 ( 0%)]  \tLoss:   88.299080\trec:   59.415615\tkl:   28.883461\n",
      "Epoch: 660 [10100/50000 (20%)]  \tLoss:   91.117950\trec:   62.449951\tkl:   28.667997\n",
      "Epoch: 660 [20100/50000 (40%)]  \tLoss:   90.429375\trec:   62.543301\tkl:   27.886068\n",
      "Epoch: 660 [30100/50000 (60%)]  \tLoss:   89.678162\trec:   61.846138\tkl:   27.832026\n",
      "Epoch: 660 [40100/50000 (80%)]  \tLoss:   92.461494\trec:   64.149582\tkl:   28.311913\n",
      "====> Epoch: 660 Average train loss: 89.3105\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0359\n",
      "Epoch: 661 [  100/50000 ( 0%)]  \tLoss:   83.996170\trec:   57.973648\tkl:   26.022524\n",
      "Epoch: 661 [10100/50000 (20%)]  \tLoss:   88.604591\trec:   59.856003\tkl:   28.748581\n",
      "Epoch: 661 [20100/50000 (40%)]  \tLoss:   87.412750\trec:   60.328247\tkl:   27.084511\n",
      "Epoch: 661 [30100/50000 (60%)]  \tLoss:   90.536758\trec:   61.893738\tkl:   28.643013\n",
      "Epoch: 661 [40100/50000 (80%)]  \tLoss:   92.359161\trec:   64.556038\tkl:   27.803120\n",
      "====> Epoch: 661 Average train loss: 89.3017\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1169\n",
      "Epoch: 662 [  100/50000 ( 0%)]  \tLoss:   92.910484\trec:   63.787022\tkl:   29.123470\n",
      "Epoch: 662 [10100/50000 (20%)]  \tLoss:   87.246376\trec:   59.415478\tkl:   27.830898\n",
      "Epoch: 662 [20100/50000 (40%)]  \tLoss:   85.597267\trec:   58.232128\tkl:   27.365135\n",
      "Epoch: 662 [30100/50000 (60%)]  \tLoss:   91.824684\trec:   63.588425\tkl:   28.236258\n",
      "Epoch: 662 [40100/50000 (80%)]  \tLoss:   88.441818\trec:   60.211922\tkl:   28.229895\n",
      "====> Epoch: 662 Average train loss: 89.2947\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1093\n",
      "Epoch: 663 [  100/50000 ( 0%)]  \tLoss:   87.309128\trec:   60.089344\tkl:   27.219784\n",
      "Epoch: 663 [10100/50000 (20%)]  \tLoss:   89.310333\trec:   61.446922\tkl:   27.863405\n",
      "Epoch: 663 [20100/50000 (40%)]  \tLoss:   86.823807\trec:   59.028767\tkl:   27.795038\n",
      "Epoch: 663 [30100/50000 (60%)]  \tLoss:   88.480896\trec:   60.516960\tkl:   27.963930\n",
      "Epoch: 663 [40100/50000 (80%)]  \tLoss:   90.189674\trec:   61.643124\tkl:   28.546553\n",
      "====> Epoch: 663 Average train loss: 89.2889\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9493\n",
      "Epoch: 664 [  100/50000 ( 0%)]  \tLoss:   86.852814\trec:   58.980602\tkl:   27.872202\n",
      "Epoch: 664 [10100/50000 (20%)]  \tLoss:   89.813171\trec:   61.486469\tkl:   28.326704\n",
      "Epoch: 664 [20100/50000 (40%)]  \tLoss:   90.956696\trec:   63.041439\tkl:   27.915258\n",
      "Epoch: 664 [30100/50000 (60%)]  \tLoss:   91.227203\trec:   63.071102\tkl:   28.156107\n",
      "Epoch: 664 [40100/50000 (80%)]  \tLoss:   93.285126\trec:   64.022041\tkl:   29.263083\n",
      "====> Epoch: 664 Average train loss: 89.2905\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0921\n",
      "Epoch: 665 [  100/50000 ( 0%)]  \tLoss:   89.279472\trec:   61.481030\tkl:   27.798441\n",
      "Epoch: 665 [10100/50000 (20%)]  \tLoss:   90.389610\trec:   62.290619\tkl:   28.098984\n",
      "Epoch: 665 [20100/50000 (40%)]  \tLoss:   87.896072\trec:   60.620831\tkl:   27.275238\n",
      "Epoch: 665 [30100/50000 (60%)]  \tLoss:   88.857849\trec:   60.895966\tkl:   27.961889\n",
      "Epoch: 665 [40100/50000 (80%)]  \tLoss:   89.926582\trec:   61.133945\tkl:   28.792633\n",
      "====> Epoch: 665 Average train loss: 89.2953\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9811\n",
      "Epoch: 666 [  100/50000 ( 0%)]  \tLoss:   86.076050\trec:   59.340752\tkl:   26.735302\n",
      "Epoch: 666 [10100/50000 (20%)]  \tLoss:   86.187241\trec:   57.984936\tkl:   28.202311\n",
      "Epoch: 666 [20100/50000 (40%)]  \tLoss:   86.612343\trec:   58.899414\tkl:   27.712929\n",
      "Epoch: 666 [30100/50000 (60%)]  \tLoss:   89.991333\trec:   61.564278\tkl:   28.427057\n",
      "Epoch: 666 [40100/50000 (80%)]  \tLoss:   87.747971\trec:   60.490204\tkl:   27.257763\n",
      "====> Epoch: 666 Average train loss: 89.2646\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9782\n",
      "Epoch: 667 [  100/50000 ( 0%)]  \tLoss:   91.495026\trec:   63.109409\tkl:   28.385620\n",
      "Epoch: 667 [10100/50000 (20%)]  \tLoss:   86.498497\trec:   58.793777\tkl:   27.704716\n",
      "Epoch: 667 [20100/50000 (40%)]  \tLoss:   90.044373\trec:   61.839714\tkl:   28.204660\n",
      "Epoch: 667 [30100/50000 (60%)]  \tLoss:   88.355118\trec:   60.230007\tkl:   28.125105\n",
      "Epoch: 667 [40100/50000 (80%)]  \tLoss:   87.421577\trec:   60.276112\tkl:   27.145464\n",
      "====> Epoch: 667 Average train loss: 89.2854\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0183\n",
      "Epoch: 668 [  100/50000 ( 0%)]  \tLoss:   89.596619\trec:   61.366829\tkl:   28.229794\n",
      "Epoch: 668 [10100/50000 (20%)]  \tLoss:   95.434502\trec:   65.831123\tkl:   29.603373\n",
      "Epoch: 668 [20100/50000 (40%)]  \tLoss:   89.851639\trec:   60.983894\tkl:   28.867746\n",
      "Epoch: 668 [30100/50000 (60%)]  \tLoss:   92.027634\trec:   63.010696\tkl:   29.016943\n",
      "Epoch: 668 [40100/50000 (80%)]  \tLoss:   94.183739\trec:   64.867546\tkl:   29.316189\n",
      "====> Epoch: 668 Average train loss: 89.2822\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9757\n",
      "Epoch: 669 [  100/50000 ( 0%)]  \tLoss:   89.313217\trec:   61.423244\tkl:   27.889975\n",
      "Epoch: 669 [10100/50000 (20%)]  \tLoss:   89.164757\trec:   61.060459\tkl:   28.104300\n",
      "Epoch: 669 [20100/50000 (40%)]  \tLoss:   86.703804\trec:   59.214207\tkl:   27.489599\n",
      "Epoch: 669 [30100/50000 (60%)]  \tLoss:   88.725166\trec:   60.502422\tkl:   28.222744\n",
      "Epoch: 669 [40100/50000 (80%)]  \tLoss:   92.148331\trec:   64.209831\tkl:   27.938496\n",
      "====> Epoch: 669 Average train loss: 89.2873\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0550\n",
      "Epoch: 670 [  100/50000 ( 0%)]  \tLoss:   85.806618\trec:   58.857300\tkl:   26.949326\n",
      "Epoch: 670 [10100/50000 (20%)]  \tLoss:   89.493286\trec:   60.570293\tkl:   28.922997\n",
      "Epoch: 670 [20100/50000 (40%)]  \tLoss:   88.327568\trec:   59.941807\tkl:   28.385759\n",
      "Epoch: 670 [30100/50000 (60%)]  \tLoss:   91.806366\trec:   62.911785\tkl:   28.894577\n",
      "Epoch: 670 [40100/50000 (80%)]  \tLoss:   87.178329\trec:   59.688389\tkl:   27.489943\n",
      "====> Epoch: 670 Average train loss: 89.2629\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0044\n",
      "Epoch: 671 [  100/50000 ( 0%)]  \tLoss:   86.612770\trec:   59.174129\tkl:   27.438637\n",
      "Epoch: 671 [10100/50000 (20%)]  \tLoss:   88.125854\trec:   60.387741\tkl:   27.738110\n",
      "Epoch: 671 [20100/50000 (40%)]  \tLoss:   87.978096\trec:   60.353054\tkl:   27.625038\n",
      "Epoch: 671 [30100/50000 (60%)]  \tLoss:   88.712280\trec:   60.416893\tkl:   28.295389\n",
      "Epoch: 671 [40100/50000 (80%)]  \tLoss:   91.172440\trec:   62.532505\tkl:   28.639931\n",
      "====> Epoch: 671 Average train loss: 89.2791\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0092\n",
      "Epoch: 672 [  100/50000 ( 0%)]  \tLoss:   89.528046\trec:   61.632370\tkl:   27.895678\n",
      "Epoch: 672 [10100/50000 (20%)]  \tLoss:   91.259384\trec:   62.633286\tkl:   28.626099\n",
      "Epoch: 672 [20100/50000 (40%)]  \tLoss:   91.107208\trec:   62.607178\tkl:   28.500034\n",
      "Epoch: 672 [30100/50000 (60%)]  \tLoss:   87.847321\trec:   59.890667\tkl:   27.956650\n",
      "Epoch: 672 [40100/50000 (80%)]  \tLoss:   87.975212\trec:   60.304276\tkl:   27.670942\n",
      "====> Epoch: 672 Average train loss: 89.2549\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9832\n",
      "Epoch: 673 [  100/50000 ( 0%)]  \tLoss:   88.344467\trec:   60.620522\tkl:   27.723955\n",
      "Epoch: 673 [10100/50000 (20%)]  \tLoss:   90.752632\trec:   61.950508\tkl:   28.802128\n",
      "Epoch: 673 [20100/50000 (40%)]  \tLoss:   86.812653\trec:   58.922089\tkl:   27.890564\n",
      "Epoch: 673 [30100/50000 (60%)]  \tLoss:   86.116753\trec:   59.348206\tkl:   26.768549\n",
      "Epoch: 673 [40100/50000 (80%)]  \tLoss:   91.428238\trec:   62.915668\tkl:   28.512577\n",
      "====> Epoch: 673 Average train loss: 89.2678\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0176\n",
      "Epoch: 674 [  100/50000 ( 0%)]  \tLoss:   88.670914\trec:   61.190193\tkl:   27.480717\n",
      "Epoch: 674 [10100/50000 (20%)]  \tLoss:   85.755753\trec:   58.943752\tkl:   26.811996\n",
      "Epoch: 674 [20100/50000 (40%)]  \tLoss:   93.298111\trec:   64.250626\tkl:   29.047489\n",
      "Epoch: 674 [30100/50000 (60%)]  \tLoss:   89.891121\trec:   62.254120\tkl:   27.637001\n",
      "Epoch: 674 [40100/50000 (80%)]  \tLoss:   93.736191\trec:   64.563515\tkl:   29.172680\n",
      "====> Epoch: 674 Average train loss: 89.2259\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9632\n",
      "Epoch: 675 [  100/50000 ( 0%)]  \tLoss:   92.352455\trec:   63.205166\tkl:   29.147299\n",
      "Epoch: 675 [10100/50000 (20%)]  \tLoss:   87.948380\trec:   59.931511\tkl:   28.016867\n",
      "Epoch: 675 [20100/50000 (40%)]  \tLoss:   92.519745\trec:   63.716053\tkl:   28.803686\n",
      "Epoch: 675 [30100/50000 (60%)]  \tLoss:   88.105019\trec:   60.243759\tkl:   27.861254\n",
      "Epoch: 675 [40100/50000 (80%)]  \tLoss:   87.685577\trec:   60.129890\tkl:   27.555681\n",
      "====> Epoch: 675 Average train loss: 89.2557\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0381\n",
      "Epoch: 676 [  100/50000 ( 0%)]  \tLoss:   88.578278\trec:   61.131660\tkl:   27.446623\n",
      "Epoch: 676 [10100/50000 (20%)]  \tLoss:   86.843513\trec:   59.322067\tkl:   27.521450\n",
      "Epoch: 676 [20100/50000 (40%)]  \tLoss:   90.330437\trec:   61.349876\tkl:   28.980560\n",
      "Epoch: 676 [30100/50000 (60%)]  \tLoss:   82.342773\trec:   56.640518\tkl:   25.702253\n",
      "Epoch: 676 [40100/50000 (80%)]  \tLoss:   91.954903\trec:   63.271172\tkl:   28.683727\n",
      "====> Epoch: 676 Average train loss: 89.2744\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1149\n",
      "Epoch: 677 [  100/50000 ( 0%)]  \tLoss:   86.721092\trec:   58.868015\tkl:   27.853071\n",
      "Epoch: 677 [10100/50000 (20%)]  \tLoss:   90.386246\trec:   62.162079\tkl:   28.224167\n",
      "Epoch: 677 [20100/50000 (40%)]  \tLoss:   89.741890\trec:   62.065277\tkl:   27.676619\n",
      "Epoch: 677 [30100/50000 (60%)]  \tLoss:   88.002335\trec:   60.501419\tkl:   27.500910\n",
      "Epoch: 677 [40100/50000 (80%)]  \tLoss:   90.427719\trec:   62.224197\tkl:   28.203526\n",
      "====> Epoch: 677 Average train loss: 89.2528\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0145\n",
      "Epoch: 678 [  100/50000 ( 0%)]  \tLoss:   90.528076\trec:   62.902225\tkl:   27.625849\n",
      "Epoch: 678 [10100/50000 (20%)]  \tLoss:   88.485191\trec:   60.475555\tkl:   28.009644\n",
      "Epoch: 678 [20100/50000 (40%)]  \tLoss:   86.062576\trec:   59.392391\tkl:   26.670179\n",
      "Epoch: 678 [30100/50000 (60%)]  \tLoss:   88.801201\trec:   60.616035\tkl:   28.185167\n",
      "Epoch: 678 [40100/50000 (80%)]  \tLoss:   89.465157\trec:   61.761204\tkl:   27.703945\n",
      "====> Epoch: 678 Average train loss: 89.2354\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0795\n",
      "Epoch: 679 [  100/50000 ( 0%)]  \tLoss:   91.366814\trec:   63.045536\tkl:   28.321283\n",
      "Epoch: 679 [10100/50000 (20%)]  \tLoss:   90.943359\trec:   62.743725\tkl:   28.199631\n",
      "Epoch: 679 [20100/50000 (40%)]  \tLoss:   88.305313\trec:   60.972874\tkl:   27.332436\n",
      "Epoch: 679 [30100/50000 (60%)]  \tLoss:   87.722633\trec:   60.765068\tkl:   26.957567\n",
      "Epoch: 679 [40100/50000 (80%)]  \tLoss:   89.932945\trec:   62.208744\tkl:   27.724203\n",
      "====> Epoch: 679 Average train loss: 89.2264\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9552\n",
      "Epoch: 680 [  100/50000 ( 0%)]  \tLoss:   86.686157\trec:   59.317371\tkl:   27.368788\n",
      "Epoch: 680 [10100/50000 (20%)]  \tLoss:   84.835861\trec:   57.698868\tkl:   27.136990\n",
      "Epoch: 680 [20100/50000 (40%)]  \tLoss:   93.112518\trec:   64.913948\tkl:   28.198568\n",
      "Epoch: 680 [30100/50000 (60%)]  \tLoss:   85.320465\trec:   57.737713\tkl:   27.582758\n",
      "Epoch: 680 [40100/50000 (80%)]  \tLoss:   92.071289\trec:   62.273788\tkl:   29.797499\n",
      "====> Epoch: 680 Average train loss: 89.2496\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0445\n",
      "Epoch: 681 [  100/50000 ( 0%)]  \tLoss:   91.988190\trec:   63.486912\tkl:   28.501278\n",
      "Epoch: 681 [10100/50000 (20%)]  \tLoss:   86.134468\trec:   59.531094\tkl:   26.603384\n",
      "Epoch: 681 [20100/50000 (40%)]  \tLoss:   86.750397\trec:   59.182705\tkl:   27.567692\n",
      "Epoch: 681 [30100/50000 (60%)]  \tLoss:   93.350967\trec:   64.065643\tkl:   29.285318\n",
      "Epoch: 681 [40100/50000 (80%)]  \tLoss:   92.776558\trec:   62.950672\tkl:   29.825893\n",
      "====> Epoch: 681 Average train loss: 89.2335\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0747\n",
      "Epoch: 682 [  100/50000 ( 0%)]  \tLoss:   87.332306\trec:   59.776638\tkl:   27.555666\n",
      "Epoch: 682 [10100/50000 (20%)]  \tLoss:   89.116646\trec:   61.175007\tkl:   27.941641\n",
      "Epoch: 682 [20100/50000 (40%)]  \tLoss:   90.684082\trec:   63.121014\tkl:   27.563066\n",
      "Epoch: 682 [30100/50000 (60%)]  \tLoss:   91.605347\trec:   62.992264\tkl:   28.613081\n",
      "Epoch: 682 [40100/50000 (80%)]  \tLoss:   84.336563\trec:   58.058075\tkl:   26.278481\n",
      "====> Epoch: 682 Average train loss: 89.2214\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0586\n",
      "Epoch: 683 [  100/50000 ( 0%)]  \tLoss:   90.875877\trec:   62.362549\tkl:   28.513325\n",
      "Epoch: 683 [10100/50000 (20%)]  \tLoss:   87.389687\trec:   58.970104\tkl:   28.419584\n",
      "Epoch: 683 [20100/50000 (40%)]  \tLoss:   84.210114\trec:   56.928070\tkl:   27.282043\n",
      "Epoch: 683 [30100/50000 (60%)]  \tLoss:   92.057304\trec:   63.524277\tkl:   28.533022\n",
      "Epoch: 683 [40100/50000 (80%)]  \tLoss:   86.891914\trec:   59.121201\tkl:   27.770708\n",
      "====> Epoch: 683 Average train loss: 89.2278\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9647\n",
      "Epoch: 684 [  100/50000 ( 0%)]  \tLoss:   87.291374\trec:   59.552029\tkl:   27.739342\n",
      "Epoch: 684 [10100/50000 (20%)]  \tLoss:   91.689629\trec:   63.352863\tkl:   28.336758\n",
      "Epoch: 684 [20100/50000 (40%)]  \tLoss:   91.897652\trec:   63.052464\tkl:   28.845184\n",
      "Epoch: 684 [30100/50000 (60%)]  \tLoss:   86.360420\trec:   58.581631\tkl:   27.778788\n",
      "Epoch: 684 [40100/50000 (80%)]  \tLoss:   89.546288\trec:   61.692978\tkl:   27.853308\n",
      "====> Epoch: 684 Average train loss: 89.2353\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8965\n",
      "Epoch: 685 [  100/50000 ( 0%)]  \tLoss:   90.410721\trec:   61.838196\tkl:   28.572523\n",
      "Epoch: 685 [10100/50000 (20%)]  \tLoss:   91.113571\trec:   62.021385\tkl:   29.092192\n",
      "Epoch: 685 [20100/50000 (40%)]  \tLoss:   89.471939\trec:   61.229980\tkl:   28.241964\n",
      "Epoch: 685 [30100/50000 (60%)]  \tLoss:   85.436745\trec:   58.458046\tkl:   26.978699\n",
      "Epoch: 685 [40100/50000 (80%)]  \tLoss:   90.460609\trec:   62.097969\tkl:   28.362644\n",
      "====> Epoch: 685 Average train loss: 89.2148\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9881\n",
      "Epoch: 686 [  100/50000 ( 0%)]  \tLoss:   86.296501\trec:   58.313457\tkl:   27.983042\n",
      "Epoch: 686 [10100/50000 (20%)]  \tLoss:   92.179070\trec:   63.775311\tkl:   28.403759\n",
      "Epoch: 686 [20100/50000 (40%)]  \tLoss:   88.616089\trec:   59.777821\tkl:   28.838276\n",
      "Epoch: 686 [30100/50000 (60%)]  \tLoss:   86.128464\trec:   58.892403\tkl:   27.236063\n",
      "Epoch: 686 [40100/50000 (80%)]  \tLoss:   87.377480\trec:   59.958675\tkl:   27.418798\n",
      "====> Epoch: 686 Average train loss: 89.1924\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8901\n",
      "Epoch: 687 [  100/50000 ( 0%)]  \tLoss:   90.700768\trec:   61.535564\tkl:   29.165205\n",
      "Epoch: 687 [10100/50000 (20%)]  \tLoss:   88.936852\trec:   60.907810\tkl:   28.029037\n",
      "Epoch: 687 [20100/50000 (40%)]  \tLoss:   92.659943\trec:   63.585224\tkl:   29.074711\n",
      "Epoch: 687 [30100/50000 (60%)]  \tLoss:   86.561798\trec:   59.210106\tkl:   27.351688\n",
      "Epoch: 687 [40100/50000 (80%)]  \tLoss:   86.747765\trec:   58.933212\tkl:   27.814550\n",
      "====> Epoch: 687 Average train loss: 89.2198\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0443\n",
      "Epoch: 688 [  100/50000 ( 0%)]  \tLoss:   86.853943\trec:   60.111088\tkl:   26.742855\n",
      "Epoch: 688 [10100/50000 (20%)]  \tLoss:   90.950645\trec:   62.948006\tkl:   28.002636\n",
      "Epoch: 688 [20100/50000 (40%)]  \tLoss:   91.665077\trec:   62.710857\tkl:   28.954220\n",
      "Epoch: 688 [30100/50000 (60%)]  \tLoss:   92.069878\trec:   63.213455\tkl:   28.856421\n",
      "Epoch: 688 [40100/50000 (80%)]  \tLoss:   88.346886\trec:   60.760742\tkl:   27.586143\n",
      "====> Epoch: 688 Average train loss: 89.1989\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0471\n",
      "Epoch: 689 [  100/50000 ( 0%)]  \tLoss:   90.362862\trec:   62.106247\tkl:   28.256611\n",
      "Epoch: 689 [10100/50000 (20%)]  \tLoss:   85.491562\trec:   58.664520\tkl:   26.827045\n",
      "Epoch: 689 [20100/50000 (40%)]  \tLoss:   86.218803\trec:   59.322109\tkl:   26.896698\n",
      "Epoch: 689 [30100/50000 (60%)]  \tLoss:   88.766754\trec:   61.222980\tkl:   27.543770\n",
      "Epoch: 689 [40100/50000 (80%)]  \tLoss:   88.071815\trec:   60.281746\tkl:   27.790073\n",
      "====> Epoch: 689 Average train loss: 89.2002\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9410\n",
      "Epoch: 690 [  100/50000 ( 0%)]  \tLoss:   87.879509\trec:   60.015995\tkl:   27.863510\n",
      "Epoch: 690 [10100/50000 (20%)]  \tLoss:   89.402466\trec:   61.149128\tkl:   28.253340\n",
      "Epoch: 690 [20100/50000 (40%)]  \tLoss:   86.935684\trec:   59.170418\tkl:   27.765259\n",
      "Epoch: 690 [30100/50000 (60%)]  \tLoss:   88.596581\trec:   59.682362\tkl:   28.914223\n",
      "Epoch: 690 [40100/50000 (80%)]  \tLoss:   88.695702\trec:   60.277275\tkl:   28.418430\n",
      "====> Epoch: 690 Average train loss: 89.1923\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0951\n",
      "Epoch: 691 [  100/50000 ( 0%)]  \tLoss:   90.539635\trec:   62.837055\tkl:   27.702583\n",
      "Epoch: 691 [10100/50000 (20%)]  \tLoss:   87.941032\trec:   60.246540\tkl:   27.694496\n",
      "Epoch: 691 [20100/50000 (40%)]  \tLoss:   88.550781\trec:   59.828758\tkl:   28.722023\n",
      "Epoch: 691 [30100/50000 (60%)]  \tLoss:   86.498810\trec:   58.838318\tkl:   27.660482\n",
      "Epoch: 691 [40100/50000 (80%)]  \tLoss:   91.714127\trec:   62.575729\tkl:   29.138395\n",
      "====> Epoch: 691 Average train loss: 89.1674\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9950\n",
      "Epoch: 692 [  100/50000 ( 0%)]  \tLoss:   91.643219\trec:   63.114315\tkl:   28.528910\n",
      "Epoch: 692 [10100/50000 (20%)]  \tLoss:   92.358101\trec:   63.168457\tkl:   29.189642\n",
      "Epoch: 692 [20100/50000 (40%)]  \tLoss:   88.781853\trec:   60.351952\tkl:   28.429907\n",
      "Epoch: 692 [30100/50000 (60%)]  \tLoss:   88.958565\trec:   61.058441\tkl:   27.900124\n",
      "Epoch: 692 [40100/50000 (80%)]  \tLoss:   87.626511\trec:   59.720119\tkl:   27.906391\n",
      "====> Epoch: 692 Average train loss: 89.2035\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0358\n",
      "Epoch: 693 [  100/50000 ( 0%)]  \tLoss:   89.594101\trec:   60.958698\tkl:   28.635399\n",
      "Epoch: 693 [10100/50000 (20%)]  \tLoss:   89.425690\trec:   60.588669\tkl:   28.837023\n",
      "Epoch: 693 [20100/50000 (40%)]  \tLoss:   90.923035\trec:   62.581280\tkl:   28.341757\n",
      "Epoch: 693 [30100/50000 (60%)]  \tLoss:   87.646782\trec:   58.898315\tkl:   28.748470\n",
      "Epoch: 693 [40100/50000 (80%)]  \tLoss:   86.691521\trec:   58.228073\tkl:   28.463451\n",
      "====> Epoch: 693 Average train loss: 89.1729\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9953\n",
      "Epoch: 694 [  100/50000 ( 0%)]  \tLoss:   87.539497\trec:   59.704193\tkl:   27.835306\n",
      "Epoch: 694 [10100/50000 (20%)]  \tLoss:   89.405464\trec:   61.390800\tkl:   28.014662\n",
      "Epoch: 694 [20100/50000 (40%)]  \tLoss:   91.081085\trec:   62.750778\tkl:   28.330301\n",
      "Epoch: 694 [30100/50000 (60%)]  \tLoss:   91.559669\trec:   62.904896\tkl:   28.654776\n",
      "Epoch: 694 [40100/50000 (80%)]  \tLoss:   89.504578\trec:   61.136360\tkl:   28.368216\n",
      "====> Epoch: 694 Average train loss: 89.1945\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9610\n",
      "Epoch: 695 [  100/50000 ( 0%)]  \tLoss:   89.475288\trec:   60.907333\tkl:   28.567963\n",
      "Epoch: 695 [10100/50000 (20%)]  \tLoss:   92.081558\trec:   62.747158\tkl:   29.334402\n",
      "Epoch: 695 [20100/50000 (40%)]  \tLoss:   92.263634\trec:   62.971382\tkl:   29.292248\n",
      "Epoch: 695 [30100/50000 (60%)]  \tLoss:   86.907082\trec:   58.916824\tkl:   27.990253\n",
      "Epoch: 695 [40100/50000 (80%)]  \tLoss:   88.105728\trec:   60.803963\tkl:   27.301765\n",
      "====> Epoch: 695 Average train loss: 89.1895\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0010\n",
      "Epoch: 696 [  100/50000 ( 0%)]  \tLoss:   88.593697\trec:   60.828041\tkl:   27.765661\n",
      "Epoch: 696 [10100/50000 (20%)]  \tLoss:   93.462624\trec:   64.948647\tkl:   28.513977\n",
      "Epoch: 696 [20100/50000 (40%)]  \tLoss:   88.990982\trec:   61.048622\tkl:   27.942360\n",
      "Epoch: 696 [30100/50000 (60%)]  \tLoss:   91.125504\trec:   63.487900\tkl:   27.637604\n",
      "Epoch: 696 [40100/50000 (80%)]  \tLoss:   90.960190\trec:   61.925095\tkl:   29.035101\n",
      "====> Epoch: 696 Average train loss: 89.1543\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0847\n",
      "Epoch: 697 [  100/50000 ( 0%)]  \tLoss:   91.070297\trec:   62.162392\tkl:   28.907909\n",
      "Epoch: 697 [10100/50000 (20%)]  \tLoss:   85.332558\trec:   58.487724\tkl:   26.844831\n",
      "Epoch: 697 [20100/50000 (40%)]  \tLoss:   89.091011\trec:   61.789608\tkl:   27.301401\n",
      "Epoch: 697 [30100/50000 (60%)]  \tLoss:   85.690338\trec:   58.580986\tkl:   27.109352\n",
      "Epoch: 697 [40100/50000 (80%)]  \tLoss:   89.397980\trec:   61.025280\tkl:   28.372694\n",
      "====> Epoch: 697 Average train loss: 89.1829\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9968\n",
      "Epoch: 698 [  100/50000 ( 0%)]  \tLoss:   88.537262\trec:   60.704334\tkl:   27.832926\n",
      "Epoch: 698 [10100/50000 (20%)]  \tLoss:   89.494926\trec:   60.645649\tkl:   28.849279\n",
      "Epoch: 698 [20100/50000 (40%)]  \tLoss:   88.545425\trec:   60.574402\tkl:   27.971020\n",
      "Epoch: 698 [30100/50000 (60%)]  \tLoss:   87.693726\trec:   60.106979\tkl:   27.586752\n",
      "Epoch: 698 [40100/50000 (80%)]  \tLoss:   91.106728\trec:   62.086346\tkl:   29.020382\n",
      "====> Epoch: 698 Average train loss: 89.1637\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9860\n",
      "Epoch: 699 [  100/50000 ( 0%)]  \tLoss:   85.580421\trec:   58.255131\tkl:   27.325287\n",
      "Epoch: 699 [10100/50000 (20%)]  \tLoss:   93.704552\trec:   65.191109\tkl:   28.513435\n",
      "Epoch: 699 [20100/50000 (40%)]  \tLoss:   88.029198\trec:   59.929638\tkl:   28.099560\n",
      "Epoch: 699 [30100/50000 (60%)]  \tLoss:   90.248573\trec:   61.997841\tkl:   28.250727\n",
      "Epoch: 699 [40100/50000 (80%)]  \tLoss:   90.212067\trec:   62.292217\tkl:   27.919855\n",
      "====> Epoch: 699 Average train loss: 89.1797\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9809\n",
      "Epoch: 700 [  100/50000 ( 0%)]  \tLoss:   93.654884\trec:   63.885632\tkl:   29.769253\n",
      "Epoch: 700 [10100/50000 (20%)]  \tLoss:   86.503708\trec:   58.619663\tkl:   27.884043\n",
      "Epoch: 700 [20100/50000 (40%)]  \tLoss:   90.121407\trec:   61.260418\tkl:   28.860985\n",
      "Epoch: 700 [30100/50000 (60%)]  \tLoss:   89.626495\trec:   60.500359\tkl:   29.126129\n",
      "Epoch: 700 [40100/50000 (80%)]  \tLoss:   90.839287\trec:   62.827068\tkl:   28.012217\n",
      "====> Epoch: 700 Average train loss: 89.1615\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9865\n",
      "Epoch: 701 [  100/50000 ( 0%)]  \tLoss:   88.774597\trec:   61.345684\tkl:   27.428917\n",
      "Epoch: 701 [10100/50000 (20%)]  \tLoss:   90.951012\trec:   61.979229\tkl:   28.971792\n",
      "Epoch: 701 [20100/50000 (40%)]  \tLoss:   89.177879\trec:   60.438622\tkl:   28.739258\n",
      "Epoch: 701 [30100/50000 (60%)]  \tLoss:   90.535866\trec:   61.746590\tkl:   28.789276\n",
      "Epoch: 701 [40100/50000 (80%)]  \tLoss:   89.038208\trec:   60.506912\tkl:   28.531296\n",
      "====> Epoch: 701 Average train loss: 89.1707\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0091\n",
      "Epoch: 702 [  100/50000 ( 0%)]  \tLoss:   88.460350\trec:   60.657749\tkl:   27.802599\n",
      "Epoch: 702 [10100/50000 (20%)]  \tLoss:   92.208878\trec:   63.645046\tkl:   28.563826\n",
      "Epoch: 702 [20100/50000 (40%)]  \tLoss:   89.289047\trec:   61.452953\tkl:   27.836100\n",
      "Epoch: 702 [30100/50000 (60%)]  \tLoss:   86.963844\trec:   59.470295\tkl:   27.493553\n",
      "Epoch: 702 [40100/50000 (80%)]  \tLoss:   86.193893\trec:   59.163788\tkl:   27.030107\n",
      "====> Epoch: 702 Average train loss: 89.1854\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9011\n",
      "Epoch: 703 [  100/50000 ( 0%)]  \tLoss:   88.159431\trec:   59.762165\tkl:   28.397270\n",
      "Epoch: 703 [10100/50000 (20%)]  \tLoss:   88.291519\trec:   59.580643\tkl:   28.710873\n",
      "Epoch: 703 [20100/50000 (40%)]  \tLoss:   88.946724\trec:   61.271336\tkl:   27.675390\n",
      "Epoch: 703 [30100/50000 (60%)]  \tLoss:   86.627159\trec:   58.797798\tkl:   27.829363\n",
      "Epoch: 703 [40100/50000 (80%)]  \tLoss:   88.875626\trec:   61.048359\tkl:   27.827263\n",
      "====> Epoch: 703 Average train loss: 89.1848\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9660\n",
      "Epoch: 704 [  100/50000 ( 0%)]  \tLoss:   88.760597\trec:   61.150513\tkl:   27.610083\n",
      "Epoch: 704 [10100/50000 (20%)]  \tLoss:   93.106094\trec:   63.602531\tkl:   29.503557\n",
      "Epoch: 704 [20100/50000 (40%)]  \tLoss:   90.437576\trec:   61.626907\tkl:   28.810671\n",
      "Epoch: 704 [30100/50000 (60%)]  \tLoss:   89.581406\trec:   61.371449\tkl:   28.209951\n",
      "Epoch: 704 [40100/50000 (80%)]  \tLoss:   87.891212\trec:   60.011776\tkl:   27.879429\n",
      "====> Epoch: 704 Average train loss: 89.1797\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9207\n",
      "Epoch: 705 [  100/50000 ( 0%)]  \tLoss:   87.347687\trec:   60.431503\tkl:   26.916183\n",
      "Epoch: 705 [10100/50000 (20%)]  \tLoss:   89.201736\trec:   60.046494\tkl:   29.155247\n",
      "Epoch: 705 [20100/50000 (40%)]  \tLoss:   90.121147\trec:   61.928329\tkl:   28.192823\n",
      "Epoch: 705 [30100/50000 (60%)]  \tLoss:   88.739792\trec:   60.444725\tkl:   28.295065\n",
      "Epoch: 705 [40100/50000 (80%)]  \tLoss:   86.305588\trec:   59.040337\tkl:   27.265253\n",
      "====> Epoch: 705 Average train loss: 89.1624\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9278\n",
      "Epoch: 706 [  100/50000 ( 0%)]  \tLoss:   88.696678\trec:   60.572109\tkl:   28.124575\n",
      "Epoch: 706 [10100/50000 (20%)]  \tLoss:   87.491913\trec:   60.599930\tkl:   26.891987\n",
      "Epoch: 706 [20100/50000 (40%)]  \tLoss:   94.184647\trec:   64.914139\tkl:   29.270512\n",
      "Epoch: 706 [30100/50000 (60%)]  \tLoss:   85.912476\trec:   58.787090\tkl:   27.125389\n",
      "Epoch: 706 [40100/50000 (80%)]  \tLoss:   84.921814\trec:   57.720760\tkl:   27.201054\n",
      "====> Epoch: 706 Average train loss: 89.1481\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0561\n",
      "Epoch: 707 [  100/50000 ( 0%)]  \tLoss:   85.863358\trec:   58.913895\tkl:   26.949459\n",
      "Epoch: 707 [10100/50000 (20%)]  \tLoss:   90.638374\trec:   62.431255\tkl:   28.207121\n",
      "Epoch: 707 [20100/50000 (40%)]  \tLoss:   91.628395\trec:   62.751492\tkl:   28.876905\n",
      "Epoch: 707 [30100/50000 (60%)]  \tLoss:   87.587852\trec:   60.081379\tkl:   27.506470\n",
      "Epoch: 707 [40100/50000 (80%)]  \tLoss:   92.264557\trec:   63.376633\tkl:   28.887922\n",
      "====> Epoch: 707 Average train loss: 89.1380\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0395\n",
      "Epoch: 708 [  100/50000 ( 0%)]  \tLoss:   86.790565\trec:   60.027493\tkl:   26.763071\n",
      "Epoch: 708 [10100/50000 (20%)]  \tLoss:   88.339256\trec:   59.920219\tkl:   28.419043\n",
      "Epoch: 708 [20100/50000 (40%)]  \tLoss:   85.746231\trec:   57.991299\tkl:   27.754936\n",
      "Epoch: 708 [30100/50000 (60%)]  \tLoss:   90.868942\trec:   61.413582\tkl:   29.455360\n",
      "Epoch: 708 [40100/50000 (80%)]  \tLoss:   88.921074\trec:   61.319778\tkl:   27.601297\n",
      "====> Epoch: 708 Average train loss: 89.1269\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9820\n",
      "Epoch: 709 [  100/50000 ( 0%)]  \tLoss:   90.181801\trec:   61.989262\tkl:   28.192541\n",
      "Epoch: 709 [10100/50000 (20%)]  \tLoss:   84.662254\trec:   57.014240\tkl:   27.648012\n",
      "Epoch: 709 [20100/50000 (40%)]  \tLoss:   83.775406\trec:   57.077347\tkl:   26.698059\n",
      "Epoch: 709 [30100/50000 (60%)]  \tLoss:   88.727654\trec:   60.462627\tkl:   28.265034\n",
      "Epoch: 709 [40100/50000 (80%)]  \tLoss:   90.589890\trec:   62.619030\tkl:   27.970860\n",
      "====> Epoch: 709 Average train loss: 89.1503\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9125\n",
      "Epoch: 710 [  100/50000 ( 0%)]  \tLoss:   89.638008\trec:   60.784538\tkl:   28.853464\n",
      "Epoch: 710 [10100/50000 (20%)]  \tLoss:   88.207420\trec:   60.127441\tkl:   28.079979\n",
      "Epoch: 710 [20100/50000 (40%)]  \tLoss:   89.946159\trec:   62.678490\tkl:   27.267673\n",
      "Epoch: 710 [30100/50000 (60%)]  \tLoss:   86.829453\trec:   59.786621\tkl:   27.042831\n",
      "Epoch: 710 [40100/50000 (80%)]  \tLoss:   93.343414\trec:   64.421631\tkl:   28.921791\n",
      "====> Epoch: 710 Average train loss: 89.1322\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9976\n",
      "Epoch: 711 [  100/50000 ( 0%)]  \tLoss:   89.934616\trec:   61.330223\tkl:   28.604397\n",
      "Epoch: 711 [10100/50000 (20%)]  \tLoss:   93.588135\trec:   64.496384\tkl:   29.091747\n",
      "Epoch: 711 [20100/50000 (40%)]  \tLoss:   89.481873\trec:   61.679707\tkl:   27.802164\n",
      "Epoch: 711 [30100/50000 (60%)]  \tLoss:   84.193848\trec:   56.388229\tkl:   27.805616\n",
      "Epoch: 711 [40100/50000 (80%)]  \tLoss:   90.336281\trec:   63.364243\tkl:   26.972036\n",
      "====> Epoch: 711 Average train loss: 89.1655\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0191\n",
      "Epoch: 712 [  100/50000 ( 0%)]  \tLoss:   88.005661\trec:   60.249069\tkl:   27.756586\n",
      "Epoch: 712 [10100/50000 (20%)]  \tLoss:   86.714333\trec:   58.490971\tkl:   28.223368\n",
      "Epoch: 712 [20100/50000 (40%)]  \tLoss:   90.464882\trec:   61.844189\tkl:   28.620697\n",
      "Epoch: 712 [30100/50000 (60%)]  \tLoss:   91.092010\trec:   62.211803\tkl:   28.880199\n",
      "Epoch: 712 [40100/50000 (80%)]  \tLoss:   87.718620\trec:   59.299149\tkl:   28.419470\n",
      "====> Epoch: 712 Average train loss: 89.1287\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9367\n",
      "Epoch: 713 [  100/50000 ( 0%)]  \tLoss:   92.015541\trec:   63.544071\tkl:   28.471474\n",
      "Epoch: 713 [10100/50000 (20%)]  \tLoss:   88.244858\trec:   59.981228\tkl:   28.263638\n",
      "Epoch: 713 [20100/50000 (40%)]  \tLoss:   90.295006\trec:   62.219505\tkl:   28.075502\n",
      "Epoch: 713 [30100/50000 (60%)]  \tLoss:   90.250542\trec:   63.097237\tkl:   27.153315\n",
      "Epoch: 713 [40100/50000 (80%)]  \tLoss:   91.076317\trec:   61.629868\tkl:   29.446449\n",
      "====> Epoch: 713 Average train loss: 89.1463\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0056\n",
      "Epoch: 714 [  100/50000 ( 0%)]  \tLoss:   89.822540\trec:   61.124783\tkl:   28.697748\n",
      "Epoch: 714 [10100/50000 (20%)]  \tLoss:   89.443222\trec:   61.156139\tkl:   28.287075\n",
      "Epoch: 714 [20100/50000 (40%)]  \tLoss:   84.791031\trec:   57.692146\tkl:   27.098890\n",
      "Epoch: 714 [30100/50000 (60%)]  \tLoss:   89.878365\trec:   62.120907\tkl:   27.757460\n",
      "Epoch: 714 [40100/50000 (80%)]  \tLoss:   90.013634\trec:   61.233173\tkl:   28.780458\n",
      "====> Epoch: 714 Average train loss: 89.1293\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9918\n",
      "Epoch: 715 [  100/50000 ( 0%)]  \tLoss:   90.997437\trec:   62.532440\tkl:   28.465004\n",
      "Epoch: 715 [10100/50000 (20%)]  \tLoss:   89.909645\trec:   61.542488\tkl:   28.367157\n",
      "Epoch: 715 [20100/50000 (40%)]  \tLoss:   86.105133\trec:   58.738533\tkl:   27.366596\n",
      "Epoch: 715 [30100/50000 (60%)]  \tLoss:   86.060753\trec:   58.120380\tkl:   27.940372\n",
      "Epoch: 715 [40100/50000 (80%)]  \tLoss:   88.468559\trec:   60.624081\tkl:   27.844479\n",
      "====> Epoch: 715 Average train loss: 89.1238\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9487\n",
      "Epoch: 716 [  100/50000 ( 0%)]  \tLoss:   86.408653\trec:   59.369648\tkl:   27.038998\n",
      "Epoch: 716 [10100/50000 (20%)]  \tLoss:   89.443253\trec:   61.197983\tkl:   28.245268\n",
      "Epoch: 716 [20100/50000 (40%)]  \tLoss:   89.753799\trec:   60.454140\tkl:   29.299660\n",
      "Epoch: 716 [30100/50000 (60%)]  \tLoss:   93.538788\trec:   64.411240\tkl:   29.127548\n",
      "Epoch: 716 [40100/50000 (80%)]  \tLoss:   88.317589\trec:   60.311142\tkl:   28.006445\n",
      "====> Epoch: 716 Average train loss: 89.1304\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8485\n",
      "Epoch: 717 [  100/50000 ( 0%)]  \tLoss:   90.987251\trec:   62.758213\tkl:   28.229044\n",
      "Epoch: 717 [10100/50000 (20%)]  \tLoss:   88.115074\trec:   60.330612\tkl:   27.784468\n",
      "Epoch: 717 [20100/50000 (40%)]  \tLoss:   89.082878\trec:   61.484463\tkl:   27.598417\n",
      "Epoch: 717 [30100/50000 (60%)]  \tLoss:   89.577690\trec:   61.579197\tkl:   27.998491\n",
      "Epoch: 717 [40100/50000 (80%)]  \tLoss:   87.528923\trec:   59.816113\tkl:   27.712807\n",
      "====> Epoch: 717 Average train loss: 89.1010\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9264\n",
      "Epoch: 718 [  100/50000 ( 0%)]  \tLoss:   87.229279\trec:   59.637707\tkl:   27.591570\n",
      "Epoch: 718 [10100/50000 (20%)]  \tLoss:   90.119667\trec:   60.980549\tkl:   29.139116\n",
      "Epoch: 718 [20100/50000 (40%)]  \tLoss:   86.690620\trec:   58.136829\tkl:   28.553789\n",
      "Epoch: 718 [30100/50000 (60%)]  \tLoss:   90.491821\trec:   61.122192\tkl:   29.369631\n",
      "Epoch: 718 [40100/50000 (80%)]  \tLoss:   87.813347\trec:   60.746548\tkl:   27.066799\n",
      "====> Epoch: 718 Average train loss: 89.1110\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0135\n",
      "Epoch: 719 [  100/50000 ( 0%)]  \tLoss:   91.376961\trec:   63.414627\tkl:   27.962334\n",
      "Epoch: 719 [10100/50000 (20%)]  \tLoss:   91.770248\trec:   62.114456\tkl:   29.655798\n",
      "Epoch: 719 [20100/50000 (40%)]  \tLoss:   90.014626\trec:   61.947578\tkl:   28.067045\n",
      "Epoch: 719 [30100/50000 (60%)]  \tLoss:   88.310417\trec:   59.986385\tkl:   28.324032\n",
      "Epoch: 719 [40100/50000 (80%)]  \tLoss:   87.942947\trec:   60.181042\tkl:   27.761909\n",
      "====> Epoch: 719 Average train loss: 89.0958\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0286\n",
      "Epoch: 720 [  100/50000 ( 0%)]  \tLoss:   92.995293\trec:   63.395828\tkl:   29.599466\n",
      "Epoch: 720 [10100/50000 (20%)]  \tLoss:   86.333862\trec:   58.163681\tkl:   28.170179\n",
      "Epoch: 720 [20100/50000 (40%)]  \tLoss:   93.737579\trec:   64.736618\tkl:   29.000961\n",
      "Epoch: 720 [30100/50000 (60%)]  \tLoss:   90.817909\trec:   62.227489\tkl:   28.590420\n",
      "Epoch: 720 [40100/50000 (80%)]  \tLoss:   86.509277\trec:   58.434975\tkl:   28.074299\n",
      "====> Epoch: 720 Average train loss: 89.1036\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.1161\n",
      "Epoch: 721 [  100/50000 ( 0%)]  \tLoss:   91.773140\trec:   63.095169\tkl:   28.677979\n",
      "Epoch: 721 [10100/50000 (20%)]  \tLoss:   90.745483\trec:   62.942471\tkl:   27.803019\n",
      "Epoch: 721 [20100/50000 (40%)]  \tLoss:   89.612221\trec:   61.344051\tkl:   28.268175\n",
      "Epoch: 721 [30100/50000 (60%)]  \tLoss:   89.858070\trec:   62.334053\tkl:   27.524021\n",
      "Epoch: 721 [40100/50000 (80%)]  \tLoss:   86.897018\trec:   59.459316\tkl:   27.437706\n",
      "====> Epoch: 721 Average train loss: 89.1265\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8843\n",
      "Epoch: 722 [  100/50000 ( 0%)]  \tLoss:   90.591164\trec:   62.547684\tkl:   28.043474\n",
      "Epoch: 722 [10100/50000 (20%)]  \tLoss:   89.676811\trec:   61.282051\tkl:   28.394766\n",
      "Epoch: 722 [20100/50000 (40%)]  \tLoss:   85.020493\trec:   57.938190\tkl:   27.082302\n",
      "Epoch: 722 [30100/50000 (60%)]  \tLoss:   90.803162\trec:   62.470482\tkl:   28.332676\n",
      "Epoch: 722 [40100/50000 (80%)]  \tLoss:   88.990372\trec:   61.071125\tkl:   27.919247\n",
      "====> Epoch: 722 Average train loss: 89.1125\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9073\n",
      "Epoch: 723 [  100/50000 ( 0%)]  \tLoss:   85.977592\trec:   59.187576\tkl:   26.790018\n",
      "Epoch: 723 [10100/50000 (20%)]  \tLoss:   90.610809\trec:   61.405849\tkl:   29.204960\n",
      "Epoch: 723 [20100/50000 (40%)]  \tLoss:   92.482491\trec:   63.509453\tkl:   28.973036\n",
      "Epoch: 723 [30100/50000 (60%)]  \tLoss:   85.657501\trec:   58.927532\tkl:   26.729963\n",
      "Epoch: 723 [40100/50000 (80%)]  \tLoss:   90.077896\trec:   61.639687\tkl:   28.438211\n",
      "====> Epoch: 723 Average train loss: 89.1057\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9474\n",
      "Epoch: 724 [  100/50000 ( 0%)]  \tLoss:   90.035957\trec:   62.079540\tkl:   27.956421\n",
      "Epoch: 724 [10100/50000 (20%)]  \tLoss:   86.242928\trec:   59.331169\tkl:   26.911760\n",
      "Epoch: 724 [20100/50000 (40%)]  \tLoss:   89.422089\trec:   61.472778\tkl:   27.949316\n",
      "Epoch: 724 [30100/50000 (60%)]  \tLoss:   90.190224\trec:   60.742947\tkl:   29.447277\n",
      "Epoch: 724 [40100/50000 (80%)]  \tLoss:   91.208229\trec:   62.182835\tkl:   29.025393\n",
      "====> Epoch: 724 Average train loss: 89.0978\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9597\n",
      "Epoch: 725 [  100/50000 ( 0%)]  \tLoss:   93.590263\trec:   65.116119\tkl:   28.474140\n",
      "Epoch: 725 [10100/50000 (20%)]  \tLoss:   85.787041\trec:   57.954826\tkl:   27.832211\n",
      "Epoch: 725 [20100/50000 (40%)]  \tLoss:   88.243279\trec:   60.818787\tkl:   27.424486\n",
      "Epoch: 725 [30100/50000 (60%)]  \tLoss:   91.086853\trec:   63.013584\tkl:   28.073273\n",
      "Epoch: 725 [40100/50000 (80%)]  \tLoss:   88.329605\trec:   61.098534\tkl:   27.231079\n",
      "====> Epoch: 725 Average train loss: 89.0956\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0467\n",
      "Epoch: 726 [  100/50000 ( 0%)]  \tLoss:   87.165352\trec:   60.308544\tkl:   26.856808\n",
      "Epoch: 726 [10100/50000 (20%)]  \tLoss:   86.547676\trec:   59.495907\tkl:   27.051762\n",
      "Epoch: 726 [20100/50000 (40%)]  \tLoss:   87.675682\trec:   59.844433\tkl:   27.831249\n",
      "Epoch: 726 [30100/50000 (60%)]  \tLoss:   90.740372\trec:   62.280331\tkl:   28.460033\n",
      "Epoch: 726 [40100/50000 (80%)]  \tLoss:   86.952820\trec:   59.306629\tkl:   27.646191\n",
      "====> Epoch: 726 Average train loss: 89.0886\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9821\n",
      "Epoch: 727 [  100/50000 ( 0%)]  \tLoss:   85.414719\trec:   57.512321\tkl:   27.902391\n",
      "Epoch: 727 [10100/50000 (20%)]  \tLoss:   88.980659\trec:   61.356674\tkl:   27.623993\n",
      "Epoch: 727 [20100/50000 (40%)]  \tLoss:   88.011192\trec:   60.455193\tkl:   27.555990\n",
      "Epoch: 727 [30100/50000 (60%)]  \tLoss:   88.487999\trec:   60.660351\tkl:   27.827646\n",
      "Epoch: 727 [40100/50000 (80%)]  \tLoss:   91.140030\trec:   63.586086\tkl:   27.553938\n",
      "====> Epoch: 727 Average train loss: 89.0972\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9115\n",
      "Epoch: 728 [  100/50000 ( 0%)]  \tLoss:   85.397850\trec:   58.348408\tkl:   27.049442\n",
      "Epoch: 728 [10100/50000 (20%)]  \tLoss:   90.337067\trec:   61.848045\tkl:   28.489023\n",
      "Epoch: 728 [20100/50000 (40%)]  \tLoss:   89.951012\trec:   61.952572\tkl:   27.998442\n",
      "Epoch: 728 [30100/50000 (60%)]  \tLoss:   87.357948\trec:   60.116882\tkl:   27.241058\n",
      "Epoch: 728 [40100/50000 (80%)]  \tLoss:   88.936600\trec:   61.765339\tkl:   27.171265\n",
      "====> Epoch: 728 Average train loss: 89.0955\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9470\n",
      "Epoch: 729 [  100/50000 ( 0%)]  \tLoss:   88.469376\trec:   59.723598\tkl:   28.745781\n",
      "Epoch: 729 [10100/50000 (20%)]  \tLoss:   81.762131\trec:   55.022781\tkl:   26.739349\n",
      "Epoch: 729 [20100/50000 (40%)]  \tLoss:   85.826225\trec:   58.280121\tkl:   27.546108\n",
      "Epoch: 729 [30100/50000 (60%)]  \tLoss:   89.343575\trec:   60.746185\tkl:   28.597382\n",
      "Epoch: 729 [40100/50000 (80%)]  \tLoss:   90.471130\trec:   61.636707\tkl:   28.834421\n",
      "====> Epoch: 729 Average train loss: 89.0735\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0286\n",
      "Epoch: 730 [  100/50000 ( 0%)]  \tLoss:   91.596718\trec:   62.129799\tkl:   29.466913\n",
      "Epoch: 730 [10100/50000 (20%)]  \tLoss:   89.971130\trec:   62.341640\tkl:   27.629490\n",
      "Epoch: 730 [20100/50000 (40%)]  \tLoss:   90.401466\trec:   62.437138\tkl:   27.964331\n",
      "Epoch: 730 [30100/50000 (60%)]  \tLoss:   86.658791\trec:   58.950184\tkl:   27.708609\n",
      "Epoch: 730 [40100/50000 (80%)]  \tLoss:   89.746948\trec:   61.144642\tkl:   28.602306\n",
      "====> Epoch: 730 Average train loss: 89.0867\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0935\n",
      "Epoch: 731 [  100/50000 ( 0%)]  \tLoss:   90.970604\trec:   62.367214\tkl:   28.603388\n",
      "Epoch: 731 [10100/50000 (20%)]  \tLoss:   86.650360\trec:   59.633110\tkl:   27.017250\n",
      "Epoch: 731 [20100/50000 (40%)]  \tLoss:   88.569580\trec:   60.467354\tkl:   28.102228\n",
      "Epoch: 731 [30100/50000 (60%)]  \tLoss:   87.586639\trec:   60.078503\tkl:   27.508135\n",
      "Epoch: 731 [40100/50000 (80%)]  \tLoss:   94.364723\trec:   65.624504\tkl:   28.740229\n",
      "====> Epoch: 731 Average train loss: 89.0786\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0341\n",
      "Epoch: 732 [  100/50000 ( 0%)]  \tLoss:   90.337196\trec:   60.938114\tkl:   29.399078\n",
      "Epoch: 732 [10100/50000 (20%)]  \tLoss:   86.108086\trec:   57.574902\tkl:   28.533182\n",
      "Epoch: 732 [20100/50000 (40%)]  \tLoss:   88.078712\trec:   60.205280\tkl:   27.873426\n",
      "Epoch: 732 [30100/50000 (60%)]  \tLoss:   88.310844\trec:   59.966404\tkl:   28.344440\n",
      "Epoch: 732 [40100/50000 (80%)]  \tLoss:   89.652382\trec:   60.197895\tkl:   29.454487\n",
      "====> Epoch: 732 Average train loss: 89.0674\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8978\n",
      "Epoch: 733 [  100/50000 ( 0%)]  \tLoss:   86.909767\trec:   58.960796\tkl:   27.948965\n",
      "Epoch: 733 [10100/50000 (20%)]  \tLoss:   91.051201\trec:   62.575500\tkl:   28.475700\n",
      "Epoch: 733 [20100/50000 (40%)]  \tLoss:   88.352280\trec:   60.524128\tkl:   27.828159\n",
      "Epoch: 733 [30100/50000 (60%)]  \tLoss:   93.517159\trec:   65.688057\tkl:   27.829098\n",
      "Epoch: 733 [40100/50000 (80%)]  \tLoss:   87.452438\trec:   59.232891\tkl:   28.219545\n",
      "====> Epoch: 733 Average train loss: 89.0762\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9908\n",
      "Epoch: 734 [  100/50000 ( 0%)]  \tLoss:   89.501030\trec:   61.248222\tkl:   28.252815\n",
      "Epoch: 734 [10100/50000 (20%)]  \tLoss:   89.895065\trec:   62.058628\tkl:   27.836439\n",
      "Epoch: 734 [20100/50000 (40%)]  \tLoss:   91.588806\trec:   63.117939\tkl:   28.470869\n",
      "Epoch: 734 [30100/50000 (60%)]  \tLoss:   88.920174\trec:   61.147228\tkl:   27.772943\n",
      "Epoch: 734 [40100/50000 (80%)]  \tLoss:   83.822128\trec:   56.470219\tkl:   27.351908\n",
      "====> Epoch: 734 Average train loss: 89.0804\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9844\n",
      "Epoch: 735 [  100/50000 ( 0%)]  \tLoss:   90.466774\trec:   61.864304\tkl:   28.602470\n",
      "Epoch: 735 [10100/50000 (20%)]  \tLoss:   90.192184\trec:   60.882557\tkl:   29.309628\n",
      "Epoch: 735 [20100/50000 (40%)]  \tLoss:   85.127968\trec:   57.668324\tkl:   27.459648\n",
      "Epoch: 735 [30100/50000 (60%)]  \tLoss:   90.591309\trec:   62.947147\tkl:   27.644165\n",
      "Epoch: 735 [40100/50000 (80%)]  \tLoss:   86.212967\trec:   59.041229\tkl:   27.171732\n",
      "====> Epoch: 735 Average train loss: 89.0556\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9998\n",
      "Epoch: 736 [  100/50000 ( 0%)]  \tLoss:   88.795830\trec:   61.099758\tkl:   27.696066\n",
      "Epoch: 736 [10100/50000 (20%)]  \tLoss:   90.444313\trec:   62.043289\tkl:   28.401030\n",
      "Epoch: 736 [20100/50000 (40%)]  \tLoss:   85.672684\trec:   57.941807\tkl:   27.730881\n",
      "Epoch: 736 [30100/50000 (60%)]  \tLoss:   85.021339\trec:   58.461121\tkl:   26.560217\n",
      "Epoch: 736 [40100/50000 (80%)]  \tLoss:   89.618179\trec:   61.492645\tkl:   28.125536\n",
      "====> Epoch: 736 Average train loss: 89.0750\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9593\n",
      "Epoch: 737 [  100/50000 ( 0%)]  \tLoss:   85.166763\trec:   57.604095\tkl:   27.562674\n",
      "Epoch: 737 [10100/50000 (20%)]  \tLoss:   91.259499\trec:   62.440918\tkl:   28.818581\n",
      "Epoch: 737 [20100/50000 (40%)]  \tLoss:   92.716988\trec:   63.860420\tkl:   28.856571\n",
      "Epoch: 737 [30100/50000 (60%)]  \tLoss:   88.047119\trec:   60.660313\tkl:   27.386808\n",
      "Epoch: 737 [40100/50000 (80%)]  \tLoss:   91.792770\trec:   62.665077\tkl:   29.127697\n",
      "====> Epoch: 737 Average train loss: 89.0618\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9796\n",
      "Epoch: 738 [  100/50000 ( 0%)]  \tLoss:   93.144371\trec:   63.322056\tkl:   29.822313\n",
      "Epoch: 738 [10100/50000 (20%)]  \tLoss:   87.161736\trec:   59.064999\tkl:   28.096737\n",
      "Epoch: 738 [20100/50000 (40%)]  \tLoss:   85.114235\trec:   57.489178\tkl:   27.625063\n",
      "Epoch: 738 [30100/50000 (60%)]  \tLoss:   90.644531\trec:   61.764114\tkl:   28.880419\n",
      "Epoch: 738 [40100/50000 (80%)]  \tLoss:   87.015083\trec:   59.275585\tkl:   27.739502\n",
      "====> Epoch: 738 Average train loss: 89.0533\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8749\n",
      "Epoch: 739 [  100/50000 ( 0%)]  \tLoss:   88.246620\trec:   60.309265\tkl:   27.937357\n",
      "Epoch: 739 [10100/50000 (20%)]  \tLoss:   90.149521\trec:   61.560379\tkl:   28.589142\n",
      "Epoch: 739 [20100/50000 (40%)]  \tLoss:   88.056892\trec:   60.870205\tkl:   27.186684\n",
      "Epoch: 739 [30100/50000 (60%)]  \tLoss:   90.357422\trec:   61.996216\tkl:   28.361210\n",
      "Epoch: 739 [40100/50000 (80%)]  \tLoss:   90.289978\trec:   61.761642\tkl:   28.528334\n",
      "====> Epoch: 739 Average train loss: 89.0559\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0115\n",
      "Epoch: 740 [  100/50000 ( 0%)]  \tLoss:   88.333038\trec:   61.010143\tkl:   27.322889\n",
      "Epoch: 740 [10100/50000 (20%)]  \tLoss:   90.414566\trec:   62.241535\tkl:   28.173031\n",
      "Epoch: 740 [20100/50000 (40%)]  \tLoss:   92.016640\trec:   63.430202\tkl:   28.586437\n",
      "Epoch: 740 [30100/50000 (60%)]  \tLoss:   91.161240\trec:   62.557087\tkl:   28.604153\n",
      "Epoch: 740 [40100/50000 (80%)]  \tLoss:   92.825058\trec:   63.460068\tkl:   29.364985\n",
      "====> Epoch: 740 Average train loss: 89.0537\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9043\n",
      "Epoch: 741 [  100/50000 ( 0%)]  \tLoss:   87.437851\trec:   58.824715\tkl:   28.613138\n",
      "Epoch: 741 [10100/50000 (20%)]  \tLoss:   89.919022\trec:   61.481060\tkl:   28.437967\n",
      "Epoch: 741 [20100/50000 (40%)]  \tLoss:   89.295670\trec:   60.456387\tkl:   28.839287\n",
      "Epoch: 741 [30100/50000 (60%)]  \tLoss:   87.680260\trec:   60.003952\tkl:   27.676308\n",
      "Epoch: 741 [40100/50000 (80%)]  \tLoss:   86.611267\trec:   59.540108\tkl:   27.071157\n",
      "====> Epoch: 741 Average train loss: 89.0535\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8553\n",
      "Epoch: 742 [  100/50000 ( 0%)]  \tLoss:   91.174767\trec:   62.594013\tkl:   28.580750\n",
      "Epoch: 742 [10100/50000 (20%)]  \tLoss:   90.054764\trec:   62.136543\tkl:   27.918217\n",
      "Epoch: 742 [20100/50000 (40%)]  \tLoss:   86.986122\trec:   58.766891\tkl:   28.219229\n",
      "Epoch: 742 [30100/50000 (60%)]  \tLoss:   88.138283\trec:   60.372612\tkl:   27.765671\n",
      "Epoch: 742 [40100/50000 (80%)]  \tLoss:   90.128906\trec:   61.840633\tkl:   28.288271\n",
      "====> Epoch: 742 Average train loss: 89.0337\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0214\n",
      "Epoch: 743 [  100/50000 ( 0%)]  \tLoss:   90.301743\trec:   61.882641\tkl:   28.419104\n",
      "Epoch: 743 [10100/50000 (20%)]  \tLoss:   89.061012\trec:   60.790813\tkl:   28.270199\n",
      "Epoch: 743 [20100/50000 (40%)]  \tLoss:   92.772911\trec:   63.727489\tkl:   29.045414\n",
      "Epoch: 743 [30100/50000 (60%)]  \tLoss:   88.850899\trec:   61.190945\tkl:   27.659946\n",
      "Epoch: 743 [40100/50000 (80%)]  \tLoss:   88.000252\trec:   61.095772\tkl:   26.904480\n",
      "====> Epoch: 743 Average train loss: 89.0394\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0344\n",
      "Epoch: 744 [  100/50000 ( 0%)]  \tLoss:   84.549118\trec:   57.423897\tkl:   27.125219\n",
      "Epoch: 744 [10100/50000 (20%)]  \tLoss:   89.205391\trec:   60.663769\tkl:   28.541620\n",
      "Epoch: 744 [20100/50000 (40%)]  \tLoss:   88.855125\trec:   60.664730\tkl:   28.190397\n",
      "Epoch: 744 [30100/50000 (60%)]  \tLoss:   90.130272\trec:   61.316940\tkl:   28.813332\n",
      "Epoch: 744 [40100/50000 (80%)]  \tLoss:   89.064636\trec:   61.532261\tkl:   27.532375\n",
      "====> Epoch: 744 Average train loss: 89.0223\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9041\n",
      "Epoch: 745 [  100/50000 ( 0%)]  \tLoss:   90.297798\trec:   61.369541\tkl:   28.928261\n",
      "Epoch: 745 [10100/50000 (20%)]  \tLoss:   89.225929\trec:   60.023262\tkl:   29.202665\n",
      "Epoch: 745 [20100/50000 (40%)]  \tLoss:   89.930397\trec:   61.864059\tkl:   28.066338\n",
      "Epoch: 745 [30100/50000 (60%)]  \tLoss:   88.707558\trec:   60.114540\tkl:   28.593012\n",
      "Epoch: 745 [40100/50000 (80%)]  \tLoss:   90.320183\trec:   62.179661\tkl:   28.140522\n",
      "====> Epoch: 745 Average train loss: 89.0224\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9305\n",
      "Epoch: 746 [  100/50000 ( 0%)]  \tLoss:   90.951950\trec:   62.030098\tkl:   28.921860\n",
      "Epoch: 746 [10100/50000 (20%)]  \tLoss:   90.376450\trec:   62.031445\tkl:   28.345009\n",
      "Epoch: 746 [20100/50000 (40%)]  \tLoss:   92.712631\trec:   63.809380\tkl:   28.903261\n",
      "Epoch: 746 [30100/50000 (60%)]  \tLoss:   88.584618\trec:   61.311131\tkl:   27.273485\n",
      "Epoch: 746 [40100/50000 (80%)]  \tLoss:   87.251083\trec:   60.123417\tkl:   27.127663\n",
      "====> Epoch: 746 Average train loss: 89.0393\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0260\n",
      "Epoch: 747 [  100/50000 ( 0%)]  \tLoss:   89.182419\trec:   61.382725\tkl:   27.799700\n",
      "Epoch: 747 [10100/50000 (20%)]  \tLoss:   92.835899\trec:   64.505951\tkl:   28.329948\n",
      "Epoch: 747 [20100/50000 (40%)]  \tLoss:   88.317970\trec:   60.214115\tkl:   28.103857\n",
      "Epoch: 747 [30100/50000 (60%)]  \tLoss:   91.484741\trec:   62.866982\tkl:   28.617758\n",
      "Epoch: 747 [40100/50000 (80%)]  \tLoss:   86.322067\trec:   59.471661\tkl:   26.850414\n",
      "====> Epoch: 747 Average train loss: 89.0458\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9254\n",
      "Epoch: 748 [  100/50000 ( 0%)]  \tLoss:   89.804764\trec:   61.065121\tkl:   28.739643\n",
      "Epoch: 748 [10100/50000 (20%)]  \tLoss:   88.150581\trec:   59.989872\tkl:   28.160707\n",
      "Epoch: 748 [20100/50000 (40%)]  \tLoss:   89.656677\trec:   62.568192\tkl:   27.088488\n",
      "Epoch: 748 [30100/50000 (60%)]  \tLoss:   91.863075\trec:   62.445370\tkl:   29.417702\n",
      "Epoch: 748 [40100/50000 (80%)]  \tLoss:   88.739845\trec:   60.716465\tkl:   28.023384\n",
      "====> Epoch: 748 Average train loss: 89.0582\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8743\n",
      "Epoch: 749 [  100/50000 ( 0%)]  \tLoss:   88.099197\trec:   60.344944\tkl:   27.754248\n",
      "Epoch: 749 [10100/50000 (20%)]  \tLoss:   86.122078\trec:   58.915031\tkl:   27.207048\n",
      "Epoch: 749 [20100/50000 (40%)]  \tLoss:   92.946533\trec:   63.978245\tkl:   28.968285\n",
      "Epoch: 749 [30100/50000 (60%)]  \tLoss:   89.202026\trec:   61.180759\tkl:   28.021263\n",
      "Epoch: 749 [40100/50000 (80%)]  \tLoss:   92.363739\trec:   63.817608\tkl:   28.546133\n",
      "====> Epoch: 749 Average train loss: 89.0205\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9137\n",
      "Epoch: 750 [  100/50000 ( 0%)]  \tLoss:   92.513283\trec:   63.809334\tkl:   28.703945\n",
      "Epoch: 750 [10100/50000 (20%)]  \tLoss:   91.009842\trec:   61.492878\tkl:   29.516968\n",
      "Epoch: 750 [20100/50000 (40%)]  \tLoss:   89.172066\trec:   61.045208\tkl:   28.126865\n",
      "Epoch: 750 [30100/50000 (60%)]  \tLoss:   88.644073\trec:   61.051804\tkl:   27.592268\n",
      "Epoch: 750 [40100/50000 (80%)]  \tLoss:   84.514679\trec:   57.370731\tkl:   27.143944\n",
      "====> Epoch: 750 Average train loss: 89.0265\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9410\n",
      "Epoch: 751 [  100/50000 ( 0%)]  \tLoss:   86.954102\trec:   59.156902\tkl:   27.797192\n",
      "Epoch: 751 [10100/50000 (20%)]  \tLoss:   89.767227\trec:   61.756130\tkl:   28.011095\n",
      "Epoch: 751 [20100/50000 (40%)]  \tLoss:   85.743904\trec:   58.058613\tkl:   27.685293\n",
      "Epoch: 751 [30100/50000 (60%)]  \tLoss:   88.903610\trec:   60.762596\tkl:   28.141020\n",
      "Epoch: 751 [40100/50000 (80%)]  \tLoss:   91.780418\trec:   63.863758\tkl:   27.916660\n",
      "====> Epoch: 751 Average train loss: 89.0311\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9196\n",
      "Epoch: 752 [  100/50000 ( 0%)]  \tLoss:   88.365417\trec:   61.004284\tkl:   27.361132\n",
      "Epoch: 752 [10100/50000 (20%)]  \tLoss:   87.746758\trec:   58.929810\tkl:   28.816948\n",
      "Epoch: 752 [20100/50000 (40%)]  \tLoss:   88.471054\trec:   60.329967\tkl:   28.141079\n",
      "Epoch: 752 [30100/50000 (60%)]  \tLoss:   84.893784\trec:   57.408272\tkl:   27.485512\n",
      "Epoch: 752 [40100/50000 (80%)]  \tLoss:   90.975075\trec:   62.105869\tkl:   28.869209\n",
      "====> Epoch: 752 Average train loss: 88.9983\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8761\n",
      "Epoch: 753 [  100/50000 ( 0%)]  \tLoss:   89.957344\trec:   61.621223\tkl:   28.336121\n",
      "Epoch: 753 [10100/50000 (20%)]  \tLoss:   89.663750\trec:   60.769062\tkl:   28.894688\n",
      "Epoch: 753 [20100/50000 (40%)]  \tLoss:   91.981308\trec:   63.343559\tkl:   28.637743\n",
      "Epoch: 753 [30100/50000 (60%)]  \tLoss:   87.441269\trec:   59.926037\tkl:   27.515226\n",
      "Epoch: 753 [40100/50000 (80%)]  \tLoss:   88.314667\trec:   59.819714\tkl:   28.494947\n",
      "====> Epoch: 753 Average train loss: 89.0145\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9204\n",
      "Epoch: 754 [  100/50000 ( 0%)]  \tLoss:   89.710136\trec:   61.117626\tkl:   28.592514\n",
      "Epoch: 754 [10100/50000 (20%)]  \tLoss:   89.350533\trec:   60.766434\tkl:   28.584101\n",
      "Epoch: 754 [20100/50000 (40%)]  \tLoss:   89.116402\trec:   60.920734\tkl:   28.195669\n",
      "Epoch: 754 [30100/50000 (60%)]  \tLoss:   87.505966\trec:   60.094707\tkl:   27.411259\n",
      "Epoch: 754 [40100/50000 (80%)]  \tLoss:   89.844101\trec:   61.049706\tkl:   28.794395\n",
      "====> Epoch: 754 Average train loss: 88.9968\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8751\n",
      "Epoch: 755 [  100/50000 ( 0%)]  \tLoss:   90.234917\trec:   61.244492\tkl:   28.990429\n",
      "Epoch: 755 [10100/50000 (20%)]  \tLoss:   89.837036\trec:   61.533020\tkl:   28.304018\n",
      "Epoch: 755 [20100/50000 (40%)]  \tLoss:   85.440445\trec:   57.888638\tkl:   27.551805\n",
      "Epoch: 755 [30100/50000 (60%)]  \tLoss:   88.137627\trec:   60.758678\tkl:   27.378944\n",
      "Epoch: 755 [40100/50000 (80%)]  \tLoss:   87.578384\trec:   59.554478\tkl:   28.023911\n",
      "====> Epoch: 755 Average train loss: 89.0113\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9680\n",
      "Epoch: 756 [  100/50000 ( 0%)]  \tLoss:   86.236031\trec:   58.099026\tkl:   28.137007\n",
      "Epoch: 756 [10100/50000 (20%)]  \tLoss:   85.245369\trec:   58.278721\tkl:   26.966644\n",
      "Epoch: 756 [20100/50000 (40%)]  \tLoss:   88.380409\trec:   60.252762\tkl:   28.127644\n",
      "Epoch: 756 [30100/50000 (60%)]  \tLoss:   89.288536\trec:   60.891556\tkl:   28.396982\n",
      "Epoch: 756 [40100/50000 (80%)]  \tLoss:   92.574020\trec:   63.682285\tkl:   28.891737\n",
      "====> Epoch: 756 Average train loss: 89.0219\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8445\n",
      "Epoch: 757 [  100/50000 ( 0%)]  \tLoss:   91.501152\trec:   62.488075\tkl:   29.013073\n",
      "Epoch: 757 [10100/50000 (20%)]  \tLoss:   92.600105\trec:   64.217400\tkl:   28.382708\n",
      "Epoch: 757 [20100/50000 (40%)]  \tLoss:   90.688026\trec:   62.247456\tkl:   28.440575\n",
      "Epoch: 757 [30100/50000 (60%)]  \tLoss:   89.315445\trec:   61.276073\tkl:   28.039379\n",
      "Epoch: 757 [40100/50000 (80%)]  \tLoss:   94.347038\trec:   65.031364\tkl:   29.315674\n",
      "====> Epoch: 757 Average train loss: 89.0124\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8540\n",
      "Epoch: 758 [  100/50000 ( 0%)]  \tLoss:   88.502205\trec:   60.628830\tkl:   27.873379\n",
      "Epoch: 758 [10100/50000 (20%)]  \tLoss:   87.639366\trec:   58.953159\tkl:   28.686205\n",
      "Epoch: 758 [20100/50000 (40%)]  \tLoss:   89.391548\trec:   61.342014\tkl:   28.049534\n",
      "Epoch: 758 [30100/50000 (60%)]  \tLoss:   89.862732\trec:   61.221943\tkl:   28.640793\n",
      "Epoch: 758 [40100/50000 (80%)]  \tLoss:   93.562050\trec:   64.254509\tkl:   29.307539\n",
      "====> Epoch: 758 Average train loss: 89.0067\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0177\n",
      "Epoch: 759 [  100/50000 ( 0%)]  \tLoss:   87.995514\trec:   60.273266\tkl:   27.722252\n",
      "Epoch: 759 [10100/50000 (20%)]  \tLoss:   93.133286\trec:   64.926277\tkl:   28.207008\n",
      "Epoch: 759 [20100/50000 (40%)]  \tLoss:   89.985756\trec:   61.856052\tkl:   28.129711\n",
      "Epoch: 759 [30100/50000 (60%)]  \tLoss:   87.357964\trec:   59.424950\tkl:   27.933018\n",
      "Epoch: 759 [40100/50000 (80%)]  \tLoss:   93.684570\trec:   64.147827\tkl:   29.536747\n",
      "====> Epoch: 759 Average train loss: 89.0028\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9954\n",
      "Epoch: 760 [  100/50000 ( 0%)]  \tLoss:   89.249321\trec:   61.102489\tkl:   28.146837\n",
      "Epoch: 760 [10100/50000 (20%)]  \tLoss:   86.208458\trec:   57.945541\tkl:   28.262909\n",
      "Epoch: 760 [20100/50000 (40%)]  \tLoss:   89.636101\trec:   61.093182\tkl:   28.542921\n",
      "Epoch: 760 [30100/50000 (60%)]  \tLoss:   88.480431\trec:   59.987804\tkl:   28.492622\n",
      "Epoch: 760 [40100/50000 (80%)]  \tLoss:   92.123756\trec:   63.452827\tkl:   28.670935\n",
      "====> Epoch: 760 Average train loss: 89.0137\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8979\n",
      "Epoch: 761 [  100/50000 ( 0%)]  \tLoss:   91.048927\trec:   62.299412\tkl:   28.749506\n",
      "Epoch: 761 [10100/50000 (20%)]  \tLoss:   85.074051\trec:   57.204636\tkl:   27.869415\n",
      "Epoch: 761 [20100/50000 (40%)]  \tLoss:   87.896606\trec:   58.749023\tkl:   29.147585\n",
      "Epoch: 761 [30100/50000 (60%)]  \tLoss:   90.414467\trec:   62.361179\tkl:   28.053286\n",
      "Epoch: 761 [40100/50000 (80%)]  \tLoss:   91.418907\trec:   61.179089\tkl:   30.239809\n",
      "====> Epoch: 761 Average train loss: 88.9871\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9631\n",
      "Epoch: 762 [  100/50000 ( 0%)]  \tLoss:   91.159317\trec:   63.125042\tkl:   28.034271\n",
      "Epoch: 762 [10100/50000 (20%)]  \tLoss:   90.135925\trec:   61.731853\tkl:   28.404072\n",
      "Epoch: 762 [20100/50000 (40%)]  \tLoss:   85.451378\trec:   58.634716\tkl:   26.816660\n",
      "Epoch: 762 [30100/50000 (60%)]  \tLoss:   90.196045\trec:   62.484890\tkl:   27.711149\n",
      "Epoch: 762 [40100/50000 (80%)]  \tLoss:   86.456299\trec:   59.319408\tkl:   27.136889\n",
      "====> Epoch: 762 Average train loss: 88.9977\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8923\n",
      "Epoch: 763 [  100/50000 ( 0%)]  \tLoss:   87.875435\trec:   60.708252\tkl:   27.167187\n",
      "Epoch: 763 [10100/50000 (20%)]  \tLoss:   89.492752\trec:   62.377361\tkl:   27.115395\n",
      "Epoch: 763 [20100/50000 (40%)]  \tLoss:   87.463608\trec:   60.089119\tkl:   27.374487\n",
      "Epoch: 763 [30100/50000 (60%)]  \tLoss:   90.593224\trec:   62.067295\tkl:   28.525932\n",
      "Epoch: 763 [40100/50000 (80%)]  \tLoss:   90.627625\trec:   61.838093\tkl:   28.789530\n",
      "====> Epoch: 763 Average train loss: 89.0089\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9403\n",
      "Epoch: 764 [  100/50000 ( 0%)]  \tLoss:   89.915016\trec:   61.546043\tkl:   28.368979\n",
      "Epoch: 764 [10100/50000 (20%)]  \tLoss:   86.819366\trec:   58.713245\tkl:   28.106117\n",
      "Epoch: 764 [20100/50000 (40%)]  \tLoss:   92.021385\trec:   63.416538\tkl:   28.604853\n",
      "Epoch: 764 [30100/50000 (60%)]  \tLoss:   87.950294\trec:   59.200390\tkl:   28.749907\n",
      "Epoch: 764 [40100/50000 (80%)]  \tLoss:   87.559776\trec:   60.277939\tkl:   27.281836\n",
      "====> Epoch: 764 Average train loss: 89.0009\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8888\n",
      "Epoch: 765 [  100/50000 ( 0%)]  \tLoss:   91.005653\trec:   61.702347\tkl:   29.303303\n",
      "Epoch: 765 [10100/50000 (20%)]  \tLoss:   89.264839\trec:   61.403496\tkl:   27.861347\n",
      "Epoch: 765 [20100/50000 (40%)]  \tLoss:   83.858238\trec:   57.203735\tkl:   26.654501\n",
      "Epoch: 765 [30100/50000 (60%)]  \tLoss:   88.410812\trec:   60.222363\tkl:   28.188444\n",
      "Epoch: 765 [40100/50000 (80%)]  \tLoss:   87.793358\trec:   60.603016\tkl:   27.190346\n",
      "====> Epoch: 765 Average train loss: 88.9747\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9060\n",
      "Epoch: 766 [  100/50000 ( 0%)]  \tLoss:   91.691147\trec:   62.978909\tkl:   28.712236\n",
      "Epoch: 766 [10100/50000 (20%)]  \tLoss:   88.683418\trec:   60.364349\tkl:   28.319061\n",
      "Epoch: 766 [20100/50000 (40%)]  \tLoss:   89.375488\trec:   60.825619\tkl:   28.549862\n",
      "Epoch: 766 [30100/50000 (60%)]  \tLoss:   87.193810\trec:   59.389404\tkl:   27.804405\n",
      "Epoch: 766 [40100/50000 (80%)]  \tLoss:   88.094826\trec:   60.063446\tkl:   28.031380\n",
      "====> Epoch: 766 Average train loss: 88.9829\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8557\n",
      "Epoch: 767 [  100/50000 ( 0%)]  \tLoss:   90.025894\trec:   60.920559\tkl:   29.105331\n",
      "Epoch: 767 [10100/50000 (20%)]  \tLoss:   91.439560\trec:   62.592556\tkl:   28.847002\n",
      "Epoch: 767 [20100/50000 (40%)]  \tLoss:   88.024345\trec:   60.234116\tkl:   27.790232\n",
      "Epoch: 767 [30100/50000 (60%)]  \tLoss:   89.330811\trec:   60.768665\tkl:   28.562143\n",
      "Epoch: 767 [40100/50000 (80%)]  \tLoss:   88.106758\trec:   59.586899\tkl:   28.519859\n",
      "====> Epoch: 767 Average train loss: 88.9844\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9174\n",
      "Epoch: 768 [  100/50000 ( 0%)]  \tLoss:   88.603394\trec:   61.049873\tkl:   27.553524\n",
      "Epoch: 768 [10100/50000 (20%)]  \tLoss:   89.086128\trec:   60.683716\tkl:   28.402416\n",
      "Epoch: 768 [20100/50000 (40%)]  \tLoss:   88.649513\trec:   60.705044\tkl:   27.944466\n",
      "Epoch: 768 [30100/50000 (60%)]  \tLoss:   86.818298\trec:   58.229237\tkl:   28.589060\n",
      "Epoch: 768 [40100/50000 (80%)]  \tLoss:   93.231422\trec:   63.897312\tkl:   29.334116\n",
      "====> Epoch: 768 Average train loss: 88.9700\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9806\n",
      "Epoch: 769 [  100/50000 ( 0%)]  \tLoss:   89.242188\trec:   60.844250\tkl:   28.397940\n",
      "Epoch: 769 [10100/50000 (20%)]  \tLoss:   86.710587\trec:   58.774540\tkl:   27.936039\n",
      "Epoch: 769 [20100/50000 (40%)]  \tLoss:   89.109970\trec:   60.173016\tkl:   28.936953\n",
      "Epoch: 769 [30100/50000 (60%)]  \tLoss:   88.551811\trec:   59.737904\tkl:   28.813915\n",
      "Epoch: 769 [40100/50000 (80%)]  \tLoss:   90.052299\trec:   61.463085\tkl:   28.589220\n",
      "====> Epoch: 769 Average train loss: 89.0052\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8407\n",
      "Epoch: 770 [  100/50000 ( 0%)]  \tLoss:   92.457573\trec:   63.303486\tkl:   29.154093\n",
      "Epoch: 770 [10100/50000 (20%)]  \tLoss:   92.258408\trec:   63.642345\tkl:   28.616058\n",
      "Epoch: 770 [20100/50000 (40%)]  \tLoss:   88.920761\trec:   60.992573\tkl:   27.928192\n",
      "Epoch: 770 [30100/50000 (60%)]  \tLoss:   85.805954\trec:   58.296101\tkl:   27.509857\n",
      "Epoch: 770 [40100/50000 (80%)]  \tLoss:   90.409019\trec:   62.461971\tkl:   27.947052\n",
      "====> Epoch: 770 Average train loss: 88.9759\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8630\n",
      "Epoch: 771 [  100/50000 ( 0%)]  \tLoss:   85.183220\trec:   57.673222\tkl:   27.509995\n",
      "Epoch: 771 [10100/50000 (20%)]  \tLoss:   88.202072\trec:   59.951340\tkl:   28.250723\n",
      "Epoch: 771 [20100/50000 (40%)]  \tLoss:   86.866608\trec:   59.080330\tkl:   27.786278\n",
      "Epoch: 771 [30100/50000 (60%)]  \tLoss:   91.305626\trec:   62.472710\tkl:   28.832920\n",
      "Epoch: 771 [40100/50000 (80%)]  \tLoss:   87.785255\trec:   59.802265\tkl:   27.982983\n",
      "====> Epoch: 771 Average train loss: 88.9482\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9433\n",
      "Epoch: 772 [  100/50000 ( 0%)]  \tLoss:   91.120720\trec:   62.744762\tkl:   28.375961\n",
      "Epoch: 772 [10100/50000 (20%)]  \tLoss:   90.412544\trec:   62.432251\tkl:   27.980297\n",
      "Epoch: 772 [20100/50000 (40%)]  \tLoss:   90.309464\trec:   61.864677\tkl:   28.444782\n",
      "Epoch: 772 [30100/50000 (60%)]  \tLoss:   88.061470\trec:   60.436062\tkl:   27.625406\n",
      "Epoch: 772 [40100/50000 (80%)]  \tLoss:   92.371124\trec:   63.886131\tkl:   28.484987\n",
      "====> Epoch: 772 Average train loss: 88.9817\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8260\n",
      "Epoch: 773 [  100/50000 ( 0%)]  \tLoss:   89.327759\trec:   60.665012\tkl:   28.662748\n",
      "Epoch: 773 [10100/50000 (20%)]  \tLoss:   88.591354\trec:   59.749783\tkl:   28.841570\n",
      "Epoch: 773 [20100/50000 (40%)]  \tLoss:   88.104805\trec:   60.276794\tkl:   27.828007\n",
      "Epoch: 773 [30100/50000 (60%)]  \tLoss:   89.698212\trec:   60.772987\tkl:   28.925226\n",
      "Epoch: 773 [40100/50000 (80%)]  \tLoss:   91.660660\trec:   64.161636\tkl:   27.499020\n",
      "====> Epoch: 773 Average train loss: 88.9450\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8404\n",
      "Epoch: 774 [  100/50000 ( 0%)]  \tLoss:   94.029526\trec:   64.785202\tkl:   29.244324\n",
      "Epoch: 774 [10100/50000 (20%)]  \tLoss:   89.213394\trec:   60.410297\tkl:   28.803101\n",
      "Epoch: 774 [20100/50000 (40%)]  \tLoss:   89.105576\trec:   60.957298\tkl:   28.148273\n",
      "Epoch: 774 [30100/50000 (60%)]  \tLoss:   85.697807\trec:   58.526726\tkl:   27.171083\n",
      "Epoch: 774 [40100/50000 (80%)]  \tLoss:   88.556160\trec:   61.244438\tkl:   27.311724\n",
      "====> Epoch: 774 Average train loss: 88.9250\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8580\n",
      "Epoch: 775 [  100/50000 ( 0%)]  \tLoss:   88.885445\trec:   60.174011\tkl:   28.711435\n",
      "Epoch: 775 [10100/50000 (20%)]  \tLoss:   89.780487\trec:   61.875179\tkl:   27.905312\n",
      "Epoch: 775 [20100/50000 (40%)]  \tLoss:   90.978806\trec:   62.017265\tkl:   28.961538\n",
      "Epoch: 775 [30100/50000 (60%)]  \tLoss:   88.236740\trec:   60.384109\tkl:   27.852631\n",
      "Epoch: 775 [40100/50000 (80%)]  \tLoss:   88.459305\trec:   60.428123\tkl:   28.031179\n",
      "====> Epoch: 775 Average train loss: 88.9540\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8736\n",
      "Epoch: 776 [  100/50000 ( 0%)]  \tLoss:   89.212189\trec:   60.701660\tkl:   28.510532\n",
      "Epoch: 776 [10100/50000 (20%)]  \tLoss:   89.440262\trec:   61.634174\tkl:   27.806087\n",
      "Epoch: 776 [20100/50000 (40%)]  \tLoss:   88.737457\trec:   60.608757\tkl:   28.128696\n",
      "Epoch: 776 [30100/50000 (60%)]  \tLoss:   87.703438\trec:   59.664131\tkl:   28.039303\n",
      "Epoch: 776 [40100/50000 (80%)]  \tLoss:   94.388443\trec:   66.031540\tkl:   28.356901\n",
      "====> Epoch: 776 Average train loss: 88.9497\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9577\n",
      "Epoch: 777 [  100/50000 ( 0%)]  \tLoss:   88.143585\trec:   59.806961\tkl:   28.336620\n",
      "Epoch: 777 [10100/50000 (20%)]  \tLoss:   87.312378\trec:   59.336296\tkl:   27.976088\n",
      "Epoch: 777 [20100/50000 (40%)]  \tLoss:   88.966034\trec:   60.435081\tkl:   28.530947\n",
      "Epoch: 777 [30100/50000 (60%)]  \tLoss:   87.969559\trec:   60.535927\tkl:   27.433632\n",
      "Epoch: 777 [40100/50000 (80%)]  \tLoss:   92.474014\trec:   63.630779\tkl:   28.843235\n",
      "====> Epoch: 777 Average train loss: 88.9397\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9256\n",
      "Epoch: 778 [  100/50000 ( 0%)]  \tLoss:   84.846115\trec:   57.849648\tkl:   26.996464\n",
      "Epoch: 778 [10100/50000 (20%)]  \tLoss:   86.499657\trec:   58.612728\tkl:   27.886925\n",
      "Epoch: 778 [20100/50000 (40%)]  \tLoss:   86.011520\trec:   58.462875\tkl:   27.548645\n",
      "Epoch: 778 [30100/50000 (60%)]  \tLoss:   89.627556\trec:   61.965778\tkl:   27.661781\n",
      "Epoch: 778 [40100/50000 (80%)]  \tLoss:   89.841972\trec:   61.716900\tkl:   28.125069\n",
      "====> Epoch: 778 Average train loss: 88.9302\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8378\n",
      "Epoch: 779 [  100/50000 ( 0%)]  \tLoss:   89.206245\trec:   61.419704\tkl:   27.786543\n",
      "Epoch: 779 [10100/50000 (20%)]  \tLoss:   88.666992\trec:   60.668163\tkl:   27.998827\n",
      "Epoch: 779 [20100/50000 (40%)]  \tLoss:   91.300026\trec:   62.736206\tkl:   28.563822\n",
      "Epoch: 779 [30100/50000 (60%)]  \tLoss:   91.561646\trec:   63.466644\tkl:   28.095001\n",
      "Epoch: 779 [40100/50000 (80%)]  \tLoss:   90.781075\trec:   62.339081\tkl:   28.441986\n",
      "====> Epoch: 779 Average train loss: 88.9396\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9736\n",
      "Epoch: 780 [  100/50000 ( 0%)]  \tLoss:   86.424324\trec:   58.960556\tkl:   27.463766\n",
      "Epoch: 780 [10100/50000 (20%)]  \tLoss:   82.773491\trec:   56.916893\tkl:   25.856602\n",
      "Epoch: 780 [20100/50000 (40%)]  \tLoss:   90.869728\trec:   61.674351\tkl:   29.195370\n",
      "Epoch: 780 [30100/50000 (60%)]  \tLoss:   90.949883\trec:   61.642338\tkl:   29.307549\n",
      "Epoch: 780 [40100/50000 (80%)]  \tLoss:   92.712723\trec:   64.179688\tkl:   28.533037\n",
      "====> Epoch: 780 Average train loss: 88.9550\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8971\n",
      "Epoch: 781 [  100/50000 ( 0%)]  \tLoss:   91.506599\trec:   62.407177\tkl:   29.099428\n",
      "Epoch: 781 [10100/50000 (20%)]  \tLoss:   90.074463\trec:   61.586895\tkl:   28.487568\n",
      "Epoch: 781 [20100/50000 (40%)]  \tLoss:   89.606033\trec:   60.651360\tkl:   28.954674\n",
      "Epoch: 781 [30100/50000 (60%)]  \tLoss:   88.605202\trec:   60.170033\tkl:   28.435171\n",
      "Epoch: 781 [40100/50000 (80%)]  \tLoss:   93.654236\trec:   64.250740\tkl:   29.403496\n",
      "====> Epoch: 781 Average train loss: 88.9274\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8741\n",
      "Epoch: 782 [  100/50000 ( 0%)]  \tLoss:   85.571739\trec:   57.837654\tkl:   27.734081\n",
      "Epoch: 782 [10100/50000 (20%)]  \tLoss:   88.986618\trec:   61.108128\tkl:   27.878490\n",
      "Epoch: 782 [20100/50000 (40%)]  \tLoss:   88.003937\trec:   60.197273\tkl:   27.806660\n",
      "Epoch: 782 [30100/50000 (60%)]  \tLoss:   87.147186\trec:   59.006859\tkl:   28.140324\n",
      "Epoch: 782 [40100/50000 (80%)]  \tLoss:   86.902519\trec:   59.976109\tkl:   26.926411\n",
      "====> Epoch: 782 Average train loss: 88.9304\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8963\n",
      "Epoch: 783 [  100/50000 ( 0%)]  \tLoss:   85.509293\trec:   58.310604\tkl:   27.198692\n",
      "Epoch: 783 [10100/50000 (20%)]  \tLoss:   86.264130\trec:   58.549961\tkl:   27.714172\n",
      "Epoch: 783 [20100/50000 (40%)]  \tLoss:   86.752739\trec:   58.965656\tkl:   27.787085\n",
      "Epoch: 783 [30100/50000 (60%)]  \tLoss:   87.382751\trec:   59.512390\tkl:   27.870363\n",
      "Epoch: 783 [40100/50000 (80%)]  \tLoss:   84.432320\trec:   56.801323\tkl:   27.630995\n",
      "====> Epoch: 783 Average train loss: 88.9343\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0839\n",
      "Epoch: 784 [  100/50000 ( 0%)]  \tLoss:   87.484352\trec:   59.559490\tkl:   27.924862\n",
      "Epoch: 784 [10100/50000 (20%)]  \tLoss:   86.007057\trec:   58.631260\tkl:   27.375801\n",
      "Epoch: 784 [20100/50000 (40%)]  \tLoss:   89.569313\trec:   60.545048\tkl:   29.024267\n",
      "Epoch: 784 [30100/50000 (60%)]  \tLoss:   91.603966\trec:   62.627655\tkl:   28.976309\n",
      "Epoch: 784 [40100/50000 (80%)]  \tLoss:   90.154716\trec:   62.088612\tkl:   28.066103\n",
      "====> Epoch: 784 Average train loss: 88.9268\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9560\n",
      "Epoch: 785 [  100/50000 ( 0%)]  \tLoss:   86.837624\trec:   59.180473\tkl:   27.657150\n",
      "Epoch: 785 [10100/50000 (20%)]  \tLoss:   89.337456\trec:   60.417576\tkl:   28.919878\n",
      "Epoch: 785 [20100/50000 (40%)]  \tLoss:   91.329803\trec:   62.896820\tkl:   28.432978\n",
      "Epoch: 785 [30100/50000 (60%)]  \tLoss:   91.948517\trec:   63.730282\tkl:   28.218237\n",
      "Epoch: 785 [40100/50000 (80%)]  \tLoss:   90.806892\trec:   61.951035\tkl:   28.855856\n",
      "====> Epoch: 785 Average train loss: 88.9378\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 92.0697\n",
      "Epoch: 786 [  100/50000 ( 0%)]  \tLoss:   88.396324\trec:   60.136448\tkl:   28.259882\n",
      "Epoch: 786 [10100/50000 (20%)]  \tLoss:   90.991264\trec:   62.662197\tkl:   28.329069\n",
      "Epoch: 786 [20100/50000 (40%)]  \tLoss:   86.027672\trec:   58.601971\tkl:   27.425697\n",
      "Epoch: 786 [30100/50000 (60%)]  \tLoss:   85.119064\trec:   57.692051\tkl:   27.427006\n",
      "Epoch: 786 [40100/50000 (80%)]  \tLoss:   89.350975\trec:   60.976501\tkl:   28.374468\n",
      "====> Epoch: 786 Average train loss: 88.9063\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8811\n",
      "Epoch: 787 [  100/50000 ( 0%)]  \tLoss:   89.039391\trec:   62.236179\tkl:   26.803215\n",
      "Epoch: 787 [10100/50000 (20%)]  \tLoss:   90.697418\trec:   62.176826\tkl:   28.520590\n",
      "Epoch: 787 [20100/50000 (40%)]  \tLoss:   86.360008\trec:   59.198620\tkl:   27.161385\n",
      "Epoch: 787 [30100/50000 (60%)]  \tLoss:   88.876381\trec:   60.695892\tkl:   28.180498\n",
      "Epoch: 787 [40100/50000 (80%)]  \tLoss:   89.043510\trec:   60.774578\tkl:   28.268932\n",
      "====> Epoch: 787 Average train loss: 88.9218\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8248\n",
      "Epoch: 788 [  100/50000 ( 0%)]  \tLoss:   93.229980\trec:   63.152477\tkl:   30.077494\n",
      "Epoch: 788 [10100/50000 (20%)]  \tLoss:   89.571968\trec:   61.286160\tkl:   28.285805\n",
      "Epoch: 788 [20100/50000 (40%)]  \tLoss:   92.226891\trec:   63.574089\tkl:   28.652798\n",
      "Epoch: 788 [30100/50000 (60%)]  \tLoss:   88.141670\trec:   59.398289\tkl:   28.743382\n",
      "Epoch: 788 [40100/50000 (80%)]  \tLoss:   90.980057\trec:   62.721512\tkl:   28.258547\n",
      "====> Epoch: 788 Average train loss: 88.9476\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8894\n",
      "Epoch: 789 [  100/50000 ( 0%)]  \tLoss:   87.715683\trec:   59.391731\tkl:   28.323944\n",
      "Epoch: 789 [10100/50000 (20%)]  \tLoss:   90.667397\trec:   61.340412\tkl:   29.326988\n",
      "Epoch: 789 [20100/50000 (40%)]  \tLoss:   86.121147\trec:   57.902832\tkl:   28.218315\n",
      "Epoch: 789 [30100/50000 (60%)]  \tLoss:   89.720428\trec:   61.684574\tkl:   28.035858\n",
      "Epoch: 789 [40100/50000 (80%)]  \tLoss:   92.629066\trec:   63.951382\tkl:   28.677691\n",
      "====> Epoch: 789 Average train loss: 88.8984\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9051\n",
      "Epoch: 790 [  100/50000 ( 0%)]  \tLoss:   87.739372\trec:   59.458584\tkl:   28.280790\n",
      "Epoch: 790 [10100/50000 (20%)]  \tLoss:   93.851227\trec:   64.877121\tkl:   28.974106\n",
      "Epoch: 790 [20100/50000 (40%)]  \tLoss:   88.034004\trec:   60.133904\tkl:   27.900103\n",
      "Epoch: 790 [30100/50000 (60%)]  \tLoss:   87.026680\trec:   59.581669\tkl:   27.445009\n",
      "Epoch: 790 [40100/50000 (80%)]  \tLoss:   91.410034\trec:   62.848156\tkl:   28.561882\n",
      "====> Epoch: 790 Average train loss: 88.8862\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8762\n",
      "Epoch: 791 [  100/50000 ( 0%)]  \tLoss:   82.694702\trec:   56.339619\tkl:   26.355093\n",
      "Epoch: 791 [10100/50000 (20%)]  \tLoss:   87.273018\trec:   60.292763\tkl:   26.980253\n",
      "Epoch: 791 [20100/50000 (40%)]  \tLoss:   90.466797\trec:   62.145546\tkl:   28.321255\n",
      "Epoch: 791 [30100/50000 (60%)]  \tLoss:   90.836342\trec:   62.375965\tkl:   28.460382\n",
      "Epoch: 791 [40100/50000 (80%)]  \tLoss:   89.364708\trec:   61.245789\tkl:   28.118919\n",
      "====> Epoch: 791 Average train loss: 88.9148\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8940\n",
      "Epoch: 792 [  100/50000 ( 0%)]  \tLoss:   88.299858\trec:   59.972870\tkl:   28.326986\n",
      "Epoch: 792 [10100/50000 (20%)]  \tLoss:   89.055191\trec:   62.096268\tkl:   26.958925\n",
      "Epoch: 792 [20100/50000 (40%)]  \tLoss:   87.817261\trec:   60.344440\tkl:   27.472816\n",
      "Epoch: 792 [30100/50000 (60%)]  \tLoss:   87.869904\trec:   60.704319\tkl:   27.165581\n",
      "Epoch: 792 [40100/50000 (80%)]  \tLoss:   91.999542\trec:   63.252480\tkl:   28.747063\n",
      "====> Epoch: 792 Average train loss: 88.9010\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8841\n",
      "Epoch: 793 [  100/50000 ( 0%)]  \tLoss:   88.134209\trec:   60.170395\tkl:   27.963816\n",
      "Epoch: 793 [10100/50000 (20%)]  \tLoss:   83.117111\trec:   55.768436\tkl:   27.348671\n",
      "Epoch: 793 [20100/50000 (40%)]  \tLoss:   91.688599\trec:   63.729481\tkl:   27.959120\n",
      "Epoch: 793 [30100/50000 (60%)]  \tLoss:   88.129333\trec:   59.093803\tkl:   29.035536\n",
      "Epoch: 793 [40100/50000 (80%)]  \tLoss:   89.475372\trec:   60.365742\tkl:   29.109625\n",
      "====> Epoch: 793 Average train loss: 88.9136\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8687\n",
      "Epoch: 794 [  100/50000 ( 0%)]  \tLoss:   88.925262\trec:   60.116562\tkl:   28.808701\n",
      "Epoch: 794 [10100/50000 (20%)]  \tLoss:   86.288071\trec:   59.501728\tkl:   26.786350\n",
      "Epoch: 794 [20100/50000 (40%)]  \tLoss:   86.878380\trec:   58.478626\tkl:   28.399750\n",
      "Epoch: 794 [30100/50000 (60%)]  \tLoss:   90.202507\trec:   62.023193\tkl:   28.179316\n",
      "Epoch: 794 [40100/50000 (80%)]  \tLoss:   87.040192\trec:   59.327309\tkl:   27.712881\n",
      "====> Epoch: 794 Average train loss: 88.8923\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8707\n",
      "Epoch: 795 [  100/50000 ( 0%)]  \tLoss:   91.663071\trec:   62.495712\tkl:   29.167360\n",
      "Epoch: 795 [10100/50000 (20%)]  \tLoss:   93.490875\trec:   65.094254\tkl:   28.396624\n",
      "Epoch: 795 [20100/50000 (40%)]  \tLoss:   91.000877\trec:   63.123241\tkl:   27.877632\n",
      "Epoch: 795 [30100/50000 (60%)]  \tLoss:   85.374840\trec:   57.641315\tkl:   27.733528\n",
      "Epoch: 795 [40100/50000 (80%)]  \tLoss:   91.259941\trec:   62.719959\tkl:   28.539986\n",
      "====> Epoch: 795 Average train loss: 88.8991\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8776\n",
      "Epoch: 796 [  100/50000 ( 0%)]  \tLoss:   90.088341\trec:   61.337635\tkl:   28.750698\n",
      "Epoch: 796 [10100/50000 (20%)]  \tLoss:   89.752769\trec:   62.200020\tkl:   27.552757\n",
      "Epoch: 796 [20100/50000 (40%)]  \tLoss:   88.172195\trec:   60.592846\tkl:   27.579350\n",
      "Epoch: 796 [30100/50000 (60%)]  \tLoss:   88.463356\trec:   60.172752\tkl:   28.290606\n",
      "Epoch: 796 [40100/50000 (80%)]  \tLoss:   91.072525\trec:   62.463963\tkl:   28.608561\n",
      "====> Epoch: 796 Average train loss: 88.8986\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7815\n",
      "Epoch: 797 [  100/50000 ( 0%)]  \tLoss:   90.950623\trec:   62.176258\tkl:   28.774370\n",
      "Epoch: 797 [10100/50000 (20%)]  \tLoss:   86.162918\trec:   58.550034\tkl:   27.612888\n",
      "Epoch: 797 [20100/50000 (40%)]  \tLoss:   85.733940\trec:   58.212528\tkl:   27.521414\n",
      "Epoch: 797 [30100/50000 (60%)]  \tLoss:   83.829109\trec:   56.743202\tkl:   27.085911\n",
      "Epoch: 797 [40100/50000 (80%)]  \tLoss:   90.915733\trec:   63.082378\tkl:   27.833357\n",
      "====> Epoch: 797 Average train loss: 88.8713\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8338\n",
      "Epoch: 798 [  100/50000 ( 0%)]  \tLoss:   87.649338\trec:   60.173084\tkl:   27.476250\n",
      "Epoch: 798 [10100/50000 (20%)]  \tLoss:   89.465797\trec:   60.926609\tkl:   28.539188\n",
      "Epoch: 798 [20100/50000 (40%)]  \tLoss:   87.036148\trec:   60.007168\tkl:   27.028988\n",
      "Epoch: 798 [30100/50000 (60%)]  \tLoss:   85.466637\trec:   56.953972\tkl:   28.512661\n",
      "Epoch: 798 [40100/50000 (80%)]  \tLoss:   87.913475\trec:   59.224686\tkl:   28.688789\n",
      "====> Epoch: 798 Average train loss: 88.8905\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9164\n",
      "Epoch: 799 [  100/50000 ( 0%)]  \tLoss:   87.453941\trec:   59.838085\tkl:   27.615858\n",
      "Epoch: 799 [10100/50000 (20%)]  \tLoss:   86.947121\trec:   58.966335\tkl:   27.980783\n",
      "Epoch: 799 [20100/50000 (40%)]  \tLoss:   86.949440\trec:   59.386765\tkl:   27.562674\n",
      "Epoch: 799 [30100/50000 (60%)]  \tLoss:   88.714958\trec:   59.542038\tkl:   29.172915\n",
      "Epoch: 799 [40100/50000 (80%)]  \tLoss:   89.355301\trec:   61.136162\tkl:   28.219137\n",
      "====> Epoch: 799 Average train loss: 88.8959\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9162\n",
      "Epoch: 800 [  100/50000 ( 0%)]  \tLoss:   89.311867\trec:   60.456692\tkl:   28.855173\n",
      "Epoch: 800 [10100/50000 (20%)]  \tLoss:   86.012436\trec:   59.297764\tkl:   26.714682\n",
      "Epoch: 800 [20100/50000 (40%)]  \tLoss:   87.221657\trec:   58.979012\tkl:   28.242643\n",
      "Epoch: 800 [30100/50000 (60%)]  \tLoss:   88.670723\trec:   60.577473\tkl:   28.093246\n",
      "Epoch: 800 [40100/50000 (80%)]  \tLoss:   90.664978\trec:   63.177090\tkl:   27.487890\n",
      "====> Epoch: 800 Average train loss: 88.9010\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9295\n",
      "Epoch: 801 [  100/50000 ( 0%)]  \tLoss:   88.469208\trec:   60.464592\tkl:   28.004614\n",
      "Epoch: 801 [10100/50000 (20%)]  \tLoss:   87.357574\trec:   59.468925\tkl:   27.888651\n",
      "Epoch: 801 [20100/50000 (40%)]  \tLoss:   92.375175\trec:   63.833515\tkl:   28.541660\n",
      "Epoch: 801 [30100/50000 (60%)]  \tLoss:   88.869141\trec:   60.859531\tkl:   28.009604\n",
      "Epoch: 801 [40100/50000 (80%)]  \tLoss:   88.341423\trec:   60.558376\tkl:   27.783047\n",
      "====> Epoch: 801 Average train loss: 88.8953\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8922\n",
      "Epoch: 802 [  100/50000 ( 0%)]  \tLoss:   84.549355\trec:   57.275635\tkl:   27.273726\n",
      "Epoch: 802 [10100/50000 (20%)]  \tLoss:   88.098640\trec:   60.331688\tkl:   27.766953\n",
      "Epoch: 802 [20100/50000 (40%)]  \tLoss:   93.633087\trec:   63.963493\tkl:   29.669584\n",
      "Epoch: 802 [30100/50000 (60%)]  \tLoss:   92.204994\trec:   62.910645\tkl:   29.294355\n",
      "Epoch: 802 [40100/50000 (80%)]  \tLoss:   87.403717\trec:   59.961704\tkl:   27.442017\n",
      "====> Epoch: 802 Average train loss: 88.8754\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9166\n",
      "Epoch: 803 [  100/50000 ( 0%)]  \tLoss:   93.844917\trec:   64.307404\tkl:   29.537510\n",
      "Epoch: 803 [10100/50000 (20%)]  \tLoss:   87.815971\trec:   59.727299\tkl:   28.088680\n",
      "Epoch: 803 [20100/50000 (40%)]  \tLoss:   90.055351\trec:   62.107449\tkl:   27.947895\n",
      "Epoch: 803 [30100/50000 (60%)]  \tLoss:   86.337921\trec:   58.559608\tkl:   27.778307\n",
      "Epoch: 803 [40100/50000 (80%)]  \tLoss:   88.564003\trec:   60.471035\tkl:   28.092964\n",
      "====> Epoch: 803 Average train loss: 88.9089\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8799\n",
      "Epoch: 804 [  100/50000 ( 0%)]  \tLoss:   86.821999\trec:   58.858265\tkl:   27.963737\n",
      "Epoch: 804 [10100/50000 (20%)]  \tLoss:   87.903954\trec:   59.273582\tkl:   28.630371\n",
      "Epoch: 804 [20100/50000 (40%)]  \tLoss:   85.429512\trec:   58.141136\tkl:   27.288376\n",
      "Epoch: 804 [30100/50000 (60%)]  \tLoss:   93.098503\trec:   64.230240\tkl:   28.868259\n",
      "Epoch: 804 [40100/50000 (80%)]  \tLoss:   89.473824\trec:   61.454178\tkl:   28.019644\n",
      "====> Epoch: 804 Average train loss: 88.8656\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8764\n",
      "Epoch: 805 [  100/50000 ( 0%)]  \tLoss:   90.016655\trec:   62.332134\tkl:   27.684528\n",
      "Epoch: 805 [10100/50000 (20%)]  \tLoss:   88.152885\trec:   60.460365\tkl:   27.692526\n",
      "Epoch: 805 [20100/50000 (40%)]  \tLoss:   91.555603\trec:   63.010647\tkl:   28.544958\n",
      "Epoch: 805 [30100/50000 (60%)]  \tLoss:   89.456451\trec:   62.320076\tkl:   27.136375\n",
      "Epoch: 805 [40100/50000 (80%)]  \tLoss:   81.028023\trec:   54.178452\tkl:   26.849575\n",
      "====> Epoch: 805 Average train loss: 88.8769\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8528\n",
      "Epoch: 806 [  100/50000 ( 0%)]  \tLoss:   90.168213\trec:   60.551361\tkl:   29.616848\n",
      "Epoch: 806 [10100/50000 (20%)]  \tLoss:   88.868843\trec:   61.542858\tkl:   27.325985\n",
      "Epoch: 806 [20100/50000 (40%)]  \tLoss:   89.262947\trec:   60.378998\tkl:   28.883951\n",
      "Epoch: 806 [30100/50000 (60%)]  \tLoss:   90.732224\trec:   63.345985\tkl:   27.386244\n",
      "Epoch: 806 [40100/50000 (80%)]  \tLoss:   91.833145\trec:   63.365459\tkl:   28.467688\n",
      "====> Epoch: 806 Average train loss: 88.8655\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9194\n",
      "Epoch: 807 [  100/50000 ( 0%)]  \tLoss:   86.978546\trec:   58.600788\tkl:   28.377756\n",
      "Epoch: 807 [10100/50000 (20%)]  \tLoss:   86.755074\trec:   58.668274\tkl:   28.086796\n",
      "Epoch: 807 [20100/50000 (40%)]  \tLoss:   89.546852\trec:   61.244080\tkl:   28.302773\n",
      "Epoch: 807 [30100/50000 (60%)]  \tLoss:   90.663071\trec:   62.202682\tkl:   28.460390\n",
      "Epoch: 807 [40100/50000 (80%)]  \tLoss:   86.885376\trec:   59.067024\tkl:   27.818352\n",
      "====> Epoch: 807 Average train loss: 88.8582\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9128\n",
      "Epoch: 808 [  100/50000 ( 0%)]  \tLoss:   85.384109\trec:   59.044258\tkl:   26.339853\n",
      "Epoch: 808 [10100/50000 (20%)]  \tLoss:   87.556343\trec:   60.123547\tkl:   27.432802\n",
      "Epoch: 808 [20100/50000 (40%)]  \tLoss:   89.932281\trec:   61.846359\tkl:   28.085928\n",
      "Epoch: 808 [30100/50000 (60%)]  \tLoss:   88.373833\trec:   59.878105\tkl:   28.495735\n",
      "Epoch: 808 [40100/50000 (80%)]  \tLoss:   87.962120\trec:   59.139832\tkl:   28.822285\n",
      "====> Epoch: 808 Average train loss: 88.8601\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8890\n",
      "Epoch: 809 [  100/50000 ( 0%)]  \tLoss:   86.817123\trec:   59.237774\tkl:   27.579350\n",
      "Epoch: 809 [10100/50000 (20%)]  \tLoss:   87.757469\trec:   60.612801\tkl:   27.144667\n",
      "Epoch: 809 [20100/50000 (40%)]  \tLoss:   90.485878\trec:   61.992645\tkl:   28.493235\n",
      "Epoch: 809 [30100/50000 (60%)]  \tLoss:   89.321518\trec:   60.637943\tkl:   28.683584\n",
      "Epoch: 809 [40100/50000 (80%)]  \tLoss:   90.371109\trec:   61.446758\tkl:   28.924351\n",
      "====> Epoch: 809 Average train loss: 88.8832\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8199\n",
      "Epoch: 810 [  100/50000 ( 0%)]  \tLoss:   86.321793\trec:   58.500488\tkl:   27.821312\n",
      "Epoch: 810 [10100/50000 (20%)]  \tLoss:   89.949005\trec:   62.457031\tkl:   27.491978\n",
      "Epoch: 810 [20100/50000 (40%)]  \tLoss:   87.437363\trec:   60.208889\tkl:   27.228476\n",
      "Epoch: 810 [30100/50000 (60%)]  \tLoss:   89.623787\trec:   61.481071\tkl:   28.142717\n",
      "Epoch: 810 [40100/50000 (80%)]  \tLoss:   92.786774\trec:   62.741074\tkl:   30.045708\n",
      "====> Epoch: 810 Average train loss: 88.8656\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8688\n",
      "Epoch: 811 [  100/50000 ( 0%)]  \tLoss:   90.094826\trec:   61.548954\tkl:   28.545868\n",
      "Epoch: 811 [10100/50000 (20%)]  \tLoss:   86.289139\trec:   57.932323\tkl:   28.356819\n",
      "Epoch: 811 [20100/50000 (40%)]  \tLoss:   88.611031\trec:   60.200809\tkl:   28.410225\n",
      "Epoch: 811 [30100/50000 (60%)]  \tLoss:   92.399170\trec:   63.291859\tkl:   29.107309\n",
      "Epoch: 811 [40100/50000 (80%)]  \tLoss:   88.941582\trec:   60.165798\tkl:   28.775780\n",
      "====> Epoch: 811 Average train loss: 88.8385\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9627\n",
      "Epoch: 812 [  100/50000 ( 0%)]  \tLoss:   89.476112\trec:   61.454853\tkl:   28.021259\n",
      "Epoch: 812 [10100/50000 (20%)]  \tLoss:   89.825798\trec:   61.287067\tkl:   28.538731\n",
      "Epoch: 812 [20100/50000 (40%)]  \tLoss:   85.132004\trec:   57.453411\tkl:   27.678589\n",
      "Epoch: 812 [30100/50000 (60%)]  \tLoss:   86.695778\trec:   59.170658\tkl:   27.525124\n",
      "Epoch: 812 [40100/50000 (80%)]  \tLoss:   89.873238\trec:   62.034389\tkl:   27.838852\n",
      "====> Epoch: 812 Average train loss: 88.8457\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8479\n",
      "Epoch: 813 [  100/50000 ( 0%)]  \tLoss:   88.352097\trec:   60.269714\tkl:   28.082380\n",
      "Epoch: 813 [10100/50000 (20%)]  \tLoss:   86.234795\trec:   58.103603\tkl:   28.131191\n",
      "Epoch: 813 [20100/50000 (40%)]  \tLoss:   87.553963\trec:   60.085598\tkl:   27.468359\n",
      "Epoch: 813 [30100/50000 (60%)]  \tLoss:   92.158432\trec:   63.011482\tkl:   29.146948\n",
      "Epoch: 813 [40100/50000 (80%)]  \tLoss:   90.105644\trec:   62.195564\tkl:   27.910078\n",
      "====> Epoch: 813 Average train loss: 88.8571\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8010\n",
      "Epoch: 814 [  100/50000 ( 0%)]  \tLoss:   90.566002\trec:   61.883320\tkl:   28.682682\n",
      "Epoch: 814 [10100/50000 (20%)]  \tLoss:   89.518631\trec:   61.069138\tkl:   28.449492\n",
      "Epoch: 814 [20100/50000 (40%)]  \tLoss:   79.366875\trec:   53.833672\tkl:   25.533207\n",
      "Epoch: 814 [30100/50000 (60%)]  \tLoss:   88.402847\trec:   60.653164\tkl:   27.749689\n",
      "Epoch: 814 [40100/50000 (80%)]  \tLoss:   91.141647\trec:   62.389915\tkl:   28.751730\n",
      "====> Epoch: 814 Average train loss: 88.8642\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7988\n",
      "Epoch: 815 [  100/50000 ( 0%)]  \tLoss:   92.185249\trec:   61.985634\tkl:   30.199621\n",
      "Epoch: 815 [10100/50000 (20%)]  \tLoss:   89.054962\trec:   60.464432\tkl:   28.590532\n",
      "Epoch: 815 [20100/50000 (40%)]  \tLoss:   90.514351\trec:   61.682049\tkl:   28.832310\n",
      "Epoch: 815 [30100/50000 (60%)]  \tLoss:   88.300583\trec:   60.088268\tkl:   28.212318\n",
      "Epoch: 815 [40100/50000 (80%)]  \tLoss:   87.065445\trec:   59.622967\tkl:   27.442474\n",
      "====> Epoch: 815 Average train loss: 88.8351\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8666\n",
      "Epoch: 816 [  100/50000 ( 0%)]  \tLoss:   91.002792\trec:   61.740891\tkl:   29.261894\n",
      "Epoch: 816 [10100/50000 (20%)]  \tLoss:   89.850876\trec:   60.845222\tkl:   29.005651\n",
      "Epoch: 816 [20100/50000 (40%)]  \tLoss:   85.610001\trec:   58.634895\tkl:   26.975101\n",
      "Epoch: 816 [30100/50000 (60%)]  \tLoss:   91.333405\trec:   62.251644\tkl:   29.081762\n",
      "Epoch: 816 [40100/50000 (80%)]  \tLoss:   87.102188\trec:   59.658737\tkl:   27.443447\n",
      "====> Epoch: 816 Average train loss: 88.8310\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9136\n",
      "Epoch: 817 [  100/50000 ( 0%)]  \tLoss:   91.104630\trec:   62.602673\tkl:   28.501947\n",
      "Epoch: 817 [10100/50000 (20%)]  \tLoss:   85.184746\trec:   57.698566\tkl:   27.486172\n",
      "Epoch: 817 [20100/50000 (40%)]  \tLoss:   86.694382\trec:   59.260632\tkl:   27.433752\n",
      "Epoch: 817 [30100/50000 (60%)]  \tLoss:   89.008514\trec:   60.397839\tkl:   28.610676\n",
      "Epoch: 817 [40100/50000 (80%)]  \tLoss:   86.004623\trec:   58.031364\tkl:   27.973263\n",
      "====> Epoch: 817 Average train loss: 88.8156\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7990\n",
      "Epoch: 818 [  100/50000 ( 0%)]  \tLoss:   89.843826\trec:   61.082146\tkl:   28.761684\n",
      "Epoch: 818 [10100/50000 (20%)]  \tLoss:   89.067749\trec:   61.565399\tkl:   27.502354\n",
      "Epoch: 818 [20100/50000 (40%)]  \tLoss:   90.788849\trec:   61.796337\tkl:   28.992510\n",
      "Epoch: 818 [30100/50000 (60%)]  \tLoss:   88.989044\trec:   61.621235\tkl:   27.367807\n",
      "Epoch: 818 [40100/50000 (80%)]  \tLoss:   89.855995\trec:   61.841797\tkl:   28.014204\n",
      "====> Epoch: 818 Average train loss: 88.8306\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9793\n",
      "Epoch: 819 [  100/50000 ( 0%)]  \tLoss:   88.136948\trec:   59.421581\tkl:   28.715376\n",
      "Epoch: 819 [10100/50000 (20%)]  \tLoss:   84.288330\trec:   56.473133\tkl:   27.815195\n",
      "Epoch: 819 [20100/50000 (40%)]  \tLoss:   91.882774\trec:   62.785023\tkl:   29.097754\n",
      "Epoch: 819 [30100/50000 (60%)]  \tLoss:   84.346054\trec:   56.038574\tkl:   28.307478\n",
      "Epoch: 819 [40100/50000 (80%)]  \tLoss:   86.784149\trec:   59.219395\tkl:   27.564758\n",
      "====> Epoch: 819 Average train loss: 88.8300\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7928\n",
      "Epoch: 820 [  100/50000 ( 0%)]  \tLoss:   86.220215\trec:   58.649891\tkl:   27.570316\n",
      "Epoch: 820 [10100/50000 (20%)]  \tLoss:   86.211700\trec:   58.137714\tkl:   28.073978\n",
      "Epoch: 820 [20100/50000 (40%)]  \tLoss:   88.210617\trec:   60.918755\tkl:   27.291861\n",
      "Epoch: 820 [30100/50000 (60%)]  \tLoss:   91.781334\trec:   62.825478\tkl:   28.955858\n",
      "Epoch: 820 [40100/50000 (80%)]  \tLoss:   90.312088\trec:   61.546326\tkl:   28.765764\n",
      "====> Epoch: 820 Average train loss: 88.8496\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8902\n",
      "Epoch: 821 [  100/50000 ( 0%)]  \tLoss:   88.956306\trec:   60.569756\tkl:   28.386557\n",
      "Epoch: 821 [10100/50000 (20%)]  \tLoss:   89.632462\trec:   61.152977\tkl:   28.479479\n",
      "Epoch: 821 [20100/50000 (40%)]  \tLoss:   90.205421\trec:   61.367477\tkl:   28.837938\n",
      "Epoch: 821 [30100/50000 (60%)]  \tLoss:   88.272141\trec:   60.015339\tkl:   28.256798\n",
      "Epoch: 821 [40100/50000 (80%)]  \tLoss:   87.493095\trec:   60.051815\tkl:   27.441278\n",
      "====> Epoch: 821 Average train loss: 88.8243\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8491\n",
      "Epoch: 822 [  100/50000 ( 0%)]  \tLoss:   86.263771\trec:   56.851063\tkl:   29.412710\n",
      "Epoch: 822 [10100/50000 (20%)]  \tLoss:   89.355919\trec:   61.557587\tkl:   27.798330\n",
      "Epoch: 822 [20100/50000 (40%)]  \tLoss:   89.363213\trec:   60.779297\tkl:   28.583918\n",
      "Epoch: 822 [30100/50000 (60%)]  \tLoss:   90.258751\trec:   62.251621\tkl:   28.007126\n",
      "Epoch: 822 [40100/50000 (80%)]  \tLoss:   86.703415\trec:   59.017372\tkl:   27.686043\n",
      "====> Epoch: 822 Average train loss: 88.8347\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8740\n",
      "Epoch: 823 [  100/50000 ( 0%)]  \tLoss:   87.674728\trec:   58.398651\tkl:   29.276075\n",
      "Epoch: 823 [10100/50000 (20%)]  \tLoss:   91.018883\trec:   62.800049\tkl:   28.218832\n",
      "Epoch: 823 [20100/50000 (40%)]  \tLoss:   94.210938\trec:   64.096924\tkl:   30.114008\n",
      "Epoch: 823 [30100/50000 (60%)]  \tLoss:   88.738670\trec:   60.353680\tkl:   28.384989\n",
      "Epoch: 823 [40100/50000 (80%)]  \tLoss:   90.954819\trec:   61.767822\tkl:   29.187006\n",
      "====> Epoch: 823 Average train loss: 88.8153\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8284\n",
      "Epoch: 824 [  100/50000 ( 0%)]  \tLoss:   84.851387\trec:   57.223534\tkl:   27.627846\n",
      "Epoch: 824 [10100/50000 (20%)]  \tLoss:   88.447136\trec:   59.786785\tkl:   28.660353\n",
      "Epoch: 824 [20100/50000 (40%)]  \tLoss:   88.095505\trec:   60.091267\tkl:   28.004242\n",
      "Epoch: 824 [30100/50000 (60%)]  \tLoss:   88.329590\trec:   59.827816\tkl:   28.501766\n",
      "Epoch: 824 [40100/50000 (80%)]  \tLoss:   89.123611\trec:   60.833984\tkl:   28.289629\n",
      "====> Epoch: 824 Average train loss: 88.8293\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9336\n",
      "Epoch: 825 [  100/50000 ( 0%)]  \tLoss:   92.102379\trec:   63.174355\tkl:   28.928032\n",
      "Epoch: 825 [10100/50000 (20%)]  \tLoss:   88.720428\trec:   60.042263\tkl:   28.678164\n",
      "Epoch: 825 [20100/50000 (40%)]  \tLoss:   90.904083\trec:   62.418015\tkl:   28.486069\n",
      "Epoch: 825 [30100/50000 (60%)]  \tLoss:   85.575615\trec:   58.218750\tkl:   27.356863\n",
      "Epoch: 825 [40100/50000 (80%)]  \tLoss:   92.054352\trec:   63.702904\tkl:   28.351444\n",
      "====> Epoch: 825 Average train loss: 88.8487\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8784\n",
      "Epoch: 826 [  100/50000 ( 0%)]  \tLoss:   86.169060\trec:   58.546616\tkl:   27.622440\n",
      "Epoch: 826 [10100/50000 (20%)]  \tLoss:   89.901207\trec:   60.843708\tkl:   29.057501\n",
      "Epoch: 826 [20100/50000 (40%)]  \tLoss:   88.998085\trec:   60.901325\tkl:   28.096752\n",
      "Epoch: 826 [30100/50000 (60%)]  \tLoss:   91.727760\trec:   63.500534\tkl:   28.227224\n",
      "Epoch: 826 [40100/50000 (80%)]  \tLoss:   92.607300\trec:   63.138382\tkl:   29.468916\n",
      "====> Epoch: 826 Average train loss: 88.8001\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8254\n",
      "Epoch: 827 [  100/50000 ( 0%)]  \tLoss:   89.384018\trec:   61.598484\tkl:   27.785542\n",
      "Epoch: 827 [10100/50000 (20%)]  \tLoss:   88.727272\trec:   61.293877\tkl:   27.433397\n",
      "Epoch: 827 [20100/50000 (40%)]  \tLoss:   88.117043\trec:   60.820568\tkl:   27.296469\n",
      "Epoch: 827 [30100/50000 (60%)]  \tLoss:   88.509140\trec:   60.477810\tkl:   28.031322\n",
      "Epoch: 827 [40100/50000 (80%)]  \tLoss:   87.365509\trec:   59.799267\tkl:   27.566235\n",
      "====> Epoch: 827 Average train loss: 88.8111\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9089\n",
      "Epoch: 828 [  100/50000 ( 0%)]  \tLoss:   84.240288\trec:   56.713737\tkl:   27.526558\n",
      "Epoch: 828 [10100/50000 (20%)]  \tLoss:   88.180641\trec:   60.365967\tkl:   27.814676\n",
      "Epoch: 828 [20100/50000 (40%)]  \tLoss:   88.094078\trec:   60.209373\tkl:   27.884712\n",
      "Epoch: 828 [30100/50000 (60%)]  \tLoss:   88.974831\trec:   60.199432\tkl:   28.775400\n",
      "Epoch: 828 [40100/50000 (80%)]  \tLoss:   86.792908\trec:   59.653740\tkl:   27.139172\n",
      "====> Epoch: 828 Average train loss: 88.8122\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8878\n",
      "Epoch: 829 [  100/50000 ( 0%)]  \tLoss:   89.325729\trec:   60.998314\tkl:   28.327415\n",
      "Epoch: 829 [10100/50000 (20%)]  \tLoss:   88.910660\trec:   61.310970\tkl:   27.599688\n",
      "Epoch: 829 [20100/50000 (40%)]  \tLoss:   88.833092\trec:   60.101223\tkl:   28.731873\n",
      "Epoch: 829 [30100/50000 (60%)]  \tLoss:   89.678452\trec:   60.760967\tkl:   28.917492\n",
      "Epoch: 829 [40100/50000 (80%)]  \tLoss:   87.478317\trec:   58.978783\tkl:   28.499533\n",
      "====> Epoch: 829 Average train loss: 88.8177\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8212\n",
      "Epoch: 830 [  100/50000 ( 0%)]  \tLoss:   89.529648\trec:   61.439892\tkl:   28.089756\n",
      "Epoch: 830 [10100/50000 (20%)]  \tLoss:   88.683868\trec:   59.616055\tkl:   29.067816\n",
      "Epoch: 830 [20100/50000 (40%)]  \tLoss:   90.387085\trec:   62.878941\tkl:   27.508144\n",
      "Epoch: 830 [30100/50000 (60%)]  \tLoss:   90.552834\trec:   62.623974\tkl:   27.928856\n",
      "Epoch: 830 [40100/50000 (80%)]  \tLoss:   89.315331\trec:   60.887146\tkl:   28.428183\n",
      "====> Epoch: 830 Average train loss: 88.8034\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9254\n",
      "Epoch: 831 [  100/50000 ( 0%)]  \tLoss:   86.439621\trec:   59.529331\tkl:   26.910286\n",
      "Epoch: 831 [10100/50000 (20%)]  \tLoss:   87.929649\trec:   59.550518\tkl:   28.379126\n",
      "Epoch: 831 [20100/50000 (40%)]  \tLoss:   87.295563\trec:   59.989178\tkl:   27.306387\n",
      "Epoch: 831 [30100/50000 (60%)]  \tLoss:   91.237930\trec:   61.987408\tkl:   29.250526\n",
      "Epoch: 831 [40100/50000 (80%)]  \tLoss:   88.239830\trec:   60.729206\tkl:   27.510624\n",
      "====> Epoch: 831 Average train loss: 88.8057\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8204\n",
      "Epoch: 832 [  100/50000 ( 0%)]  \tLoss:   84.033722\trec:   56.784470\tkl:   27.249247\n",
      "Epoch: 832 [10100/50000 (20%)]  \tLoss:   84.922081\trec:   57.369930\tkl:   27.552145\n",
      "Epoch: 832 [20100/50000 (40%)]  \tLoss:   85.097694\trec:   57.460472\tkl:   27.637217\n",
      "Epoch: 832 [30100/50000 (60%)]  \tLoss:   89.431831\trec:   61.766365\tkl:   27.665468\n",
      "Epoch: 832 [40100/50000 (80%)]  \tLoss:   88.075981\trec:   59.826229\tkl:   28.249752\n",
      "====> Epoch: 832 Average train loss: 88.7686\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9000\n",
      "Epoch: 833 [  100/50000 ( 0%)]  \tLoss:   86.701454\trec:   58.686104\tkl:   28.015354\n",
      "Epoch: 833 [10100/50000 (20%)]  \tLoss:   88.348534\trec:   59.925419\tkl:   28.423113\n",
      "Epoch: 833 [20100/50000 (40%)]  \tLoss:   89.776299\trec:   61.847301\tkl:   27.928993\n",
      "Epoch: 833 [30100/50000 (60%)]  \tLoss:   89.232246\trec:   60.886444\tkl:   28.345800\n",
      "Epoch: 833 [40100/50000 (80%)]  \tLoss:   86.503746\trec:   58.683025\tkl:   27.820719\n",
      "====> Epoch: 833 Average train loss: 88.8041\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8362\n",
      "Epoch: 834 [  100/50000 ( 0%)]  \tLoss:   89.012985\trec:   60.759262\tkl:   28.253727\n",
      "Epoch: 834 [10100/50000 (20%)]  \tLoss:   86.263092\trec:   58.852104\tkl:   27.410990\n",
      "Epoch: 834 [20100/50000 (40%)]  \tLoss:   86.883102\trec:   59.567120\tkl:   27.315981\n",
      "Epoch: 834 [30100/50000 (60%)]  \tLoss:   88.814178\trec:   60.714657\tkl:   28.099525\n",
      "Epoch: 834 [40100/50000 (80%)]  \tLoss:   89.429352\trec:   60.525036\tkl:   28.904316\n",
      "====> Epoch: 834 Average train loss: 88.7932\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9720\n",
      "Epoch: 835 [  100/50000 ( 0%)]  \tLoss:   86.070564\trec:   58.695293\tkl:   27.375273\n",
      "Epoch: 835 [10100/50000 (20%)]  \tLoss:   91.654831\trec:   62.581600\tkl:   29.073233\n",
      "Epoch: 835 [20100/50000 (40%)]  \tLoss:   88.178513\trec:   59.598015\tkl:   28.580492\n",
      "Epoch: 835 [30100/50000 (60%)]  \tLoss:   83.998329\trec:   56.936195\tkl:   27.062134\n",
      "Epoch: 835 [40100/50000 (80%)]  \tLoss:   87.304001\trec:   59.650879\tkl:   27.653130\n",
      "====> Epoch: 835 Average train loss: 88.7809\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9092\n",
      "Epoch: 836 [  100/50000 ( 0%)]  \tLoss:   88.056129\trec:   59.459774\tkl:   28.596361\n",
      "Epoch: 836 [10100/50000 (20%)]  \tLoss:   87.105835\trec:   58.515022\tkl:   28.590813\n",
      "Epoch: 836 [20100/50000 (40%)]  \tLoss:   91.354607\trec:   62.317020\tkl:   29.037582\n",
      "Epoch: 836 [30100/50000 (60%)]  \tLoss:   91.035576\trec:   62.750717\tkl:   28.284861\n",
      "Epoch: 836 [40100/50000 (80%)]  \tLoss:   89.446686\trec:   61.438259\tkl:   28.008430\n",
      "====> Epoch: 836 Average train loss: 88.7902\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9416\n",
      "Epoch: 837 [  100/50000 ( 0%)]  \tLoss:   89.906265\trec:   61.471336\tkl:   28.434927\n",
      "Epoch: 837 [10100/50000 (20%)]  \tLoss:   86.851570\trec:   58.565727\tkl:   28.285841\n",
      "Epoch: 837 [20100/50000 (40%)]  \tLoss:   90.740288\trec:   61.748295\tkl:   28.991999\n",
      "Epoch: 837 [30100/50000 (60%)]  \tLoss:   88.547325\trec:   59.442600\tkl:   29.104721\n",
      "Epoch: 837 [40100/50000 (80%)]  \tLoss:   89.990791\trec:   60.904663\tkl:   29.086126\n",
      "====> Epoch: 837 Average train loss: 88.8090\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9342\n",
      "Epoch: 838 [  100/50000 ( 0%)]  \tLoss:   91.876892\trec:   62.930252\tkl:   28.946634\n",
      "Epoch: 838 [10100/50000 (20%)]  \tLoss:   90.128044\trec:   61.488228\tkl:   28.639824\n",
      "Epoch: 838 [20100/50000 (40%)]  \tLoss:   88.419571\trec:   60.559475\tkl:   27.860086\n",
      "Epoch: 838 [30100/50000 (60%)]  \tLoss:   91.576111\trec:   62.678818\tkl:   28.897289\n",
      "Epoch: 838 [40100/50000 (80%)]  \tLoss:   89.436256\trec:   61.629833\tkl:   27.806427\n",
      "====> Epoch: 838 Average train loss: 88.7812\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9391\n",
      "Epoch: 839 [  100/50000 ( 0%)]  \tLoss:   87.784164\trec:   59.493958\tkl:   28.290213\n",
      "Epoch: 839 [10100/50000 (20%)]  \tLoss:   88.776756\trec:   60.900799\tkl:   27.875952\n",
      "Epoch: 839 [20100/50000 (40%)]  \tLoss:   89.435524\trec:   61.049023\tkl:   28.386509\n",
      "Epoch: 839 [30100/50000 (60%)]  \tLoss:   88.567970\trec:   60.520947\tkl:   28.047016\n",
      "Epoch: 839 [40100/50000 (80%)]  \tLoss:   89.572029\trec:   61.724121\tkl:   27.847910\n",
      "====> Epoch: 839 Average train loss: 88.7866\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9999\n",
      "Epoch: 840 [  100/50000 ( 0%)]  \tLoss:   92.310066\trec:   62.796398\tkl:   29.513666\n",
      "Epoch: 840 [10100/50000 (20%)]  \tLoss:   84.030319\trec:   57.416550\tkl:   26.613770\n",
      "Epoch: 840 [20100/50000 (40%)]  \tLoss:   85.782265\trec:   58.364521\tkl:   27.417738\n",
      "Epoch: 840 [30100/50000 (60%)]  \tLoss:   89.478882\trec:   61.012363\tkl:   28.466522\n",
      "Epoch: 840 [40100/50000 (80%)]  \tLoss:   90.041542\trec:   61.333221\tkl:   28.708324\n",
      "====> Epoch: 840 Average train loss: 88.7828\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8712\n",
      "Epoch: 841 [  100/50000 ( 0%)]  \tLoss:   89.142578\trec:   61.452324\tkl:   27.690248\n",
      "Epoch: 841 [10100/50000 (20%)]  \tLoss:   86.687286\trec:   59.214600\tkl:   27.472683\n",
      "Epoch: 841 [20100/50000 (40%)]  \tLoss:   89.303574\trec:   60.467812\tkl:   28.835756\n",
      "Epoch: 841 [30100/50000 (60%)]  \tLoss:   89.012711\trec:   59.997158\tkl:   29.015551\n",
      "Epoch: 841 [40100/50000 (80%)]  \tLoss:   88.296211\trec:   60.428661\tkl:   27.867548\n",
      "====> Epoch: 841 Average train loss: 88.7825\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8233\n",
      "Epoch: 842 [  100/50000 ( 0%)]  \tLoss:   85.049042\trec:   57.697567\tkl:   27.351469\n",
      "Epoch: 842 [10100/50000 (20%)]  \tLoss:   88.394821\trec:   60.553024\tkl:   27.841801\n",
      "Epoch: 842 [20100/50000 (40%)]  \tLoss:   88.751396\trec:   60.631718\tkl:   28.119680\n",
      "Epoch: 842 [30100/50000 (60%)]  \tLoss:   89.654434\trec:   61.871685\tkl:   27.782753\n",
      "Epoch: 842 [40100/50000 (80%)]  \tLoss:   90.094070\trec:   61.730774\tkl:   28.363295\n",
      "====> Epoch: 842 Average train loss: 88.7785\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8779\n",
      "Epoch: 843 [  100/50000 ( 0%)]  \tLoss:   86.908203\trec:   59.201893\tkl:   27.706303\n",
      "Epoch: 843 [10100/50000 (20%)]  \tLoss:   88.302750\trec:   60.198799\tkl:   28.103956\n",
      "Epoch: 843 [20100/50000 (40%)]  \tLoss:   87.420830\trec:   58.567417\tkl:   28.853413\n",
      "Epoch: 843 [30100/50000 (60%)]  \tLoss:   87.963318\trec:   60.790398\tkl:   27.172920\n",
      "Epoch: 843 [40100/50000 (80%)]  \tLoss:   89.807228\trec:   60.940205\tkl:   28.867022\n",
      "====> Epoch: 843 Average train loss: 88.7711\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9198\n",
      "Epoch: 844 [  100/50000 ( 0%)]  \tLoss:   87.363945\trec:   59.536510\tkl:   27.827431\n",
      "Epoch: 844 [10100/50000 (20%)]  \tLoss:   90.229668\trec:   61.709705\tkl:   28.519955\n",
      "Epoch: 844 [20100/50000 (40%)]  \tLoss:   81.647957\trec:   55.448231\tkl:   26.199726\n",
      "Epoch: 844 [30100/50000 (60%)]  \tLoss:   93.060036\trec:   63.695427\tkl:   29.364609\n",
      "Epoch: 844 [40100/50000 (80%)]  \tLoss:   84.150314\trec:   57.302254\tkl:   26.848061\n",
      "====> Epoch: 844 Average train loss: 88.7806\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9234\n",
      "Epoch: 845 [  100/50000 ( 0%)]  \tLoss:   87.055832\trec:   59.613670\tkl:   27.442158\n",
      "Epoch: 845 [10100/50000 (20%)]  \tLoss:   88.098534\trec:   59.661297\tkl:   28.437231\n",
      "Epoch: 845 [20100/50000 (40%)]  \tLoss:   88.329109\trec:   60.740009\tkl:   27.589102\n",
      "Epoch: 845 [30100/50000 (60%)]  \tLoss:   90.002663\trec:   61.467285\tkl:   28.535383\n",
      "Epoch: 845 [40100/50000 (80%)]  \tLoss:   90.528862\trec:   61.792988\tkl:   28.735876\n",
      "====> Epoch: 845 Average train loss: 88.7627\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8046\n",
      "Epoch: 846 [  100/50000 ( 0%)]  \tLoss:   87.338234\trec:   60.132481\tkl:   27.205753\n",
      "Epoch: 846 [10100/50000 (20%)]  \tLoss:   87.070007\trec:   58.945072\tkl:   28.124935\n",
      "Epoch: 846 [20100/50000 (40%)]  \tLoss:   87.865997\trec:   59.111931\tkl:   28.754066\n",
      "Epoch: 846 [30100/50000 (60%)]  \tLoss:   88.018692\trec:   60.440380\tkl:   27.578312\n",
      "Epoch: 846 [40100/50000 (80%)]  \tLoss:   89.320717\trec:   60.247604\tkl:   29.073114\n",
      "====> Epoch: 846 Average train loss: 88.7602\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8524\n",
      "Epoch: 847 [  100/50000 ( 0%)]  \tLoss:   85.917847\trec:   58.132919\tkl:   27.784925\n",
      "Epoch: 847 [10100/50000 (20%)]  \tLoss:   85.302071\trec:   57.497265\tkl:   27.804800\n",
      "Epoch: 847 [20100/50000 (40%)]  \tLoss:   91.036461\trec:   62.429424\tkl:   28.607042\n",
      "Epoch: 847 [30100/50000 (60%)]  \tLoss:   87.057549\trec:   58.787842\tkl:   28.269707\n",
      "Epoch: 847 [40100/50000 (80%)]  \tLoss:   90.365150\trec:   61.692696\tkl:   28.672464\n",
      "====> Epoch: 847 Average train loss: 88.7599\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9625\n",
      "Epoch: 848 [  100/50000 ( 0%)]  \tLoss:   88.941833\trec:   60.251335\tkl:   28.690498\n",
      "Epoch: 848 [10100/50000 (20%)]  \tLoss:   88.531425\trec:   59.887383\tkl:   28.644037\n",
      "Epoch: 848 [20100/50000 (40%)]  \tLoss:   87.214806\trec:   58.810436\tkl:   28.404360\n",
      "Epoch: 848 [30100/50000 (60%)]  \tLoss:   91.745049\trec:   63.098221\tkl:   28.646826\n",
      "Epoch: 848 [40100/50000 (80%)]  \tLoss:   86.727539\trec:   59.375790\tkl:   27.351748\n",
      "====> Epoch: 848 Average train loss: 88.7585\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7607\n",
      "Epoch: 849 [  100/50000 ( 0%)]  \tLoss:   87.895241\trec:   59.804138\tkl:   28.091101\n",
      "Epoch: 849 [10100/50000 (20%)]  \tLoss:   87.646660\trec:   59.111958\tkl:   28.534706\n",
      "Epoch: 849 [20100/50000 (40%)]  \tLoss:   87.355881\trec:   60.058018\tkl:   27.297863\n",
      "Epoch: 849 [30100/50000 (60%)]  \tLoss:   87.654266\trec:   60.911644\tkl:   26.742622\n",
      "Epoch: 849 [40100/50000 (80%)]  \tLoss:   90.642860\trec:   62.526936\tkl:   28.115923\n",
      "====> Epoch: 849 Average train loss: 88.7704\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8080\n",
      "Epoch: 850 [  100/50000 ( 0%)]  \tLoss:   90.848160\trec:   61.844250\tkl:   29.003916\n",
      "Epoch: 850 [10100/50000 (20%)]  \tLoss:   85.884315\trec:   58.500595\tkl:   27.383720\n",
      "Epoch: 850 [20100/50000 (40%)]  \tLoss:   90.170273\trec:   61.871502\tkl:   28.298769\n",
      "Epoch: 850 [30100/50000 (60%)]  \tLoss:   92.541893\trec:   63.718761\tkl:   28.823132\n",
      "Epoch: 850 [40100/50000 (80%)]  \tLoss:   88.937202\trec:   60.241680\tkl:   28.695526\n",
      "====> Epoch: 850 Average train loss: 88.7594\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8717\n",
      "Epoch: 851 [  100/50000 ( 0%)]  \tLoss:   87.681541\trec:   58.901489\tkl:   28.780052\n",
      "Epoch: 851 [10100/50000 (20%)]  \tLoss:   91.561333\trec:   63.446018\tkl:   28.115316\n",
      "Epoch: 851 [20100/50000 (40%)]  \tLoss:   85.344109\trec:   57.449001\tkl:   27.895107\n",
      "Epoch: 851 [30100/50000 (60%)]  \tLoss:   88.602333\trec:   60.002895\tkl:   28.599440\n",
      "Epoch: 851 [40100/50000 (80%)]  \tLoss:   88.659645\trec:   60.420815\tkl:   28.238827\n",
      "====> Epoch: 851 Average train loss: 88.7794\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9029\n",
      "Epoch: 852 [  100/50000 ( 0%)]  \tLoss:   87.670395\trec:   59.766884\tkl:   27.903515\n",
      "Epoch: 852 [10100/50000 (20%)]  \tLoss:   88.102242\trec:   60.564964\tkl:   27.537285\n",
      "Epoch: 852 [20100/50000 (40%)]  \tLoss:   94.808182\trec:   65.567009\tkl:   29.241177\n",
      "Epoch: 852 [30100/50000 (60%)]  \tLoss:   88.818924\trec:   60.864616\tkl:   27.954311\n",
      "Epoch: 852 [40100/50000 (80%)]  \tLoss:   89.438087\trec:   61.683483\tkl:   27.754601\n",
      "====> Epoch: 852 Average train loss: 88.7407\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8743\n",
      "Epoch: 853 [  100/50000 ( 0%)]  \tLoss:   89.073837\trec:   60.251900\tkl:   28.821938\n",
      "Epoch: 853 [10100/50000 (20%)]  \tLoss:   88.000992\trec:   58.981201\tkl:   29.019789\n",
      "Epoch: 853 [20100/50000 (40%)]  \tLoss:   90.315773\trec:   61.706188\tkl:   28.609581\n",
      "Epoch: 853 [30100/50000 (60%)]  \tLoss:   89.566612\trec:   61.379055\tkl:   28.187551\n",
      "Epoch: 853 [40100/50000 (80%)]  \tLoss:   86.384430\trec:   58.318474\tkl:   28.065962\n",
      "====> Epoch: 853 Average train loss: 88.7633\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8382\n",
      "Epoch: 854 [  100/50000 ( 0%)]  \tLoss:   89.599792\trec:   61.224560\tkl:   28.375235\n",
      "Epoch: 854 [10100/50000 (20%)]  \tLoss:   87.845039\trec:   59.326073\tkl:   28.518965\n",
      "Epoch: 854 [20100/50000 (40%)]  \tLoss:   87.067146\trec:   57.727196\tkl:   29.339952\n",
      "Epoch: 854 [30100/50000 (60%)]  \tLoss:   94.693634\trec:   65.220406\tkl:   29.473223\n",
      "Epoch: 854 [40100/50000 (80%)]  \tLoss:   88.247772\trec:   59.974758\tkl:   28.273018\n",
      "====> Epoch: 854 Average train loss: 88.7784\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9598\n",
      "Epoch: 855 [  100/50000 ( 0%)]  \tLoss:   91.677383\trec:   63.647030\tkl:   28.030352\n",
      "Epoch: 855 [10100/50000 (20%)]  \tLoss:   90.006470\trec:   60.900013\tkl:   29.106459\n",
      "Epoch: 855 [20100/50000 (40%)]  \tLoss:   89.224571\trec:   61.123161\tkl:   28.101400\n",
      "Epoch: 855 [30100/50000 (60%)]  \tLoss:   91.232948\trec:   62.424091\tkl:   28.808855\n",
      "Epoch: 855 [40100/50000 (80%)]  \tLoss:   89.946228\trec:   62.153584\tkl:   27.792648\n",
      "====> Epoch: 855 Average train loss: 88.7424\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8875\n",
      "Epoch: 856 [  100/50000 ( 0%)]  \tLoss:   92.023598\trec:   61.464725\tkl:   30.558878\n",
      "Epoch: 856 [10100/50000 (20%)]  \tLoss:   87.037361\trec:   59.931606\tkl:   27.105759\n",
      "Epoch: 856 [20100/50000 (40%)]  \tLoss:   89.696274\trec:   61.237381\tkl:   28.458897\n",
      "Epoch: 856 [30100/50000 (60%)]  \tLoss:   87.368797\trec:   59.466015\tkl:   27.902784\n",
      "Epoch: 856 [40100/50000 (80%)]  \tLoss:   89.483582\trec:   61.504559\tkl:   27.979021\n",
      "====> Epoch: 856 Average train loss: 88.7444\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8381\n",
      "Epoch: 857 [  100/50000 ( 0%)]  \tLoss:   90.543510\trec:   62.672661\tkl:   27.870850\n",
      "Epoch: 857 [10100/50000 (20%)]  \tLoss:   86.643608\trec:   58.759705\tkl:   27.883902\n",
      "Epoch: 857 [20100/50000 (40%)]  \tLoss:   89.389061\trec:   60.966648\tkl:   28.422417\n",
      "Epoch: 857 [30100/50000 (60%)]  \tLoss:   90.415840\trec:   62.059364\tkl:   28.356474\n",
      "Epoch: 857 [40100/50000 (80%)]  \tLoss:   86.555466\trec:   58.745029\tkl:   27.810434\n",
      "====> Epoch: 857 Average train loss: 88.7371\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8112\n",
      "Epoch: 858 [  100/50000 ( 0%)]  \tLoss:   88.528282\trec:   60.399258\tkl:   28.129021\n",
      "Epoch: 858 [10100/50000 (20%)]  \tLoss:   90.532257\trec:   61.652927\tkl:   28.879328\n",
      "Epoch: 858 [20100/50000 (40%)]  \tLoss:   91.533272\trec:   62.210632\tkl:   29.322636\n",
      "Epoch: 858 [30100/50000 (60%)]  \tLoss:   89.122421\trec:   60.813744\tkl:   28.308676\n",
      "Epoch: 858 [40100/50000 (80%)]  \tLoss:   91.600174\trec:   62.634594\tkl:   28.965580\n",
      "====> Epoch: 858 Average train loss: 88.7518\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8826\n",
      "Epoch: 859 [  100/50000 ( 0%)]  \tLoss:   87.019493\trec:   59.181454\tkl:   27.838032\n",
      "Epoch: 859 [10100/50000 (20%)]  \tLoss:   90.114510\trec:   61.442001\tkl:   28.672514\n",
      "Epoch: 859 [20100/50000 (40%)]  \tLoss:   88.620834\trec:   60.588181\tkl:   28.032661\n",
      "Epoch: 859 [30100/50000 (60%)]  \tLoss:   86.905411\trec:   59.200253\tkl:   27.705160\n",
      "Epoch: 859 [40100/50000 (80%)]  \tLoss:   88.405815\trec:   60.633816\tkl:   27.772003\n",
      "====> Epoch: 859 Average train loss: 88.7456\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9155\n",
      "Epoch: 860 [  100/50000 ( 0%)]  \tLoss:   88.305138\trec:   60.331882\tkl:   27.973246\n",
      "Epoch: 860 [10100/50000 (20%)]  \tLoss:   87.502983\trec:   59.906261\tkl:   27.596725\n",
      "Epoch: 860 [20100/50000 (40%)]  \tLoss:   87.848595\trec:   59.401535\tkl:   28.447056\n",
      "Epoch: 860 [30100/50000 (60%)]  \tLoss:   85.739296\trec:   58.721756\tkl:   27.017538\n",
      "Epoch: 860 [40100/50000 (80%)]  \tLoss:   89.708183\trec:   61.825985\tkl:   27.882202\n",
      "====> Epoch: 860 Average train loss: 88.7648\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8931\n",
      "Epoch: 861 [  100/50000 ( 0%)]  \tLoss:   89.978592\trec:   61.908459\tkl:   28.070129\n",
      "Epoch: 861 [10100/50000 (20%)]  \tLoss:   88.654663\trec:   60.399246\tkl:   28.255415\n",
      "Epoch: 861 [20100/50000 (40%)]  \tLoss:   87.223358\trec:   58.858593\tkl:   28.364767\n",
      "Epoch: 861 [30100/50000 (60%)]  \tLoss:   88.702789\trec:   59.862312\tkl:   28.840483\n",
      "Epoch: 861 [40100/50000 (80%)]  \tLoss:   92.286423\trec:   63.485203\tkl:   28.801220\n",
      "====> Epoch: 861 Average train loss: 88.7338\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9567\n",
      "Epoch: 862 [  100/50000 ( 0%)]  \tLoss:   87.283615\trec:   59.502495\tkl:   27.781122\n",
      "Epoch: 862 [10100/50000 (20%)]  \tLoss:   87.503403\trec:   59.577518\tkl:   27.925890\n",
      "Epoch: 862 [20100/50000 (40%)]  \tLoss:   91.171600\trec:   62.480057\tkl:   28.691538\n",
      "Epoch: 862 [30100/50000 (60%)]  \tLoss:   90.470230\trec:   62.374592\tkl:   28.095638\n",
      "Epoch: 862 [40100/50000 (80%)]  \tLoss:   88.016785\trec:   60.428562\tkl:   27.588223\n",
      "====> Epoch: 862 Average train loss: 88.7247\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8175\n",
      "Epoch: 863 [  100/50000 ( 0%)]  \tLoss:   88.417755\trec:   60.898865\tkl:   27.518881\n",
      "Epoch: 863 [10100/50000 (20%)]  \tLoss:   86.167381\trec:   58.559963\tkl:   27.607416\n",
      "Epoch: 863 [20100/50000 (40%)]  \tLoss:   89.833237\trec:   61.322136\tkl:   28.511105\n",
      "Epoch: 863 [30100/50000 (60%)]  \tLoss:   92.569061\trec:   63.408630\tkl:   29.160435\n",
      "Epoch: 863 [40100/50000 (80%)]  \tLoss:   84.539558\trec:   57.094364\tkl:   27.445192\n",
      "====> Epoch: 863 Average train loss: 88.7341\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8971\n",
      "Epoch: 864 [  100/50000 ( 0%)]  \tLoss:   92.304428\trec:   63.429970\tkl:   28.874468\n",
      "Epoch: 864 [10100/50000 (20%)]  \tLoss:   83.996765\trec:   56.827507\tkl:   27.169258\n",
      "Epoch: 864 [20100/50000 (40%)]  \tLoss:   91.090973\trec:   61.749168\tkl:   29.341810\n",
      "Epoch: 864 [30100/50000 (60%)]  \tLoss:   88.696342\trec:   60.439587\tkl:   28.256762\n",
      "Epoch: 864 [40100/50000 (80%)]  \tLoss:   89.625145\trec:   60.507507\tkl:   29.117634\n",
      "====> Epoch: 864 Average train loss: 88.7040\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9163\n",
      "Epoch: 865 [  100/50000 ( 0%)]  \tLoss:   86.179077\trec:   58.901180\tkl:   27.277901\n",
      "Epoch: 865 [10100/50000 (20%)]  \tLoss:   86.637100\trec:   59.062328\tkl:   27.574770\n",
      "Epoch: 865 [20100/50000 (40%)]  \tLoss:   87.990562\trec:   59.847675\tkl:   28.142895\n",
      "Epoch: 865 [30100/50000 (60%)]  \tLoss:   87.395973\trec:   59.531693\tkl:   27.864277\n",
      "Epoch: 865 [40100/50000 (80%)]  \tLoss:   88.569565\trec:   60.171688\tkl:   28.397879\n",
      "====> Epoch: 865 Average train loss: 88.7428\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7718\n",
      "Epoch: 866 [  100/50000 ( 0%)]  \tLoss:   88.567802\trec:   61.343216\tkl:   27.224583\n",
      "Epoch: 866 [10100/50000 (20%)]  \tLoss:   91.161003\trec:   63.074810\tkl:   28.086195\n",
      "Epoch: 866 [20100/50000 (40%)]  \tLoss:   86.586441\trec:   59.161903\tkl:   27.424545\n",
      "Epoch: 866 [30100/50000 (60%)]  \tLoss:   87.886421\trec:   60.189888\tkl:   27.696543\n",
      "Epoch: 866 [40100/50000 (80%)]  \tLoss:   90.593155\trec:   62.175491\tkl:   28.417658\n",
      "====> Epoch: 866 Average train loss: 88.7321\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7289\n",
      "Epoch: 867 [  100/50000 ( 0%)]  \tLoss:   89.136772\trec:   61.150078\tkl:   27.986694\n",
      "Epoch: 867 [10100/50000 (20%)]  \tLoss:   89.928551\trec:   61.505234\tkl:   28.423319\n",
      "Epoch: 867 [20100/50000 (40%)]  \tLoss:   88.134995\trec:   59.716728\tkl:   28.418270\n",
      "Epoch: 867 [30100/50000 (60%)]  \tLoss:   90.282097\trec:   61.437782\tkl:   28.844316\n",
      "Epoch: 867 [40100/50000 (80%)]  \tLoss:   87.450974\trec:   59.663769\tkl:   27.787207\n",
      "====> Epoch: 867 Average train loss: 88.7112\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8251\n",
      "Epoch: 868 [  100/50000 ( 0%)]  \tLoss:   89.480850\trec:   61.328201\tkl:   28.152643\n",
      "Epoch: 868 [10100/50000 (20%)]  \tLoss:   89.445610\trec:   60.691662\tkl:   28.753950\n",
      "Epoch: 868 [20100/50000 (40%)]  \tLoss:   85.977104\trec:   58.905067\tkl:   27.072039\n",
      "Epoch: 868 [30100/50000 (60%)]  \tLoss:   91.073982\trec:   61.592255\tkl:   29.481724\n",
      "Epoch: 868 [40100/50000 (80%)]  \tLoss:   86.947067\trec:   58.830757\tkl:   28.116308\n",
      "====> Epoch: 868 Average train loss: 88.7336\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8456\n",
      "Epoch: 869 [  100/50000 ( 0%)]  \tLoss:   88.314743\trec:   59.883823\tkl:   28.430918\n",
      "Epoch: 869 [10100/50000 (20%)]  \tLoss:   90.624718\trec:   62.248993\tkl:   28.375723\n",
      "Epoch: 869 [20100/50000 (40%)]  \tLoss:   84.482437\trec:   57.457745\tkl:   27.024696\n",
      "Epoch: 869 [30100/50000 (60%)]  \tLoss:   90.518326\trec:   61.644268\tkl:   28.874060\n",
      "Epoch: 869 [40100/50000 (80%)]  \tLoss:   87.489471\trec:   60.574516\tkl:   26.914961\n",
      "====> Epoch: 869 Average train loss: 88.7365\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8053\n",
      "Epoch: 870 [  100/50000 ( 0%)]  \tLoss:   90.333862\trec:   61.441643\tkl:   28.892225\n",
      "Epoch: 870 [10100/50000 (20%)]  \tLoss:   87.368149\trec:   59.283207\tkl:   28.084946\n",
      "Epoch: 870 [20100/50000 (40%)]  \tLoss:   91.659340\trec:   62.490871\tkl:   29.168474\n",
      "Epoch: 870 [30100/50000 (60%)]  \tLoss:   90.537811\trec:   61.910225\tkl:   28.627592\n",
      "Epoch: 870 [40100/50000 (80%)]  \tLoss:   89.149483\trec:   61.208183\tkl:   27.941298\n",
      "====> Epoch: 870 Average train loss: 88.6922\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8004\n",
      "Epoch: 871 [  100/50000 ( 0%)]  \tLoss:   90.363983\trec:   61.688381\tkl:   28.675600\n",
      "Epoch: 871 [10100/50000 (20%)]  \tLoss:   88.890839\trec:   61.531937\tkl:   27.358900\n",
      "Epoch: 871 [20100/50000 (40%)]  \tLoss:   90.706680\trec:   62.227009\tkl:   28.479670\n",
      "Epoch: 871 [30100/50000 (60%)]  \tLoss:   89.726151\trec:   61.777504\tkl:   27.948648\n",
      "Epoch: 871 [40100/50000 (80%)]  \tLoss:   91.226990\trec:   62.142021\tkl:   29.084974\n",
      "====> Epoch: 871 Average train loss: 88.7198\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8775\n",
      "Epoch: 872 [  100/50000 ( 0%)]  \tLoss:   87.922516\trec:   58.818447\tkl:   29.104067\n",
      "Epoch: 872 [10100/50000 (20%)]  \tLoss:   88.266357\trec:   61.440865\tkl:   26.825493\n",
      "Epoch: 872 [20100/50000 (40%)]  \tLoss:   89.867058\trec:   61.675774\tkl:   28.191280\n",
      "Epoch: 872 [30100/50000 (60%)]  \tLoss:   88.476974\trec:   60.224178\tkl:   28.252798\n",
      "Epoch: 872 [40100/50000 (80%)]  \tLoss:   86.597885\trec:   59.015347\tkl:   27.582539\n",
      "====> Epoch: 872 Average train loss: 88.7090\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8129\n",
      "Epoch: 873 [  100/50000 ( 0%)]  \tLoss:   88.579559\trec:   60.507568\tkl:   28.071991\n",
      "Epoch: 873 [10100/50000 (20%)]  \tLoss:   87.720436\trec:   60.099216\tkl:   27.621218\n",
      "Epoch: 873 [20100/50000 (40%)]  \tLoss:   88.290665\trec:   59.950665\tkl:   28.340002\n",
      "Epoch: 873 [30100/50000 (60%)]  \tLoss:   87.512138\trec:   59.888641\tkl:   27.623495\n",
      "Epoch: 873 [40100/50000 (80%)]  \tLoss:   85.133232\trec:   57.072517\tkl:   28.060709\n",
      "====> Epoch: 873 Average train loss: 88.7196\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7776\n",
      "Epoch: 874 [  100/50000 ( 0%)]  \tLoss:   90.569069\trec:   61.752441\tkl:   28.816629\n",
      "Epoch: 874 [10100/50000 (20%)]  \tLoss:   85.989845\trec:   57.304848\tkl:   28.684990\n",
      "Epoch: 874 [20100/50000 (40%)]  \tLoss:   85.627426\trec:   58.953217\tkl:   26.674212\n",
      "Epoch: 874 [30100/50000 (60%)]  \tLoss:   87.672546\trec:   60.236492\tkl:   27.436054\n",
      "Epoch: 874 [40100/50000 (80%)]  \tLoss:   90.526466\trec:   61.788383\tkl:   28.738085\n",
      "====> Epoch: 874 Average train loss: 88.6924\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7976\n",
      "Epoch: 875 [  100/50000 ( 0%)]  \tLoss:   90.616364\trec:   61.812107\tkl:   28.804255\n",
      "Epoch: 875 [10100/50000 (20%)]  \tLoss:   91.988380\trec:   62.482323\tkl:   29.506054\n",
      "Epoch: 875 [20100/50000 (40%)]  \tLoss:   87.532974\trec:   60.114037\tkl:   27.418940\n",
      "Epoch: 875 [30100/50000 (60%)]  \tLoss:   92.944458\trec:   62.627216\tkl:   30.317247\n",
      "Epoch: 875 [40100/50000 (80%)]  \tLoss:   86.926910\trec:   58.880341\tkl:   28.046574\n",
      "====> Epoch: 875 Average train loss: 88.7199\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7666\n",
      "Epoch: 876 [  100/50000 ( 0%)]  \tLoss:   85.343849\trec:   58.202282\tkl:   27.141562\n",
      "Epoch: 876 [10100/50000 (20%)]  \tLoss:   87.686508\trec:   59.483776\tkl:   28.202736\n",
      "Epoch: 876 [20100/50000 (40%)]  \tLoss:   87.677071\trec:   60.151035\tkl:   27.526030\n",
      "Epoch: 876 [30100/50000 (60%)]  \tLoss:   85.420486\trec:   57.632450\tkl:   27.788042\n",
      "Epoch: 876 [40100/50000 (80%)]  \tLoss:   86.970932\trec:   59.030342\tkl:   27.940599\n",
      "====> Epoch: 876 Average train loss: 88.6962\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8538\n",
      "Epoch: 877 [  100/50000 ( 0%)]  \tLoss:   86.738335\trec:   57.829147\tkl:   28.909189\n",
      "Epoch: 877 [10100/50000 (20%)]  \tLoss:   87.945679\trec:   59.544052\tkl:   28.401634\n",
      "Epoch: 877 [20100/50000 (40%)]  \tLoss:   92.736061\trec:   63.038895\tkl:   29.697166\n",
      "Epoch: 877 [30100/50000 (60%)]  \tLoss:   84.240776\trec:   57.355488\tkl:   26.885290\n",
      "Epoch: 877 [40100/50000 (80%)]  \tLoss:   88.077354\trec:   59.287029\tkl:   28.790321\n",
      "====> Epoch: 877 Average train loss: 88.6963\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7524\n",
      "Epoch: 878 [  100/50000 ( 0%)]  \tLoss:   84.862160\trec:   57.419643\tkl:   27.442514\n",
      "Epoch: 878 [10100/50000 (20%)]  \tLoss:   91.616470\trec:   63.226841\tkl:   28.389631\n",
      "Epoch: 878 [20100/50000 (40%)]  \tLoss:   84.969337\trec:   58.114861\tkl:   26.854471\n",
      "Epoch: 878 [30100/50000 (60%)]  \tLoss:   87.528511\trec:   59.167839\tkl:   28.360670\n",
      "Epoch: 878 [40100/50000 (80%)]  \tLoss:   91.814278\trec:   63.764843\tkl:   28.049438\n",
      "====> Epoch: 878 Average train loss: 88.6986\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8121\n",
      "Epoch: 879 [  100/50000 ( 0%)]  \tLoss:   87.648621\trec:   59.013485\tkl:   28.635134\n",
      "Epoch: 879 [10100/50000 (20%)]  \tLoss:   93.419708\trec:   64.551300\tkl:   28.868412\n",
      "Epoch: 879 [20100/50000 (40%)]  \tLoss:   85.269859\trec:   57.639832\tkl:   27.630024\n",
      "Epoch: 879 [30100/50000 (60%)]  \tLoss:   89.678696\trec:   61.190010\tkl:   28.488691\n",
      "Epoch: 879 [40100/50000 (80%)]  \tLoss:   87.722946\trec:   60.513985\tkl:   27.208969\n",
      "====> Epoch: 879 Average train loss: 88.6893\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9103\n",
      "Epoch: 880 [  100/50000 ( 0%)]  \tLoss:   86.572319\trec:   59.742714\tkl:   26.829609\n",
      "Epoch: 880 [10100/50000 (20%)]  \tLoss:   92.988052\trec:   63.933285\tkl:   29.054771\n",
      "Epoch: 880 [20100/50000 (40%)]  \tLoss:   86.128036\trec:   57.564079\tkl:   28.563955\n",
      "Epoch: 880 [30100/50000 (60%)]  \tLoss:   89.775307\trec:   60.799427\tkl:   28.975887\n",
      "Epoch: 880 [40100/50000 (80%)]  \tLoss:   92.615837\trec:   63.899239\tkl:   28.716606\n",
      "====> Epoch: 880 Average train loss: 88.6857\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9622\n",
      "Epoch: 881 [  100/50000 ( 0%)]  \tLoss:   87.881172\trec:   59.170135\tkl:   28.711039\n",
      "Epoch: 881 [10100/50000 (20%)]  \tLoss:   89.766228\trec:   61.412563\tkl:   28.353672\n",
      "Epoch: 881 [20100/50000 (40%)]  \tLoss:   86.053329\trec:   57.887859\tkl:   28.165466\n",
      "Epoch: 881 [30100/50000 (60%)]  \tLoss:   90.940605\trec:   62.159206\tkl:   28.781395\n",
      "Epoch: 881 [40100/50000 (80%)]  \tLoss:   89.003555\trec:   60.705311\tkl:   28.298241\n",
      "====> Epoch: 881 Average train loss: 88.6819\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8226\n",
      "Epoch: 882 [  100/50000 ( 0%)]  \tLoss:   86.936356\trec:   58.890751\tkl:   28.045605\n",
      "Epoch: 882 [10100/50000 (20%)]  \tLoss:   87.260796\trec:   59.188396\tkl:   28.072397\n",
      "Epoch: 882 [20100/50000 (40%)]  \tLoss:   86.695587\trec:   59.125690\tkl:   27.569887\n",
      "Epoch: 882 [30100/50000 (60%)]  \tLoss:   89.612526\trec:   61.268154\tkl:   28.344374\n",
      "Epoch: 882 [40100/50000 (80%)]  \tLoss:   88.740997\trec:   60.209675\tkl:   28.531315\n",
      "====> Epoch: 882 Average train loss: 88.6895\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7853\n",
      "Epoch: 883 [  100/50000 ( 0%)]  \tLoss:   89.814713\trec:   61.521355\tkl:   28.293358\n",
      "Epoch: 883 [10100/50000 (20%)]  \tLoss:   89.091835\trec:   60.796825\tkl:   28.295012\n",
      "Epoch: 883 [20100/50000 (40%)]  \tLoss:   91.282158\trec:   62.071613\tkl:   29.210545\n",
      "Epoch: 883 [30100/50000 (60%)]  \tLoss:   89.692123\trec:   61.302586\tkl:   28.389544\n",
      "Epoch: 883 [40100/50000 (80%)]  \tLoss:   92.891571\trec:   62.706615\tkl:   30.184956\n",
      "====> Epoch: 883 Average train loss: 88.6672\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8847\n",
      "Epoch: 884 [  100/50000 ( 0%)]  \tLoss:   88.906616\trec:   61.535271\tkl:   27.371347\n",
      "Epoch: 884 [10100/50000 (20%)]  \tLoss:   88.169922\trec:   59.623158\tkl:   28.546757\n",
      "Epoch: 884 [20100/50000 (40%)]  \tLoss:   91.817490\trec:   62.458260\tkl:   29.359228\n",
      "Epoch: 884 [30100/50000 (60%)]  \tLoss:   93.017754\trec:   63.731342\tkl:   29.286411\n",
      "Epoch: 884 [40100/50000 (80%)]  \tLoss:   88.484306\trec:   60.287361\tkl:   28.196939\n",
      "====> Epoch: 884 Average train loss: 88.6878\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8575\n",
      "Epoch: 885 [  100/50000 ( 0%)]  \tLoss:   85.422195\trec:   58.005215\tkl:   27.416983\n",
      "Epoch: 885 [10100/50000 (20%)]  \tLoss:   88.592026\trec:   60.554203\tkl:   28.037827\n",
      "Epoch: 885 [20100/50000 (40%)]  \tLoss:   91.895973\trec:   62.587376\tkl:   29.308603\n",
      "Epoch: 885 [30100/50000 (60%)]  \tLoss:   88.975761\trec:   60.944138\tkl:   28.031616\n",
      "Epoch: 885 [40100/50000 (80%)]  \tLoss:   88.675262\trec:   59.767948\tkl:   28.907316\n",
      "====> Epoch: 885 Average train loss: 88.6613\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8393\n",
      "Epoch: 886 [  100/50000 ( 0%)]  \tLoss:   91.183807\trec:   62.491058\tkl:   28.692751\n",
      "Epoch: 886 [10100/50000 (20%)]  \tLoss:   87.527184\trec:   59.400917\tkl:   28.126272\n",
      "Epoch: 886 [20100/50000 (40%)]  \tLoss:   89.622993\trec:   60.980911\tkl:   28.642084\n",
      "Epoch: 886 [30100/50000 (60%)]  \tLoss:   90.013771\trec:   62.377853\tkl:   27.635912\n",
      "Epoch: 886 [40100/50000 (80%)]  \tLoss:   88.581108\trec:   60.849003\tkl:   27.732103\n",
      "====> Epoch: 886 Average train loss: 88.6672\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8872\n",
      "Epoch: 887 [  100/50000 ( 0%)]  \tLoss:   88.333046\trec:   60.362099\tkl:   27.970947\n",
      "Epoch: 887 [10100/50000 (20%)]  \tLoss:   88.737061\trec:   60.224712\tkl:   28.512348\n",
      "Epoch: 887 [20100/50000 (40%)]  \tLoss:   92.363800\trec:   62.421154\tkl:   29.942638\n",
      "Epoch: 887 [30100/50000 (60%)]  \tLoss:   90.206017\trec:   61.306347\tkl:   28.899662\n",
      "Epoch: 887 [40100/50000 (80%)]  \tLoss:   89.115547\trec:   60.959263\tkl:   28.156288\n",
      "====> Epoch: 887 Average train loss: 88.6709\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8935\n",
      "Epoch: 888 [  100/50000 ( 0%)]  \tLoss:   88.192085\trec:   60.415985\tkl:   27.776100\n",
      "Epoch: 888 [10100/50000 (20%)]  \tLoss:   89.742439\trec:   60.645164\tkl:   29.097275\n",
      "Epoch: 888 [20100/50000 (40%)]  \tLoss:   90.991287\trec:   62.830879\tkl:   28.160410\n",
      "Epoch: 888 [30100/50000 (60%)]  \tLoss:   89.402908\trec:   61.454506\tkl:   27.948402\n",
      "Epoch: 888 [40100/50000 (80%)]  \tLoss:   89.783478\trec:   60.436497\tkl:   29.346972\n",
      "====> Epoch: 888 Average train loss: 88.6691\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8313\n",
      "Epoch: 889 [  100/50000 ( 0%)]  \tLoss:   86.036934\trec:   58.535770\tkl:   27.501163\n",
      "Epoch: 889 [10100/50000 (20%)]  \tLoss:   89.818298\trec:   61.470345\tkl:   28.347954\n",
      "Epoch: 889 [20100/50000 (40%)]  \tLoss:   85.645584\trec:   58.094612\tkl:   27.550972\n",
      "Epoch: 889 [30100/50000 (60%)]  \tLoss:   91.032234\trec:   62.168037\tkl:   28.864197\n",
      "Epoch: 889 [40100/50000 (80%)]  \tLoss:   89.514099\trec:   61.628525\tkl:   27.885576\n",
      "====> Epoch: 889 Average train loss: 88.6687\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8850\n",
      "Epoch: 890 [  100/50000 ( 0%)]  \tLoss:   86.301933\trec:   59.290077\tkl:   27.011850\n",
      "Epoch: 890 [10100/50000 (20%)]  \tLoss:   91.445389\trec:   62.662292\tkl:   28.783100\n",
      "Epoch: 890 [20100/50000 (40%)]  \tLoss:   87.263596\trec:   60.017899\tkl:   27.245689\n",
      "Epoch: 890 [30100/50000 (60%)]  \tLoss:   88.722008\trec:   60.646862\tkl:   28.075146\n",
      "Epoch: 890 [40100/50000 (80%)]  \tLoss:   91.279716\trec:   63.113251\tkl:   28.166464\n",
      "====> Epoch: 890 Average train loss: 88.6806\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8080\n",
      "Epoch: 891 [  100/50000 ( 0%)]  \tLoss:   87.930748\trec:   59.775955\tkl:   28.154797\n",
      "Epoch: 891 [10100/50000 (20%)]  \tLoss:   89.736198\trec:   61.234131\tkl:   28.502069\n",
      "Epoch: 891 [20100/50000 (40%)]  \tLoss:   93.595268\trec:   64.596222\tkl:   28.999046\n",
      "Epoch: 891 [30100/50000 (60%)]  \tLoss:   87.597000\trec:   59.588036\tkl:   28.008965\n",
      "Epoch: 891 [40100/50000 (80%)]  \tLoss:   89.900642\trec:   60.984085\tkl:   28.916552\n",
      "====> Epoch: 891 Average train loss: 88.6893\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8329\n",
      "Epoch: 892 [  100/50000 ( 0%)]  \tLoss:   89.447479\trec:   60.890701\tkl:   28.556772\n",
      "Epoch: 892 [10100/50000 (20%)]  \tLoss:   89.086182\trec:   61.068466\tkl:   28.017714\n",
      "Epoch: 892 [20100/50000 (40%)]  \tLoss:   85.304840\trec:   57.936794\tkl:   27.368042\n",
      "Epoch: 892 [30100/50000 (60%)]  \tLoss:   87.541855\trec:   59.050781\tkl:   28.491076\n",
      "Epoch: 892 [40100/50000 (80%)]  \tLoss:   87.965759\trec:   59.230846\tkl:   28.734911\n",
      "====> Epoch: 892 Average train loss: 88.6427\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8424\n",
      "Epoch: 893 [  100/50000 ( 0%)]  \tLoss:   91.754044\trec:   63.237587\tkl:   28.516453\n",
      "Epoch: 893 [10100/50000 (20%)]  \tLoss:   92.177055\trec:   62.417519\tkl:   29.759541\n",
      "Epoch: 893 [20100/50000 (40%)]  \tLoss:   90.613335\trec:   61.616081\tkl:   28.997255\n",
      "Epoch: 893 [30100/50000 (60%)]  \tLoss:   84.616600\trec:   56.722332\tkl:   27.894272\n",
      "Epoch: 893 [40100/50000 (80%)]  \tLoss:   87.752266\trec:   59.643494\tkl:   28.108767\n",
      "====> Epoch: 893 Average train loss: 88.6421\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7130\n",
      "Epoch: 894 [  100/50000 ( 0%)]  \tLoss:   87.400749\trec:   58.800175\tkl:   28.600576\n",
      "Epoch: 894 [10100/50000 (20%)]  \tLoss:   89.040886\trec:   61.336296\tkl:   27.704592\n",
      "Epoch: 894 [20100/50000 (40%)]  \tLoss:   88.113495\trec:   59.242901\tkl:   28.870590\n",
      "Epoch: 894 [30100/50000 (60%)]  \tLoss:   85.666275\trec:   58.444901\tkl:   27.221376\n",
      "Epoch: 894 [40100/50000 (80%)]  \tLoss:   84.454216\trec:   56.067741\tkl:   28.386469\n",
      "====> Epoch: 894 Average train loss: 88.6767\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8157\n",
      "Epoch: 895 [  100/50000 ( 0%)]  \tLoss:   89.622665\trec:   60.750702\tkl:   28.871962\n",
      "Epoch: 895 [10100/50000 (20%)]  \tLoss:   90.171158\trec:   60.634579\tkl:   29.536583\n",
      "Epoch: 895 [20100/50000 (40%)]  \tLoss:   87.278572\trec:   59.542786\tkl:   27.735786\n",
      "Epoch: 895 [30100/50000 (60%)]  \tLoss:   86.724899\trec:   57.691669\tkl:   29.033226\n",
      "Epoch: 895 [40100/50000 (80%)]  \tLoss:   85.844170\trec:   58.815418\tkl:   27.028749\n",
      "====> Epoch: 895 Average train loss: 88.6565\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.6733\n",
      "Epoch: 896 [  100/50000 ( 0%)]  \tLoss:   91.408989\trec:   62.977753\tkl:   28.431240\n",
      "Epoch: 896 [10100/50000 (20%)]  \tLoss:   90.793983\trec:   61.903095\tkl:   28.890892\n",
      "Epoch: 896 [20100/50000 (40%)]  \tLoss:   89.937065\trec:   61.709496\tkl:   28.227577\n",
      "Epoch: 896 [30100/50000 (60%)]  \tLoss:   87.342606\trec:   59.882763\tkl:   27.459843\n",
      "Epoch: 896 [40100/50000 (80%)]  \tLoss:   88.003067\trec:   59.969624\tkl:   28.033442\n",
      "====> Epoch: 896 Average train loss: 88.6452\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7697\n",
      "Epoch: 897 [  100/50000 ( 0%)]  \tLoss:   87.672119\trec:   59.214863\tkl:   28.457254\n",
      "Epoch: 897 [10100/50000 (20%)]  \tLoss:   93.612335\trec:   64.117218\tkl:   29.495117\n",
      "Epoch: 897 [20100/50000 (40%)]  \tLoss:   90.455025\trec:   62.444931\tkl:   28.010098\n",
      "Epoch: 897 [30100/50000 (60%)]  \tLoss:   84.870918\trec:   57.201214\tkl:   27.669704\n",
      "Epoch: 897 [40100/50000 (80%)]  \tLoss:   88.590828\trec:   60.317608\tkl:   28.273222\n",
      "====> Epoch: 897 Average train loss: 88.6663\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8923\n",
      "Epoch: 898 [  100/50000 ( 0%)]  \tLoss:   87.688385\trec:   59.561913\tkl:   28.126474\n",
      "Epoch: 898 [10100/50000 (20%)]  \tLoss:   91.515358\trec:   61.979832\tkl:   29.535524\n",
      "Epoch: 898 [20100/50000 (40%)]  \tLoss:   88.451775\trec:   59.977695\tkl:   28.474087\n",
      "Epoch: 898 [30100/50000 (60%)]  \tLoss:   89.558807\trec:   60.733425\tkl:   28.825382\n",
      "Epoch: 898 [40100/50000 (80%)]  \tLoss:   89.093628\trec:   61.197746\tkl:   27.895884\n",
      "====> Epoch: 898 Average train loss: 88.6640\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9059\n",
      "Epoch: 899 [  100/50000 ( 0%)]  \tLoss:   89.452896\trec:   60.868221\tkl:   28.584677\n",
      "Epoch: 899 [10100/50000 (20%)]  \tLoss:   84.127548\trec:   57.015781\tkl:   27.111767\n",
      "Epoch: 899 [20100/50000 (40%)]  \tLoss:   91.979996\trec:   62.767220\tkl:   29.212782\n",
      "Epoch: 899 [30100/50000 (60%)]  \tLoss:   85.911980\trec:   58.461941\tkl:   27.450039\n",
      "Epoch: 899 [40100/50000 (80%)]  \tLoss:   87.652695\trec:   59.820515\tkl:   27.832172\n",
      "====> Epoch: 899 Average train loss: 88.6390\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8501\n",
      "Epoch: 900 [  100/50000 ( 0%)]  \tLoss:   89.423256\trec:   60.644627\tkl:   28.778627\n",
      "Epoch: 900 [10100/50000 (20%)]  \tLoss:   86.529663\trec:   59.649975\tkl:   26.879696\n",
      "Epoch: 900 [20100/50000 (40%)]  \tLoss:   90.760056\trec:   62.296944\tkl:   28.463112\n",
      "Epoch: 900 [30100/50000 (60%)]  \tLoss:   89.370682\trec:   61.290970\tkl:   28.079708\n",
      "Epoch: 900 [40100/50000 (80%)]  \tLoss:   91.011330\trec:   62.724567\tkl:   28.286753\n",
      "====> Epoch: 900 Average train loss: 88.6524\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8554\n",
      "Epoch: 901 [  100/50000 ( 0%)]  \tLoss:   91.089844\trec:   62.123222\tkl:   28.966621\n",
      "Epoch: 901 [10100/50000 (20%)]  \tLoss:   89.023643\trec:   60.733856\tkl:   28.289785\n",
      "Epoch: 901 [20100/50000 (40%)]  \tLoss:   86.886642\trec:   59.328571\tkl:   27.558065\n",
      "Epoch: 901 [30100/50000 (60%)]  \tLoss:   87.918747\trec:   59.610901\tkl:   28.307840\n",
      "Epoch: 901 [40100/50000 (80%)]  \tLoss:   90.289696\trec:   61.764816\tkl:   28.524879\n",
      "====> Epoch: 901 Average train loss: 88.6606\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7893\n",
      "Epoch: 902 [  100/50000 ( 0%)]  \tLoss:   88.513863\trec:   60.986797\tkl:   27.527065\n",
      "Epoch: 902 [10100/50000 (20%)]  \tLoss:   92.821831\trec:   64.037575\tkl:   28.784262\n",
      "Epoch: 902 [20100/50000 (40%)]  \tLoss:   81.847946\trec:   55.729149\tkl:   26.118797\n",
      "Epoch: 902 [30100/50000 (60%)]  \tLoss:   85.563141\trec:   57.612782\tkl:   27.950365\n",
      "Epoch: 902 [40100/50000 (80%)]  \tLoss:   87.928154\trec:   59.489704\tkl:   28.438450\n",
      "====> Epoch: 902 Average train loss: 88.6543\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8148\n",
      "Epoch: 903 [  100/50000 ( 0%)]  \tLoss:   89.656425\trec:   60.707714\tkl:   28.948709\n",
      "Epoch: 903 [10100/50000 (20%)]  \tLoss:   85.077606\trec:   57.663254\tkl:   27.414352\n",
      "Epoch: 903 [20100/50000 (40%)]  \tLoss:   85.260391\trec:   57.505955\tkl:   27.754435\n",
      "Epoch: 903 [30100/50000 (60%)]  \tLoss:   89.958496\trec:   61.545937\tkl:   28.412558\n",
      "Epoch: 903 [40100/50000 (80%)]  \tLoss:   83.581764\trec:   56.467129\tkl:   27.114641\n",
      "====> Epoch: 903 Average train loss: 88.6562\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8287\n",
      "Epoch: 904 [  100/50000 ( 0%)]  \tLoss:   86.773903\trec:   58.660461\tkl:   28.113447\n",
      "Epoch: 904 [10100/50000 (20%)]  \tLoss:   88.112831\trec:   60.209705\tkl:   27.903124\n",
      "Epoch: 904 [20100/50000 (40%)]  \tLoss:   93.338844\trec:   64.280914\tkl:   29.057934\n",
      "Epoch: 904 [30100/50000 (60%)]  \tLoss:   88.816406\trec:   59.781868\tkl:   29.034540\n",
      "Epoch: 904 [40100/50000 (80%)]  \tLoss:   89.524223\trec:   60.807747\tkl:   28.716478\n",
      "====> Epoch: 904 Average train loss: 88.6521\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8343\n",
      "Epoch: 905 [  100/50000 ( 0%)]  \tLoss:   89.816795\trec:   60.257607\tkl:   29.559189\n",
      "Epoch: 905 [10100/50000 (20%)]  \tLoss:   86.433960\trec:   57.714520\tkl:   28.719448\n",
      "Epoch: 905 [20100/50000 (40%)]  \tLoss:   88.572563\trec:   61.095829\tkl:   27.476736\n",
      "Epoch: 905 [30100/50000 (60%)]  \tLoss:   89.448982\trec:   61.665703\tkl:   27.783278\n",
      "Epoch: 905 [40100/50000 (80%)]  \tLoss:   88.552284\trec:   59.718163\tkl:   28.834126\n",
      "====> Epoch: 905 Average train loss: 88.6386\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8226\n",
      "Epoch: 906 [  100/50000 ( 0%)]  \tLoss:   84.104851\trec:   56.159599\tkl:   27.945255\n",
      "Epoch: 906 [10100/50000 (20%)]  \tLoss:   84.735779\trec:   58.315220\tkl:   26.420555\n",
      "Epoch: 906 [20100/50000 (40%)]  \tLoss:   90.228867\trec:   61.457043\tkl:   28.771826\n",
      "Epoch: 906 [30100/50000 (60%)]  \tLoss:   88.739182\trec:   60.711182\tkl:   28.028002\n",
      "Epoch: 906 [40100/50000 (80%)]  \tLoss:   85.215294\trec:   58.311634\tkl:   26.903656\n",
      "====> Epoch: 906 Average train loss: 88.6118\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7475\n",
      "Epoch: 907 [  100/50000 ( 0%)]  \tLoss:   91.052650\trec:   61.665966\tkl:   29.386688\n",
      "Epoch: 907 [10100/50000 (20%)]  \tLoss:   87.512009\trec:   59.295837\tkl:   28.216173\n",
      "Epoch: 907 [20100/50000 (40%)]  \tLoss:   90.415878\trec:   62.273357\tkl:   28.142513\n",
      "Epoch: 907 [30100/50000 (60%)]  \tLoss:   89.816597\trec:   60.783035\tkl:   29.033569\n",
      "Epoch: 907 [40100/50000 (80%)]  \tLoss:   89.311882\trec:   60.394733\tkl:   28.917149\n",
      "====> Epoch: 907 Average train loss: 88.6246\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8179\n",
      "Epoch: 908 [  100/50000 ( 0%)]  \tLoss:   83.047127\trec:   55.824726\tkl:   27.222397\n",
      "Epoch: 908 [10100/50000 (20%)]  \tLoss:   86.317673\trec:   58.504196\tkl:   27.813480\n",
      "Epoch: 908 [20100/50000 (40%)]  \tLoss:   89.065063\trec:   60.524105\tkl:   28.540962\n",
      "Epoch: 908 [30100/50000 (60%)]  \tLoss:   85.960503\trec:   58.354614\tkl:   27.605892\n",
      "Epoch: 908 [40100/50000 (80%)]  \tLoss:   90.640831\trec:   61.234943\tkl:   29.405880\n",
      "====> Epoch: 908 Average train loss: 88.6381\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7946\n",
      "Epoch: 909 [  100/50000 ( 0%)]  \tLoss:   88.924118\trec:   60.332012\tkl:   28.592110\n",
      "Epoch: 909 [10100/50000 (20%)]  \tLoss:   90.539024\trec:   61.842381\tkl:   28.696640\n",
      "Epoch: 909 [20100/50000 (40%)]  \tLoss:   89.078979\trec:   60.701439\tkl:   28.377542\n",
      "Epoch: 909 [30100/50000 (60%)]  \tLoss:   91.892120\trec:   62.965584\tkl:   28.926533\n",
      "Epoch: 909 [40100/50000 (80%)]  \tLoss:   89.442749\trec:   61.538643\tkl:   27.904106\n",
      "====> Epoch: 909 Average train loss: 88.6187\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7129\n",
      "Epoch: 910 [  100/50000 ( 0%)]  \tLoss:   90.470116\trec:   61.797703\tkl:   28.672407\n",
      "Epoch: 910 [10100/50000 (20%)]  \tLoss:   87.168686\trec:   59.740147\tkl:   27.428539\n",
      "Epoch: 910 [20100/50000 (40%)]  \tLoss:   92.627594\trec:   63.259346\tkl:   29.368252\n",
      "Epoch: 910 [30100/50000 (60%)]  \tLoss:   88.832359\trec:   60.636765\tkl:   28.195601\n",
      "Epoch: 910 [40100/50000 (80%)]  \tLoss:   92.465294\trec:   63.456055\tkl:   29.009233\n",
      "====> Epoch: 910 Average train loss: 88.6179\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8728\n",
      "Epoch: 911 [  100/50000 ( 0%)]  \tLoss:   85.248497\trec:   57.553047\tkl:   27.695446\n",
      "Epoch: 911 [10100/50000 (20%)]  \tLoss:   89.826889\trec:   61.096462\tkl:   28.730434\n",
      "Epoch: 911 [20100/50000 (40%)]  \tLoss:   87.506592\trec:   58.797752\tkl:   28.708836\n",
      "Epoch: 911 [30100/50000 (60%)]  \tLoss:   87.695152\trec:   59.528709\tkl:   28.166439\n",
      "Epoch: 911 [40100/50000 (80%)]  \tLoss:   89.071228\trec:   61.228882\tkl:   27.842342\n",
      "====> Epoch: 911 Average train loss: 88.5990\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8442\n",
      "Epoch: 912 [  100/50000 ( 0%)]  \tLoss:   89.080429\trec:   60.860008\tkl:   28.220419\n",
      "Epoch: 912 [10100/50000 (20%)]  \tLoss:   89.327324\trec:   60.911629\tkl:   28.415693\n",
      "Epoch: 912 [20100/50000 (40%)]  \tLoss:   91.014610\trec:   62.369480\tkl:   28.645121\n",
      "Epoch: 912 [30100/50000 (60%)]  \tLoss:   84.425797\trec:   57.667938\tkl:   26.757864\n",
      "Epoch: 912 [40100/50000 (80%)]  \tLoss:   92.810287\trec:   63.050838\tkl:   29.759447\n",
      "====> Epoch: 912 Average train loss: 88.6227\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8155\n",
      "Epoch: 913 [  100/50000 ( 0%)]  \tLoss:   87.752632\trec:   59.518681\tkl:   28.233959\n",
      "Epoch: 913 [10100/50000 (20%)]  \tLoss:   88.829887\trec:   60.195423\tkl:   28.634468\n",
      "Epoch: 913 [20100/50000 (40%)]  \tLoss:   90.092514\trec:   61.160252\tkl:   28.932264\n",
      "Epoch: 913 [30100/50000 (60%)]  \tLoss:   84.637672\trec:   56.640739\tkl:   27.996939\n",
      "Epoch: 913 [40100/50000 (80%)]  \tLoss:   89.174965\trec:   61.721230\tkl:   27.453737\n",
      "====> Epoch: 913 Average train loss: 88.6172\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7611\n",
      "Epoch: 914 [  100/50000 ( 0%)]  \tLoss:   86.132751\trec:   58.452160\tkl:   27.680586\n",
      "Epoch: 914 [10100/50000 (20%)]  \tLoss:   89.331169\trec:   61.391811\tkl:   27.939362\n",
      "Epoch: 914 [20100/50000 (40%)]  \tLoss:   86.108025\trec:   57.872532\tkl:   28.235491\n",
      "Epoch: 914 [30100/50000 (60%)]  \tLoss:   89.010429\trec:   60.837460\tkl:   28.172968\n",
      "Epoch: 914 [40100/50000 (80%)]  \tLoss:   86.318947\trec:   58.772430\tkl:   27.546513\n",
      "====> Epoch: 914 Average train loss: 88.6274\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7764\n",
      "Epoch: 915 [  100/50000 ( 0%)]  \tLoss:   87.588776\trec:   59.812851\tkl:   27.775930\n",
      "Epoch: 915 [10100/50000 (20%)]  \tLoss:   87.173096\trec:   59.944538\tkl:   27.228552\n",
      "Epoch: 915 [20100/50000 (40%)]  \tLoss:   86.401970\trec:   58.169853\tkl:   28.232124\n",
      "Epoch: 915 [30100/50000 (60%)]  \tLoss:   87.209595\trec:   59.698853\tkl:   27.510750\n",
      "Epoch: 915 [40100/50000 (80%)]  \tLoss:   90.083458\trec:   61.125534\tkl:   28.957914\n",
      "====> Epoch: 915 Average train loss: 88.6065\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8292\n",
      "Epoch: 916 [  100/50000 ( 0%)]  \tLoss:   90.989685\trec:   62.188744\tkl:   28.800941\n",
      "Epoch: 916 [10100/50000 (20%)]  \tLoss:   87.310112\trec:   58.764271\tkl:   28.545839\n",
      "Epoch: 916 [20100/50000 (40%)]  \tLoss:   91.380333\trec:   61.931190\tkl:   29.449144\n",
      "Epoch: 916 [30100/50000 (60%)]  \tLoss:   87.166168\trec:   58.882259\tkl:   28.283909\n",
      "Epoch: 916 [40100/50000 (80%)]  \tLoss:   88.124557\trec:   60.323017\tkl:   27.801540\n",
      "====> Epoch: 916 Average train loss: 88.6246\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8131\n",
      "Epoch: 917 [  100/50000 ( 0%)]  \tLoss:   89.462723\trec:   61.480930\tkl:   27.981789\n",
      "Epoch: 917 [10100/50000 (20%)]  \tLoss:   91.218803\trec:   62.362431\tkl:   28.856371\n",
      "Epoch: 917 [20100/50000 (40%)]  \tLoss:   90.761406\trec:   61.579502\tkl:   29.181904\n",
      "Epoch: 917 [30100/50000 (60%)]  \tLoss:   86.174919\trec:   58.382221\tkl:   27.792704\n",
      "Epoch: 917 [40100/50000 (80%)]  \tLoss:   87.477066\trec:   59.564789\tkl:   27.912285\n",
      "====> Epoch: 917 Average train loss: 88.5965\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8268\n",
      "Epoch: 918 [  100/50000 ( 0%)]  \tLoss:   89.745270\trec:   61.572079\tkl:   28.173193\n",
      "Epoch: 918 [10100/50000 (20%)]  \tLoss:   88.837067\trec:   59.580498\tkl:   29.256577\n",
      "Epoch: 918 [20100/50000 (40%)]  \tLoss:   88.096924\trec:   60.169746\tkl:   27.927177\n",
      "Epoch: 918 [30100/50000 (60%)]  \tLoss:   88.398628\trec:   60.208221\tkl:   28.190405\n",
      "Epoch: 918 [40100/50000 (80%)]  \tLoss:   95.759979\trec:   66.493599\tkl:   29.266376\n",
      "====> Epoch: 918 Average train loss: 88.5951\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8976\n",
      "Epoch: 919 [  100/50000 ( 0%)]  \tLoss:   90.644653\trec:   60.384811\tkl:   30.259840\n",
      "Epoch: 919 [10100/50000 (20%)]  \tLoss:   87.107719\trec:   59.609753\tkl:   27.497969\n",
      "Epoch: 919 [20100/50000 (40%)]  \tLoss:   87.795151\trec:   59.743477\tkl:   28.051674\n",
      "Epoch: 919 [30100/50000 (60%)]  \tLoss:   87.852013\trec:   59.922337\tkl:   27.929674\n",
      "Epoch: 919 [40100/50000 (80%)]  \tLoss:   90.709839\trec:   62.037354\tkl:   28.672485\n",
      "====> Epoch: 919 Average train loss: 88.5897\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7720\n",
      "Epoch: 920 [  100/50000 ( 0%)]  \tLoss:   87.846779\trec:   58.935883\tkl:   28.910896\n",
      "Epoch: 920 [10100/50000 (20%)]  \tLoss:   87.223457\trec:   58.582977\tkl:   28.640482\n",
      "Epoch: 920 [20100/50000 (40%)]  \tLoss:   87.578827\trec:   59.085606\tkl:   28.493221\n",
      "Epoch: 920 [30100/50000 (60%)]  \tLoss:   89.595390\trec:   60.755756\tkl:   28.839628\n",
      "Epoch: 920 [40100/50000 (80%)]  \tLoss:   87.337257\trec:   58.738892\tkl:   28.598362\n",
      "====> Epoch: 920 Average train loss: 88.5938\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8010\n",
      "Epoch: 921 [  100/50000 ( 0%)]  \tLoss:   86.902145\trec:   59.037861\tkl:   27.864292\n",
      "Epoch: 921 [10100/50000 (20%)]  \tLoss:   85.282120\trec:   57.713371\tkl:   27.568743\n",
      "Epoch: 921 [20100/50000 (40%)]  \tLoss:   90.314713\trec:   61.876465\tkl:   28.438251\n",
      "Epoch: 921 [30100/50000 (60%)]  \tLoss:   92.163887\trec:   62.531883\tkl:   29.632002\n",
      "Epoch: 921 [40100/50000 (80%)]  \tLoss:   89.267525\trec:   60.270046\tkl:   28.997482\n",
      "====> Epoch: 921 Average train loss: 88.6017\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8296\n",
      "Epoch: 922 [  100/50000 ( 0%)]  \tLoss:   85.362335\trec:   58.250404\tkl:   27.111929\n",
      "Epoch: 922 [10100/50000 (20%)]  \tLoss:   92.333435\trec:   64.134109\tkl:   28.199327\n",
      "Epoch: 922 [20100/50000 (40%)]  \tLoss:   85.383995\trec:   57.489433\tkl:   27.894558\n",
      "Epoch: 922 [30100/50000 (60%)]  \tLoss:   84.964600\trec:   58.122425\tkl:   26.842171\n",
      "Epoch: 922 [40100/50000 (80%)]  \tLoss:   88.070992\trec:   61.054569\tkl:   27.016430\n",
      "====> Epoch: 922 Average train loss: 88.6087\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8382\n",
      "Epoch: 923 [  100/50000 ( 0%)]  \tLoss:   88.437675\trec:   60.775379\tkl:   27.662300\n",
      "Epoch: 923 [10100/50000 (20%)]  \tLoss:   87.990509\trec:   59.696400\tkl:   28.294104\n",
      "Epoch: 923 [20100/50000 (40%)]  \tLoss:   88.680748\trec:   61.553856\tkl:   27.126894\n",
      "Epoch: 923 [30100/50000 (60%)]  \tLoss:   87.585640\trec:   60.421688\tkl:   27.163954\n",
      "Epoch: 923 [40100/50000 (80%)]  \tLoss:   89.541542\trec:   60.593822\tkl:   28.947721\n",
      "====> Epoch: 923 Average train loss: 88.6023\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8354\n",
      "Epoch: 924 [  100/50000 ( 0%)]  \tLoss:   82.843300\trec:   56.665569\tkl:   26.177727\n",
      "Epoch: 924 [10100/50000 (20%)]  \tLoss:   89.015411\trec:   60.950134\tkl:   28.065268\n",
      "Epoch: 924 [20100/50000 (40%)]  \tLoss:   88.457039\trec:   59.930351\tkl:   28.526690\n",
      "Epoch: 924 [30100/50000 (60%)]  \tLoss:   88.769707\trec:   60.573017\tkl:   28.196693\n",
      "Epoch: 924 [40100/50000 (80%)]  \tLoss:   87.120079\trec:   59.258240\tkl:   27.861837\n",
      "====> Epoch: 924 Average train loss: 88.6009\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8282\n",
      "Epoch: 925 [  100/50000 ( 0%)]  \tLoss:   88.202652\trec:   59.508759\tkl:   28.693892\n",
      "Epoch: 925 [10100/50000 (20%)]  \tLoss:   85.414528\trec:   57.555283\tkl:   27.859249\n",
      "Epoch: 925 [20100/50000 (40%)]  \tLoss:   89.543114\trec:   61.127949\tkl:   28.415165\n",
      "Epoch: 925 [30100/50000 (60%)]  \tLoss:   89.898834\trec:   61.893650\tkl:   28.005182\n",
      "Epoch: 925 [40100/50000 (80%)]  \tLoss:   90.175209\trec:   61.771610\tkl:   28.403599\n",
      "====> Epoch: 925 Average train loss: 88.5949\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7629\n",
      "Epoch: 926 [  100/50000 ( 0%)]  \tLoss:   89.416794\trec:   61.361816\tkl:   28.054976\n",
      "Epoch: 926 [10100/50000 (20%)]  \tLoss:   91.746147\trec:   63.028633\tkl:   28.717516\n",
      "Epoch: 926 [20100/50000 (40%)]  \tLoss:   88.870171\trec:   59.761189\tkl:   29.108982\n",
      "Epoch: 926 [30100/50000 (60%)]  \tLoss:   88.635925\trec:   61.290009\tkl:   27.345921\n",
      "Epoch: 926 [40100/50000 (80%)]  \tLoss:   89.257423\trec:   61.198891\tkl:   28.058525\n",
      "====> Epoch: 926 Average train loss: 88.5828\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7813\n",
      "Epoch: 927 [  100/50000 ( 0%)]  \tLoss:   86.960876\trec:   58.841415\tkl:   28.119465\n",
      "Epoch: 927 [10100/50000 (20%)]  \tLoss:   85.108871\trec:   57.279400\tkl:   27.829479\n",
      "Epoch: 927 [20100/50000 (40%)]  \tLoss:   89.768517\trec:   60.871494\tkl:   28.897017\n",
      "Epoch: 927 [30100/50000 (60%)]  \tLoss:   91.388283\trec:   63.246052\tkl:   28.142229\n",
      "Epoch: 927 [40100/50000 (80%)]  \tLoss:   91.287247\trec:   62.979919\tkl:   28.307323\n",
      "====> Epoch: 927 Average train loss: 88.6008\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7509\n",
      "Epoch: 928 [  100/50000 ( 0%)]  \tLoss:   87.618393\trec:   59.131687\tkl:   28.486704\n",
      "Epoch: 928 [10100/50000 (20%)]  \tLoss:   88.064812\trec:   60.457451\tkl:   27.607361\n",
      "Epoch: 928 [20100/50000 (40%)]  \tLoss:   84.882362\trec:   58.145447\tkl:   26.736916\n",
      "Epoch: 928 [30100/50000 (60%)]  \tLoss:   88.077805\trec:   60.355545\tkl:   27.722258\n",
      "Epoch: 928 [40100/50000 (80%)]  \tLoss:   87.436554\trec:   59.091621\tkl:   28.344931\n",
      "====> Epoch: 928 Average train loss: 88.5931\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7736\n",
      "Epoch: 929 [  100/50000 ( 0%)]  \tLoss:   88.833328\trec:   60.362831\tkl:   28.470495\n",
      "Epoch: 929 [10100/50000 (20%)]  \tLoss:   92.210854\trec:   63.337933\tkl:   28.872925\n",
      "Epoch: 929 [20100/50000 (40%)]  \tLoss:   89.933647\trec:   60.966110\tkl:   28.967539\n",
      "Epoch: 929 [30100/50000 (60%)]  \tLoss:   89.135933\trec:   60.482555\tkl:   28.653378\n",
      "Epoch: 929 [40100/50000 (80%)]  \tLoss:   91.120308\trec:   62.751518\tkl:   28.368795\n",
      "====> Epoch: 929 Average train loss: 88.5916\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7685\n",
      "Epoch: 930 [  100/50000 ( 0%)]  \tLoss:   88.427498\trec:   59.947685\tkl:   28.479815\n",
      "Epoch: 930 [10100/50000 (20%)]  \tLoss:   93.771172\trec:   63.997803\tkl:   29.773363\n",
      "Epoch: 930 [20100/50000 (40%)]  \tLoss:   90.179810\trec:   61.427811\tkl:   28.751999\n",
      "Epoch: 930 [30100/50000 (60%)]  \tLoss:   86.999512\trec:   58.999344\tkl:   28.000162\n",
      "Epoch: 930 [40100/50000 (80%)]  \tLoss:   89.685303\trec:   60.777855\tkl:   28.907446\n",
      "====> Epoch: 930 Average train loss: 88.5866\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7835\n",
      "Epoch: 931 [  100/50000 ( 0%)]  \tLoss:   90.317223\trec:   61.338448\tkl:   28.978783\n",
      "Epoch: 931 [10100/50000 (20%)]  \tLoss:   86.534332\trec:   58.679657\tkl:   27.854675\n",
      "Epoch: 931 [20100/50000 (40%)]  \tLoss:   89.064491\trec:   60.587093\tkl:   28.477392\n",
      "Epoch: 931 [30100/50000 (60%)]  \tLoss:   86.067772\trec:   58.306019\tkl:   27.761757\n",
      "Epoch: 931 [40100/50000 (80%)]  \tLoss:   89.736794\trec:   61.306507\tkl:   28.430283\n",
      "====> Epoch: 931 Average train loss: 88.5745\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8947\n",
      "Epoch: 932 [  100/50000 ( 0%)]  \tLoss:   89.590469\trec:   61.150948\tkl:   28.439516\n",
      "Epoch: 932 [10100/50000 (20%)]  \tLoss:   88.503319\trec:   59.262753\tkl:   29.240561\n",
      "Epoch: 932 [20100/50000 (40%)]  \tLoss:   88.245659\trec:   59.419415\tkl:   28.826254\n",
      "Epoch: 932 [30100/50000 (60%)]  \tLoss:   88.366745\trec:   60.266083\tkl:   28.100660\n",
      "Epoch: 932 [40100/50000 (80%)]  \tLoss:   85.023796\trec:   57.764023\tkl:   27.259775\n",
      "====> Epoch: 932 Average train loss: 88.5816\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.6875\n",
      "Epoch: 933 [  100/50000 ( 0%)]  \tLoss:   90.140007\trec:   61.956879\tkl:   28.183132\n",
      "Epoch: 933 [10100/50000 (20%)]  \tLoss:   88.476128\trec:   60.487549\tkl:   27.988583\n",
      "Epoch: 933 [20100/50000 (40%)]  \tLoss:   90.042839\trec:   61.162968\tkl:   28.879873\n",
      "Epoch: 933 [30100/50000 (60%)]  \tLoss:   85.984245\trec:   58.211884\tkl:   27.772366\n",
      "Epoch: 933 [40100/50000 (80%)]  \tLoss:   87.986641\trec:   59.838360\tkl:   28.148285\n",
      "====> Epoch: 933 Average train loss: 88.5603\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7076\n",
      "Epoch: 934 [  100/50000 ( 0%)]  \tLoss:   90.496033\trec:   61.556072\tkl:   28.939964\n",
      "Epoch: 934 [10100/50000 (20%)]  \tLoss:   83.398048\trec:   55.594032\tkl:   27.804018\n",
      "Epoch: 934 [20100/50000 (40%)]  \tLoss:   86.337502\trec:   58.707676\tkl:   27.629829\n",
      "Epoch: 934 [30100/50000 (60%)]  \tLoss:   89.526871\trec:   61.658047\tkl:   27.868822\n",
      "Epoch: 934 [40100/50000 (80%)]  \tLoss:   93.204994\trec:   63.060982\tkl:   30.144012\n",
      "====> Epoch: 934 Average train loss: 88.5999\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7163\n",
      "Epoch: 935 [  100/50000 ( 0%)]  \tLoss:   90.065796\trec:   61.465828\tkl:   28.599966\n",
      "Epoch: 935 [10100/50000 (20%)]  \tLoss:   90.471207\trec:   62.219875\tkl:   28.251335\n",
      "Epoch: 935 [20100/50000 (40%)]  \tLoss:   85.928223\trec:   58.366531\tkl:   27.561689\n",
      "Epoch: 935 [30100/50000 (60%)]  \tLoss:   85.755020\trec:   58.706928\tkl:   27.048090\n",
      "Epoch: 935 [40100/50000 (80%)]  \tLoss:   85.399864\trec:   58.338120\tkl:   27.061743\n",
      "====> Epoch: 935 Average train loss: 88.5710\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8387\n",
      "Epoch: 936 [  100/50000 ( 0%)]  \tLoss:   92.196686\trec:   62.743080\tkl:   29.453608\n",
      "Epoch: 936 [10100/50000 (20%)]  \tLoss:   89.910339\trec:   60.642761\tkl:   29.267578\n",
      "Epoch: 936 [20100/50000 (40%)]  \tLoss:   89.384018\trec:   61.218945\tkl:   28.165075\n",
      "Epoch: 936 [30100/50000 (60%)]  \tLoss:   90.485001\trec:   61.905594\tkl:   28.579407\n",
      "Epoch: 936 [40100/50000 (80%)]  \tLoss:   86.212440\trec:   58.618584\tkl:   27.593857\n",
      "====> Epoch: 936 Average train loss: 88.5810\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.6542\n",
      "Epoch: 937 [  100/50000 ( 0%)]  \tLoss:   89.133034\trec:   60.122986\tkl:   29.010046\n",
      "Epoch: 937 [10100/50000 (20%)]  \tLoss:   89.582848\trec:   60.339569\tkl:   29.243279\n",
      "Epoch: 937 [20100/50000 (40%)]  \tLoss:   90.907829\trec:   61.860779\tkl:   29.047049\n",
      "Epoch: 937 [30100/50000 (60%)]  \tLoss:   84.021408\trec:   56.672783\tkl:   27.348623\n",
      "Epoch: 937 [40100/50000 (80%)]  \tLoss:   91.787224\trec:   62.596561\tkl:   29.190659\n",
      "====> Epoch: 937 Average train loss: 88.5781\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7722\n",
      "Epoch: 938 [  100/50000 ( 0%)]  \tLoss:   86.502419\trec:   58.690144\tkl:   27.812273\n",
      "Epoch: 938 [10100/50000 (20%)]  \tLoss:   93.560600\trec:   63.987148\tkl:   29.573452\n",
      "Epoch: 938 [20100/50000 (40%)]  \tLoss:   90.423790\trec:   61.670750\tkl:   28.753042\n",
      "Epoch: 938 [30100/50000 (60%)]  \tLoss:   91.149704\trec:   62.705944\tkl:   28.443754\n",
      "Epoch: 938 [40100/50000 (80%)]  \tLoss:   90.015762\trec:   62.232700\tkl:   27.783056\n",
      "====> Epoch: 938 Average train loss: 88.5448\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8134\n",
      "Epoch: 939 [  100/50000 ( 0%)]  \tLoss:   86.480934\trec:   58.695995\tkl:   27.784935\n",
      "Epoch: 939 [10100/50000 (20%)]  \tLoss:   89.829506\trec:   61.181404\tkl:   28.648109\n",
      "Epoch: 939 [20100/50000 (40%)]  \tLoss:   89.190262\trec:   61.125706\tkl:   28.064558\n",
      "Epoch: 939 [30100/50000 (60%)]  \tLoss:   85.551872\trec:   58.162186\tkl:   27.389692\n",
      "Epoch: 939 [40100/50000 (80%)]  \tLoss:   90.915504\trec:   61.837715\tkl:   29.077795\n",
      "====> Epoch: 939 Average train loss: 88.5698\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7794\n",
      "Epoch: 940 [  100/50000 ( 0%)]  \tLoss:   86.681915\trec:   58.729206\tkl:   27.952702\n",
      "Epoch: 940 [10100/50000 (20%)]  \tLoss:   87.793945\trec:   59.737572\tkl:   28.056374\n",
      "Epoch: 940 [20100/50000 (40%)]  \tLoss:   86.680565\trec:   58.696735\tkl:   27.983826\n",
      "Epoch: 940 [30100/50000 (60%)]  \tLoss:   92.905167\trec:   63.776566\tkl:   29.128595\n",
      "Epoch: 940 [40100/50000 (80%)]  \tLoss:   86.420761\trec:   58.864902\tkl:   27.555864\n",
      "====> Epoch: 940 Average train loss: 88.5705\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7473\n",
      "Epoch: 941 [  100/50000 ( 0%)]  \tLoss:   86.302277\trec:   58.504822\tkl:   27.797451\n",
      "Epoch: 941 [10100/50000 (20%)]  \tLoss:   89.755447\trec:   61.527771\tkl:   28.227671\n",
      "Epoch: 941 [20100/50000 (40%)]  \tLoss:   85.808456\trec:   58.305733\tkl:   27.502724\n",
      "Epoch: 941 [30100/50000 (60%)]  \tLoss:   85.821053\trec:   57.360935\tkl:   28.460119\n",
      "Epoch: 941 [40100/50000 (80%)]  \tLoss:   91.323746\trec:   62.721928\tkl:   28.601816\n",
      "====> Epoch: 941 Average train loss: 88.5502\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8321\n",
      "Epoch: 942 [  100/50000 ( 0%)]  \tLoss:   87.662910\trec:   59.659393\tkl:   28.003515\n",
      "Epoch: 942 [10100/50000 (20%)]  \tLoss:   88.704353\trec:   60.111599\tkl:   28.592752\n",
      "Epoch: 942 [20100/50000 (40%)]  \tLoss:   86.065056\trec:   59.031063\tkl:   27.033997\n",
      "Epoch: 942 [30100/50000 (60%)]  \tLoss:   88.579376\trec:   59.432529\tkl:   29.146845\n",
      "Epoch: 942 [40100/50000 (80%)]  \tLoss:   86.975723\trec:   59.318344\tkl:   27.657377\n",
      "====> Epoch: 942 Average train loss: 88.5593\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8561\n",
      "Epoch: 943 [  100/50000 ( 0%)]  \tLoss:   81.561752\trec:   53.887451\tkl:   27.674307\n",
      "Epoch: 943 [10100/50000 (20%)]  \tLoss:   86.365738\trec:   58.616615\tkl:   27.749128\n",
      "Epoch: 943 [20100/50000 (40%)]  \tLoss:   89.138283\trec:   60.205585\tkl:   28.932699\n",
      "Epoch: 943 [30100/50000 (60%)]  \tLoss:   86.219292\trec:   58.124729\tkl:   28.094564\n",
      "Epoch: 943 [40100/50000 (80%)]  \tLoss:   90.423950\trec:   62.022064\tkl:   28.401890\n",
      "====> Epoch: 943 Average train loss: 88.5645\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7874\n",
      "Epoch: 944 [  100/50000 ( 0%)]  \tLoss:   87.547714\trec:   59.562263\tkl:   27.985449\n",
      "Epoch: 944 [10100/50000 (20%)]  \tLoss:   91.215401\trec:   62.092285\tkl:   29.123117\n",
      "Epoch: 944 [20100/50000 (40%)]  \tLoss:   89.352379\trec:   60.794647\tkl:   28.557739\n",
      "Epoch: 944 [30100/50000 (60%)]  \tLoss:   90.862411\trec:   62.272343\tkl:   28.590071\n",
      "Epoch: 944 [40100/50000 (80%)]  \tLoss:   84.090752\trec:   56.533249\tkl:   27.557497\n",
      "====> Epoch: 944 Average train loss: 88.5278\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8019\n",
      "Epoch: 945 [  100/50000 ( 0%)]  \tLoss:   85.437813\trec:   57.096794\tkl:   28.341011\n",
      "Epoch: 945 [10100/50000 (20%)]  \tLoss:   89.728386\trec:   61.424248\tkl:   28.304142\n",
      "Epoch: 945 [20100/50000 (40%)]  \tLoss:   87.564293\trec:   59.530075\tkl:   28.034214\n",
      "Epoch: 945 [30100/50000 (60%)]  \tLoss:   84.437988\trec:   57.107304\tkl:   27.330679\n",
      "Epoch: 945 [40100/50000 (80%)]  \tLoss:   89.209274\trec:   60.774040\tkl:   28.435228\n",
      "====> Epoch: 945 Average train loss: 88.5433\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7564\n",
      "Epoch: 946 [  100/50000 ( 0%)]  \tLoss:   88.442535\trec:   60.560234\tkl:   27.882303\n",
      "Epoch: 946 [10100/50000 (20%)]  \tLoss:   90.639366\trec:   61.451904\tkl:   29.187460\n",
      "Epoch: 946 [20100/50000 (40%)]  \tLoss:   83.896378\trec:   56.984257\tkl:   26.912121\n",
      "Epoch: 946 [30100/50000 (60%)]  \tLoss:   88.238991\trec:   59.796082\tkl:   28.442909\n",
      "Epoch: 946 [40100/50000 (80%)]  \tLoss:   91.306885\trec:   62.912968\tkl:   28.393915\n",
      "====> Epoch: 946 Average train loss: 88.5677\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8215\n",
      "Epoch: 947 [  100/50000 ( 0%)]  \tLoss:   85.879715\trec:   58.707890\tkl:   27.171827\n",
      "Epoch: 947 [10100/50000 (20%)]  \tLoss:   86.160820\trec:   58.112915\tkl:   28.047909\n",
      "Epoch: 947 [20100/50000 (40%)]  \tLoss:   87.293037\trec:   58.752075\tkl:   28.540962\n",
      "Epoch: 947 [30100/50000 (60%)]  \tLoss:   91.799477\trec:   61.808739\tkl:   29.990740\n",
      "Epoch: 947 [40100/50000 (80%)]  \tLoss:   91.223419\trec:   62.152901\tkl:   29.070522\n",
      "====> Epoch: 947 Average train loss: 88.5361\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8149\n",
      "Epoch: 948 [  100/50000 ( 0%)]  \tLoss:   87.978767\trec:   59.579300\tkl:   28.399466\n",
      "Epoch: 948 [10100/50000 (20%)]  \tLoss:   89.549255\trec:   62.083672\tkl:   27.465584\n",
      "Epoch: 948 [20100/50000 (40%)]  \tLoss:   91.013779\trec:   62.717224\tkl:   28.296553\n",
      "Epoch: 948 [30100/50000 (60%)]  \tLoss:   87.535568\trec:   59.478573\tkl:   28.056997\n",
      "Epoch: 948 [40100/50000 (80%)]  \tLoss:   87.982597\trec:   60.099445\tkl:   27.883144\n",
      "====> Epoch: 948 Average train loss: 88.5541\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8581\n",
      "Epoch: 949 [  100/50000 ( 0%)]  \tLoss:   90.397774\trec:   61.722298\tkl:   28.675470\n",
      "Epoch: 949 [10100/50000 (20%)]  \tLoss:   89.651649\trec:   60.846287\tkl:   28.805361\n",
      "Epoch: 949 [20100/50000 (40%)]  \tLoss:   85.294243\trec:   57.636009\tkl:   27.658234\n",
      "Epoch: 949 [30100/50000 (60%)]  \tLoss:   92.733482\trec:   63.990654\tkl:   28.742832\n",
      "Epoch: 949 [40100/50000 (80%)]  \tLoss:   93.118507\trec:   63.640869\tkl:   29.477634\n",
      "====> Epoch: 949 Average train loss: 88.5493\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7729\n",
      "Epoch: 950 [  100/50000 ( 0%)]  \tLoss:   88.939354\trec:   60.174042\tkl:   28.765306\n",
      "Epoch: 950 [10100/50000 (20%)]  \tLoss:   87.159531\trec:   58.825336\tkl:   28.334188\n",
      "Epoch: 950 [20100/50000 (40%)]  \tLoss:   87.960236\trec:   58.938320\tkl:   29.021914\n",
      "Epoch: 950 [30100/50000 (60%)]  \tLoss:   85.334549\trec:   57.981785\tkl:   27.352762\n",
      "Epoch: 950 [40100/50000 (80%)]  \tLoss:   89.102264\trec:   60.480171\tkl:   28.622099\n",
      "====> Epoch: 950 Average train loss: 88.5246\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8759\n",
      "Epoch: 951 [  100/50000 ( 0%)]  \tLoss:   92.194000\trec:   63.400673\tkl:   28.793331\n",
      "Epoch: 951 [10100/50000 (20%)]  \tLoss:   90.981201\trec:   62.399380\tkl:   28.581821\n",
      "Epoch: 951 [20100/50000 (40%)]  \tLoss:   88.916977\trec:   61.193359\tkl:   27.723625\n",
      "Epoch: 951 [30100/50000 (60%)]  \tLoss:   89.728569\trec:   61.224434\tkl:   28.504145\n",
      "Epoch: 951 [40100/50000 (80%)]  \tLoss:   83.506699\trec:   57.238384\tkl:   26.268314\n",
      "====> Epoch: 951 Average train loss: 88.5436\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7217\n",
      "Epoch: 952 [  100/50000 ( 0%)]  \tLoss:   91.012383\trec:   61.309872\tkl:   29.702505\n",
      "Epoch: 952 [10100/50000 (20%)]  \tLoss:   89.937622\trec:   61.320175\tkl:   28.617451\n",
      "Epoch: 952 [20100/50000 (40%)]  \tLoss:   87.545776\trec:   59.323162\tkl:   28.222616\n",
      "Epoch: 952 [30100/50000 (60%)]  \tLoss:   89.544189\trec:   61.558418\tkl:   27.985771\n",
      "Epoch: 952 [40100/50000 (80%)]  \tLoss:   90.298630\trec:   61.444736\tkl:   28.853899\n",
      "====> Epoch: 952 Average train loss: 88.5364\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7256\n",
      "Epoch: 953 [  100/50000 ( 0%)]  \tLoss:   88.948402\trec:   60.183826\tkl:   28.764580\n",
      "Epoch: 953 [10100/50000 (20%)]  \tLoss:   93.368896\trec:   64.050781\tkl:   29.318115\n",
      "Epoch: 953 [20100/50000 (40%)]  \tLoss:   89.328979\trec:   60.644802\tkl:   28.684183\n",
      "Epoch: 953 [30100/50000 (60%)]  \tLoss:   88.627480\trec:   59.726875\tkl:   28.900600\n",
      "Epoch: 953 [40100/50000 (80%)]  \tLoss:   89.550652\trec:   61.375637\tkl:   28.175011\n",
      "====> Epoch: 953 Average train loss: 88.5514\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.6981\n",
      "Epoch: 954 [  100/50000 ( 0%)]  \tLoss:   88.814491\trec:   59.747585\tkl:   29.066900\n",
      "Epoch: 954 [10100/50000 (20%)]  \tLoss:   90.036766\trec:   61.402763\tkl:   28.634003\n",
      "Epoch: 954 [20100/50000 (40%)]  \tLoss:   88.626930\trec:   59.760868\tkl:   28.866062\n",
      "Epoch: 954 [30100/50000 (60%)]  \tLoss:   88.148293\trec:   60.840790\tkl:   27.307497\n",
      "Epoch: 954 [40100/50000 (80%)]  \tLoss:   88.571373\trec:   59.529533\tkl:   29.041843\n",
      "====> Epoch: 954 Average train loss: 88.5317\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7912\n",
      "Epoch: 955 [  100/50000 ( 0%)]  \tLoss:   86.764214\trec:   58.700310\tkl:   28.063908\n",
      "Epoch: 955 [10100/50000 (20%)]  \tLoss:   90.334557\trec:   62.032883\tkl:   28.301676\n",
      "Epoch: 955 [20100/50000 (40%)]  \tLoss:   90.225990\trec:   60.802216\tkl:   29.423784\n",
      "Epoch: 955 [30100/50000 (60%)]  \tLoss:   91.782028\trec:   62.439266\tkl:   29.342760\n",
      "Epoch: 955 [40100/50000 (80%)]  \tLoss:   87.035118\trec:   58.514206\tkl:   28.520912\n",
      "====> Epoch: 955 Average train loss: 88.5409\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7884\n",
      "Epoch: 956 [  100/50000 ( 0%)]  \tLoss:   87.551338\trec:   59.856277\tkl:   27.695055\n",
      "Epoch: 956 [10100/50000 (20%)]  \tLoss:   84.535728\trec:   57.586060\tkl:   26.949675\n",
      "Epoch: 956 [20100/50000 (40%)]  \tLoss:   87.953979\trec:   59.987183\tkl:   27.966806\n",
      "Epoch: 956 [30100/50000 (60%)]  \tLoss:   91.609863\trec:   62.332382\tkl:   29.277485\n",
      "Epoch: 956 [40100/50000 (80%)]  \tLoss:   88.967690\trec:   60.671612\tkl:   28.296089\n",
      "====> Epoch: 956 Average train loss: 88.5382\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7623\n",
      "Epoch: 957 [  100/50000 ( 0%)]  \tLoss:   89.083755\trec:   61.263908\tkl:   27.819847\n",
      "Epoch: 957 [10100/50000 (20%)]  \tLoss:   87.828087\trec:   59.530544\tkl:   28.297539\n",
      "Epoch: 957 [20100/50000 (40%)]  \tLoss:   91.440338\trec:   63.126579\tkl:   28.313761\n",
      "Epoch: 957 [30100/50000 (60%)]  \tLoss:   89.409531\trec:   60.728352\tkl:   28.681181\n",
      "Epoch: 957 [40100/50000 (80%)]  \tLoss:   86.624939\trec:   59.729549\tkl:   26.895393\n",
      "====> Epoch: 957 Average train loss: 88.5444\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7695\n",
      "Epoch: 958 [  100/50000 ( 0%)]  \tLoss:   88.317001\trec:   59.214851\tkl:   29.102150\n",
      "Epoch: 958 [10100/50000 (20%)]  \tLoss:   89.148979\trec:   60.661289\tkl:   28.487690\n",
      "Epoch: 958 [20100/50000 (40%)]  \tLoss:   88.220253\trec:   59.719170\tkl:   28.501089\n",
      "Epoch: 958 [30100/50000 (60%)]  \tLoss:   88.512680\trec:   60.953793\tkl:   27.558893\n",
      "Epoch: 958 [40100/50000 (80%)]  \tLoss:   87.891991\trec:   59.656063\tkl:   28.235928\n",
      "====> Epoch: 958 Average train loss: 88.5288\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7331\n",
      "Epoch: 959 [  100/50000 ( 0%)]  \tLoss:   89.542580\trec:   61.931171\tkl:   27.611408\n",
      "Epoch: 959 [10100/50000 (20%)]  \tLoss:   90.463966\trec:   61.578392\tkl:   28.885571\n",
      "Epoch: 959 [20100/50000 (40%)]  \tLoss:   86.627068\trec:   59.443039\tkl:   27.184029\n",
      "Epoch: 959 [30100/50000 (60%)]  \tLoss:   84.631607\trec:   56.524002\tkl:   28.107607\n",
      "Epoch: 959 [40100/50000 (80%)]  \tLoss:   91.800644\trec:   62.988396\tkl:   28.812243\n",
      "====> Epoch: 959 Average train loss: 88.5240\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7109\n",
      "Epoch: 960 [  100/50000 ( 0%)]  \tLoss:   87.341896\trec:   59.137390\tkl:   28.204506\n",
      "Epoch: 960 [10100/50000 (20%)]  \tLoss:   88.233116\trec:   60.169804\tkl:   28.063311\n",
      "Epoch: 960 [20100/50000 (40%)]  \tLoss:   89.484566\trec:   61.209229\tkl:   28.275337\n",
      "Epoch: 960 [30100/50000 (60%)]  \tLoss:   83.778633\trec:   56.137566\tkl:   27.641060\n",
      "Epoch: 960 [40100/50000 (80%)]  \tLoss:   84.855255\trec:   58.201992\tkl:   26.653263\n",
      "====> Epoch: 960 Average train loss: 88.5205\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8344\n",
      "Epoch: 961 [  100/50000 ( 0%)]  \tLoss:   88.932106\trec:   60.285984\tkl:   28.646120\n",
      "Epoch: 961 [10100/50000 (20%)]  \tLoss:   85.182968\trec:   57.780342\tkl:   27.402622\n",
      "Epoch: 961 [20100/50000 (40%)]  \tLoss:   91.240517\trec:   62.341557\tkl:   28.898956\n",
      "Epoch: 961 [30100/50000 (60%)]  \tLoss:   89.107185\trec:   60.291576\tkl:   28.815613\n",
      "Epoch: 961 [40100/50000 (80%)]  \tLoss:   91.534042\trec:   62.526897\tkl:   29.007143\n",
      "====> Epoch: 961 Average train loss: 88.5203\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7649\n",
      "Epoch: 962 [  100/50000 ( 0%)]  \tLoss:   91.430504\trec:   62.236736\tkl:   29.193764\n",
      "Epoch: 962 [10100/50000 (20%)]  \tLoss:   86.996063\trec:   58.648914\tkl:   28.347147\n",
      "Epoch: 962 [20100/50000 (40%)]  \tLoss:   84.766853\trec:   57.248299\tkl:   27.518555\n",
      "Epoch: 962 [30100/50000 (60%)]  \tLoss:   90.057945\trec:   60.989784\tkl:   29.068159\n",
      "Epoch: 962 [40100/50000 (80%)]  \tLoss:   87.361656\trec:   59.146252\tkl:   28.215405\n",
      "====> Epoch: 962 Average train loss: 88.5176\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8304\n",
      "Epoch: 963 [  100/50000 ( 0%)]  \tLoss:   89.005424\trec:   59.968025\tkl:   29.037401\n",
      "Epoch: 963 [10100/50000 (20%)]  \tLoss:   90.972511\trec:   62.133621\tkl:   28.838884\n",
      "Epoch: 963 [20100/50000 (40%)]  \tLoss:   86.870415\trec:   58.900261\tkl:   27.970156\n",
      "Epoch: 963 [30100/50000 (60%)]  \tLoss:   87.998787\trec:   59.546810\tkl:   28.451982\n",
      "Epoch: 963 [40100/50000 (80%)]  \tLoss:   88.821587\trec:   60.426647\tkl:   28.394941\n",
      "====> Epoch: 963 Average train loss: 88.5067\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7818\n",
      "Epoch: 964 [  100/50000 ( 0%)]  \tLoss:   89.064148\trec:   60.302158\tkl:   28.761992\n",
      "Epoch: 964 [10100/50000 (20%)]  \tLoss:   85.106056\trec:   57.442215\tkl:   27.663837\n",
      "Epoch: 964 [20100/50000 (40%)]  \tLoss:   86.668358\trec:   59.012138\tkl:   27.656225\n",
      "Epoch: 964 [30100/50000 (60%)]  \tLoss:   88.383377\trec:   60.126980\tkl:   28.256395\n",
      "Epoch: 964 [40100/50000 (80%)]  \tLoss:   87.173981\trec:   59.101830\tkl:   28.072153\n",
      "====> Epoch: 964 Average train loss: 88.5215\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7656\n",
      "Epoch: 965 [  100/50000 ( 0%)]  \tLoss:   84.928925\trec:   57.941357\tkl:   26.987562\n",
      "Epoch: 965 [10100/50000 (20%)]  \tLoss:   91.866798\trec:   62.796707\tkl:   29.070087\n",
      "Epoch: 965 [20100/50000 (40%)]  \tLoss:   89.604637\trec:   61.258125\tkl:   28.346514\n",
      "Epoch: 965 [30100/50000 (60%)]  \tLoss:   88.894318\trec:   60.351608\tkl:   28.542700\n",
      "Epoch: 965 [40100/50000 (80%)]  \tLoss:   90.016518\trec:   61.965790\tkl:   28.050737\n",
      "====> Epoch: 965 Average train loss: 88.5098\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7505\n",
      "Epoch: 966 [  100/50000 ( 0%)]  \tLoss:   87.340096\trec:   59.280571\tkl:   28.059526\n",
      "Epoch: 966 [10100/50000 (20%)]  \tLoss:   86.977150\trec:   58.795994\tkl:   28.181156\n",
      "Epoch: 966 [20100/50000 (40%)]  \tLoss:   87.619576\trec:   58.594696\tkl:   29.024881\n",
      "Epoch: 966 [30100/50000 (60%)]  \tLoss:   89.643921\trec:   61.632736\tkl:   28.011187\n",
      "Epoch: 966 [40100/50000 (80%)]  \tLoss:   88.150612\trec:   60.485371\tkl:   27.665245\n",
      "====> Epoch: 966 Average train loss: 88.5089\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7679\n",
      "Epoch: 967 [  100/50000 ( 0%)]  \tLoss:   89.907166\trec:   61.402977\tkl:   28.504194\n",
      "Epoch: 967 [10100/50000 (20%)]  \tLoss:   87.365250\trec:   59.108391\tkl:   28.256865\n",
      "Epoch: 967 [20100/50000 (40%)]  \tLoss:   91.038376\trec:   62.230522\tkl:   28.807858\n",
      "Epoch: 967 [30100/50000 (60%)]  \tLoss:   90.610191\trec:   61.039646\tkl:   29.570543\n",
      "Epoch: 967 [40100/50000 (80%)]  \tLoss:   88.917114\trec:   59.714340\tkl:   29.202778\n",
      "====> Epoch: 967 Average train loss: 88.5155\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8069\n",
      "Epoch: 968 [  100/50000 ( 0%)]  \tLoss:   90.657166\trec:   61.707764\tkl:   28.949402\n",
      "Epoch: 968 [10100/50000 (20%)]  \tLoss:   91.760704\trec:   62.539246\tkl:   29.221455\n",
      "Epoch: 968 [20100/50000 (40%)]  \tLoss:   88.160614\trec:   59.439743\tkl:   28.720871\n",
      "Epoch: 968 [30100/50000 (60%)]  \tLoss:   84.760254\trec:   56.612362\tkl:   28.147888\n",
      "Epoch: 968 [40100/50000 (80%)]  \tLoss:   90.332642\trec:   61.758224\tkl:   28.574419\n",
      "====> Epoch: 968 Average train loss: 88.5041\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7541\n",
      "Epoch: 969 [  100/50000 ( 0%)]  \tLoss:   88.949982\trec:   61.209446\tkl:   27.740532\n",
      "Epoch: 969 [10100/50000 (20%)]  \tLoss:   90.334557\trec:   61.774696\tkl:   28.559862\n",
      "Epoch: 969 [20100/50000 (40%)]  \tLoss:   85.309937\trec:   56.971054\tkl:   28.338888\n",
      "Epoch: 969 [30100/50000 (60%)]  \tLoss:   88.967293\trec:   60.686691\tkl:   28.280600\n",
      "Epoch: 969 [40100/50000 (80%)]  \tLoss:   90.707191\trec:   62.203728\tkl:   28.503464\n",
      "====> Epoch: 969 Average train loss: 88.4958\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9013\n",
      "Epoch: 970 [  100/50000 ( 0%)]  \tLoss:   87.560509\trec:   58.788963\tkl:   28.771545\n",
      "Epoch: 970 [10100/50000 (20%)]  \tLoss:   83.779686\trec:   57.157997\tkl:   26.621689\n",
      "Epoch: 970 [20100/50000 (40%)]  \tLoss:   90.699020\trec:   61.848164\tkl:   28.850864\n",
      "Epoch: 970 [30100/50000 (60%)]  \tLoss:   90.236465\trec:   60.953415\tkl:   29.283047\n",
      "Epoch: 970 [40100/50000 (80%)]  \tLoss:   88.584686\trec:   60.580391\tkl:   28.004295\n",
      "====> Epoch: 970 Average train loss: 88.5065\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7369\n",
      "Epoch: 971 [  100/50000 ( 0%)]  \tLoss:   86.129677\trec:   58.646641\tkl:   27.483036\n",
      "Epoch: 971 [10100/50000 (20%)]  \tLoss:   87.280411\trec:   59.329075\tkl:   27.951328\n",
      "Epoch: 971 [20100/50000 (40%)]  \tLoss:   87.290390\trec:   59.193707\tkl:   28.096689\n",
      "Epoch: 971 [30100/50000 (60%)]  \tLoss:   89.149689\trec:   60.799786\tkl:   28.349899\n",
      "Epoch: 971 [40100/50000 (80%)]  \tLoss:   89.668976\trec:   60.898735\tkl:   28.770237\n",
      "====> Epoch: 971 Average train loss: 88.4967\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7830\n",
      "Epoch: 972 [  100/50000 ( 0%)]  \tLoss:   91.309448\trec:   62.935486\tkl:   28.373970\n",
      "Epoch: 972 [10100/50000 (20%)]  \tLoss:   84.312309\trec:   56.987888\tkl:   27.324423\n",
      "Epoch: 972 [20100/50000 (40%)]  \tLoss:   85.934738\trec:   57.902481\tkl:   28.032255\n",
      "Epoch: 972 [30100/50000 (60%)]  \tLoss:   85.102989\trec:   57.143913\tkl:   27.959072\n",
      "Epoch: 972 [40100/50000 (80%)]  \tLoss:   86.688683\trec:   58.913349\tkl:   27.775331\n",
      "====> Epoch: 972 Average train loss: 88.4878\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7085\n",
      "Epoch: 973 [  100/50000 ( 0%)]  \tLoss:   88.746017\trec:   61.914394\tkl:   26.831623\n",
      "Epoch: 973 [10100/50000 (20%)]  \tLoss:   86.410934\trec:   57.852737\tkl:   28.558193\n",
      "Epoch: 973 [20100/50000 (40%)]  \tLoss:   90.384117\trec:   61.974804\tkl:   28.409311\n",
      "Epoch: 973 [30100/50000 (60%)]  \tLoss:   90.748856\trec:   61.462391\tkl:   29.286461\n",
      "Epoch: 973 [40100/50000 (80%)]  \tLoss:   85.850723\trec:   58.312546\tkl:   27.538168\n",
      "====> Epoch: 973 Average train loss: 88.4952\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7576\n",
      "Epoch: 974 [  100/50000 ( 0%)]  \tLoss:   87.350349\trec:   60.201317\tkl:   27.149027\n",
      "Epoch: 974 [10100/50000 (20%)]  \tLoss:   87.387253\trec:   58.668999\tkl:   28.718258\n",
      "Epoch: 974 [20100/50000 (40%)]  \tLoss:   88.700562\trec:   60.123703\tkl:   28.576855\n",
      "Epoch: 974 [30100/50000 (60%)]  \tLoss:   91.662216\trec:   62.232136\tkl:   29.430075\n",
      "Epoch: 974 [40100/50000 (80%)]  \tLoss:   88.761063\trec:   60.588482\tkl:   28.172583\n",
      "====> Epoch: 974 Average train loss: 88.4851\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7585\n",
      "Epoch: 975 [  100/50000 ( 0%)]  \tLoss:   89.303894\trec:   60.752144\tkl:   28.551752\n",
      "Epoch: 975 [10100/50000 (20%)]  \tLoss:   85.694077\trec:   58.139030\tkl:   27.555054\n",
      "Epoch: 975 [20100/50000 (40%)]  \tLoss:   91.453033\trec:   62.724201\tkl:   28.728830\n",
      "Epoch: 975 [30100/50000 (60%)]  \tLoss:   89.912361\trec:   60.948330\tkl:   28.964033\n",
      "Epoch: 975 [40100/50000 (80%)]  \tLoss:   87.060860\trec:   59.003475\tkl:   28.057377\n",
      "====> Epoch: 975 Average train loss: 88.4772\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7651\n",
      "Epoch: 976 [  100/50000 ( 0%)]  \tLoss:   85.293457\trec:   57.657909\tkl:   27.635544\n",
      "Epoch: 976 [10100/50000 (20%)]  \tLoss:   90.637794\trec:   61.621563\tkl:   29.016228\n",
      "Epoch: 976 [20100/50000 (40%)]  \tLoss:   89.617615\trec:   61.295258\tkl:   28.322357\n",
      "Epoch: 976 [30100/50000 (60%)]  \tLoss:   87.647758\trec:   59.803650\tkl:   27.844110\n",
      "Epoch: 976 [40100/50000 (80%)]  \tLoss:   88.126297\trec:   59.700668\tkl:   28.425629\n",
      "====> Epoch: 976 Average train loss: 88.4979\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.9111\n",
      "Epoch: 977 [  100/50000 ( 0%)]  \tLoss:   90.929390\trec:   62.364250\tkl:   28.565136\n",
      "Epoch: 977 [10100/50000 (20%)]  \tLoss:   85.488609\trec:   58.280193\tkl:   27.208418\n",
      "Epoch: 977 [20100/50000 (40%)]  \tLoss:   85.176834\trec:   58.501534\tkl:   26.675306\n",
      "Epoch: 977 [30100/50000 (60%)]  \tLoss:   88.778770\trec:   59.330975\tkl:   29.447788\n",
      "Epoch: 977 [40100/50000 (80%)]  \tLoss:   86.305817\trec:   58.262997\tkl:   28.042826\n",
      "====> Epoch: 977 Average train loss: 88.4784\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7290\n",
      "Epoch: 978 [  100/50000 ( 0%)]  \tLoss:   83.061211\trec:   56.556942\tkl:   26.504267\n",
      "Epoch: 978 [10100/50000 (20%)]  \tLoss:   91.637985\trec:   62.365131\tkl:   29.272854\n",
      "Epoch: 978 [20100/50000 (40%)]  \tLoss:   92.829842\trec:   63.975437\tkl:   28.854399\n",
      "Epoch: 978 [30100/50000 (60%)]  \tLoss:   91.928787\trec:   62.064365\tkl:   29.864428\n",
      "Epoch: 978 [40100/50000 (80%)]  \tLoss:   88.757477\trec:   60.098789\tkl:   28.658686\n",
      "====> Epoch: 978 Average train loss: 88.4882\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.6751\n",
      "Epoch: 979 [  100/50000 ( 0%)]  \tLoss:   91.081940\trec:   62.109829\tkl:   28.972111\n",
      "Epoch: 979 [10100/50000 (20%)]  \tLoss:   92.070610\trec:   62.796833\tkl:   29.273779\n",
      "Epoch: 979 [20100/50000 (40%)]  \tLoss:   83.839546\trec:   57.421181\tkl:   26.418364\n",
      "Epoch: 979 [30100/50000 (60%)]  \tLoss:   88.550156\trec:   60.951035\tkl:   27.599125\n",
      "Epoch: 979 [40100/50000 (80%)]  \tLoss:   87.604698\trec:   59.548489\tkl:   28.056206\n",
      "====> Epoch: 979 Average train loss: 88.4708\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8506\n",
      "Epoch: 980 [  100/50000 ( 0%)]  \tLoss:   85.068291\trec:   58.268753\tkl:   26.799536\n",
      "Epoch: 980 [10100/50000 (20%)]  \tLoss:   84.974365\trec:   57.526337\tkl:   27.448025\n",
      "Epoch: 980 [20100/50000 (40%)]  \tLoss:   89.756035\trec:   60.059002\tkl:   29.697031\n",
      "Epoch: 980 [30100/50000 (60%)]  \tLoss:   88.671631\trec:   60.372021\tkl:   28.299608\n",
      "Epoch: 980 [40100/50000 (80%)]  \tLoss:   83.538513\trec:   56.770473\tkl:   26.768036\n",
      "====> Epoch: 980 Average train loss: 88.4705\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.6768\n",
      "Epoch: 981 [  100/50000 ( 0%)]  \tLoss:   89.776192\trec:   62.649441\tkl:   27.126753\n",
      "Epoch: 981 [10100/50000 (20%)]  \tLoss:   83.145386\trec:   55.571377\tkl:   27.574015\n",
      "Epoch: 981 [20100/50000 (40%)]  \tLoss:   85.567329\trec:   58.328728\tkl:   27.238604\n",
      "Epoch: 981 [30100/50000 (60%)]  \tLoss:   90.433472\trec:   61.334335\tkl:   29.099134\n",
      "Epoch: 981 [40100/50000 (80%)]  \tLoss:   83.978935\trec:   56.635986\tkl:   27.342949\n",
      "====> Epoch: 981 Average train loss: 88.4886\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8521\n",
      "Epoch: 982 [  100/50000 ( 0%)]  \tLoss:   87.688087\trec:   58.900219\tkl:   28.787868\n",
      "Epoch: 982 [10100/50000 (20%)]  \tLoss:   94.985214\trec:   65.650024\tkl:   29.335184\n",
      "Epoch: 982 [20100/50000 (40%)]  \tLoss:   88.613113\trec:   60.306999\tkl:   28.306110\n",
      "Epoch: 982 [30100/50000 (60%)]  \tLoss:   87.327286\trec:   59.353062\tkl:   27.974228\n",
      "Epoch: 982 [40100/50000 (80%)]  \tLoss:   90.147789\trec:   61.798779\tkl:   28.349016\n",
      "====> Epoch: 982 Average train loss: 88.4865\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7642\n",
      "Epoch: 983 [  100/50000 ( 0%)]  \tLoss:   88.737755\trec:   61.111736\tkl:   27.626011\n",
      "Epoch: 983 [10100/50000 (20%)]  \tLoss:   86.768013\trec:   59.251629\tkl:   27.516386\n",
      "Epoch: 983 [20100/50000 (40%)]  \tLoss:   88.979042\trec:   60.215176\tkl:   28.763868\n",
      "Epoch: 983 [30100/50000 (60%)]  \tLoss:   87.363609\trec:   58.796211\tkl:   28.567396\n",
      "Epoch: 983 [40100/50000 (80%)]  \tLoss:   88.586891\trec:   59.536484\tkl:   29.050415\n",
      "====> Epoch: 983 Average train loss: 88.5001\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7606\n",
      "Epoch: 984 [  100/50000 ( 0%)]  \tLoss:   88.688217\trec:   60.520744\tkl:   28.167477\n",
      "Epoch: 984 [10100/50000 (20%)]  \tLoss:   89.616127\trec:   60.813519\tkl:   28.802614\n",
      "Epoch: 984 [20100/50000 (40%)]  \tLoss:   89.308220\trec:   61.025890\tkl:   28.282333\n",
      "Epoch: 984 [30100/50000 (60%)]  \tLoss:   87.929474\trec:   58.738239\tkl:   29.191235\n",
      "Epoch: 984 [40100/50000 (80%)]  \tLoss:   85.321236\trec:   58.711845\tkl:   26.609392\n",
      "====> Epoch: 984 Average train loss: 88.4714\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8177\n",
      "Epoch: 985 [  100/50000 ( 0%)]  \tLoss:   90.860428\trec:   61.731159\tkl:   29.129263\n",
      "Epoch: 985 [10100/50000 (20%)]  \tLoss:   89.343193\trec:   59.362480\tkl:   29.980715\n",
      "Epoch: 985 [20100/50000 (40%)]  \tLoss:   91.316963\trec:   62.826649\tkl:   28.490313\n",
      "Epoch: 985 [30100/50000 (60%)]  \tLoss:   88.659737\trec:   59.921162\tkl:   28.738571\n",
      "Epoch: 985 [40100/50000 (80%)]  \tLoss:   89.390656\trec:   60.637295\tkl:   28.753359\n",
      "====> Epoch: 985 Average train loss: 88.4617\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7682\n",
      "Epoch: 986 [  100/50000 ( 0%)]  \tLoss:   91.013611\trec:   62.494274\tkl:   28.519333\n",
      "Epoch: 986 [10100/50000 (20%)]  \tLoss:   88.058456\trec:   59.343777\tkl:   28.714672\n",
      "Epoch: 986 [20100/50000 (40%)]  \tLoss:   87.160294\trec:   59.746208\tkl:   27.414082\n",
      "Epoch: 986 [30100/50000 (60%)]  \tLoss:   92.036339\trec:   63.779297\tkl:   28.257038\n",
      "Epoch: 986 [40100/50000 (80%)]  \tLoss:   88.088516\trec:   59.529499\tkl:   28.559008\n",
      "====> Epoch: 986 Average train loss: 88.4466\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7719\n",
      "Epoch: 987 [  100/50000 ( 0%)]  \tLoss:   85.857185\trec:   58.262371\tkl:   27.594816\n",
      "Epoch: 987 [10100/50000 (20%)]  \tLoss:   91.468338\trec:   63.144016\tkl:   28.324316\n",
      "Epoch: 987 [20100/50000 (40%)]  \tLoss:   87.432671\trec:   59.005810\tkl:   28.426859\n",
      "Epoch: 987 [30100/50000 (60%)]  \tLoss:   84.775703\trec:   56.796631\tkl:   27.979073\n",
      "Epoch: 987 [40100/50000 (80%)]  \tLoss:   91.556618\trec:   62.862728\tkl:   28.693886\n",
      "====> Epoch: 987 Average train loss: 88.4795\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7767\n",
      "Epoch: 988 [  100/50000 ( 0%)]  \tLoss:   87.003555\trec:   58.252388\tkl:   28.751167\n",
      "Epoch: 988 [10100/50000 (20%)]  \tLoss:   87.208710\trec:   59.728943\tkl:   27.479767\n",
      "Epoch: 988 [20100/50000 (40%)]  \tLoss:   89.331459\trec:   60.388416\tkl:   28.943047\n",
      "Epoch: 988 [30100/50000 (60%)]  \tLoss:   88.814278\trec:   60.778912\tkl:   28.035366\n",
      "Epoch: 988 [40100/50000 (80%)]  \tLoss:   86.434029\trec:   58.732742\tkl:   27.701288\n",
      "====> Epoch: 988 Average train loss: 88.4597\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7470\n",
      "Epoch: 989 [  100/50000 ( 0%)]  \tLoss:   89.487892\trec:   61.610874\tkl:   27.877022\n",
      "Epoch: 989 [10100/50000 (20%)]  \tLoss:   85.927856\trec:   58.293682\tkl:   27.634178\n",
      "Epoch: 989 [20100/50000 (40%)]  \tLoss:   86.330734\trec:   57.384995\tkl:   28.945736\n",
      "Epoch: 989 [30100/50000 (60%)]  \tLoss:   91.508530\trec:   63.496460\tkl:   28.012075\n",
      "Epoch: 989 [40100/50000 (80%)]  \tLoss:   89.316017\trec:   61.550583\tkl:   27.765434\n",
      "====> Epoch: 989 Average train loss: 88.4656\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7111\n",
      "Epoch: 990 [  100/50000 ( 0%)]  \tLoss:   86.300117\trec:   58.605537\tkl:   27.694580\n",
      "Epoch: 990 [10100/50000 (20%)]  \tLoss:   86.215034\trec:   58.240646\tkl:   27.974394\n",
      "Epoch: 990 [20100/50000 (40%)]  \tLoss:   90.005524\trec:   61.161575\tkl:   28.843945\n",
      "Epoch: 990 [30100/50000 (60%)]  \tLoss:   88.539879\trec:   60.159023\tkl:   28.380863\n",
      "Epoch: 990 [40100/50000 (80%)]  \tLoss:   88.678535\trec:   60.690929\tkl:   27.987602\n",
      "====> Epoch: 990 Average train loss: 88.4646\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7712\n",
      "Epoch: 991 [  100/50000 ( 0%)]  \tLoss:   89.962891\trec:   62.171776\tkl:   27.791107\n",
      "Epoch: 991 [10100/50000 (20%)]  \tLoss:   89.346024\trec:   60.688103\tkl:   28.657919\n",
      "Epoch: 991 [20100/50000 (40%)]  \tLoss:   88.990173\trec:   60.489349\tkl:   28.500830\n",
      "Epoch: 991 [30100/50000 (60%)]  \tLoss:   88.435112\trec:   60.827209\tkl:   27.607903\n",
      "Epoch: 991 [40100/50000 (80%)]  \tLoss:   91.590546\trec:   63.688393\tkl:   27.902157\n",
      "====> Epoch: 991 Average train loss: 88.4684\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7698\n",
      "Epoch: 992 [  100/50000 ( 0%)]  \tLoss:   86.036484\trec:   57.815525\tkl:   28.220957\n",
      "Epoch: 992 [10100/50000 (20%)]  \tLoss:   87.178123\trec:   60.127197\tkl:   27.050924\n",
      "Epoch: 992 [20100/50000 (40%)]  \tLoss:   90.249023\trec:   61.243008\tkl:   29.006020\n",
      "Epoch: 992 [30100/50000 (60%)]  \tLoss:   89.946053\trec:   61.265495\tkl:   28.680555\n",
      "Epoch: 992 [40100/50000 (80%)]  \tLoss:   92.098640\trec:   62.445019\tkl:   29.653622\n",
      "====> Epoch: 992 Average train loss: 88.4596\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8347\n",
      "Epoch: 993 [  100/50000 ( 0%)]  \tLoss:   89.217720\trec:   61.200085\tkl:   28.017633\n",
      "Epoch: 993 [10100/50000 (20%)]  \tLoss:   91.470161\trec:   62.201649\tkl:   29.268517\n",
      "Epoch: 993 [20100/50000 (40%)]  \tLoss:   87.299545\trec:   59.122253\tkl:   28.177298\n",
      "Epoch: 993 [30100/50000 (60%)]  \tLoss:   87.031815\trec:   60.154106\tkl:   26.877705\n",
      "Epoch: 993 [40100/50000 (80%)]  \tLoss:   88.043427\trec:   59.743340\tkl:   28.300087\n",
      "====> Epoch: 993 Average train loss: 88.4727\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7728\n",
      "Epoch: 994 [  100/50000 ( 0%)]  \tLoss:   85.728592\trec:   58.160019\tkl:   27.568579\n",
      "Epoch: 994 [10100/50000 (20%)]  \tLoss:   88.347771\trec:   60.429363\tkl:   27.918413\n",
      "Epoch: 994 [20100/50000 (40%)]  \tLoss:   94.820045\trec:   65.169083\tkl:   29.650967\n",
      "Epoch: 994 [30100/50000 (60%)]  \tLoss:   89.453514\trec:   60.753582\tkl:   28.699926\n",
      "Epoch: 994 [40100/50000 (80%)]  \tLoss:   85.678917\trec:   57.974075\tkl:   27.704838\n",
      "====> Epoch: 994 Average train loss: 88.4445\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.6911\n",
      "Epoch: 995 [  100/50000 ( 0%)]  \tLoss:   90.827477\trec:   62.087841\tkl:   28.739634\n",
      "Epoch: 995 [10100/50000 (20%)]  \tLoss:   91.109001\trec:   62.223469\tkl:   28.885529\n",
      "Epoch: 995 [20100/50000 (40%)]  \tLoss:   90.725441\trec:   61.281883\tkl:   29.443552\n",
      "Epoch: 995 [30100/50000 (60%)]  \tLoss:   86.281952\trec:   59.280796\tkl:   27.001154\n",
      "Epoch: 995 [40100/50000 (80%)]  \tLoss:   88.615456\trec:   59.505898\tkl:   29.109562\n",
      "====> Epoch: 995 Average train loss: 88.4604\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7323\n",
      "Epoch: 996 [  100/50000 ( 0%)]  \tLoss:   92.253922\trec:   63.550018\tkl:   28.703905\n",
      "Epoch: 996 [10100/50000 (20%)]  \tLoss:   89.065834\trec:   60.831982\tkl:   28.233862\n",
      "Epoch: 996 [20100/50000 (40%)]  \tLoss:   89.894608\trec:   61.743622\tkl:   28.150986\n",
      "Epoch: 996 [30100/50000 (60%)]  \tLoss:   91.168159\trec:   61.315132\tkl:   29.853037\n",
      "Epoch: 996 [40100/50000 (80%)]  \tLoss:   87.316666\trec:   59.728771\tkl:   27.587894\n",
      "====> Epoch: 996 Average train loss: 88.4515\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7803\n",
      "Epoch: 997 [  100/50000 ( 0%)]  \tLoss:   84.536774\trec:   56.770447\tkl:   27.766325\n",
      "Epoch: 997 [10100/50000 (20%)]  \tLoss:   84.782364\trec:   57.154648\tkl:   27.627714\n",
      "Epoch: 997 [20100/50000 (40%)]  \tLoss:   90.557358\trec:   61.569881\tkl:   28.987478\n",
      "Epoch: 997 [30100/50000 (60%)]  \tLoss:   93.182648\trec:   64.171684\tkl:   29.010960\n",
      "Epoch: 997 [40100/50000 (80%)]  \tLoss:   87.014839\trec:   59.029980\tkl:   27.984858\n",
      "====> Epoch: 997 Average train loss: 88.4355\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7376\n",
      "Epoch: 998 [  100/50000 ( 0%)]  \tLoss:   83.988602\trec:   56.813858\tkl:   27.174747\n",
      "Epoch: 998 [10100/50000 (20%)]  \tLoss:   87.260544\trec:   59.421043\tkl:   27.839506\n",
      "Epoch: 998 [20100/50000 (40%)]  \tLoss:   87.889313\trec:   59.863129\tkl:   28.026192\n",
      "Epoch: 998 [30100/50000 (60%)]  \tLoss:   87.363457\trec:   58.786366\tkl:   28.577095\n",
      "Epoch: 998 [40100/50000 (80%)]  \tLoss:   86.425049\trec:   59.008877\tkl:   27.416168\n",
      "====> Epoch: 998 Average train loss: 88.4564\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.8628\n",
      "Epoch: 999 [  100/50000 ( 0%)]  \tLoss:   86.346367\trec:   58.286411\tkl:   28.059950\n",
      "Epoch: 999 [10100/50000 (20%)]  \tLoss:   93.126053\trec:   64.338173\tkl:   28.787876\n",
      "Epoch: 999 [20100/50000 (40%)]  \tLoss:   88.271599\trec:   60.468792\tkl:   27.802807\n",
      "Epoch: 999 [30100/50000 (60%)]  \tLoss:   88.066132\trec:   59.735405\tkl:   28.330727\n",
      "Epoch: 999 [40100/50000 (80%)]  \tLoss:   86.240585\trec:   59.672665\tkl:   26.567919\n",
      "====> Epoch: 999 Average train loss: 88.4279\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.6744\n",
      "Epoch: 1000 [  100/50000 ( 0%)]  \tLoss:   89.156441\trec:   60.804043\tkl:   28.352407\n",
      "Epoch: 1000 [10100/50000 (20%)]  \tLoss:   87.720505\trec:   59.031082\tkl:   28.689419\n",
      "Epoch: 1000 [20100/50000 (40%)]  \tLoss:   89.696983\trec:   60.901814\tkl:   28.795162\n",
      "Epoch: 1000 [30100/50000 (60%)]  \tLoss:   87.102592\trec:   58.824104\tkl:   28.278496\n",
      "Epoch: 1000 [40100/50000 (80%)]  \tLoss:   90.553116\trec:   62.250736\tkl:   28.302378\n",
      "====> Epoch: 1000 Average train loss: 88.4343\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 91.7932\n",
      "====> Validation set loss: 91.8404\n",
      "Computing log-likelihood on test set\n",
      "Progress: 0.00%\n",
      "Progress: 10.00%\n",
      "Progress: 20.00%\n",
      "Progress: 30.00%\n",
      "Progress: 40.00%\n",
      "Progress: 50.00%\n",
      "Progress: 60.00%\n",
      "Progress: 70.00%\n",
      "Progress: 80.00%\n",
      "Progress: 90.00%\n",
      "====> Test set loss: 91.0233\n",
      "====> Test set log-likelihood: 87.1156\n"
     ]
    }
   ],
   "source": [
    "%run main_experiment_VAE.py -nf 80"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOCANXVkoRmsdKlR5Y6Zjt1",
   "name": "test80.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
