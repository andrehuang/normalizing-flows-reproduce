{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1398,
     "status": "ok",
     "timestamp": 1616446317850,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "E8yewXh1koRs",
    "outputId": "f66720f4-cebf-4f1b-e123-7b5737abfdaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3316,
     "status": "ok",
     "timestamp": 1616446319775,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "3CuMkXZXkpy6",
    "outputId": "80cc132e-f30b-4dcc-9832-3bd020b3a514"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3313,
     "status": "ok",
     "timestamp": 1616446319776,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "brrj5JJKkrGf",
    "outputId": "0ee877ea-bf99-48a1-861e-082588d035d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "cd \"/content/drive/MyDrive/Colab Notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11181568,
     "status": "ok",
     "timestamp": 1616457498035,
     "user": {
      "displayName": "Ella Mi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Yd9plYYGR0Sqt_VU1TCQay-uWSh0kQaJ9mbj=s64",
      "userId": "08375240338561811389"
     },
     "user_tz": 0
    },
    "id": "jxGIEOGXks39",
    "outputId": "2df4424f-b7bd-4f0a-ee82-e2875aa848e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.2414\n",
      "Epoch: 378 [  100/50000 ( 0%)]  \tLoss:   88.052765\trec:   62.977119\tkl:   25.075647\n",
      "Epoch: 378 [10100/50000 (20%)]  \tLoss:   90.868668\trec:   63.859322\tkl:   27.009352\n",
      "Epoch: 378 [20100/50000 (40%)]  \tLoss:   86.046455\trec:   60.533749\tkl:   25.512707\n",
      "Epoch: 378 [30100/50000 (60%)]  \tLoss:   90.910233\trec:   64.343925\tkl:   26.566305\n",
      "Epoch: 378 [40100/50000 (80%)]  \tLoss:   93.025620\trec:   66.181252\tkl:   26.844378\n",
      "====> Epoch: 378 Average train loss: 91.6302\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.2186\n",
      "Epoch: 379 [  100/50000 ( 0%)]  \tLoss:   90.274010\trec:   62.973045\tkl:   27.300966\n",
      "Epoch: 379 [10100/50000 (20%)]  \tLoss:   95.282570\trec:   67.467560\tkl:   27.815006\n",
      "Epoch: 379 [20100/50000 (40%)]  \tLoss:   89.587509\trec:   63.333664\tkl:   26.253841\n",
      "Epoch: 379 [30100/50000 (60%)]  \tLoss:   91.071602\trec:   64.883240\tkl:   26.188362\n",
      "Epoch: 379 [40100/50000 (80%)]  \tLoss:   90.565620\trec:   63.904976\tkl:   26.660648\n",
      "====> Epoch: 379 Average train loss: 91.5932\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.2575\n",
      "Epoch: 380 [  100/50000 ( 0%)]  \tLoss:   89.085678\trec:   63.228447\tkl:   25.857235\n",
      "Epoch: 380 [10100/50000 (20%)]  \tLoss:   90.521309\trec:   64.305115\tkl:   26.216194\n",
      "Epoch: 380 [20100/50000 (40%)]  \tLoss:   93.420708\trec:   66.490524\tkl:   26.930193\n",
      "Epoch: 380 [30100/50000 (60%)]  \tLoss:   91.239647\trec:   64.365135\tkl:   26.874508\n",
      "Epoch: 380 [40100/50000 (80%)]  \tLoss:   94.170074\trec:   66.982765\tkl:   27.187319\n",
      "====> Epoch: 380 Average train loss: 91.6137\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.2157\n",
      "Epoch: 381 [  100/50000 ( 0%)]  \tLoss:   92.639694\trec:   66.282150\tkl:   26.357546\n",
      "Epoch: 381 [10100/50000 (20%)]  \tLoss:   90.443146\trec:   64.691772\tkl:   25.751366\n",
      "Epoch: 381 [20100/50000 (40%)]  \tLoss:   92.521957\trec:   65.883865\tkl:   26.638098\n",
      "Epoch: 381 [30100/50000 (60%)]  \tLoss:   88.072510\trec:   62.355915\tkl:   25.716593\n",
      "Epoch: 381 [40100/50000 (80%)]  \tLoss:   91.040634\trec:   64.882622\tkl:   26.158005\n",
      "====> Epoch: 381 Average train loss: 91.5523\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1464\n",
      "Epoch: 382 [  100/50000 ( 0%)]  \tLoss:   87.903435\trec:   61.340912\tkl:   26.562519\n",
      "Epoch: 382 [10100/50000 (20%)]  \tLoss:   94.468552\trec:   67.854271\tkl:   26.614281\n",
      "Epoch: 382 [20100/50000 (40%)]  \tLoss:   93.779648\trec:   66.516510\tkl:   27.263134\n",
      "Epoch: 382 [30100/50000 (60%)]  \tLoss:   90.403748\trec:   64.049019\tkl:   26.354729\n",
      "Epoch: 382 [40100/50000 (80%)]  \tLoss:   92.055954\trec:   65.471344\tkl:   26.584604\n",
      "====> Epoch: 382 Average train loss: 91.6007\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1870\n",
      "Epoch: 383 [  100/50000 ( 0%)]  \tLoss:   92.014381\trec:   65.428894\tkl:   26.585485\n",
      "Epoch: 383 [10100/50000 (20%)]  \tLoss:   92.708130\trec:   65.660904\tkl:   27.047224\n",
      "Epoch: 383 [20100/50000 (40%)]  \tLoss:   91.225456\trec:   64.499855\tkl:   26.725603\n",
      "Epoch: 383 [30100/50000 (60%)]  \tLoss:   92.634888\trec:   66.708923\tkl:   25.925966\n",
      "Epoch: 383 [40100/50000 (80%)]  \tLoss:   91.057724\trec:   63.877674\tkl:   27.180048\n",
      "====> Epoch: 383 Average train loss: 91.5771\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1406\n",
      "Epoch: 384 [  100/50000 ( 0%)]  \tLoss:   93.582100\trec:   66.437012\tkl:   27.145088\n",
      "Epoch: 384 [10100/50000 (20%)]  \tLoss:   87.802948\trec:   62.153320\tkl:   25.649624\n",
      "Epoch: 384 [20100/50000 (40%)]  \tLoss:   93.130173\trec:   66.485947\tkl:   26.644228\n",
      "Epoch: 384 [30100/50000 (60%)]  \tLoss:   93.643982\trec:   66.634743\tkl:   27.009239\n",
      "Epoch: 384 [40100/50000 (80%)]  \tLoss:   88.512077\trec:   62.668827\tkl:   25.843254\n",
      "====> Epoch: 384 Average train loss: 91.5446\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.2415\n",
      "Epoch: 385 [  100/50000 ( 0%)]  \tLoss:   87.809006\trec:   61.647087\tkl:   26.161913\n",
      "Epoch: 385 [10100/50000 (20%)]  \tLoss:   91.765015\trec:   65.439590\tkl:   26.325430\n",
      "Epoch: 385 [20100/50000 (40%)]  \tLoss:   90.351616\trec:   63.532177\tkl:   26.819445\n",
      "Epoch: 385 [30100/50000 (60%)]  \tLoss:   96.954453\trec:   68.718094\tkl:   28.236362\n",
      "Epoch: 385 [40100/50000 (80%)]  \tLoss:   88.865593\trec:   63.200146\tkl:   25.665445\n",
      "====> Epoch: 385 Average train loss: 91.5606\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1657\n",
      "Epoch: 386 [  100/50000 ( 0%)]  \tLoss:   90.858513\trec:   64.113403\tkl:   26.745108\n",
      "Epoch: 386 [10100/50000 (20%)]  \tLoss:   95.035484\trec:   67.028122\tkl:   28.007362\n",
      "Epoch: 386 [20100/50000 (40%)]  \tLoss:   93.329803\trec:   67.014816\tkl:   26.314993\n",
      "Epoch: 386 [30100/50000 (60%)]  \tLoss:   97.810997\trec:   70.235870\tkl:   27.575117\n",
      "Epoch: 386 [40100/50000 (80%)]  \tLoss:   93.108849\trec:   65.986229\tkl:   27.122612\n",
      "====> Epoch: 386 Average train loss: 91.5479\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1502\n",
      "Epoch: 387 [  100/50000 ( 0%)]  \tLoss:   90.156929\trec:   63.519974\tkl:   26.636953\n",
      "Epoch: 387 [10100/50000 (20%)]  \tLoss:   90.329643\trec:   64.085632\tkl:   26.244011\n",
      "Epoch: 387 [20100/50000 (40%)]  \tLoss:   91.527016\trec:   64.811249\tkl:   26.715769\n",
      "Epoch: 387 [30100/50000 (60%)]  \tLoss:   95.114616\trec:   68.004433\tkl:   27.110186\n",
      "Epoch: 387 [40100/50000 (80%)]  \tLoss:   92.277596\trec:   65.765381\tkl:   26.512217\n",
      "====> Epoch: 387 Average train loss: 91.5452\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0837\n",
      "Epoch: 388 [  100/50000 ( 0%)]  \tLoss:   90.229843\trec:   63.798489\tkl:   26.431356\n",
      "Epoch: 388 [10100/50000 (20%)]  \tLoss:   92.220230\trec:   65.565437\tkl:   26.654795\n",
      "Epoch: 388 [20100/50000 (40%)]  \tLoss:   98.356895\trec:   71.033897\tkl:   27.322992\n",
      "Epoch: 388 [30100/50000 (60%)]  \tLoss:   92.392868\trec:   65.561974\tkl:   26.830889\n",
      "Epoch: 388 [40100/50000 (80%)]  \tLoss:   89.480446\trec:   63.730305\tkl:   25.750135\n",
      "====> Epoch: 388 Average train loss: 91.5179\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1880\n",
      "Epoch: 389 [  100/50000 ( 0%)]  \tLoss:   94.169518\trec:   66.908005\tkl:   27.261513\n",
      "Epoch: 389 [10100/50000 (20%)]  \tLoss:   89.532486\trec:   63.133171\tkl:   26.399319\n",
      "Epoch: 389 [20100/50000 (40%)]  \tLoss:   90.000893\trec:   63.171883\tkl:   26.829018\n",
      "Epoch: 389 [30100/50000 (60%)]  \tLoss:   86.237228\trec:   60.611950\tkl:   25.625273\n",
      "Epoch: 389 [40100/50000 (80%)]  \tLoss:   94.348251\trec:   67.596008\tkl:   26.752237\n",
      "====> Epoch: 389 Average train loss: 91.5374\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.2211\n",
      "Epoch: 390 [  100/50000 ( 0%)]  \tLoss:   93.363396\trec:   66.473923\tkl:   26.889477\n",
      "Epoch: 390 [10100/50000 (20%)]  \tLoss:   94.885117\trec:   66.792603\tkl:   28.092512\n",
      "Epoch: 390 [20100/50000 (40%)]  \tLoss:   89.612869\trec:   63.430595\tkl:   26.182272\n",
      "Epoch: 390 [30100/50000 (60%)]  \tLoss:   90.309517\trec:   63.531757\tkl:   26.777763\n",
      "Epoch: 390 [40100/50000 (80%)]  \tLoss:   95.214790\trec:   68.195984\tkl:   27.018810\n",
      "====> Epoch: 390 Average train loss: 91.5211\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0935\n",
      "Epoch: 391 [  100/50000 ( 0%)]  \tLoss:   90.082848\trec:   63.553825\tkl:   26.529026\n",
      "Epoch: 391 [10100/50000 (20%)]  \tLoss:   89.589897\trec:   63.952225\tkl:   25.637678\n",
      "Epoch: 391 [20100/50000 (40%)]  \tLoss:   90.684799\trec:   64.483047\tkl:   26.201757\n",
      "Epoch: 391 [30100/50000 (60%)]  \tLoss:   86.833435\trec:   61.724636\tkl:   25.108803\n",
      "Epoch: 391 [40100/50000 (80%)]  \tLoss:   87.643311\trec:   61.615849\tkl:   26.027462\n",
      "====> Epoch: 391 Average train loss: 91.5063\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1145\n",
      "Epoch: 392 [  100/50000 ( 0%)]  \tLoss:   97.058044\trec:   69.157059\tkl:   27.900984\n",
      "Epoch: 392 [10100/50000 (20%)]  \tLoss:   87.065933\trec:   62.130501\tkl:   24.935432\n",
      "Epoch: 392 [20100/50000 (40%)]  \tLoss:   87.102905\trec:   60.421600\tkl:   26.681313\n",
      "Epoch: 392 [30100/50000 (60%)]  \tLoss:   90.079620\trec:   64.289825\tkl:   25.789797\n",
      "Epoch: 392 [40100/50000 (80%)]  \tLoss:   88.835014\trec:   62.554886\tkl:   26.280128\n",
      "====> Epoch: 392 Average train loss: 91.4945\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1984\n",
      "Epoch: 393 [  100/50000 ( 0%)]  \tLoss:   91.482338\trec:   65.009895\tkl:   26.472448\n",
      "Epoch: 393 [10100/50000 (20%)]  \tLoss:   88.107986\trec:   61.694099\tkl:   26.413891\n",
      "Epoch: 393 [20100/50000 (40%)]  \tLoss:   90.650299\trec:   63.489880\tkl:   27.160418\n",
      "Epoch: 393 [30100/50000 (60%)]  \tLoss:   87.792168\trec:   61.245197\tkl:   26.546963\n",
      "Epoch: 393 [40100/50000 (80%)]  \tLoss:   91.123047\trec:   64.169579\tkl:   26.953466\n",
      "====> Epoch: 393 Average train loss: 91.5079\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1297\n",
      "Epoch: 394 [  100/50000 ( 0%)]  \tLoss:   93.306190\trec:   66.200783\tkl:   27.105415\n",
      "Epoch: 394 [10100/50000 (20%)]  \tLoss:   96.650429\trec:   68.826393\tkl:   27.824036\n",
      "Epoch: 394 [20100/50000 (40%)]  \tLoss:   90.409599\trec:   63.720722\tkl:   26.688879\n",
      "Epoch: 394 [30100/50000 (60%)]  \tLoss:   93.073425\trec:   66.386032\tkl:   26.687391\n",
      "Epoch: 394 [40100/50000 (80%)]  \tLoss:   93.337990\trec:   66.186844\tkl:   27.151144\n",
      "====> Epoch: 394 Average train loss: 91.4877\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.2467\n",
      "Epoch: 395 [  100/50000 ( 0%)]  \tLoss:   92.378242\trec:   65.350868\tkl:   27.027378\n",
      "Epoch: 395 [10100/50000 (20%)]  \tLoss:   93.185272\trec:   66.538208\tkl:   26.647055\n",
      "Epoch: 395 [20100/50000 (40%)]  \tLoss:   93.088623\trec:   66.392761\tkl:   26.695856\n",
      "Epoch: 395 [30100/50000 (60%)]  \tLoss:   86.756577\trec:   61.417812\tkl:   25.338770\n",
      "Epoch: 395 [40100/50000 (80%)]  \tLoss:   87.561974\trec:   61.632439\tkl:   25.929531\n",
      "====> Epoch: 395 Average train loss: 91.5119\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.2194\n",
      "Epoch: 396 [  100/50000 ( 0%)]  \tLoss:   90.424965\trec:   63.433495\tkl:   26.991474\n",
      "Epoch: 396 [10100/50000 (20%)]  \tLoss:   86.803276\trec:   61.372246\tkl:   25.431032\n",
      "Epoch: 396 [20100/50000 (40%)]  \tLoss:   91.487183\trec:   65.021950\tkl:   26.465237\n",
      "Epoch: 396 [30100/50000 (60%)]  \tLoss:   92.527344\trec:   66.218994\tkl:   26.308344\n",
      "Epoch: 396 [40100/50000 (80%)]  \tLoss:   92.520073\trec:   66.076988\tkl:   26.443085\n",
      "====> Epoch: 396 Average train loss: 91.4707\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0633\n",
      "Epoch: 397 [  100/50000 ( 0%)]  \tLoss:   88.800957\trec:   62.920235\tkl:   25.880720\n",
      "Epoch: 397 [10100/50000 (20%)]  \tLoss:   93.340416\trec:   65.913742\tkl:   27.426682\n",
      "Epoch: 397 [20100/50000 (40%)]  \tLoss:   92.083008\trec:   65.976044\tkl:   26.106968\n",
      "Epoch: 397 [30100/50000 (60%)]  \tLoss:   85.447159\trec:   60.297565\tkl:   25.149588\n",
      "Epoch: 397 [40100/50000 (80%)]  \tLoss:   94.466042\trec:   67.014961\tkl:   27.451080\n",
      "====> Epoch: 397 Average train loss: 91.4803\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0977\n",
      "Epoch: 398 [  100/50000 ( 0%)]  \tLoss:   90.161064\trec:   63.565681\tkl:   26.595383\n",
      "Epoch: 398 [10100/50000 (20%)]  \tLoss:   90.854431\trec:   64.797043\tkl:   26.057390\n",
      "Epoch: 398 [20100/50000 (40%)]  \tLoss:   92.140366\trec:   65.887558\tkl:   26.252815\n",
      "Epoch: 398 [30100/50000 (60%)]  \tLoss:   87.044228\trec:   60.327831\tkl:   26.716396\n",
      "Epoch: 398 [40100/50000 (80%)]  \tLoss:   90.214394\trec:   64.981079\tkl:   25.233311\n",
      "====> Epoch: 398 Average train loss: 91.4626\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0797\n",
      "Epoch: 399 [  100/50000 ( 0%)]  \tLoss:   90.815048\trec:   63.567474\tkl:   27.247570\n",
      "Epoch: 399 [10100/50000 (20%)]  \tLoss:   90.558632\trec:   63.584690\tkl:   26.973942\n",
      "Epoch: 399 [20100/50000 (40%)]  \tLoss:   89.765602\trec:   63.504692\tkl:   26.260916\n",
      "Epoch: 399 [30100/50000 (60%)]  \tLoss:   93.261452\trec:   65.801949\tkl:   27.459503\n",
      "Epoch: 399 [40100/50000 (80%)]  \tLoss:   92.172035\trec:   65.737251\tkl:   26.434790\n",
      "====> Epoch: 399 Average train loss: 91.4680\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0133\n",
      "Epoch: 400 [  100/50000 ( 0%)]  \tLoss:   90.558807\trec:   63.973125\tkl:   26.585684\n",
      "Epoch: 400 [10100/50000 (20%)]  \tLoss:   97.022675\trec:   69.434326\tkl:   27.588345\n",
      "Epoch: 400 [20100/50000 (40%)]  \tLoss:   94.370399\trec:   66.732384\tkl:   27.638012\n",
      "Epoch: 400 [30100/50000 (60%)]  \tLoss:   90.012321\trec:   63.090214\tkl:   26.922113\n",
      "Epoch: 400 [40100/50000 (80%)]  \tLoss:   88.025787\trec:   61.628696\tkl:   26.397097\n",
      "====> Epoch: 400 Average train loss: 91.4388\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1122\n",
      "Epoch: 401 [  100/50000 ( 0%)]  \tLoss:   87.734077\trec:   61.715267\tkl:   26.018810\n",
      "Epoch: 401 [10100/50000 (20%)]  \tLoss:   90.103989\trec:   63.819118\tkl:   26.284874\n",
      "Epoch: 401 [20100/50000 (40%)]  \tLoss:   88.452087\trec:   62.728291\tkl:   25.723793\n",
      "Epoch: 401 [30100/50000 (60%)]  \tLoss:   91.592735\trec:   64.672119\tkl:   26.920620\n",
      "Epoch: 401 [40100/50000 (80%)]  \tLoss:   88.589455\trec:   62.919140\tkl:   25.670315\n",
      "====> Epoch: 401 Average train loss: 91.4473\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1743\n",
      "Epoch: 402 [  100/50000 ( 0%)]  \tLoss:   93.565651\trec:   66.408737\tkl:   27.156916\n",
      "Epoch: 402 [10100/50000 (20%)]  \tLoss:   95.314590\trec:   67.050064\tkl:   28.264519\n",
      "Epoch: 402 [20100/50000 (40%)]  \tLoss:   92.953163\trec:   66.242966\tkl:   26.710196\n",
      "Epoch: 402 [30100/50000 (60%)]  \tLoss:   93.069344\trec:   65.665962\tkl:   27.403381\n",
      "Epoch: 402 [40100/50000 (80%)]  \tLoss:   88.718552\trec:   62.302841\tkl:   26.415718\n",
      "====> Epoch: 402 Average train loss: 91.4560\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.2255\n",
      "Epoch: 403 [  100/50000 ( 0%)]  \tLoss:   91.182907\trec:   64.959839\tkl:   26.223074\n",
      "Epoch: 403 [10100/50000 (20%)]  \tLoss:   95.618690\trec:   68.412521\tkl:   27.206169\n",
      "Epoch: 403 [20100/50000 (40%)]  \tLoss:   92.445175\trec:   66.404114\tkl:   26.041059\n",
      "Epoch: 403 [30100/50000 (60%)]  \tLoss:   92.928886\trec:   66.683472\tkl:   26.245413\n",
      "Epoch: 403 [40100/50000 (80%)]  \tLoss:   88.885880\trec:   63.411854\tkl:   25.474020\n",
      "====> Epoch: 403 Average train loss: 91.4392\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0980\n",
      "Epoch: 404 [  100/50000 ( 0%)]  \tLoss:   92.505959\trec:   65.717888\tkl:   26.788076\n",
      "Epoch: 404 [10100/50000 (20%)]  \tLoss:   92.248848\trec:   64.629066\tkl:   27.619778\n",
      "Epoch: 404 [20100/50000 (40%)]  \tLoss:   91.690269\trec:   65.272659\tkl:   26.417616\n",
      "Epoch: 404 [30100/50000 (60%)]  \tLoss:   89.103203\trec:   63.752762\tkl:   25.350439\n",
      "Epoch: 404 [40100/50000 (80%)]  \tLoss:   93.439888\trec:   66.441582\tkl:   26.998308\n",
      "====> Epoch: 404 Average train loss: 91.4351\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1393\n",
      "Epoch: 405 [  100/50000 ( 0%)]  \tLoss:   91.977417\trec:   64.743797\tkl:   27.233627\n",
      "Epoch: 405 [10100/50000 (20%)]  \tLoss:   90.276520\trec:   63.812832\tkl:   26.463692\n",
      "Epoch: 405 [20100/50000 (40%)]  \tLoss:   90.029236\trec:   63.515648\tkl:   26.513586\n",
      "Epoch: 405 [30100/50000 (60%)]  \tLoss:   90.917534\trec:   64.398872\tkl:   26.518660\n",
      "Epoch: 405 [40100/50000 (80%)]  \tLoss:   93.147720\trec:   66.145187\tkl:   27.002537\n",
      "====> Epoch: 405 Average train loss: 91.4003\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1341\n",
      "Epoch: 406 [  100/50000 ( 0%)]  \tLoss:   93.328995\trec:   66.710602\tkl:   26.618391\n",
      "Epoch: 406 [10100/50000 (20%)]  \tLoss:   89.993881\trec:   63.649315\tkl:   26.344566\n",
      "Epoch: 406 [20100/50000 (40%)]  \tLoss:   89.739311\trec:   64.050888\tkl:   25.688429\n",
      "Epoch: 406 [30100/50000 (60%)]  \tLoss:   92.150818\trec:   65.542862\tkl:   26.607962\n",
      "Epoch: 406 [40100/50000 (80%)]  \tLoss:   92.244156\trec:   65.409302\tkl:   26.834848\n",
      "====> Epoch: 406 Average train loss: 91.4037\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1326\n",
      "Epoch: 407 [  100/50000 ( 0%)]  \tLoss:   93.354454\trec:   67.213234\tkl:   26.141214\n",
      "Epoch: 407 [10100/50000 (20%)]  \tLoss:   89.241013\trec:   63.048611\tkl:   26.192406\n",
      "Epoch: 407 [20100/50000 (40%)]  \tLoss:   87.844551\trec:   61.996677\tkl:   25.847866\n",
      "Epoch: 407 [30100/50000 (60%)]  \tLoss:   90.957710\trec:   65.293457\tkl:   25.664259\n",
      "Epoch: 407 [40100/50000 (80%)]  \tLoss:   93.249916\trec:   66.694527\tkl:   26.555387\n",
      "====> Epoch: 407 Average train loss: 91.3954\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1589\n",
      "Epoch: 408 [  100/50000 ( 0%)]  \tLoss:   93.696121\trec:   66.992569\tkl:   26.703547\n",
      "Epoch: 408 [10100/50000 (20%)]  \tLoss:   94.611549\trec:   68.368446\tkl:   26.243105\n",
      "Epoch: 408 [20100/50000 (40%)]  \tLoss:   92.504402\trec:   66.114021\tkl:   26.390381\n",
      "Epoch: 408 [30100/50000 (60%)]  \tLoss:   93.411964\trec:   66.274010\tkl:   27.137949\n",
      "Epoch: 408 [40100/50000 (80%)]  \tLoss:   90.886696\trec:   64.179611\tkl:   26.707088\n",
      "====> Epoch: 408 Average train loss: 91.4051\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1213\n",
      "Epoch: 409 [  100/50000 ( 0%)]  \tLoss:   92.034706\trec:   65.708534\tkl:   26.326176\n",
      "Epoch: 409 [10100/50000 (20%)]  \tLoss:   86.576347\trec:   60.082996\tkl:   26.493351\n",
      "Epoch: 409 [20100/50000 (40%)]  \tLoss:   95.514877\trec:   68.524315\tkl:   26.990561\n",
      "Epoch: 409 [30100/50000 (60%)]  \tLoss:   92.415253\trec:   66.056984\tkl:   26.358271\n",
      "Epoch: 409 [40100/50000 (80%)]  \tLoss:   92.401505\trec:   64.887184\tkl:   27.514317\n",
      "====> Epoch: 409 Average train loss: 91.4064\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0141\n",
      "Epoch: 410 [  100/50000 ( 0%)]  \tLoss:   91.676773\trec:   64.498306\tkl:   27.178471\n",
      "Epoch: 410 [10100/50000 (20%)]  \tLoss:   94.259293\trec:   67.024162\tkl:   27.235134\n",
      "Epoch: 410 [20100/50000 (40%)]  \tLoss:   91.737770\trec:   64.402733\tkl:   27.335043\n",
      "Epoch: 410 [30100/50000 (60%)]  \tLoss:   86.509499\trec:   61.626972\tkl:   24.882528\n",
      "Epoch: 410 [40100/50000 (80%)]  \tLoss:   92.468758\trec:   65.040787\tkl:   27.427971\n",
      "====> Epoch: 410 Average train loss: 91.3923\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0680\n",
      "Epoch: 411 [  100/50000 ( 0%)]  \tLoss:   94.825867\trec:   68.121727\tkl:   26.704142\n",
      "Epoch: 411 [10100/50000 (20%)]  \tLoss:   90.914566\trec:   64.508232\tkl:   26.406332\n",
      "Epoch: 411 [20100/50000 (40%)]  \tLoss:   94.096992\trec:   66.784073\tkl:   27.312914\n",
      "Epoch: 411 [30100/50000 (60%)]  \tLoss:   90.486023\trec:   64.054688\tkl:   26.431335\n",
      "Epoch: 411 [40100/50000 (80%)]  \tLoss:   93.253250\trec:   66.474922\tkl:   26.778330\n",
      "====> Epoch: 411 Average train loss: 91.3696\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0097\n",
      "Epoch: 412 [  100/50000 ( 0%)]  \tLoss:   89.964386\trec:   64.150536\tkl:   25.813845\n",
      "Epoch: 412 [10100/50000 (20%)]  \tLoss:   92.810844\trec:   65.681755\tkl:   27.129091\n",
      "Epoch: 412 [20100/50000 (40%)]  \tLoss:   96.042625\trec:   68.450722\tkl:   27.591906\n",
      "Epoch: 412 [30100/50000 (60%)]  \tLoss:   89.214561\trec:   63.138203\tkl:   26.076357\n",
      "Epoch: 412 [40100/50000 (80%)]  \tLoss:   96.816246\trec:   69.541992\tkl:   27.274261\n",
      "====> Epoch: 412 Average train loss: 91.3812\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0891\n",
      "Epoch: 413 [  100/50000 ( 0%)]  \tLoss:   94.875885\trec:   68.167229\tkl:   26.708658\n",
      "Epoch: 413 [10100/50000 (20%)]  \tLoss:   87.518677\trec:   61.441013\tkl:   26.077665\n",
      "Epoch: 413 [20100/50000 (40%)]  \tLoss:   89.245758\trec:   63.114326\tkl:   26.131430\n",
      "Epoch: 413 [30100/50000 (60%)]  \tLoss:   94.674057\trec:   67.321991\tkl:   27.352068\n",
      "Epoch: 413 [40100/50000 (80%)]  \tLoss:   89.789238\trec:   64.153107\tkl:   25.636127\n",
      "====> Epoch: 413 Average train loss: 91.3649\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0770\n",
      "Epoch: 414 [  100/50000 ( 0%)]  \tLoss:   88.005241\trec:   61.523376\tkl:   26.481865\n",
      "Epoch: 414 [10100/50000 (20%)]  \tLoss:   91.670486\trec:   64.628464\tkl:   27.042019\n",
      "Epoch: 414 [20100/50000 (40%)]  \tLoss:   95.560890\trec:   68.556656\tkl:   27.004232\n",
      "Epoch: 414 [30100/50000 (60%)]  \tLoss:   90.784477\trec:   63.996532\tkl:   26.787947\n",
      "Epoch: 414 [40100/50000 (80%)]  \tLoss:   93.096275\trec:   66.046600\tkl:   27.049675\n",
      "====> Epoch: 414 Average train loss: 91.3799\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1605\n",
      "Epoch: 415 [  100/50000 ( 0%)]  \tLoss:   85.886589\trec:   60.482414\tkl:   25.404171\n",
      "Epoch: 415 [10100/50000 (20%)]  \tLoss:   89.867722\trec:   63.222595\tkl:   26.645124\n",
      "Epoch: 415 [20100/50000 (40%)]  \tLoss:   87.665558\trec:   61.421967\tkl:   26.243587\n",
      "Epoch: 415 [30100/50000 (60%)]  \tLoss:   95.026726\trec:   67.568146\tkl:   27.458584\n",
      "Epoch: 415 [40100/50000 (80%)]  \tLoss:   91.159561\trec:   65.330147\tkl:   25.829416\n",
      "====> Epoch: 415 Average train loss: 91.3733\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0717\n",
      "Epoch: 416 [  100/50000 ( 0%)]  \tLoss:   88.158661\trec:   62.102829\tkl:   26.055830\n",
      "Epoch: 416 [10100/50000 (20%)]  \tLoss:   90.337318\trec:   64.021957\tkl:   26.315371\n",
      "Epoch: 416 [20100/50000 (40%)]  \tLoss:   87.721481\trec:   61.457233\tkl:   26.264242\n",
      "Epoch: 416 [30100/50000 (60%)]  \tLoss:   90.282516\trec:   63.451004\tkl:   26.831516\n",
      "Epoch: 416 [40100/50000 (80%)]  \tLoss:   95.049492\trec:   68.044273\tkl:   27.005209\n",
      "====> Epoch: 416 Average train loss: 91.3574\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9991\n",
      "Epoch: 417 [  100/50000 ( 0%)]  \tLoss:   92.364136\trec:   65.640343\tkl:   26.723801\n",
      "Epoch: 417 [10100/50000 (20%)]  \tLoss:   94.628960\trec:   67.428299\tkl:   27.200668\n",
      "Epoch: 417 [20100/50000 (40%)]  \tLoss:   87.299683\trec:   62.420071\tkl:   24.879612\n",
      "Epoch: 417 [30100/50000 (60%)]  \tLoss:   93.297821\trec:   66.576569\tkl:   26.721256\n",
      "Epoch: 417 [40100/50000 (80%)]  \tLoss:   88.483612\trec:   62.773571\tkl:   25.710039\n",
      "====> Epoch: 417 Average train loss: 91.3426\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1635\n",
      "Epoch: 418 [  100/50000 ( 0%)]  \tLoss:   95.250404\trec:   67.272087\tkl:   27.978325\n",
      "Epoch: 418 [10100/50000 (20%)]  \tLoss:   93.441246\trec:   67.055496\tkl:   26.385754\n",
      "Epoch: 418 [20100/50000 (40%)]  \tLoss:   92.684395\trec:   65.963799\tkl:   26.720592\n",
      "Epoch: 418 [30100/50000 (60%)]  \tLoss:   92.572563\trec:   65.932442\tkl:   26.640123\n",
      "Epoch: 418 [40100/50000 (80%)]  \tLoss:   90.713669\trec:   64.487030\tkl:   26.226645\n",
      "====> Epoch: 418 Average train loss: 91.3476\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0803\n",
      "Epoch: 419 [  100/50000 ( 0%)]  \tLoss:   93.150040\trec:   66.272957\tkl:   26.877075\n",
      "Epoch: 419 [10100/50000 (20%)]  \tLoss:   90.098106\trec:   63.569859\tkl:   26.528242\n",
      "Epoch: 419 [20100/50000 (40%)]  \tLoss:   91.809860\trec:   65.524376\tkl:   26.285484\n",
      "Epoch: 419 [30100/50000 (60%)]  \tLoss:   91.364021\trec:   64.670349\tkl:   26.693666\n",
      "Epoch: 419 [40100/50000 (80%)]  \tLoss:   90.224770\trec:   63.328720\tkl:   26.896053\n",
      "====> Epoch: 419 Average train loss: 91.3095\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0637\n",
      "Epoch: 420 [  100/50000 ( 0%)]  \tLoss:   95.046471\trec:   67.886856\tkl:   27.159618\n",
      "Epoch: 420 [10100/50000 (20%)]  \tLoss:   89.716393\trec:   62.598148\tkl:   27.118244\n",
      "Epoch: 420 [20100/50000 (40%)]  \tLoss:   91.622292\trec:   64.562042\tkl:   27.060255\n",
      "Epoch: 420 [30100/50000 (60%)]  \tLoss:   89.674042\trec:   63.771023\tkl:   25.903011\n",
      "Epoch: 420 [40100/50000 (80%)]  \tLoss:   91.570465\trec:   65.398949\tkl:   26.171518\n",
      "====> Epoch: 420 Average train loss: 91.3175\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0843\n",
      "Epoch: 421 [  100/50000 ( 0%)]  \tLoss:   92.603844\trec:   65.864258\tkl:   26.739590\n",
      "Epoch: 421 [10100/50000 (20%)]  \tLoss:   93.380043\trec:   67.158463\tkl:   26.221584\n",
      "Epoch: 421 [20100/50000 (40%)]  \tLoss:   87.979912\trec:   62.632782\tkl:   25.347126\n",
      "Epoch: 421 [30100/50000 (60%)]  \tLoss:   90.804443\trec:   63.528397\tkl:   27.276047\n",
      "Epoch: 421 [40100/50000 (80%)]  \tLoss:   92.088882\trec:   65.430099\tkl:   26.658787\n",
      "====> Epoch: 421 Average train loss: 91.3275\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0239\n",
      "Epoch: 422 [  100/50000 ( 0%)]  \tLoss:   92.634308\trec:   65.957245\tkl:   26.677057\n",
      "Epoch: 422 [10100/50000 (20%)]  \tLoss:   92.664307\trec:   65.378830\tkl:   27.285480\n",
      "Epoch: 422 [20100/50000 (40%)]  \tLoss:   91.454453\trec:   64.782951\tkl:   26.671505\n",
      "Epoch: 422 [30100/50000 (60%)]  \tLoss:   89.156845\trec:   63.445869\tkl:   25.710976\n",
      "Epoch: 422 [40100/50000 (80%)]  \tLoss:   96.999664\trec:   69.386719\tkl:   27.612944\n",
      "====> Epoch: 422 Average train loss: 91.3000\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1579\n",
      "Epoch: 423 [  100/50000 ( 0%)]  \tLoss:   87.000801\trec:   61.114197\tkl:   25.886597\n",
      "Epoch: 423 [10100/50000 (20%)]  \tLoss:   92.787781\trec:   66.118378\tkl:   26.669401\n",
      "Epoch: 423 [20100/50000 (40%)]  \tLoss:   90.775986\trec:   64.516350\tkl:   26.259640\n",
      "Epoch: 423 [30100/50000 (60%)]  \tLoss:   92.893532\trec:   66.175476\tkl:   26.718052\n",
      "Epoch: 423 [40100/50000 (80%)]  \tLoss:   96.030899\trec:   68.861801\tkl:   27.169102\n",
      "====> Epoch: 423 Average train loss: 91.2808\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9622\n",
      "Epoch: 424 [  100/50000 ( 0%)]  \tLoss:   90.949219\trec:   64.247360\tkl:   26.701851\n",
      "Epoch: 424 [10100/50000 (20%)]  \tLoss:   91.623398\trec:   65.142929\tkl:   26.480471\n",
      "Epoch: 424 [20100/50000 (40%)]  \tLoss:   90.097656\trec:   64.231812\tkl:   25.865835\n",
      "Epoch: 424 [30100/50000 (60%)]  \tLoss:   95.617096\trec:   67.940819\tkl:   27.676279\n",
      "Epoch: 424 [40100/50000 (80%)]  \tLoss:   92.320366\trec:   65.366753\tkl:   26.953617\n",
      "====> Epoch: 424 Average train loss: 91.2992\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0485\n",
      "Epoch: 425 [  100/50000 ( 0%)]  \tLoss:   88.566963\trec:   62.547478\tkl:   26.019484\n",
      "Epoch: 425 [10100/50000 (20%)]  \tLoss:   90.474754\trec:   63.815926\tkl:   26.658827\n",
      "Epoch: 425 [20100/50000 (40%)]  \tLoss:   94.007645\trec:   67.059654\tkl:   26.947992\n",
      "Epoch: 425 [30100/50000 (60%)]  \tLoss:   88.840248\trec:   62.428661\tkl:   26.411594\n",
      "Epoch: 425 [40100/50000 (80%)]  \tLoss:   96.009743\trec:   68.186172\tkl:   27.823565\n",
      "====> Epoch: 425 Average train loss: 91.2720\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0141\n",
      "Epoch: 426 [  100/50000 ( 0%)]  \tLoss:   93.899063\trec:   66.187965\tkl:   27.711096\n",
      "Epoch: 426 [10100/50000 (20%)]  \tLoss:   95.469902\trec:   68.012634\tkl:   27.457260\n",
      "Epoch: 426 [20100/50000 (40%)]  \tLoss:   89.076988\trec:   63.639721\tkl:   25.437273\n",
      "Epoch: 426 [30100/50000 (60%)]  \tLoss:   92.837929\trec:   66.394775\tkl:   26.443153\n",
      "Epoch: 426 [40100/50000 (80%)]  \tLoss:   88.964119\trec:   62.157970\tkl:   26.806149\n",
      "====> Epoch: 426 Average train loss: 91.2980\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0982\n",
      "Epoch: 427 [  100/50000 ( 0%)]  \tLoss:   90.528122\trec:   64.049126\tkl:   26.478994\n",
      "Epoch: 427 [10100/50000 (20%)]  \tLoss:   91.196396\trec:   64.463333\tkl:   26.733063\n",
      "Epoch: 427 [20100/50000 (40%)]  \tLoss:   89.033394\trec:   63.020309\tkl:   26.013088\n",
      "Epoch: 427 [30100/50000 (60%)]  \tLoss:   93.011124\trec:   65.892082\tkl:   27.119040\n",
      "Epoch: 427 [40100/50000 (80%)]  \tLoss:   92.838181\trec:   66.018585\tkl:   26.819599\n",
      "====> Epoch: 427 Average train loss: 91.2817\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0521\n",
      "Epoch: 428 [  100/50000 ( 0%)]  \tLoss:   92.740547\trec:   65.944855\tkl:   26.795694\n",
      "Epoch: 428 [10100/50000 (20%)]  \tLoss:   89.129379\trec:   63.051514\tkl:   26.077868\n",
      "Epoch: 428 [20100/50000 (40%)]  \tLoss:   92.886734\trec:   65.499130\tkl:   27.387604\n",
      "Epoch: 428 [30100/50000 (60%)]  \tLoss:   93.195045\trec:   66.844337\tkl:   26.350704\n",
      "Epoch: 428 [40100/50000 (80%)]  \tLoss:   90.657539\trec:   63.664276\tkl:   26.993256\n",
      "====> Epoch: 428 Average train loss: 91.2888\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0705\n",
      "Epoch: 429 [  100/50000 ( 0%)]  \tLoss:   90.364746\trec:   63.832230\tkl:   26.532515\n",
      "Epoch: 429 [10100/50000 (20%)]  \tLoss:   93.717400\trec:   66.493164\tkl:   27.224239\n",
      "Epoch: 429 [20100/50000 (40%)]  \tLoss:   90.822739\trec:   64.555656\tkl:   26.267092\n",
      "Epoch: 429 [30100/50000 (60%)]  \tLoss:   94.396385\trec:   67.472740\tkl:   26.923649\n",
      "Epoch: 429 [40100/50000 (80%)]  \tLoss:   90.726662\trec:   64.430313\tkl:   26.296343\n",
      "====> Epoch: 429 Average train loss: 91.2671\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9340\n",
      "Epoch: 430 [  100/50000 ( 0%)]  \tLoss:   87.766945\trec:   62.286766\tkl:   25.480173\n",
      "Epoch: 430 [10100/50000 (20%)]  \tLoss:   90.688454\trec:   64.661514\tkl:   26.026945\n",
      "Epoch: 430 [20100/50000 (40%)]  \tLoss:   94.794380\trec:   68.126480\tkl:   26.667902\n",
      "Epoch: 430 [30100/50000 (60%)]  \tLoss:   91.998123\trec:   65.395576\tkl:   26.602541\n",
      "Epoch: 430 [40100/50000 (80%)]  \tLoss:   92.306129\trec:   64.825722\tkl:   27.480408\n",
      "====> Epoch: 430 Average train loss: 91.2512\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1238\n",
      "Epoch: 431 [  100/50000 ( 0%)]  \tLoss:   91.189957\trec:   64.809616\tkl:   26.380337\n",
      "Epoch: 431 [10100/50000 (20%)]  \tLoss:   94.616539\trec:   67.045975\tkl:   27.570566\n",
      "Epoch: 431 [20100/50000 (40%)]  \tLoss:   90.943222\trec:   64.546082\tkl:   26.397142\n",
      "Epoch: 431 [30100/50000 (60%)]  \tLoss:   86.773926\trec:   61.490395\tkl:   25.283525\n",
      "Epoch: 431 [40100/50000 (80%)]  \tLoss:   92.327080\trec:   65.482170\tkl:   26.844910\n",
      "====> Epoch: 431 Average train loss: 91.2659\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9691\n",
      "Epoch: 432 [  100/50000 ( 0%)]  \tLoss:   92.591385\trec:   65.701431\tkl:   26.889961\n",
      "Epoch: 432 [10100/50000 (20%)]  \tLoss:   92.138687\trec:   64.419670\tkl:   27.719028\n",
      "Epoch: 432 [20100/50000 (40%)]  \tLoss:   89.916382\trec:   63.328743\tkl:   26.587641\n",
      "Epoch: 432 [30100/50000 (60%)]  \tLoss:   92.879295\trec:   65.506783\tkl:   27.372505\n",
      "Epoch: 432 [40100/50000 (80%)]  \tLoss:   90.939682\trec:   63.406078\tkl:   27.533609\n",
      "====> Epoch: 432 Average train loss: 91.2405\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0045\n",
      "Epoch: 433 [  100/50000 ( 0%)]  \tLoss:   97.058670\trec:   68.982697\tkl:   28.075975\n",
      "Epoch: 433 [10100/50000 (20%)]  \tLoss:   94.071686\trec:   67.132584\tkl:   26.939100\n",
      "Epoch: 433 [20100/50000 (40%)]  \tLoss:   90.280251\trec:   63.830605\tkl:   26.449646\n",
      "Epoch: 433 [30100/50000 (60%)]  \tLoss:   92.374550\trec:   65.871460\tkl:   26.503082\n",
      "Epoch: 433 [40100/50000 (80%)]  \tLoss:   88.651154\trec:   62.136833\tkl:   26.514317\n",
      "====> Epoch: 433 Average train loss: 91.2234\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9739\n",
      "Epoch: 434 [  100/50000 ( 0%)]  \tLoss:   85.512802\trec:   59.536003\tkl:   25.976793\n",
      "Epoch: 434 [10100/50000 (20%)]  \tLoss:   94.674393\trec:   67.636627\tkl:   27.037760\n",
      "Epoch: 434 [20100/50000 (40%)]  \tLoss:   93.715721\trec:   66.120499\tkl:   27.595219\n",
      "Epoch: 434 [30100/50000 (60%)]  \tLoss:   88.720230\trec:   62.241669\tkl:   26.478563\n",
      "Epoch: 434 [40100/50000 (80%)]  \tLoss:   90.993080\trec:   65.082672\tkl:   25.910414\n",
      "====> Epoch: 434 Average train loss: 91.2477\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0720\n",
      "Epoch: 435 [  100/50000 ( 0%)]  \tLoss:   89.574478\trec:   63.053856\tkl:   26.520622\n",
      "Epoch: 435 [10100/50000 (20%)]  \tLoss:   92.884193\trec:   66.459923\tkl:   26.424276\n",
      "Epoch: 435 [20100/50000 (40%)]  \tLoss:   90.797989\trec:   64.038071\tkl:   26.759911\n",
      "Epoch: 435 [30100/50000 (60%)]  \tLoss:   87.341408\trec:   61.868942\tkl:   25.472466\n",
      "Epoch: 435 [40100/50000 (80%)]  \tLoss:   93.921074\trec:   66.657585\tkl:   27.263493\n",
      "====> Epoch: 435 Average train loss: 91.2325\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0291\n",
      "Epoch: 436 [  100/50000 ( 0%)]  \tLoss:   92.160934\trec:   65.373749\tkl:   26.787191\n",
      "Epoch: 436 [10100/50000 (20%)]  \tLoss:   89.012169\trec:   62.540047\tkl:   26.472116\n",
      "Epoch: 436 [20100/50000 (40%)]  \tLoss:   87.920723\trec:   62.136898\tkl:   25.783827\n",
      "Epoch: 436 [30100/50000 (60%)]  \tLoss:   94.696846\trec:   67.763832\tkl:   26.933010\n",
      "Epoch: 436 [40100/50000 (80%)]  \tLoss:   90.957710\trec:   64.226013\tkl:   26.731697\n",
      "====> Epoch: 436 Average train loss: 91.2326\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9793\n",
      "Epoch: 437 [  100/50000 ( 0%)]  \tLoss:   87.426529\trec:   61.547874\tkl:   25.878654\n",
      "Epoch: 437 [10100/50000 (20%)]  \tLoss:   93.118263\trec:   66.400383\tkl:   26.717875\n",
      "Epoch: 437 [20100/50000 (40%)]  \tLoss:   92.178398\trec:   64.682037\tkl:   27.496367\n",
      "Epoch: 437 [30100/50000 (60%)]  \tLoss:   85.790649\trec:   59.968002\tkl:   25.822653\n",
      "Epoch: 437 [40100/50000 (80%)]  \tLoss:   90.259880\trec:   63.925606\tkl:   26.334272\n",
      "====> Epoch: 437 Average train loss: 91.2013\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0606\n",
      "Epoch: 438 [  100/50000 ( 0%)]  \tLoss:   91.463982\trec:   65.256668\tkl:   26.207319\n",
      "Epoch: 438 [10100/50000 (20%)]  \tLoss:   88.116875\trec:   62.050957\tkl:   26.065918\n",
      "Epoch: 438 [20100/50000 (40%)]  \tLoss:   90.585548\trec:   64.201637\tkl:   26.383905\n",
      "Epoch: 438 [30100/50000 (60%)]  \tLoss:   88.892281\trec:   62.577274\tkl:   26.315014\n",
      "Epoch: 438 [40100/50000 (80%)]  \tLoss:   91.894920\trec:   65.191269\tkl:   26.703651\n",
      "====> Epoch: 438 Average train loss: 91.2311\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9392\n",
      "Epoch: 439 [  100/50000 ( 0%)]  \tLoss:   94.740135\trec:   67.380356\tkl:   27.359785\n",
      "Epoch: 439 [10100/50000 (20%)]  \tLoss:   92.675545\trec:   66.104118\tkl:   26.571424\n",
      "Epoch: 439 [20100/50000 (40%)]  \tLoss:   92.355171\trec:   65.481674\tkl:   26.873495\n",
      "Epoch: 439 [30100/50000 (60%)]  \tLoss:   91.760307\trec:   65.166321\tkl:   26.593990\n",
      "Epoch: 439 [40100/50000 (80%)]  \tLoss:   89.832245\trec:   64.125488\tkl:   25.706755\n",
      "====> Epoch: 439 Average train loss: 91.1823\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9887\n",
      "Epoch: 440 [  100/50000 ( 0%)]  \tLoss:   94.789314\trec:   67.328453\tkl:   27.460861\n",
      "Epoch: 440 [10100/50000 (20%)]  \tLoss:   88.320602\trec:   61.964851\tkl:   26.355747\n",
      "Epoch: 440 [20100/50000 (40%)]  \tLoss:   88.448120\trec:   62.667011\tkl:   25.781113\n",
      "Epoch: 440 [30100/50000 (60%)]  \tLoss:   92.980568\trec:   66.394760\tkl:   26.585808\n",
      "Epoch: 440 [40100/50000 (80%)]  \tLoss:   88.185402\trec:   61.523396\tkl:   26.661999\n",
      "====> Epoch: 440 Average train loss: 91.2163\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9853\n",
      "Epoch: 441 [  100/50000 ( 0%)]  \tLoss:   95.219109\trec:   67.606911\tkl:   27.612194\n",
      "Epoch: 441 [10100/50000 (20%)]  \tLoss:   89.446640\trec:   62.142742\tkl:   27.303902\n",
      "Epoch: 441 [20100/50000 (40%)]  \tLoss:   89.864174\trec:   64.130348\tkl:   25.733828\n",
      "Epoch: 441 [30100/50000 (60%)]  \tLoss:   91.380791\trec:   65.240112\tkl:   26.140676\n",
      "Epoch: 441 [40100/50000 (80%)]  \tLoss:   91.740753\trec:   65.097900\tkl:   26.642849\n",
      "====> Epoch: 441 Average train loss: 91.2040\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9525\n",
      "Epoch: 442 [  100/50000 ( 0%)]  \tLoss:   88.804741\trec:   62.374588\tkl:   26.430151\n",
      "Epoch: 442 [10100/50000 (20%)]  \tLoss:   89.014282\trec:   62.609543\tkl:   26.404737\n",
      "Epoch: 442 [20100/50000 (40%)]  \tLoss:   87.636734\trec:   62.306824\tkl:   25.329914\n",
      "Epoch: 442 [30100/50000 (60%)]  \tLoss:   95.581482\trec:   68.638718\tkl:   26.942762\n",
      "Epoch: 442 [40100/50000 (80%)]  \tLoss:   91.453720\trec:   64.765556\tkl:   26.688166\n",
      "====> Epoch: 442 Average train loss: 91.1869\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0124\n",
      "Epoch: 443 [  100/50000 ( 0%)]  \tLoss:   93.275787\trec:   65.848877\tkl:   27.426914\n",
      "Epoch: 443 [10100/50000 (20%)]  \tLoss:   92.955582\trec:   66.231285\tkl:   26.724295\n",
      "Epoch: 443 [20100/50000 (40%)]  \tLoss:   89.321388\trec:   63.226093\tkl:   26.095293\n",
      "Epoch: 443 [30100/50000 (60%)]  \tLoss:   85.367409\trec:   59.137905\tkl:   26.229506\n",
      "Epoch: 443 [40100/50000 (80%)]  \tLoss:   88.739357\trec:   62.189342\tkl:   26.550011\n",
      "====> Epoch: 443 Average train loss: 91.1691\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9643\n",
      "Epoch: 444 [  100/50000 ( 0%)]  \tLoss:   87.332733\trec:   61.298870\tkl:   26.033859\n",
      "Epoch: 444 [10100/50000 (20%)]  \tLoss:   89.694565\trec:   63.344868\tkl:   26.349699\n",
      "Epoch: 444 [20100/50000 (40%)]  \tLoss:   85.889442\trec:   59.836628\tkl:   26.052814\n",
      "Epoch: 444 [30100/50000 (60%)]  \tLoss:   91.321541\trec:   65.116714\tkl:   26.204828\n",
      "Epoch: 444 [40100/50000 (80%)]  \tLoss:   89.683739\trec:   63.570766\tkl:   26.112970\n",
      "====> Epoch: 444 Average train loss: 91.1630\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8781\n",
      "Epoch: 445 [  100/50000 ( 0%)]  \tLoss:   87.477768\trec:   61.200409\tkl:   26.277365\n",
      "Epoch: 445 [10100/50000 (20%)]  \tLoss:   90.914169\trec:   65.055717\tkl:   25.858444\n",
      "Epoch: 445 [20100/50000 (40%)]  \tLoss:   88.334213\trec:   62.257099\tkl:   26.077122\n",
      "Epoch: 445 [30100/50000 (60%)]  \tLoss:   95.745346\trec:   68.233376\tkl:   27.511974\n",
      "Epoch: 445 [40100/50000 (80%)]  \tLoss:   91.612648\trec:   65.291588\tkl:   26.321054\n",
      "====> Epoch: 445 Average train loss: 91.1609\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0299\n",
      "Epoch: 446 [  100/50000 ( 0%)]  \tLoss:   90.088921\trec:   63.743114\tkl:   26.345804\n",
      "Epoch: 446 [10100/50000 (20%)]  \tLoss:   93.032890\trec:   65.909790\tkl:   27.123091\n",
      "Epoch: 446 [20100/50000 (40%)]  \tLoss:   93.431602\trec:   65.647316\tkl:   27.784281\n",
      "Epoch: 446 [30100/50000 (60%)]  \tLoss:   89.774849\trec:   63.044960\tkl:   26.729893\n",
      "Epoch: 446 [40100/50000 (80%)]  \tLoss:   89.044685\trec:   62.469990\tkl:   26.574699\n",
      "====> Epoch: 446 Average train loss: 91.1711\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8662\n",
      "Epoch: 447 [  100/50000 ( 0%)]  \tLoss:   93.535713\trec:   66.685425\tkl:   26.850281\n",
      "Epoch: 447 [10100/50000 (20%)]  \tLoss:   86.732674\trec:   60.887341\tkl:   25.845333\n",
      "Epoch: 447 [20100/50000 (40%)]  \tLoss:   91.478127\trec:   65.250107\tkl:   26.228018\n",
      "Epoch: 447 [30100/50000 (60%)]  \tLoss:   97.179939\trec:   69.878815\tkl:   27.301121\n",
      "Epoch: 447 [40100/50000 (80%)]  \tLoss:   93.791870\trec:   67.126396\tkl:   26.665483\n",
      "====> Epoch: 447 Average train loss: 91.1770\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9652\n",
      "Epoch: 448 [  100/50000 ( 0%)]  \tLoss:   86.094803\trec:   60.457981\tkl:   25.636824\n",
      "Epoch: 448 [10100/50000 (20%)]  \tLoss:   90.369324\trec:   64.490021\tkl:   25.879307\n",
      "Epoch: 448 [20100/50000 (40%)]  \tLoss:   90.584351\trec:   63.832531\tkl:   26.751822\n",
      "Epoch: 448 [30100/50000 (60%)]  \tLoss:   90.119186\trec:   63.421791\tkl:   26.697395\n",
      "Epoch: 448 [40100/50000 (80%)]  \tLoss:   92.831520\trec:   65.371544\tkl:   27.459982\n",
      "====> Epoch: 448 Average train loss: 91.1451\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.1378\n",
      "Epoch: 449 [  100/50000 ( 0%)]  \tLoss:   88.596306\trec:   61.362392\tkl:   27.233913\n",
      "Epoch: 449 [10100/50000 (20%)]  \tLoss:   88.714531\trec:   62.080227\tkl:   26.634306\n",
      "Epoch: 449 [20100/50000 (40%)]  \tLoss:   93.596413\trec:   66.502831\tkl:   27.093580\n",
      "Epoch: 449 [30100/50000 (60%)]  \tLoss:   90.652878\trec:   64.406952\tkl:   26.245928\n",
      "Epoch: 449 [40100/50000 (80%)]  \tLoss:   91.373184\trec:   64.442062\tkl:   26.931122\n",
      "====> Epoch: 449 Average train loss: 91.1586\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8803\n",
      "Epoch: 450 [  100/50000 ( 0%)]  \tLoss:   87.412071\trec:   61.038921\tkl:   26.373152\n",
      "Epoch: 450 [10100/50000 (20%)]  \tLoss:   91.052071\trec:   64.756073\tkl:   26.295994\n",
      "Epoch: 450 [20100/50000 (40%)]  \tLoss:   95.671814\trec:   68.438774\tkl:   27.233034\n",
      "Epoch: 450 [30100/50000 (60%)]  \tLoss:   87.455505\trec:   61.972771\tkl:   25.482738\n",
      "Epoch: 450 [40100/50000 (80%)]  \tLoss:   92.689110\trec:   65.583717\tkl:   27.105392\n",
      "====> Epoch: 450 Average train loss: 91.1450\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8522\n",
      "Epoch: 451 [  100/50000 ( 0%)]  \tLoss:   88.850479\trec:   63.787083\tkl:   25.063393\n",
      "Epoch: 451 [10100/50000 (20%)]  \tLoss:   89.751015\trec:   63.105404\tkl:   26.645609\n",
      "Epoch: 451 [20100/50000 (40%)]  \tLoss:   88.200890\trec:   62.399139\tkl:   25.801744\n",
      "Epoch: 451 [30100/50000 (60%)]  \tLoss:   93.697105\trec:   67.275139\tkl:   26.421967\n",
      "Epoch: 451 [40100/50000 (80%)]  \tLoss:   93.744415\trec:   66.673996\tkl:   27.070417\n",
      "====> Epoch: 451 Average train loss: 91.1398\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8606\n",
      "Epoch: 452 [  100/50000 ( 0%)]  \tLoss:   91.495758\trec:   64.764450\tkl:   26.731310\n",
      "Epoch: 452 [10100/50000 (20%)]  \tLoss:   90.107857\trec:   63.212414\tkl:   26.895447\n",
      "Epoch: 452 [20100/50000 (40%)]  \tLoss:   93.948380\trec:   66.966881\tkl:   26.981491\n",
      "Epoch: 452 [30100/50000 (60%)]  \tLoss:   89.896851\trec:   64.201294\tkl:   25.695551\n",
      "Epoch: 452 [40100/50000 (80%)]  \tLoss:   91.583290\trec:   64.401779\tkl:   27.181511\n",
      "====> Epoch: 452 Average train loss: 91.1238\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 94.0217\n",
      "Epoch: 453 [  100/50000 ( 0%)]  \tLoss:   91.730576\trec:   65.243713\tkl:   26.486860\n",
      "Epoch: 453 [10100/50000 (20%)]  \tLoss:   92.437790\trec:   64.786499\tkl:   27.651291\n",
      "Epoch: 453 [20100/50000 (40%)]  \tLoss:   89.826088\trec:   62.723877\tkl:   27.102217\n",
      "Epoch: 453 [30100/50000 (60%)]  \tLoss:   87.102867\trec:   61.008289\tkl:   26.094578\n",
      "Epoch: 453 [40100/50000 (80%)]  \tLoss:   92.049316\trec:   65.312180\tkl:   26.737135\n",
      "====> Epoch: 453 Average train loss: 91.1259\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9155\n",
      "Epoch: 454 [  100/50000 ( 0%)]  \tLoss:   90.632713\trec:   64.233261\tkl:   26.399454\n",
      "Epoch: 454 [10100/50000 (20%)]  \tLoss:   91.636383\trec:   64.721786\tkl:   26.914595\n",
      "Epoch: 454 [20100/50000 (40%)]  \tLoss:   92.733498\trec:   66.394943\tkl:   26.338552\n",
      "Epoch: 454 [30100/50000 (60%)]  \tLoss:   92.485542\trec:   66.030319\tkl:   26.455229\n",
      "Epoch: 454 [40100/50000 (80%)]  \tLoss:   89.474342\trec:   62.346828\tkl:   27.127514\n",
      "====> Epoch: 454 Average train loss: 91.1217\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9808\n",
      "Epoch: 455 [  100/50000 ( 0%)]  \tLoss:   95.724190\trec:   67.864372\tkl:   27.859814\n",
      "Epoch: 455 [10100/50000 (20%)]  \tLoss:   95.081276\trec:   67.339775\tkl:   27.741501\n",
      "Epoch: 455 [20100/50000 (40%)]  \tLoss:   92.927567\trec:   65.725388\tkl:   27.202183\n",
      "Epoch: 455 [30100/50000 (60%)]  \tLoss:   89.222420\trec:   63.156845\tkl:   26.065573\n",
      "Epoch: 455 [40100/50000 (80%)]  \tLoss:   90.574959\trec:   63.416737\tkl:   27.158216\n",
      "====> Epoch: 455 Average train loss: 91.1311\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8417\n",
      "Epoch: 456 [  100/50000 ( 0%)]  \tLoss:   97.494209\trec:   68.958412\tkl:   28.535797\n",
      "Epoch: 456 [10100/50000 (20%)]  \tLoss:   90.242065\trec:   64.828087\tkl:   25.413988\n",
      "Epoch: 456 [20100/50000 (40%)]  \tLoss:   90.241112\trec:   64.026077\tkl:   26.215038\n",
      "Epoch: 456 [30100/50000 (60%)]  \tLoss:   91.494598\trec:   64.771515\tkl:   26.723085\n",
      "Epoch: 456 [40100/50000 (80%)]  \tLoss:   92.098007\trec:   65.634689\tkl:   26.463318\n",
      "====> Epoch: 456 Average train loss: 91.1056\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9849\n",
      "Epoch: 457 [  100/50000 ( 0%)]  \tLoss:   89.221596\trec:   62.426159\tkl:   26.795433\n",
      "Epoch: 457 [10100/50000 (20%)]  \tLoss:   87.878639\trec:   62.574802\tkl:   25.303841\n",
      "Epoch: 457 [20100/50000 (40%)]  \tLoss:   88.865044\trec:   62.763260\tkl:   26.101789\n",
      "Epoch: 457 [30100/50000 (60%)]  \tLoss:   91.296402\trec:   65.360458\tkl:   25.935949\n",
      "Epoch: 457 [40100/50000 (80%)]  \tLoss:   91.416847\trec:   64.149376\tkl:   27.267469\n",
      "====> Epoch: 457 Average train loss: 91.0745\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8753\n",
      "Epoch: 458 [  100/50000 ( 0%)]  \tLoss:   90.236855\trec:   63.507820\tkl:   26.729031\n",
      "Epoch: 458 [10100/50000 (20%)]  \tLoss:   90.022072\trec:   63.026821\tkl:   26.995253\n",
      "Epoch: 458 [20100/50000 (40%)]  \tLoss:   92.456108\trec:   64.767616\tkl:   27.688490\n",
      "Epoch: 458 [30100/50000 (60%)]  \tLoss:   91.251953\trec:   64.969513\tkl:   26.282438\n",
      "Epoch: 458 [40100/50000 (80%)]  \tLoss:   92.438263\trec:   66.393211\tkl:   26.045053\n",
      "====> Epoch: 458 Average train loss: 91.1001\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8149\n",
      "Epoch: 459 [  100/50000 ( 0%)]  \tLoss:   93.616310\trec:   66.686913\tkl:   26.929392\n",
      "Epoch: 459 [10100/50000 (20%)]  \tLoss:   86.548141\trec:   60.202991\tkl:   26.345148\n",
      "Epoch: 459 [20100/50000 (40%)]  \tLoss:   95.565956\trec:   67.766701\tkl:   27.799259\n",
      "Epoch: 459 [30100/50000 (60%)]  \tLoss:   92.436798\trec:   65.195992\tkl:   27.240810\n",
      "Epoch: 459 [40100/50000 (80%)]  \tLoss:   93.860771\trec:   67.077629\tkl:   26.783144\n",
      "====> Epoch: 459 Average train loss: 91.1037\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9311\n",
      "Epoch: 460 [  100/50000 ( 0%)]  \tLoss:   91.330368\trec:   65.289543\tkl:   26.040833\n",
      "Epoch: 460 [10100/50000 (20%)]  \tLoss:   88.386642\trec:   62.691708\tkl:   25.694925\n",
      "Epoch: 460 [20100/50000 (40%)]  \tLoss:   90.797859\trec:   63.854214\tkl:   26.943644\n",
      "Epoch: 460 [30100/50000 (60%)]  \tLoss:   94.630424\trec:   67.771393\tkl:   26.859030\n",
      "Epoch: 460 [40100/50000 (80%)]  \tLoss:   90.654961\trec:   63.105282\tkl:   27.549683\n",
      "====> Epoch: 460 Average train loss: 91.0994\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9733\n",
      "Epoch: 461 [  100/50000 ( 0%)]  \tLoss:   93.521347\trec:   66.942291\tkl:   26.579052\n",
      "Epoch: 461 [10100/50000 (20%)]  \tLoss:   84.058769\trec:   59.674946\tkl:   24.383820\n",
      "Epoch: 461 [20100/50000 (40%)]  \tLoss:   93.011131\trec:   65.713440\tkl:   27.297689\n",
      "Epoch: 461 [30100/50000 (60%)]  \tLoss:   88.729782\trec:   62.634197\tkl:   26.095585\n",
      "Epoch: 461 [40100/50000 (80%)]  \tLoss:   92.454979\trec:   65.741844\tkl:   26.713139\n",
      "====> Epoch: 461 Average train loss: 91.0873\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8997\n",
      "Epoch: 462 [  100/50000 ( 0%)]  \tLoss:   92.522636\trec:   65.870544\tkl:   26.652094\n",
      "Epoch: 462 [10100/50000 (20%)]  \tLoss:   89.974106\trec:   63.505310\tkl:   26.468796\n",
      "Epoch: 462 [20100/50000 (40%)]  \tLoss:   90.551987\trec:   64.428825\tkl:   26.123169\n",
      "Epoch: 462 [30100/50000 (60%)]  \tLoss:   89.927895\trec:   63.848003\tkl:   26.079897\n",
      "Epoch: 462 [40100/50000 (80%)]  \tLoss:   92.319298\trec:   64.987228\tkl:   27.332062\n",
      "====> Epoch: 462 Average train loss: 91.0670\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8316\n",
      "Epoch: 463 [  100/50000 ( 0%)]  \tLoss:   87.711258\trec:   61.941311\tkl:   25.769945\n",
      "Epoch: 463 [10100/50000 (20%)]  \tLoss:   91.212303\trec:   65.096642\tkl:   26.115658\n",
      "Epoch: 463 [20100/50000 (40%)]  \tLoss:   91.580475\trec:   65.405060\tkl:   26.175419\n",
      "Epoch: 463 [30100/50000 (60%)]  \tLoss:   89.800636\trec:   62.988392\tkl:   26.812243\n",
      "Epoch: 463 [40100/50000 (80%)]  \tLoss:   93.012314\trec:   65.509315\tkl:   27.502998\n",
      "====> Epoch: 463 Average train loss: 91.0636\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9658\n",
      "Epoch: 464 [  100/50000 ( 0%)]  \tLoss:   95.088768\trec:   68.169655\tkl:   26.919113\n",
      "Epoch: 464 [10100/50000 (20%)]  \tLoss:   92.266563\trec:   65.142143\tkl:   27.124413\n",
      "Epoch: 464 [20100/50000 (40%)]  \tLoss:   92.900848\trec:   65.657906\tkl:   27.242939\n",
      "Epoch: 464 [30100/50000 (60%)]  \tLoss:   88.218369\trec:   61.666027\tkl:   26.552341\n",
      "Epoch: 464 [40100/50000 (80%)]  \tLoss:   92.642387\trec:   64.651237\tkl:   27.991150\n",
      "====> Epoch: 464 Average train loss: 91.0818\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8887\n",
      "Epoch: 465 [  100/50000 ( 0%)]  \tLoss:   92.993965\trec:   65.443810\tkl:   27.550146\n",
      "Epoch: 465 [10100/50000 (20%)]  \tLoss:   86.298004\trec:   60.497162\tkl:   25.800846\n",
      "Epoch: 465 [20100/50000 (40%)]  \tLoss:   90.110367\trec:   63.875652\tkl:   26.234715\n",
      "Epoch: 465 [30100/50000 (60%)]  \tLoss:   92.561867\trec:   65.575386\tkl:   26.986477\n",
      "Epoch: 465 [40100/50000 (80%)]  \tLoss:   88.560112\trec:   62.141758\tkl:   26.418356\n",
      "====> Epoch: 465 Average train loss: 91.0597\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9676\n",
      "Epoch: 466 [  100/50000 ( 0%)]  \tLoss:   91.973358\trec:   65.113953\tkl:   26.859406\n",
      "Epoch: 466 [10100/50000 (20%)]  \tLoss:   94.008629\trec:   66.980309\tkl:   27.028322\n",
      "Epoch: 466 [20100/50000 (40%)]  \tLoss:   94.325790\trec:   67.947823\tkl:   26.377968\n",
      "Epoch: 466 [30100/50000 (60%)]  \tLoss:   95.348152\trec:   68.557060\tkl:   26.791096\n",
      "Epoch: 466 [40100/50000 (80%)]  \tLoss:   91.430672\trec:   64.773087\tkl:   26.657587\n",
      "====> Epoch: 466 Average train loss: 91.0620\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9130\n",
      "Epoch: 467 [  100/50000 ( 0%)]  \tLoss:   89.356277\trec:   63.448132\tkl:   25.908142\n",
      "Epoch: 467 [10100/50000 (20%)]  \tLoss:   91.736969\trec:   64.921982\tkl:   26.814987\n",
      "Epoch: 467 [20100/50000 (40%)]  \tLoss:   92.098457\trec:   65.030945\tkl:   27.067509\n",
      "Epoch: 467 [30100/50000 (60%)]  \tLoss:   93.128029\trec:   66.618774\tkl:   26.509253\n",
      "Epoch: 467 [40100/50000 (80%)]  \tLoss:   91.212860\trec:   65.132607\tkl:   26.080252\n",
      "====> Epoch: 467 Average train loss: 91.0471\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9542\n",
      "Epoch: 468 [  100/50000 ( 0%)]  \tLoss:   93.543564\trec:   66.264641\tkl:   27.278921\n",
      "Epoch: 468 [10100/50000 (20%)]  \tLoss:   90.913399\trec:   64.182297\tkl:   26.731098\n",
      "Epoch: 468 [20100/50000 (40%)]  \tLoss:   91.010231\trec:   64.146530\tkl:   26.863705\n",
      "Epoch: 468 [30100/50000 (60%)]  \tLoss:   88.721756\trec:   62.773926\tkl:   25.947832\n",
      "Epoch: 468 [40100/50000 (80%)]  \tLoss:   91.364998\trec:   64.166344\tkl:   27.198652\n",
      "====> Epoch: 468 Average train loss: 91.0344\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8237\n",
      "Epoch: 469 [  100/50000 ( 0%)]  \tLoss:   95.159470\trec:   67.944595\tkl:   27.214872\n",
      "Epoch: 469 [10100/50000 (20%)]  \tLoss:   88.995934\trec:   62.296856\tkl:   26.699087\n",
      "Epoch: 469 [20100/50000 (40%)]  \tLoss:   91.830879\trec:   64.601509\tkl:   27.229366\n",
      "Epoch: 469 [30100/50000 (60%)]  \tLoss:   91.537109\trec:   64.665749\tkl:   26.871365\n",
      "Epoch: 469 [40100/50000 (80%)]  \tLoss:   91.516884\trec:   65.380081\tkl:   26.136803\n",
      "====> Epoch: 469 Average train loss: 91.0281\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9537\n",
      "Epoch: 470 [  100/50000 ( 0%)]  \tLoss:   91.775475\trec:   63.931309\tkl:   27.844168\n",
      "Epoch: 470 [10100/50000 (20%)]  \tLoss:   91.756607\trec:   64.597694\tkl:   27.158915\n",
      "Epoch: 470 [20100/50000 (40%)]  \tLoss:   90.998520\trec:   65.190712\tkl:   25.807814\n",
      "Epoch: 470 [30100/50000 (60%)]  \tLoss:   94.447205\trec:   67.397980\tkl:   27.049232\n",
      "Epoch: 470 [40100/50000 (80%)]  \tLoss:   92.735115\trec:   66.546257\tkl:   26.188864\n",
      "====> Epoch: 470 Average train loss: 91.0292\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8912\n",
      "Epoch: 471 [  100/50000 ( 0%)]  \tLoss:   91.782700\trec:   65.159187\tkl:   26.623518\n",
      "Epoch: 471 [10100/50000 (20%)]  \tLoss:   92.169746\trec:   65.386139\tkl:   26.783600\n",
      "Epoch: 471 [20100/50000 (40%)]  \tLoss:   91.118073\trec:   64.693825\tkl:   26.424250\n",
      "Epoch: 471 [30100/50000 (60%)]  \tLoss:   89.809326\trec:   62.695072\tkl:   27.114250\n",
      "Epoch: 471 [40100/50000 (80%)]  \tLoss:   90.631714\trec:   63.757812\tkl:   26.873903\n",
      "====> Epoch: 471 Average train loss: 91.0172\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8529\n",
      "Epoch: 472 [  100/50000 ( 0%)]  \tLoss:   90.325157\trec:   63.645302\tkl:   26.679855\n",
      "Epoch: 472 [10100/50000 (20%)]  \tLoss:   89.897850\trec:   62.963860\tkl:   26.933990\n",
      "Epoch: 472 [20100/50000 (40%)]  \tLoss:   97.469635\trec:   68.732216\tkl:   28.737423\n",
      "Epoch: 472 [30100/50000 (60%)]  \tLoss:   89.892441\trec:   64.321930\tkl:   25.570513\n",
      "Epoch: 472 [40100/50000 (80%)]  \tLoss:   92.520622\trec:   66.029556\tkl:   26.491062\n",
      "====> Epoch: 472 Average train loss: 91.0405\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9430\n",
      "Epoch: 473 [  100/50000 ( 0%)]  \tLoss:   90.893257\trec:   64.215759\tkl:   26.677500\n",
      "Epoch: 473 [10100/50000 (20%)]  \tLoss:   94.323006\trec:   67.217751\tkl:   27.105263\n",
      "Epoch: 473 [20100/50000 (40%)]  \tLoss:   87.011871\trec:   61.111416\tkl:   25.900454\n",
      "Epoch: 473 [30100/50000 (60%)]  \tLoss:   92.461525\trec:   65.533195\tkl:   26.928328\n",
      "Epoch: 473 [40100/50000 (80%)]  \tLoss:   93.222107\trec:   66.594093\tkl:   26.628012\n",
      "====> Epoch: 473 Average train loss: 91.0105\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8809\n",
      "Epoch: 474 [  100/50000 ( 0%)]  \tLoss:   93.011032\trec:   66.267120\tkl:   26.743910\n",
      "Epoch: 474 [10100/50000 (20%)]  \tLoss:   95.612892\trec:   68.497002\tkl:   27.115889\n",
      "Epoch: 474 [20100/50000 (40%)]  \tLoss:   90.719810\trec:   64.499588\tkl:   26.220226\n",
      "Epoch: 474 [30100/50000 (60%)]  \tLoss:   94.876541\trec:   67.410370\tkl:   27.466173\n",
      "Epoch: 474 [40100/50000 (80%)]  \tLoss:   92.708237\trec:   65.262093\tkl:   27.446142\n",
      "====> Epoch: 474 Average train loss: 90.9914\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9426\n",
      "Epoch: 475 [  100/50000 ( 0%)]  \tLoss:   89.606667\trec:   63.437576\tkl:   26.169088\n",
      "Epoch: 475 [10100/50000 (20%)]  \tLoss:   94.750565\trec:   67.779289\tkl:   26.971281\n",
      "Epoch: 475 [20100/50000 (40%)]  \tLoss:   87.023239\trec:   60.863045\tkl:   26.160194\n",
      "Epoch: 475 [30100/50000 (60%)]  \tLoss:   87.878555\trec:   62.421425\tkl:   25.457129\n",
      "Epoch: 475 [40100/50000 (80%)]  \tLoss:   89.747215\trec:   62.998981\tkl:   26.748234\n",
      "====> Epoch: 475 Average train loss: 90.9793\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9405\n",
      "Epoch: 476 [  100/50000 ( 0%)]  \tLoss:   90.129440\trec:   63.610771\tkl:   26.518673\n",
      "Epoch: 476 [10100/50000 (20%)]  \tLoss:   93.043007\trec:   66.263641\tkl:   26.779360\n",
      "Epoch: 476 [20100/50000 (40%)]  \tLoss:   94.487030\trec:   66.846886\tkl:   27.640141\n",
      "Epoch: 476 [30100/50000 (60%)]  \tLoss:   91.084572\trec:   64.107224\tkl:   26.977346\n",
      "Epoch: 476 [40100/50000 (80%)]  \tLoss:   89.150566\trec:   63.279453\tkl:   25.871117\n",
      "====> Epoch: 476 Average train loss: 90.9932\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8821\n",
      "Epoch: 477 [  100/50000 ( 0%)]  \tLoss:   93.058159\trec:   66.759811\tkl:   26.298349\n",
      "Epoch: 477 [10100/50000 (20%)]  \tLoss:   91.062859\trec:   63.858662\tkl:   27.204199\n",
      "Epoch: 477 [20100/50000 (40%)]  \tLoss:   90.185890\trec:   64.244156\tkl:   25.941729\n",
      "Epoch: 477 [30100/50000 (60%)]  \tLoss:   91.199890\trec:   63.937557\tkl:   27.262331\n",
      "Epoch: 477 [40100/50000 (80%)]  \tLoss:   89.808876\trec:   63.672871\tkl:   26.136007\n",
      "====> Epoch: 477 Average train loss: 91.0008\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8542\n",
      "Epoch: 478 [  100/50000 ( 0%)]  \tLoss:   91.956543\trec:   65.026367\tkl:   26.930176\n",
      "Epoch: 478 [10100/50000 (20%)]  \tLoss:   89.790192\trec:   63.730652\tkl:   26.059546\n",
      "Epoch: 478 [20100/50000 (40%)]  \tLoss:   87.742172\trec:   61.395733\tkl:   26.346447\n",
      "Epoch: 478 [30100/50000 (60%)]  \tLoss:   88.420586\trec:   62.948895\tkl:   25.471691\n",
      "Epoch: 478 [40100/50000 (80%)]  \tLoss:   93.947685\trec:   67.134460\tkl:   26.813219\n",
      "====> Epoch: 478 Average train loss: 90.9889\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9428\n",
      "Epoch: 479 [  100/50000 ( 0%)]  \tLoss:   92.859619\trec:   65.681076\tkl:   27.178541\n",
      "Epoch: 479 [10100/50000 (20%)]  \tLoss:   87.251671\trec:   61.172131\tkl:   26.079538\n",
      "Epoch: 479 [20100/50000 (40%)]  \tLoss:   93.577148\trec:   67.045731\tkl:   26.531412\n",
      "Epoch: 479 [30100/50000 (60%)]  \tLoss:   87.633652\trec:   61.578564\tkl:   26.055090\n",
      "Epoch: 479 [40100/50000 (80%)]  \tLoss:   91.423035\trec:   64.136559\tkl:   27.286472\n",
      "====> Epoch: 479 Average train loss: 91.0075\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8702\n",
      "Epoch: 480 [  100/50000 ( 0%)]  \tLoss:   91.067039\trec:   64.396523\tkl:   26.670517\n",
      "Epoch: 480 [10100/50000 (20%)]  \tLoss:   96.400230\trec:   68.822975\tkl:   27.577257\n",
      "Epoch: 480 [20100/50000 (40%)]  \tLoss:   91.053856\trec:   64.290276\tkl:   26.763578\n",
      "Epoch: 480 [30100/50000 (60%)]  \tLoss:   93.719063\trec:   67.236588\tkl:   26.482477\n",
      "Epoch: 480 [40100/50000 (80%)]  \tLoss:   92.754379\trec:   65.205482\tkl:   27.548901\n",
      "====> Epoch: 480 Average train loss: 90.9899\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9085\n",
      "Epoch: 481 [  100/50000 ( 0%)]  \tLoss:   90.265282\trec:   63.067516\tkl:   27.197762\n",
      "Epoch: 481 [10100/50000 (20%)]  \tLoss:   91.677078\trec:   64.604805\tkl:   27.072268\n",
      "Epoch: 481 [20100/50000 (40%)]  \tLoss:   92.786781\trec:   66.406906\tkl:   26.379875\n",
      "Epoch: 481 [30100/50000 (60%)]  \tLoss:   88.075722\trec:   61.389160\tkl:   26.686563\n",
      "Epoch: 481 [40100/50000 (80%)]  \tLoss:   95.440025\trec:   67.728279\tkl:   27.711748\n",
      "====> Epoch: 481 Average train loss: 90.9597\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9939\n",
      "Epoch: 482 [  100/50000 ( 0%)]  \tLoss:   87.767273\trec:   61.398602\tkl:   26.368670\n",
      "Epoch: 482 [10100/50000 (20%)]  \tLoss:   91.615776\trec:   65.423088\tkl:   26.192692\n",
      "Epoch: 482 [20100/50000 (40%)]  \tLoss:   94.961853\trec:   68.311920\tkl:   26.649933\n",
      "Epoch: 482 [30100/50000 (60%)]  \tLoss:   91.852592\trec:   64.950310\tkl:   26.902290\n",
      "Epoch: 482 [40100/50000 (80%)]  \tLoss:   90.624237\trec:   63.969929\tkl:   26.654306\n",
      "====> Epoch: 482 Average train loss: 90.9755\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8516\n",
      "Epoch: 483 [  100/50000 ( 0%)]  \tLoss:   92.303429\trec:   65.670341\tkl:   26.633085\n",
      "Epoch: 483 [10100/50000 (20%)]  \tLoss:   87.716652\trec:   62.134861\tkl:   25.581783\n",
      "Epoch: 483 [20100/50000 (40%)]  \tLoss:   94.078857\trec:   66.609268\tkl:   27.469591\n",
      "Epoch: 483 [30100/50000 (60%)]  \tLoss:   90.187675\trec:   63.173080\tkl:   27.014589\n",
      "Epoch: 483 [40100/50000 (80%)]  \tLoss:   86.132927\trec:   60.916180\tkl:   25.216749\n",
      "====> Epoch: 483 Average train loss: 90.9451\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7279\n",
      "Epoch: 484 [  100/50000 ( 0%)]  \tLoss:   84.932266\trec:   59.827557\tkl:   25.104706\n",
      "Epoch: 484 [10100/50000 (20%)]  \tLoss:   87.755859\trec:   62.132870\tkl:   25.622982\n",
      "Epoch: 484 [20100/50000 (40%)]  \tLoss:   88.278496\trec:   62.612087\tkl:   25.666405\n",
      "Epoch: 484 [30100/50000 (60%)]  \tLoss:   89.926186\trec:   64.266090\tkl:   25.660105\n",
      "Epoch: 484 [40100/50000 (80%)]  \tLoss:   92.074928\trec:   64.926178\tkl:   27.148752\n",
      "====> Epoch: 484 Average train loss: 90.9524\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7814\n",
      "Epoch: 485 [  100/50000 ( 0%)]  \tLoss:   88.259666\trec:   62.584743\tkl:   25.674917\n",
      "Epoch: 485 [10100/50000 (20%)]  \tLoss:   90.959305\trec:   63.927647\tkl:   27.031656\n",
      "Epoch: 485 [20100/50000 (40%)]  \tLoss:   90.380104\trec:   63.672966\tkl:   26.707136\n",
      "Epoch: 485 [30100/50000 (60%)]  \tLoss:   90.457909\trec:   63.755947\tkl:   26.701967\n",
      "Epoch: 485 [40100/50000 (80%)]  \tLoss:   92.784920\trec:   66.186134\tkl:   26.598785\n",
      "====> Epoch: 485 Average train loss: 90.9473\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8438\n",
      "Epoch: 486 [  100/50000 ( 0%)]  \tLoss:   90.058624\trec:   63.214977\tkl:   26.843639\n",
      "Epoch: 486 [10100/50000 (20%)]  \tLoss:   86.327499\trec:   61.319920\tkl:   25.007572\n",
      "Epoch: 486 [20100/50000 (40%)]  \tLoss:   90.371864\trec:   63.240520\tkl:   27.131340\n",
      "Epoch: 486 [30100/50000 (60%)]  \tLoss:   90.268539\trec:   63.344616\tkl:   26.923927\n",
      "Epoch: 486 [40100/50000 (80%)]  \tLoss:   90.596901\trec:   63.280487\tkl:   27.316416\n",
      "====> Epoch: 486 Average train loss: 90.9674\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9922\n",
      "Epoch: 487 [  100/50000 ( 0%)]  \tLoss:   93.186668\trec:   65.781609\tkl:   27.405058\n",
      "Epoch: 487 [10100/50000 (20%)]  \tLoss:   91.851471\trec:   65.345490\tkl:   26.505981\n",
      "Epoch: 487 [20100/50000 (40%)]  \tLoss:   89.445099\trec:   62.873505\tkl:   26.571587\n",
      "Epoch: 487 [30100/50000 (60%)]  \tLoss:   88.869781\trec:   62.728710\tkl:   26.141069\n",
      "Epoch: 487 [40100/50000 (80%)]  \tLoss:   90.813766\trec:   64.326431\tkl:   26.487328\n",
      "====> Epoch: 487 Average train loss: 90.9317\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9179\n",
      "Epoch: 488 [  100/50000 ( 0%)]  \tLoss:   91.396393\trec:   63.586384\tkl:   27.810011\n",
      "Epoch: 488 [10100/50000 (20%)]  \tLoss:   90.950722\trec:   63.864513\tkl:   27.086208\n",
      "Epoch: 488 [20100/50000 (40%)]  \tLoss:   93.275673\trec:   67.071953\tkl:   26.203712\n",
      "Epoch: 488 [30100/50000 (60%)]  \tLoss:   92.874588\trec:   65.534416\tkl:   27.340179\n",
      "Epoch: 488 [40100/50000 (80%)]  \tLoss:   90.009590\trec:   63.911491\tkl:   26.098093\n",
      "====> Epoch: 488 Average train loss: 90.9380\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8214\n",
      "Epoch: 489 [  100/50000 ( 0%)]  \tLoss:   90.820992\trec:   65.010742\tkl:   25.810251\n",
      "Epoch: 489 [10100/50000 (20%)]  \tLoss:   92.719009\trec:   65.830605\tkl:   26.888407\n",
      "Epoch: 489 [20100/50000 (40%)]  \tLoss:   94.823318\trec:   67.221916\tkl:   27.601410\n",
      "Epoch: 489 [30100/50000 (60%)]  \tLoss:   91.114487\trec:   64.543922\tkl:   26.570566\n",
      "Epoch: 489 [40100/50000 (80%)]  \tLoss:   88.490067\trec:   62.287441\tkl:   26.202629\n",
      "====> Epoch: 489 Average train loss: 90.9575\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9733\n",
      "Epoch: 490 [  100/50000 ( 0%)]  \tLoss:   91.366386\trec:   64.714958\tkl:   26.651430\n",
      "Epoch: 490 [10100/50000 (20%)]  \tLoss:   90.572945\trec:   64.287056\tkl:   26.285889\n",
      "Epoch: 490 [20100/50000 (40%)]  \tLoss:   92.722115\trec:   65.850616\tkl:   26.871504\n",
      "Epoch: 490 [30100/50000 (60%)]  \tLoss:   91.960243\trec:   64.977089\tkl:   26.983154\n",
      "Epoch: 490 [40100/50000 (80%)]  \tLoss:   93.077324\trec:   65.743355\tkl:   27.333967\n",
      "====> Epoch: 490 Average train loss: 90.9233\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9435\n",
      "Epoch: 491 [  100/50000 ( 0%)]  \tLoss:   93.139786\trec:   66.754875\tkl:   26.384909\n",
      "Epoch: 491 [10100/50000 (20%)]  \tLoss:   90.496407\trec:   63.386902\tkl:   27.109499\n",
      "Epoch: 491 [20100/50000 (40%)]  \tLoss:   92.339607\trec:   64.848213\tkl:   27.491398\n",
      "Epoch: 491 [30100/50000 (60%)]  \tLoss:   94.492126\trec:   67.381035\tkl:   27.111094\n",
      "Epoch: 491 [40100/50000 (80%)]  \tLoss:   91.548126\trec:   65.033936\tkl:   26.514183\n",
      "====> Epoch: 491 Average train loss: 90.9483\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8233\n",
      "Epoch: 492 [  100/50000 ( 0%)]  \tLoss:   91.064941\trec:   64.523209\tkl:   26.541731\n",
      "Epoch: 492 [10100/50000 (20%)]  \tLoss:   90.545525\trec:   63.434753\tkl:   27.110771\n",
      "Epoch: 492 [20100/50000 (40%)]  \tLoss:   92.723534\trec:   65.860878\tkl:   26.862656\n",
      "Epoch: 492 [30100/50000 (60%)]  \tLoss:   88.118523\trec:   61.444424\tkl:   26.674103\n",
      "Epoch: 492 [40100/50000 (80%)]  \tLoss:   94.316910\trec:   66.767342\tkl:   27.549570\n",
      "====> Epoch: 492 Average train loss: 90.9164\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8175\n",
      "Epoch: 493 [  100/50000 ( 0%)]  \tLoss:   90.969421\trec:   63.718243\tkl:   27.251183\n",
      "Epoch: 493 [10100/50000 (20%)]  \tLoss:   93.331863\trec:   65.999039\tkl:   27.332825\n",
      "Epoch: 493 [20100/50000 (40%)]  \tLoss:   92.863556\trec:   66.249458\tkl:   26.614092\n",
      "Epoch: 493 [30100/50000 (60%)]  \tLoss:   88.567276\trec:   61.766117\tkl:   26.801157\n",
      "Epoch: 493 [40100/50000 (80%)]  \tLoss:   92.696289\trec:   65.872765\tkl:   26.823517\n",
      "====> Epoch: 493 Average train loss: 90.8920\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8551\n",
      "Epoch: 494 [  100/50000 ( 0%)]  \tLoss:   92.874626\trec:   66.301796\tkl:   26.572836\n",
      "Epoch: 494 [10100/50000 (20%)]  \tLoss:   90.012985\trec:   63.159798\tkl:   26.853188\n",
      "Epoch: 494 [20100/50000 (40%)]  \tLoss:   95.287163\trec:   68.651855\tkl:   26.635307\n",
      "Epoch: 494 [30100/50000 (60%)]  \tLoss:   91.310371\trec:   64.356667\tkl:   26.953701\n",
      "Epoch: 494 [40100/50000 (80%)]  \tLoss:   91.644234\trec:   64.805382\tkl:   26.838850\n",
      "====> Epoch: 494 Average train loss: 90.8686\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8707\n",
      "Epoch: 495 [  100/50000 ( 0%)]  \tLoss:   89.966537\trec:   63.299744\tkl:   26.666801\n",
      "Epoch: 495 [10100/50000 (20%)]  \tLoss:   91.956909\trec:   64.826523\tkl:   27.130388\n",
      "Epoch: 495 [20100/50000 (40%)]  \tLoss:   90.989464\trec:   63.995205\tkl:   26.994257\n",
      "Epoch: 495 [30100/50000 (60%)]  \tLoss:   90.018829\trec:   63.634445\tkl:   26.384378\n",
      "Epoch: 495 [40100/50000 (80%)]  \tLoss:   90.296242\trec:   62.945839\tkl:   27.350399\n",
      "====> Epoch: 495 Average train loss: 90.8885\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8167\n",
      "Epoch: 496 [  100/50000 ( 0%)]  \tLoss:   93.790314\trec:   66.028152\tkl:   27.762156\n",
      "Epoch: 496 [10100/50000 (20%)]  \tLoss:   88.207771\trec:   62.131573\tkl:   26.076200\n",
      "Epoch: 496 [20100/50000 (40%)]  \tLoss:   89.775352\trec:   63.954678\tkl:   25.820679\n",
      "Epoch: 496 [30100/50000 (60%)]  \tLoss:   93.190903\trec:   65.355774\tkl:   27.835129\n",
      "Epoch: 496 [40100/50000 (80%)]  \tLoss:   92.477501\trec:   65.559288\tkl:   26.918217\n",
      "====> Epoch: 496 Average train loss: 90.8895\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8491\n",
      "Epoch: 497 [  100/50000 ( 0%)]  \tLoss:   93.254356\trec:   65.700851\tkl:   27.553507\n",
      "Epoch: 497 [10100/50000 (20%)]  \tLoss:   91.516777\trec:   65.571983\tkl:   25.944794\n",
      "Epoch: 497 [20100/50000 (40%)]  \tLoss:   86.531013\trec:   61.139587\tkl:   25.391426\n",
      "Epoch: 497 [30100/50000 (60%)]  \tLoss:   91.024902\trec:   64.544159\tkl:   26.480736\n",
      "Epoch: 497 [40100/50000 (80%)]  \tLoss:   89.806023\trec:   63.051365\tkl:   26.754658\n",
      "====> Epoch: 497 Average train loss: 90.8813\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7506\n",
      "Epoch: 498 [  100/50000 ( 0%)]  \tLoss:   87.796310\trec:   61.553913\tkl:   26.242393\n",
      "Epoch: 498 [10100/50000 (20%)]  \tLoss:   91.753403\trec:   64.591827\tkl:   27.161579\n",
      "Epoch: 498 [20100/50000 (40%)]  \tLoss:   87.803520\trec:   61.068066\tkl:   26.735460\n",
      "Epoch: 498 [30100/50000 (60%)]  \tLoss:   91.830093\trec:   64.509880\tkl:   27.320211\n",
      "Epoch: 498 [40100/50000 (80%)]  \tLoss:   91.983803\trec:   65.618301\tkl:   26.365507\n",
      "====> Epoch: 498 Average train loss: 90.8900\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7632\n",
      "Epoch: 499 [  100/50000 ( 0%)]  \tLoss:   91.147461\trec:   64.580040\tkl:   26.567427\n",
      "Epoch: 499 [10100/50000 (20%)]  \tLoss:   86.650215\trec:   60.959457\tkl:   25.690756\n",
      "Epoch: 499 [20100/50000 (40%)]  \tLoss:   89.993126\trec:   63.431957\tkl:   26.561165\n",
      "Epoch: 499 [30100/50000 (60%)]  \tLoss:   97.906776\trec:   70.208405\tkl:   27.698374\n",
      "Epoch: 499 [40100/50000 (80%)]  \tLoss:   89.423325\trec:   62.784973\tkl:   26.638357\n",
      "====> Epoch: 499 Average train loss: 90.8865\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7526\n",
      "Epoch: 500 [  100/50000 ( 0%)]  \tLoss:   89.080582\trec:   62.594280\tkl:   26.486303\n",
      "Epoch: 500 [10100/50000 (20%)]  \tLoss:   94.263435\trec:   67.313957\tkl:   26.949486\n",
      "Epoch: 500 [20100/50000 (40%)]  \tLoss:   90.024033\trec:   62.934753\tkl:   27.089277\n",
      "Epoch: 500 [30100/50000 (60%)]  \tLoss:   89.621033\trec:   63.552650\tkl:   26.068378\n",
      "Epoch: 500 [40100/50000 (80%)]  \tLoss:   91.123764\trec:   64.037476\tkl:   27.086290\n",
      "====> Epoch: 500 Average train loss: 90.8593\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8036\n",
      "Epoch: 501 [  100/50000 ( 0%)]  \tLoss:   93.493896\trec:   66.775513\tkl:   26.718378\n",
      "Epoch: 501 [10100/50000 (20%)]  \tLoss:   91.182831\trec:   65.234985\tkl:   25.947845\n",
      "Epoch: 501 [20100/50000 (40%)]  \tLoss:   90.724716\trec:   64.271317\tkl:   26.453400\n",
      "Epoch: 501 [30100/50000 (60%)]  \tLoss:   89.778107\trec:   62.923870\tkl:   26.854231\n",
      "Epoch: 501 [40100/50000 (80%)]  \tLoss:   90.625191\trec:   64.158905\tkl:   26.466286\n",
      "====> Epoch: 501 Average train loss: 90.8743\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8870\n",
      "Epoch: 502 [  100/50000 ( 0%)]  \tLoss:   91.645439\trec:   65.693062\tkl:   25.952370\n",
      "Epoch: 502 [10100/50000 (20%)]  \tLoss:   95.201385\trec:   67.549934\tkl:   27.651445\n",
      "Epoch: 502 [20100/50000 (40%)]  \tLoss:   88.941170\trec:   62.190193\tkl:   26.750978\n",
      "Epoch: 502 [30100/50000 (60%)]  \tLoss:   89.972061\trec:   63.803017\tkl:   26.169043\n",
      "Epoch: 502 [40100/50000 (80%)]  \tLoss:   93.528496\trec:   65.835976\tkl:   27.692522\n",
      "====> Epoch: 502 Average train loss: 90.8658\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8348\n",
      "Epoch: 503 [  100/50000 ( 0%)]  \tLoss:   90.168190\trec:   63.096092\tkl:   27.072102\n",
      "Epoch: 503 [10100/50000 (20%)]  \tLoss:   91.808464\trec:   64.211807\tkl:   27.596659\n",
      "Epoch: 503 [20100/50000 (40%)]  \tLoss:   89.887283\trec:   63.118210\tkl:   26.769072\n",
      "Epoch: 503 [30100/50000 (60%)]  \tLoss:   94.063835\trec:   67.469421\tkl:   26.594412\n",
      "Epoch: 503 [40100/50000 (80%)]  \tLoss:   88.741333\trec:   62.888683\tkl:   25.852648\n",
      "====> Epoch: 503 Average train loss: 90.8471\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8225\n",
      "Epoch: 504 [  100/50000 ( 0%)]  \tLoss:   90.516373\trec:   64.468437\tkl:   26.047935\n",
      "Epoch: 504 [10100/50000 (20%)]  \tLoss:   90.146049\trec:   63.580273\tkl:   26.565786\n",
      "Epoch: 504 [20100/50000 (40%)]  \tLoss:   87.468826\trec:   61.327549\tkl:   26.141273\n",
      "Epoch: 504 [30100/50000 (60%)]  \tLoss:   90.931160\trec:   64.011475\tkl:   26.919685\n",
      "Epoch: 504 [40100/50000 (80%)]  \tLoss:   94.920135\trec:   67.217598\tkl:   27.702534\n",
      "====> Epoch: 504 Average train loss: 90.8664\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7746\n",
      "Epoch: 505 [  100/50000 ( 0%)]  \tLoss:   90.811913\trec:   64.302658\tkl:   26.509253\n",
      "Epoch: 505 [10100/50000 (20%)]  \tLoss:   89.282158\trec:   63.259346\tkl:   26.022812\n",
      "Epoch: 505 [20100/50000 (40%)]  \tLoss:   88.396523\trec:   62.253479\tkl:   26.143036\n",
      "Epoch: 505 [30100/50000 (60%)]  \tLoss:   88.998711\trec:   62.393223\tkl:   26.605492\n",
      "Epoch: 505 [40100/50000 (80%)]  \tLoss:   91.063667\trec:   64.980011\tkl:   26.083664\n",
      "====> Epoch: 505 Average train loss: 90.8550\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8005\n",
      "Epoch: 506 [  100/50000 ( 0%)]  \tLoss:   93.165192\trec:   66.265503\tkl:   26.899681\n",
      "Epoch: 506 [10100/50000 (20%)]  \tLoss:   92.433754\trec:   64.567047\tkl:   27.866711\n",
      "Epoch: 506 [20100/50000 (40%)]  \tLoss:   88.702026\trec:   62.113548\tkl:   26.588478\n",
      "Epoch: 506 [30100/50000 (60%)]  \tLoss:   91.426559\trec:   64.750885\tkl:   26.675669\n",
      "Epoch: 506 [40100/50000 (80%)]  \tLoss:   95.725327\trec:   68.542595\tkl:   27.182730\n",
      "====> Epoch: 506 Average train loss: 90.8467\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8464\n",
      "Epoch: 507 [  100/50000 ( 0%)]  \tLoss:   88.520859\trec:   61.645031\tkl:   26.875822\n",
      "Epoch: 507 [10100/50000 (20%)]  \tLoss:   91.556427\trec:   64.476280\tkl:   27.080149\n",
      "Epoch: 507 [20100/50000 (40%)]  \tLoss:   91.136192\trec:   64.805748\tkl:   26.330437\n",
      "Epoch: 507 [30100/50000 (60%)]  \tLoss:   92.368378\trec:   65.458183\tkl:   26.910192\n",
      "Epoch: 507 [40100/50000 (80%)]  \tLoss:   97.567940\trec:   68.661308\tkl:   28.906628\n",
      "====> Epoch: 507 Average train loss: 90.8287\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7589\n",
      "Epoch: 508 [  100/50000 ( 0%)]  \tLoss:   87.244354\trec:   60.936234\tkl:   26.308117\n",
      "Epoch: 508 [10100/50000 (20%)]  \tLoss:   86.952675\trec:   61.824852\tkl:   25.127825\n",
      "Epoch: 508 [20100/50000 (40%)]  \tLoss:   89.533379\trec:   62.996765\tkl:   26.536613\n",
      "Epoch: 508 [30100/50000 (60%)]  \tLoss:   91.383286\trec:   64.229294\tkl:   27.153992\n",
      "Epoch: 508 [40100/50000 (80%)]  \tLoss:   90.558960\trec:   63.730537\tkl:   26.828423\n",
      "====> Epoch: 508 Average train loss: 90.8518\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7827\n",
      "Epoch: 509 [  100/50000 ( 0%)]  \tLoss:   91.164497\trec:   63.833412\tkl:   27.331091\n",
      "Epoch: 509 [10100/50000 (20%)]  \tLoss:   89.492104\trec:   62.975067\tkl:   26.517046\n",
      "Epoch: 509 [20100/50000 (40%)]  \tLoss:   87.946426\trec:   60.684719\tkl:   27.261702\n",
      "Epoch: 509 [30100/50000 (60%)]  \tLoss:   93.545509\trec:   65.925667\tkl:   27.619843\n",
      "Epoch: 509 [40100/50000 (80%)]  \tLoss:   88.796593\trec:   62.396004\tkl:   26.400585\n",
      "====> Epoch: 509 Average train loss: 90.8155\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7225\n",
      "Epoch: 510 [  100/50000 ( 0%)]  \tLoss:   88.298454\trec:   61.509331\tkl:   26.789125\n",
      "Epoch: 510 [10100/50000 (20%)]  \tLoss:   87.789009\trec:   61.268208\tkl:   26.520803\n",
      "Epoch: 510 [20100/50000 (40%)]  \tLoss:   84.603813\trec:   58.653149\tkl:   25.950668\n",
      "Epoch: 510 [30100/50000 (60%)]  \tLoss:   91.703041\trec:   64.081017\tkl:   27.622036\n",
      "Epoch: 510 [40100/50000 (80%)]  \tLoss:   92.493172\trec:   65.418083\tkl:   27.075085\n",
      "====> Epoch: 510 Average train loss: 90.8466\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8255\n",
      "Epoch: 511 [  100/50000 ( 0%)]  \tLoss:   90.127693\trec:   63.997822\tkl:   26.129873\n",
      "Epoch: 511 [10100/50000 (20%)]  \tLoss:   92.251831\trec:   65.785454\tkl:   26.466381\n",
      "Epoch: 511 [20100/50000 (40%)]  \tLoss:   89.953445\trec:   63.305565\tkl:   26.647879\n",
      "Epoch: 511 [30100/50000 (60%)]  \tLoss:   91.538582\trec:   64.974449\tkl:   26.564131\n",
      "Epoch: 511 [40100/50000 (80%)]  \tLoss:   87.497124\trec:   62.051815\tkl:   25.445312\n",
      "====> Epoch: 511 Average train loss: 90.8146\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7524\n",
      "Epoch: 512 [  100/50000 ( 0%)]  \tLoss:   91.485558\trec:   64.465485\tkl:   27.020067\n",
      "Epoch: 512 [10100/50000 (20%)]  \tLoss:   93.019180\trec:   65.943161\tkl:   27.076019\n",
      "Epoch: 512 [20100/50000 (40%)]  \tLoss:   87.277794\trec:   61.113728\tkl:   26.164064\n",
      "Epoch: 512 [30100/50000 (60%)]  \tLoss:   92.152573\trec:   64.896828\tkl:   27.255749\n",
      "Epoch: 512 [40100/50000 (80%)]  \tLoss:   91.068260\trec:   65.442207\tkl:   25.626055\n",
      "====> Epoch: 512 Average train loss: 90.7984\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7934\n",
      "Epoch: 513 [  100/50000 ( 0%)]  \tLoss:   91.789726\trec:   64.769806\tkl:   27.019918\n",
      "Epoch: 513 [10100/50000 (20%)]  \tLoss:   87.047302\trec:   61.117947\tkl:   25.929350\n",
      "Epoch: 513 [20100/50000 (40%)]  \tLoss:   93.079391\trec:   65.342583\tkl:   27.736811\n",
      "Epoch: 513 [30100/50000 (60%)]  \tLoss:   94.411385\trec:   67.522217\tkl:   26.889170\n",
      "Epoch: 513 [40100/50000 (80%)]  \tLoss:   92.300400\trec:   66.142784\tkl:   26.157612\n",
      "====> Epoch: 513 Average train loss: 90.7952\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7795\n",
      "Epoch: 514 [  100/50000 ( 0%)]  \tLoss:   91.681137\trec:   64.823639\tkl:   26.857502\n",
      "Epoch: 514 [10100/50000 (20%)]  \tLoss:   91.427292\trec:   64.827095\tkl:   26.600195\n",
      "Epoch: 514 [20100/50000 (40%)]  \tLoss:   88.805893\trec:   62.783115\tkl:   26.022783\n",
      "Epoch: 514 [30100/50000 (60%)]  \tLoss:   90.885887\trec:   64.317131\tkl:   26.568752\n",
      "Epoch: 514 [40100/50000 (80%)]  \tLoss:   90.929062\trec:   63.054958\tkl:   27.874096\n",
      "====> Epoch: 514 Average train loss: 90.7760\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7836\n",
      "Epoch: 515 [  100/50000 ( 0%)]  \tLoss:   93.856934\trec:   66.417511\tkl:   27.439426\n",
      "Epoch: 515 [10100/50000 (20%)]  \tLoss:   90.024780\trec:   63.617630\tkl:   26.407158\n",
      "Epoch: 515 [20100/50000 (40%)]  \tLoss:   88.999405\trec:   62.259003\tkl:   26.740396\n",
      "Epoch: 515 [30100/50000 (60%)]  \tLoss:   91.116089\trec:   65.412781\tkl:   25.703316\n",
      "Epoch: 515 [40100/50000 (80%)]  \tLoss:   90.161171\trec:   64.050461\tkl:   26.110710\n",
      "====> Epoch: 515 Average train loss: 90.8009\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7669\n",
      "Epoch: 516 [  100/50000 ( 0%)]  \tLoss:   86.540939\trec:   61.310871\tkl:   25.230061\n",
      "Epoch: 516 [10100/50000 (20%)]  \tLoss:   91.297417\trec:   64.220573\tkl:   27.076851\n",
      "Epoch: 516 [20100/50000 (40%)]  \tLoss:   91.733276\trec:   64.856239\tkl:   26.877037\n",
      "Epoch: 516 [30100/50000 (60%)]  \tLoss:   89.935547\trec:   63.010639\tkl:   26.924902\n",
      "Epoch: 516 [40100/50000 (80%)]  \tLoss:   87.480240\trec:   61.117790\tkl:   26.362450\n",
      "====> Epoch: 516 Average train loss: 90.7951\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7900\n",
      "Epoch: 517 [  100/50000 ( 0%)]  \tLoss:   93.202248\trec:   65.665527\tkl:   27.536716\n",
      "Epoch: 517 [10100/50000 (20%)]  \tLoss:   90.204773\trec:   63.279499\tkl:   26.925272\n",
      "Epoch: 517 [20100/50000 (40%)]  \tLoss:   94.308174\trec:   66.908333\tkl:   27.399837\n",
      "Epoch: 517 [30100/50000 (60%)]  \tLoss:   89.376953\trec:   63.113270\tkl:   26.263680\n",
      "Epoch: 517 [40100/50000 (80%)]  \tLoss:   86.124268\trec:   60.205574\tkl:   25.918694\n",
      "====> Epoch: 517 Average train loss: 90.8219\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7584\n",
      "Epoch: 518 [  100/50000 ( 0%)]  \tLoss:   88.280731\trec:   62.232327\tkl:   26.048401\n",
      "Epoch: 518 [10100/50000 (20%)]  \tLoss:   92.172264\trec:   64.286659\tkl:   27.885607\n",
      "Epoch: 518 [20100/50000 (40%)]  \tLoss:   91.168495\trec:   64.637939\tkl:   26.530556\n",
      "Epoch: 518 [30100/50000 (60%)]  \tLoss:   88.974014\trec:   62.631062\tkl:   26.342949\n",
      "Epoch: 518 [40100/50000 (80%)]  \tLoss:   93.235054\trec:   66.070724\tkl:   27.164330\n",
      "====> Epoch: 518 Average train loss: 90.7525\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7043\n",
      "Epoch: 519 [  100/50000 ( 0%)]  \tLoss:   89.889923\trec:   63.273647\tkl:   26.616276\n",
      "Epoch: 519 [10100/50000 (20%)]  \tLoss:   90.726227\trec:   63.880466\tkl:   26.845766\n",
      "Epoch: 519 [20100/50000 (40%)]  \tLoss:   89.531914\trec:   62.997692\tkl:   26.534224\n",
      "Epoch: 519 [30100/50000 (60%)]  \tLoss:   86.956596\trec:   60.622845\tkl:   26.333754\n",
      "Epoch: 519 [40100/50000 (80%)]  \tLoss:   94.473618\trec:   67.166504\tkl:   27.307116\n",
      "====> Epoch: 519 Average train loss: 90.7694\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8174\n",
      "Epoch: 520 [  100/50000 ( 0%)]  \tLoss:   90.262558\trec:   63.612347\tkl:   26.650211\n",
      "Epoch: 520 [10100/50000 (20%)]  \tLoss:   90.856567\trec:   63.605476\tkl:   27.251093\n",
      "Epoch: 520 [20100/50000 (40%)]  \tLoss:   89.923653\trec:   63.474266\tkl:   26.449381\n",
      "Epoch: 520 [30100/50000 (60%)]  \tLoss:   93.672508\trec:   66.537354\tkl:   27.135155\n",
      "Epoch: 520 [40100/50000 (80%)]  \tLoss:   89.697807\trec:   63.445202\tkl:   26.252607\n",
      "====> Epoch: 520 Average train loss: 90.7660\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7695\n",
      "Epoch: 521 [  100/50000 ( 0%)]  \tLoss:   93.827812\trec:   66.530792\tkl:   27.297020\n",
      "Epoch: 521 [10100/50000 (20%)]  \tLoss:   89.911720\trec:   63.499466\tkl:   26.412249\n",
      "Epoch: 521 [20100/50000 (40%)]  \tLoss:   91.032570\trec:   64.260513\tkl:   26.772053\n",
      "Epoch: 521 [30100/50000 (60%)]  \tLoss:   92.187859\trec:   64.867943\tkl:   27.319916\n",
      "Epoch: 521 [40100/50000 (80%)]  \tLoss:   91.612900\trec:   64.762665\tkl:   26.850237\n",
      "====> Epoch: 521 Average train loss: 90.7911\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8537\n",
      "Epoch: 522 [  100/50000 ( 0%)]  \tLoss:   90.744820\trec:   64.034538\tkl:   26.710285\n",
      "Epoch: 522 [10100/50000 (20%)]  \tLoss:   92.326851\trec:   65.789970\tkl:   26.536879\n",
      "Epoch: 522 [20100/50000 (40%)]  \tLoss:   90.590446\trec:   62.715366\tkl:   27.875078\n",
      "Epoch: 522 [30100/50000 (60%)]  \tLoss:   88.385620\trec:   62.284286\tkl:   26.101337\n",
      "Epoch: 522 [40100/50000 (80%)]  \tLoss:   88.427261\trec:   61.822720\tkl:   26.604549\n",
      "====> Epoch: 522 Average train loss: 90.7754\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7296\n",
      "Epoch: 523 [  100/50000 ( 0%)]  \tLoss:   94.991615\trec:   67.511398\tkl:   27.480219\n",
      "Epoch: 523 [10100/50000 (20%)]  \tLoss:   91.162018\trec:   64.665428\tkl:   26.496590\n",
      "Epoch: 523 [20100/50000 (40%)]  \tLoss:   90.130859\trec:   63.334705\tkl:   26.796154\n",
      "Epoch: 523 [30100/50000 (60%)]  \tLoss:   90.974930\trec:   63.821934\tkl:   27.153000\n",
      "Epoch: 523 [40100/50000 (80%)]  \tLoss:   91.806694\trec:   65.538788\tkl:   26.267910\n",
      "====> Epoch: 523 Average train loss: 90.7518\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7888\n",
      "Epoch: 524 [  100/50000 ( 0%)]  \tLoss:   94.179039\trec:   66.341103\tkl:   27.837936\n",
      "Epoch: 524 [10100/50000 (20%)]  \tLoss:   89.294044\trec:   63.519032\tkl:   25.775009\n",
      "Epoch: 524 [20100/50000 (40%)]  \tLoss:   88.532364\trec:   62.487270\tkl:   26.045095\n",
      "Epoch: 524 [30100/50000 (60%)]  \tLoss:   92.082977\trec:   65.495743\tkl:   26.587231\n",
      "Epoch: 524 [40100/50000 (80%)]  \tLoss:   89.041832\trec:   63.284267\tkl:   25.757572\n",
      "====> Epoch: 524 Average train loss: 90.7714\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8069\n",
      "Epoch: 525 [  100/50000 ( 0%)]  \tLoss:   90.032196\trec:   63.450317\tkl:   26.581882\n",
      "Epoch: 525 [10100/50000 (20%)]  \tLoss:   93.017166\trec:   65.855949\tkl:   27.161221\n",
      "Epoch: 525 [20100/50000 (40%)]  \tLoss:   91.783638\trec:   64.716850\tkl:   27.066790\n",
      "Epoch: 525 [30100/50000 (60%)]  \tLoss:   91.789917\trec:   64.229996\tkl:   27.559927\n",
      "Epoch: 525 [40100/50000 (80%)]  \tLoss:   90.538139\trec:   63.597282\tkl:   26.940865\n",
      "====> Epoch: 525 Average train loss: 90.7577\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7233\n",
      "Epoch: 526 [  100/50000 ( 0%)]  \tLoss:   88.910500\trec:   61.880711\tkl:   27.029781\n",
      "Epoch: 526 [10100/50000 (20%)]  \tLoss:   88.670830\trec:   62.336346\tkl:   26.334484\n",
      "Epoch: 526 [20100/50000 (40%)]  \tLoss:   88.430420\trec:   61.557224\tkl:   26.873196\n",
      "Epoch: 526 [30100/50000 (60%)]  \tLoss:   92.890984\trec:   66.897614\tkl:   25.993370\n",
      "Epoch: 526 [40100/50000 (80%)]  \tLoss:   88.679588\trec:   63.011219\tkl:   25.668371\n",
      "====> Epoch: 526 Average train loss: 90.7468\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7642\n",
      "Epoch: 527 [  100/50000 ( 0%)]  \tLoss:   89.067039\trec:   62.945202\tkl:   26.121836\n",
      "Epoch: 527 [10100/50000 (20%)]  \tLoss:   90.969353\trec:   63.952080\tkl:   27.017273\n",
      "Epoch: 527 [20100/50000 (40%)]  \tLoss:   93.826698\trec:   65.792732\tkl:   28.033966\n",
      "Epoch: 527 [30100/50000 (60%)]  \tLoss:   90.306061\trec:   64.080193\tkl:   26.225870\n",
      "Epoch: 527 [40100/50000 (80%)]  \tLoss:   91.604103\trec:   64.171837\tkl:   27.432264\n",
      "====> Epoch: 527 Average train loss: 90.7524\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8036\n",
      "Epoch: 528 [  100/50000 ( 0%)]  \tLoss:   91.974800\trec:   66.033066\tkl:   25.941740\n",
      "Epoch: 528 [10100/50000 (20%)]  \tLoss:   94.494942\trec:   66.998405\tkl:   27.496538\n",
      "Epoch: 528 [20100/50000 (40%)]  \tLoss:   88.425194\trec:   62.678299\tkl:   25.746895\n",
      "Epoch: 528 [30100/50000 (60%)]  \tLoss:   88.617439\trec:   62.571922\tkl:   26.045519\n",
      "Epoch: 528 [40100/50000 (80%)]  \tLoss:   89.968140\trec:   63.435623\tkl:   26.532524\n",
      "====> Epoch: 528 Average train loss: 90.7424\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7939\n",
      "Epoch: 529 [  100/50000 ( 0%)]  \tLoss:   90.747734\trec:   63.287624\tkl:   27.460112\n",
      "Epoch: 529 [10100/50000 (20%)]  \tLoss:   88.494530\trec:   61.769745\tkl:   26.724783\n",
      "Epoch: 529 [20100/50000 (40%)]  \tLoss:   87.248100\trec:   61.189236\tkl:   26.058861\n",
      "Epoch: 529 [30100/50000 (60%)]  \tLoss:   88.849159\trec:   62.552559\tkl:   26.296600\n",
      "Epoch: 529 [40100/50000 (80%)]  \tLoss:   92.358894\trec:   65.231667\tkl:   27.127234\n",
      "====> Epoch: 529 Average train loss: 90.7486\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7594\n",
      "Epoch: 530 [  100/50000 ( 0%)]  \tLoss:   93.296539\trec:   66.507584\tkl:   26.788954\n",
      "Epoch: 530 [10100/50000 (20%)]  \tLoss:   91.328735\trec:   64.424149\tkl:   26.904589\n",
      "Epoch: 530 [20100/50000 (40%)]  \tLoss:   91.763397\trec:   65.970818\tkl:   25.792582\n",
      "Epoch: 530 [30100/50000 (60%)]  \tLoss:   88.538742\trec:   61.661205\tkl:   26.877531\n",
      "Epoch: 530 [40100/50000 (80%)]  \tLoss:   90.161102\trec:   62.859306\tkl:   27.301796\n",
      "====> Epoch: 530 Average train loss: 90.7127\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7961\n",
      "Epoch: 531 [  100/50000 ( 0%)]  \tLoss:   87.754044\trec:   60.767029\tkl:   26.987017\n",
      "Epoch: 531 [10100/50000 (20%)]  \tLoss:   91.702881\trec:   64.987534\tkl:   26.715343\n",
      "Epoch: 531 [20100/50000 (40%)]  \tLoss:   94.383560\trec:   66.192406\tkl:   28.191156\n",
      "Epoch: 531 [30100/50000 (60%)]  \tLoss:   96.000885\trec:   68.743729\tkl:   27.257154\n",
      "Epoch: 531 [40100/50000 (80%)]  \tLoss:   91.439880\trec:   64.302551\tkl:   27.137323\n",
      "====> Epoch: 531 Average train loss: 90.7129\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7079\n",
      "Epoch: 532 [  100/50000 ( 0%)]  \tLoss:   91.472534\trec:   64.230545\tkl:   27.241989\n",
      "Epoch: 532 [10100/50000 (20%)]  \tLoss:   87.736694\trec:   61.116009\tkl:   26.620687\n",
      "Epoch: 532 [20100/50000 (40%)]  \tLoss:   95.567192\trec:   68.891838\tkl:   26.675362\n",
      "Epoch: 532 [30100/50000 (60%)]  \tLoss:   89.541115\trec:   63.462513\tkl:   26.078604\n",
      "Epoch: 532 [40100/50000 (80%)]  \tLoss:   90.337502\trec:   63.325649\tkl:   27.011845\n",
      "====> Epoch: 532 Average train loss: 90.7283\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7968\n",
      "Epoch: 533 [  100/50000 ( 0%)]  \tLoss:   91.411118\trec:   64.833366\tkl:   26.577755\n",
      "Epoch: 533 [10100/50000 (20%)]  \tLoss:   91.869774\trec:   65.859207\tkl:   26.010567\n",
      "Epoch: 533 [20100/50000 (40%)]  \tLoss:   89.520279\trec:   62.965923\tkl:   26.554361\n",
      "Epoch: 533 [30100/50000 (60%)]  \tLoss:   92.626305\trec:   65.261482\tkl:   27.364826\n",
      "Epoch: 533 [40100/50000 (80%)]  \tLoss:   91.615463\trec:   64.959801\tkl:   26.655666\n",
      "====> Epoch: 533 Average train loss: 90.7082\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8352\n",
      "Epoch: 534 [  100/50000 ( 0%)]  \tLoss:   93.169159\trec:   66.258522\tkl:   26.910635\n",
      "Epoch: 534 [10100/50000 (20%)]  \tLoss:   91.339294\trec:   63.752056\tkl:   27.587238\n",
      "Epoch: 534 [20100/50000 (40%)]  \tLoss:   88.603516\trec:   63.037498\tkl:   25.566015\n",
      "Epoch: 534 [30100/50000 (60%)]  \tLoss:   87.158279\trec:   61.952068\tkl:   25.206211\n",
      "Epoch: 534 [40100/50000 (80%)]  \tLoss:   92.967804\trec:   66.405060\tkl:   26.562738\n",
      "====> Epoch: 534 Average train loss: 90.7209\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6612\n",
      "Epoch: 535 [  100/50000 ( 0%)]  \tLoss:   90.577820\trec:   63.808807\tkl:   26.769012\n",
      "Epoch: 535 [10100/50000 (20%)]  \tLoss:   93.176933\trec:   66.242851\tkl:   26.934076\n",
      "Epoch: 535 [20100/50000 (40%)]  \tLoss:   93.078751\trec:   66.081802\tkl:   26.996948\n",
      "Epoch: 535 [30100/50000 (60%)]  \tLoss:   93.328094\trec:   66.979858\tkl:   26.348240\n",
      "Epoch: 535 [40100/50000 (80%)]  \tLoss:   87.931206\trec:   61.735790\tkl:   26.195414\n",
      "====> Epoch: 535 Average train loss: 90.7192\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6790\n",
      "Epoch: 536 [  100/50000 ( 0%)]  \tLoss:   89.278923\trec:   62.423676\tkl:   26.855246\n",
      "Epoch: 536 [10100/50000 (20%)]  \tLoss:   93.433556\trec:   66.149483\tkl:   27.284077\n",
      "Epoch: 536 [20100/50000 (40%)]  \tLoss:   91.731621\trec:   64.580605\tkl:   27.151018\n",
      "Epoch: 536 [30100/50000 (60%)]  \tLoss:   89.684555\trec:   62.449619\tkl:   27.234941\n",
      "Epoch: 536 [40100/50000 (80%)]  \tLoss:   91.304131\trec:   64.276428\tkl:   27.027702\n",
      "====> Epoch: 536 Average train loss: 90.7090\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6643\n",
      "Epoch: 537 [  100/50000 ( 0%)]  \tLoss:   92.042641\trec:   64.721657\tkl:   27.320992\n",
      "Epoch: 537 [10100/50000 (20%)]  \tLoss:   91.488808\trec:   64.507217\tkl:   26.981596\n",
      "Epoch: 537 [20100/50000 (40%)]  \tLoss:   89.646034\trec:   63.312054\tkl:   26.333984\n",
      "Epoch: 537 [30100/50000 (60%)]  \tLoss:   95.822021\trec:   67.856590\tkl:   27.965431\n",
      "Epoch: 537 [40100/50000 (80%)]  \tLoss:   92.497383\trec:   64.837196\tkl:   27.660185\n",
      "====> Epoch: 537 Average train loss: 90.6941\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6561\n",
      "Epoch: 538 [  100/50000 ( 0%)]  \tLoss:   92.526382\trec:   66.269119\tkl:   26.257271\n",
      "Epoch: 538 [10100/50000 (20%)]  \tLoss:   91.776482\trec:   64.683296\tkl:   27.093185\n",
      "Epoch: 538 [20100/50000 (40%)]  \tLoss:   93.362732\trec:   65.857124\tkl:   27.505606\n",
      "Epoch: 538 [30100/50000 (60%)]  \tLoss:   88.637741\trec:   63.285225\tkl:   25.352516\n",
      "Epoch: 538 [40100/50000 (80%)]  \tLoss:   91.436852\trec:   64.908470\tkl:   26.528378\n",
      "====> Epoch: 538 Average train loss: 90.7131\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7741\n",
      "Epoch: 539 [  100/50000 ( 0%)]  \tLoss:   85.080933\trec:   59.551136\tkl:   25.529799\n",
      "Epoch: 539 [10100/50000 (20%)]  \tLoss:   90.914932\trec:   63.151001\tkl:   27.763933\n",
      "Epoch: 539 [20100/50000 (40%)]  \tLoss:   86.446892\trec:   61.276012\tkl:   25.170877\n",
      "Epoch: 539 [30100/50000 (60%)]  \tLoss:   94.225876\trec:   66.989212\tkl:   27.236670\n",
      "Epoch: 539 [40100/50000 (80%)]  \tLoss:   92.375381\trec:   65.157959\tkl:   27.217419\n",
      "====> Epoch: 539 Average train loss: 90.7113\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7232\n",
      "Epoch: 540 [  100/50000 ( 0%)]  \tLoss:   89.552574\trec:   62.653721\tkl:   26.898855\n",
      "Epoch: 540 [10100/50000 (20%)]  \tLoss:   89.315796\trec:   62.395683\tkl:   26.920115\n",
      "Epoch: 540 [20100/50000 (40%)]  \tLoss:   87.787498\trec:   61.671169\tkl:   26.116323\n",
      "Epoch: 540 [30100/50000 (60%)]  \tLoss:   90.344604\trec:   63.746063\tkl:   26.598545\n",
      "Epoch: 540 [40100/50000 (80%)]  \tLoss:   86.466904\trec:   60.765224\tkl:   25.701677\n",
      "====> Epoch: 540 Average train loss: 90.6950\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7526\n",
      "Epoch: 541 [  100/50000 ( 0%)]  \tLoss:   93.488571\trec:   65.791412\tkl:   27.697153\n",
      "Epoch: 541 [10100/50000 (20%)]  \tLoss:   90.399757\trec:   64.177231\tkl:   26.222521\n",
      "Epoch: 541 [20100/50000 (40%)]  \tLoss:   89.889412\trec:   63.036228\tkl:   26.853180\n",
      "Epoch: 541 [30100/50000 (60%)]  \tLoss:   90.462868\trec:   63.032494\tkl:   27.430372\n",
      "Epoch: 541 [40100/50000 (80%)]  \tLoss:   88.054138\trec:   61.246670\tkl:   26.807468\n",
      "====> Epoch: 541 Average train loss: 90.6823\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6998\n",
      "Epoch: 542 [  100/50000 ( 0%)]  \tLoss:   89.260132\trec:   62.614315\tkl:   26.645824\n",
      "Epoch: 542 [10100/50000 (20%)]  \tLoss:   88.564041\trec:   62.211449\tkl:   26.352594\n",
      "Epoch: 542 [20100/50000 (40%)]  \tLoss:   92.451843\trec:   65.605553\tkl:   26.846289\n",
      "Epoch: 542 [30100/50000 (60%)]  \tLoss:   89.579575\trec:   62.944649\tkl:   26.634932\n",
      "Epoch: 542 [40100/50000 (80%)]  \tLoss:   94.087120\trec:   67.414604\tkl:   26.672508\n",
      "====> Epoch: 542 Average train loss: 90.6748\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7822\n",
      "Epoch: 543 [  100/50000 ( 0%)]  \tLoss:   86.820366\trec:   60.886364\tkl:   25.934000\n",
      "Epoch: 543 [10100/50000 (20%)]  \tLoss:   88.633896\trec:   62.509842\tkl:   26.124052\n",
      "Epoch: 543 [20100/50000 (40%)]  \tLoss:   91.446915\trec:   64.582146\tkl:   26.864759\n",
      "Epoch: 543 [30100/50000 (60%)]  \tLoss:   90.614960\trec:   63.803337\tkl:   26.811615\n",
      "Epoch: 543 [40100/50000 (80%)]  \tLoss:   90.430870\trec:   63.695210\tkl:   26.735661\n",
      "====> Epoch: 543 Average train loss: 90.6716\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6141\n",
      "Epoch: 544 [  100/50000 ( 0%)]  \tLoss:   87.470573\trec:   61.468719\tkl:   26.001858\n",
      "Epoch: 544 [10100/50000 (20%)]  \tLoss:   94.849991\trec:   67.328957\tkl:   27.521036\n",
      "Epoch: 544 [20100/50000 (40%)]  \tLoss:   92.418159\trec:   65.522873\tkl:   26.895285\n",
      "Epoch: 544 [30100/50000 (60%)]  \tLoss:   94.396645\trec:   66.348282\tkl:   28.048370\n",
      "Epoch: 544 [40100/50000 (80%)]  \tLoss:   93.659943\trec:   66.320061\tkl:   27.339882\n",
      "====> Epoch: 544 Average train loss: 90.6312\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7140\n",
      "Epoch: 545 [  100/50000 ( 0%)]  \tLoss:   92.368690\trec:   64.477364\tkl:   27.891325\n",
      "Epoch: 545 [10100/50000 (20%)]  \tLoss:   93.172836\trec:   66.671822\tkl:   26.501017\n",
      "Epoch: 545 [20100/50000 (40%)]  \tLoss:   90.167862\trec:   63.169872\tkl:   26.997986\n",
      "Epoch: 545 [30100/50000 (60%)]  \tLoss:   88.976639\trec:   61.969334\tkl:   27.007309\n",
      "Epoch: 545 [40100/50000 (80%)]  \tLoss:   89.475876\trec:   62.038822\tkl:   27.437050\n",
      "====> Epoch: 545 Average train loss: 90.6532\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7039\n",
      "Epoch: 546 [  100/50000 ( 0%)]  \tLoss:   89.910301\trec:   63.025043\tkl:   26.885256\n",
      "Epoch: 546 [10100/50000 (20%)]  \tLoss:   89.955666\trec:   64.238426\tkl:   25.717237\n",
      "Epoch: 546 [20100/50000 (40%)]  \tLoss:   90.814392\trec:   64.393517\tkl:   26.420876\n",
      "Epoch: 546 [30100/50000 (60%)]  \tLoss:   88.823814\trec:   62.822536\tkl:   26.001276\n",
      "Epoch: 546 [40100/50000 (80%)]  \tLoss:   92.397850\trec:   65.399200\tkl:   26.998653\n",
      "====> Epoch: 546 Average train loss: 90.6264\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7491\n",
      "Epoch: 547 [  100/50000 ( 0%)]  \tLoss:   87.690392\trec:   61.896507\tkl:   25.793879\n",
      "Epoch: 547 [10100/50000 (20%)]  \tLoss:   94.253044\trec:   66.928131\tkl:   27.324917\n",
      "Epoch: 547 [20100/50000 (40%)]  \tLoss:   92.000076\trec:   64.964142\tkl:   27.035940\n",
      "Epoch: 547 [30100/50000 (60%)]  \tLoss:   92.328575\trec:   65.897110\tkl:   26.431459\n",
      "Epoch: 547 [40100/50000 (80%)]  \tLoss:   92.742538\trec:   66.011559\tkl:   26.730970\n",
      "====> Epoch: 547 Average train loss: 90.6694\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6262\n",
      "Epoch: 548 [  100/50000 ( 0%)]  \tLoss:   89.491196\trec:   63.294910\tkl:   26.196285\n",
      "Epoch: 548 [10100/50000 (20%)]  \tLoss:   93.367096\trec:   66.283859\tkl:   27.083237\n",
      "Epoch: 548 [20100/50000 (40%)]  \tLoss:   91.788521\trec:   64.471931\tkl:   27.316589\n",
      "Epoch: 548 [30100/50000 (60%)]  \tLoss:   91.366440\trec:   65.140274\tkl:   26.226170\n",
      "Epoch: 548 [40100/50000 (80%)]  \tLoss:   92.035233\trec:   65.305855\tkl:   26.729380\n",
      "====> Epoch: 548 Average train loss: 90.6165\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6922\n",
      "Epoch: 549 [  100/50000 ( 0%)]  \tLoss:   89.055870\trec:   62.375423\tkl:   26.680447\n",
      "Epoch: 549 [10100/50000 (20%)]  \tLoss:   92.583046\trec:   64.464005\tkl:   28.119045\n",
      "Epoch: 549 [20100/50000 (40%)]  \tLoss:   88.187622\trec:   61.612461\tkl:   26.575165\n",
      "Epoch: 549 [30100/50000 (60%)]  \tLoss:   89.285591\trec:   62.703720\tkl:   26.581875\n",
      "Epoch: 549 [40100/50000 (80%)]  \tLoss:   88.698143\trec:   62.013260\tkl:   26.684877\n",
      "====> Epoch: 549 Average train loss: 90.6204\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7737\n",
      "Epoch: 550 [  100/50000 ( 0%)]  \tLoss:   93.004395\trec:   65.347176\tkl:   27.657217\n",
      "Epoch: 550 [10100/50000 (20%)]  \tLoss:   85.798340\trec:   59.875038\tkl:   25.923306\n",
      "Epoch: 550 [20100/50000 (40%)]  \tLoss:   93.718796\trec:   66.452538\tkl:   27.266256\n",
      "Epoch: 550 [30100/50000 (60%)]  \tLoss:   89.680153\trec:   63.170044\tkl:   26.510111\n",
      "Epoch: 550 [40100/50000 (80%)]  \tLoss:   91.430222\trec:   63.979588\tkl:   27.450636\n",
      "====> Epoch: 550 Average train loss: 90.6365\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7700\n",
      "Epoch: 551 [  100/50000 ( 0%)]  \tLoss:   90.357422\trec:   63.253689\tkl:   27.103733\n",
      "Epoch: 551 [10100/50000 (20%)]  \tLoss:   87.733788\trec:   60.738510\tkl:   26.995283\n",
      "Epoch: 551 [20100/50000 (40%)]  \tLoss:   85.854523\trec:   60.237644\tkl:   25.616875\n",
      "Epoch: 551 [30100/50000 (60%)]  \tLoss:   92.208946\trec:   65.490768\tkl:   26.718168\n",
      "Epoch: 551 [40100/50000 (80%)]  \tLoss:   92.085075\trec:   65.417526\tkl:   26.667549\n",
      "====> Epoch: 551 Average train loss: 90.6318\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8314\n",
      "Epoch: 552 [  100/50000 ( 0%)]  \tLoss:   92.619057\trec:   65.460297\tkl:   27.158770\n",
      "Epoch: 552 [10100/50000 (20%)]  \tLoss:   87.984024\trec:   62.150082\tkl:   25.833944\n",
      "Epoch: 552 [20100/50000 (40%)]  \tLoss:   93.251694\trec:   66.811272\tkl:   26.440424\n",
      "Epoch: 552 [30100/50000 (60%)]  \tLoss:   90.909248\trec:   63.998718\tkl:   26.910526\n",
      "Epoch: 552 [40100/50000 (80%)]  \tLoss:   92.564606\trec:   66.390785\tkl:   26.173828\n",
      "====> Epoch: 552 Average train loss: 90.6422\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8120\n",
      "Epoch: 553 [  100/50000 ( 0%)]  \tLoss:   91.422615\trec:   64.848175\tkl:   26.574442\n",
      "Epoch: 553 [10100/50000 (20%)]  \tLoss:   92.797150\trec:   65.941277\tkl:   26.855869\n",
      "Epoch: 553 [20100/50000 (40%)]  \tLoss:   91.996666\trec:   65.635841\tkl:   26.360832\n",
      "Epoch: 553 [30100/50000 (60%)]  \tLoss:   90.304161\trec:   64.188286\tkl:   26.115871\n",
      "Epoch: 553 [40100/50000 (80%)]  \tLoss:   92.549614\trec:   65.418152\tkl:   27.131464\n",
      "====> Epoch: 553 Average train loss: 90.6290\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5883\n",
      "Epoch: 554 [  100/50000 ( 0%)]  \tLoss:   89.606071\trec:   62.529140\tkl:   27.076939\n",
      "Epoch: 554 [10100/50000 (20%)]  \tLoss:   93.672066\trec:   66.766449\tkl:   26.905619\n",
      "Epoch: 554 [20100/50000 (40%)]  \tLoss:   91.870056\trec:   64.901489\tkl:   26.968561\n",
      "Epoch: 554 [30100/50000 (60%)]  \tLoss:   90.518562\trec:   63.558987\tkl:   26.959574\n",
      "Epoch: 554 [40100/50000 (80%)]  \tLoss:   94.502159\trec:   66.996056\tkl:   27.506096\n",
      "====> Epoch: 554 Average train loss: 90.6366\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8053\n",
      "Epoch: 555 [  100/50000 ( 0%)]  \tLoss:   84.653435\trec:   59.071220\tkl:   25.582211\n",
      "Epoch: 555 [10100/50000 (20%)]  \tLoss:   89.577850\trec:   62.582127\tkl:   26.995724\n",
      "Epoch: 555 [20100/50000 (40%)]  \tLoss:   91.505577\trec:   64.271729\tkl:   27.233843\n",
      "Epoch: 555 [30100/50000 (60%)]  \tLoss:   93.297798\trec:   65.676826\tkl:   27.620968\n",
      "Epoch: 555 [40100/50000 (80%)]  \tLoss:   94.284355\trec:   66.991333\tkl:   27.293022\n",
      "====> Epoch: 555 Average train loss: 90.6120\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6640\n",
      "Epoch: 556 [  100/50000 ( 0%)]  \tLoss:   88.340523\trec:   61.963604\tkl:   26.376921\n",
      "Epoch: 556 [10100/50000 (20%)]  \tLoss:   84.317566\trec:   58.075127\tkl:   26.242439\n",
      "Epoch: 556 [20100/50000 (40%)]  \tLoss:   90.079247\trec:   63.658333\tkl:   26.420916\n",
      "Epoch: 556 [30100/50000 (60%)]  \tLoss:   89.189873\trec:   62.469719\tkl:   26.720154\n",
      "Epoch: 556 [40100/50000 (80%)]  \tLoss:   90.606590\trec:   63.417664\tkl:   27.188927\n",
      "====> Epoch: 556 Average train loss: 90.6044\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6218\n",
      "Epoch: 557 [  100/50000 ( 0%)]  \tLoss:   92.014542\trec:   64.003319\tkl:   28.011217\n",
      "Epoch: 557 [10100/50000 (20%)]  \tLoss:   90.761620\trec:   64.702667\tkl:   26.058950\n",
      "Epoch: 557 [20100/50000 (40%)]  \tLoss:   89.699936\trec:   62.478100\tkl:   27.221840\n",
      "Epoch: 557 [30100/50000 (60%)]  \tLoss:   91.303337\trec:   64.080742\tkl:   27.222597\n",
      "Epoch: 557 [40100/50000 (80%)]  \tLoss:   89.026756\trec:   63.007019\tkl:   26.019741\n",
      "====> Epoch: 557 Average train loss: 90.6235\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7093\n",
      "Epoch: 558 [  100/50000 ( 0%)]  \tLoss:   92.737419\trec:   65.957642\tkl:   26.779785\n",
      "Epoch: 558 [10100/50000 (20%)]  \tLoss:   91.657906\trec:   64.474373\tkl:   27.183537\n",
      "Epoch: 558 [20100/50000 (40%)]  \tLoss:   86.447342\trec:   59.709911\tkl:   26.737436\n",
      "Epoch: 558 [30100/50000 (60%)]  \tLoss:   95.444511\trec:   67.888374\tkl:   27.556137\n",
      "Epoch: 558 [40100/50000 (80%)]  \tLoss:   89.392654\trec:   63.019863\tkl:   26.372793\n",
      "====> Epoch: 558 Average train loss: 90.6007\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6330\n",
      "Epoch: 559 [  100/50000 ( 0%)]  \tLoss:   87.816574\trec:   61.503975\tkl:   26.312597\n",
      "Epoch: 559 [10100/50000 (20%)]  \tLoss:   84.376961\trec:   58.233860\tkl:   26.143097\n",
      "Epoch: 559 [20100/50000 (40%)]  \tLoss:   93.066193\trec:   65.713791\tkl:   27.352394\n",
      "Epoch: 559 [30100/50000 (60%)]  \tLoss:   86.779480\trec:   60.517879\tkl:   26.261600\n",
      "Epoch: 559 [40100/50000 (80%)]  \tLoss:   91.472237\trec:   64.684639\tkl:   26.787594\n",
      "====> Epoch: 559 Average train loss: 90.6124\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7562\n",
      "Epoch: 560 [  100/50000 ( 0%)]  \tLoss:   89.488213\trec:   62.228275\tkl:   27.259939\n",
      "Epoch: 560 [10100/50000 (20%)]  \tLoss:   87.942566\trec:   61.316540\tkl:   26.626028\n",
      "Epoch: 560 [20100/50000 (40%)]  \tLoss:   90.334587\trec:   63.252724\tkl:   27.081865\n",
      "Epoch: 560 [30100/50000 (60%)]  \tLoss:   91.133644\trec:   63.736675\tkl:   27.396969\n",
      "Epoch: 560 [40100/50000 (80%)]  \tLoss:   91.147110\trec:   63.792793\tkl:   27.354321\n",
      "====> Epoch: 560 Average train loss: 90.6136\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6497\n",
      "Epoch: 561 [  100/50000 ( 0%)]  \tLoss:   89.945496\trec:   62.715378\tkl:   27.230114\n",
      "Epoch: 561 [10100/50000 (20%)]  \tLoss:   90.011963\trec:   63.396042\tkl:   26.615917\n",
      "Epoch: 561 [20100/50000 (40%)]  \tLoss:   89.976051\trec:   63.400105\tkl:   26.575949\n",
      "Epoch: 561 [30100/50000 (60%)]  \tLoss:   89.287537\trec:   62.255749\tkl:   27.031792\n",
      "Epoch: 561 [40100/50000 (80%)]  \tLoss:   88.309082\trec:   61.972816\tkl:   26.336260\n",
      "====> Epoch: 561 Average train loss: 90.5797\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5843\n",
      "Epoch: 562 [  100/50000 ( 0%)]  \tLoss:   88.400352\trec:   62.146084\tkl:   26.254265\n",
      "Epoch: 562 [10100/50000 (20%)]  \tLoss:   93.650093\trec:   66.897926\tkl:   26.752171\n",
      "Epoch: 562 [20100/50000 (40%)]  \tLoss:   89.656197\trec:   63.554401\tkl:   26.101793\n",
      "Epoch: 562 [30100/50000 (60%)]  \tLoss:   95.098122\trec:   67.375343\tkl:   27.722782\n",
      "Epoch: 562 [40100/50000 (80%)]  \tLoss:   91.081062\trec:   64.165894\tkl:   26.915169\n",
      "====> Epoch: 562 Average train loss: 90.6049\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6901\n",
      "Epoch: 563 [  100/50000 ( 0%)]  \tLoss:   87.167664\trec:   60.248055\tkl:   26.919609\n",
      "Epoch: 563 [10100/50000 (20%)]  \tLoss:   91.601959\trec:   64.576111\tkl:   27.025846\n",
      "Epoch: 563 [20100/50000 (40%)]  \tLoss:   94.447411\trec:   66.358238\tkl:   28.089174\n",
      "Epoch: 563 [30100/50000 (60%)]  \tLoss:   89.280479\trec:   62.977821\tkl:   26.302656\n",
      "Epoch: 563 [40100/50000 (80%)]  \tLoss:   88.467476\trec:   61.420013\tkl:   27.047462\n",
      "====> Epoch: 563 Average train loss: 90.5679\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6987\n",
      "Epoch: 564 [  100/50000 ( 0%)]  \tLoss:   89.996239\trec:   63.718994\tkl:   26.277245\n",
      "Epoch: 564 [10100/50000 (20%)]  \tLoss:   88.960464\trec:   63.175957\tkl:   25.784506\n",
      "Epoch: 564 [20100/50000 (40%)]  \tLoss:   92.951111\trec:   65.457352\tkl:   27.493765\n",
      "Epoch: 564 [30100/50000 (60%)]  \tLoss:   90.001427\trec:   63.028515\tkl:   26.972912\n",
      "Epoch: 564 [40100/50000 (80%)]  \tLoss:   92.703560\trec:   65.461861\tkl:   27.241699\n",
      "====> Epoch: 564 Average train loss: 90.5846\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5648\n",
      "Epoch: 565 [  100/50000 ( 0%)]  \tLoss:   89.156441\trec:   62.858799\tkl:   26.297651\n",
      "Epoch: 565 [10100/50000 (20%)]  \tLoss:   90.284302\trec:   63.511951\tkl:   26.772352\n",
      "Epoch: 565 [20100/50000 (40%)]  \tLoss:   88.679649\trec:   63.228516\tkl:   25.451128\n",
      "Epoch: 565 [30100/50000 (60%)]  \tLoss:   95.247002\trec:   67.949478\tkl:   27.297516\n",
      "Epoch: 565 [40100/50000 (80%)]  \tLoss:   91.493019\trec:   63.300869\tkl:   28.192148\n",
      "====> Epoch: 565 Average train loss: 90.5783\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7171\n",
      "Epoch: 566 [  100/50000 ( 0%)]  \tLoss:   86.564903\trec:   60.728916\tkl:   25.835989\n",
      "Epoch: 566 [10100/50000 (20%)]  \tLoss:   90.181740\trec:   63.785244\tkl:   26.396496\n",
      "Epoch: 566 [20100/50000 (40%)]  \tLoss:   87.920563\trec:   62.205002\tkl:   25.715567\n",
      "Epoch: 566 [30100/50000 (60%)]  \tLoss:   93.595116\trec:   65.846573\tkl:   27.748539\n",
      "Epoch: 566 [40100/50000 (80%)]  \tLoss:   89.146431\trec:   62.909584\tkl:   26.236847\n",
      "====> Epoch: 566 Average train loss: 90.5406\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7479\n",
      "Epoch: 567 [  100/50000 ( 0%)]  \tLoss:   89.333130\trec:   63.369732\tkl:   25.963402\n",
      "Epoch: 567 [10100/50000 (20%)]  \tLoss:   89.056763\trec:   62.528130\tkl:   26.528637\n",
      "Epoch: 567 [20100/50000 (40%)]  \tLoss:   89.946236\trec:   63.383327\tkl:   26.562912\n",
      "Epoch: 567 [30100/50000 (60%)]  \tLoss:   89.817245\trec:   62.906063\tkl:   26.911186\n",
      "Epoch: 567 [40100/50000 (80%)]  \tLoss:   91.560234\trec:   64.824867\tkl:   26.735369\n",
      "====> Epoch: 567 Average train loss: 90.5628\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.8198\n",
      "Epoch: 568 [  100/50000 ( 0%)]  \tLoss:   88.547058\trec:   61.697655\tkl:   26.849401\n",
      "Epoch: 568 [10100/50000 (20%)]  \tLoss:   87.161057\trec:   60.710571\tkl:   26.450487\n",
      "Epoch: 568 [20100/50000 (40%)]  \tLoss:   88.505478\trec:   62.308552\tkl:   26.196920\n",
      "Epoch: 568 [30100/50000 (60%)]  \tLoss:   92.256416\trec:   65.803658\tkl:   26.452761\n",
      "Epoch: 568 [40100/50000 (80%)]  \tLoss:   88.034439\trec:   61.753071\tkl:   26.281368\n",
      "====> Epoch: 568 Average train loss: 90.5553\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7174\n",
      "Epoch: 569 [  100/50000 ( 0%)]  \tLoss:   89.384941\trec:   63.374599\tkl:   26.010342\n",
      "Epoch: 569 [10100/50000 (20%)]  \tLoss:   90.597145\trec:   64.312866\tkl:   26.284281\n",
      "Epoch: 569 [20100/50000 (40%)]  \tLoss:   90.003380\trec:   63.040932\tkl:   26.962448\n",
      "Epoch: 569 [30100/50000 (60%)]  \tLoss:   90.081482\trec:   62.927746\tkl:   27.153734\n",
      "Epoch: 569 [40100/50000 (80%)]  \tLoss:   94.004601\trec:   66.366524\tkl:   27.638071\n",
      "====> Epoch: 569 Average train loss: 90.5683\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7091\n",
      "Epoch: 570 [  100/50000 ( 0%)]  \tLoss:   92.617867\trec:   64.913971\tkl:   27.703894\n",
      "Epoch: 570 [10100/50000 (20%)]  \tLoss:   94.141144\trec:   66.700539\tkl:   27.440598\n",
      "Epoch: 570 [20100/50000 (40%)]  \tLoss:   92.299171\trec:   65.462082\tkl:   26.837090\n",
      "Epoch: 570 [30100/50000 (60%)]  \tLoss:   90.202477\trec:   63.939976\tkl:   26.262505\n",
      "Epoch: 570 [40100/50000 (80%)]  \tLoss:   94.818275\trec:   67.303917\tkl:   27.514370\n",
      "====> Epoch: 570 Average train loss: 90.5597\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6375\n",
      "Epoch: 571 [  100/50000 ( 0%)]  \tLoss:   89.731522\trec:   62.952908\tkl:   26.778608\n",
      "Epoch: 571 [10100/50000 (20%)]  \tLoss:   91.790054\trec:   65.164116\tkl:   26.625933\n",
      "Epoch: 571 [20100/50000 (40%)]  \tLoss:   92.546753\trec:   66.014404\tkl:   26.532351\n",
      "Epoch: 571 [30100/50000 (60%)]  \tLoss:   88.456444\trec:   61.731934\tkl:   26.724510\n",
      "Epoch: 571 [40100/50000 (80%)]  \tLoss:   91.594009\trec:   64.298218\tkl:   27.295794\n",
      "====> Epoch: 571 Average train loss: 90.5807\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5678\n",
      "Epoch: 572 [  100/50000 ( 0%)]  \tLoss:   91.901413\trec:   65.116737\tkl:   26.784679\n",
      "Epoch: 572 [10100/50000 (20%)]  \tLoss:   89.915611\trec:   63.025936\tkl:   26.889677\n",
      "Epoch: 572 [20100/50000 (40%)]  \tLoss:   89.699081\trec:   63.558968\tkl:   26.140114\n",
      "Epoch: 572 [30100/50000 (60%)]  \tLoss:   89.121796\trec:   62.425907\tkl:   26.695885\n",
      "Epoch: 572 [40100/50000 (80%)]  \tLoss:   87.910294\trec:   60.587166\tkl:   27.323122\n",
      "====> Epoch: 572 Average train loss: 90.5709\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6578\n",
      "Epoch: 573 [  100/50000 ( 0%)]  \tLoss:   91.673500\trec:   64.180893\tkl:   27.492611\n",
      "Epoch: 573 [10100/50000 (20%)]  \tLoss:   93.522583\trec:   65.565155\tkl:   27.957432\n",
      "Epoch: 573 [20100/50000 (40%)]  \tLoss:   91.313599\trec:   63.740673\tkl:   27.572927\n",
      "Epoch: 573 [30100/50000 (60%)]  \tLoss:   96.243530\trec:   68.851295\tkl:   27.392239\n",
      "Epoch: 573 [40100/50000 (80%)]  \tLoss:   90.848473\trec:   63.705643\tkl:   27.142830\n",
      "====> Epoch: 573 Average train loss: 90.5264\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6709\n",
      "Epoch: 574 [  100/50000 ( 0%)]  \tLoss:   90.960587\trec:   64.787514\tkl:   26.173075\n",
      "Epoch: 574 [10100/50000 (20%)]  \tLoss:   85.760330\trec:   60.218338\tkl:   25.541994\n",
      "Epoch: 574 [20100/50000 (40%)]  \tLoss:   91.736183\trec:   64.778114\tkl:   26.958063\n",
      "Epoch: 574 [30100/50000 (60%)]  \tLoss:   91.853775\trec:   65.307220\tkl:   26.546555\n",
      "Epoch: 574 [40100/50000 (80%)]  \tLoss:   89.914665\trec:   62.920486\tkl:   26.994177\n",
      "====> Epoch: 574 Average train loss: 90.5360\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7063\n",
      "Epoch: 575 [  100/50000 ( 0%)]  \tLoss:   89.616035\trec:   63.123631\tkl:   26.492405\n",
      "Epoch: 575 [10100/50000 (20%)]  \tLoss:   92.880249\trec:   66.009277\tkl:   26.870972\n",
      "Epoch: 575 [20100/50000 (40%)]  \tLoss:   91.098686\trec:   64.582947\tkl:   26.515741\n",
      "Epoch: 575 [30100/50000 (60%)]  \tLoss:   93.238007\trec:   66.389023\tkl:   26.848986\n",
      "Epoch: 575 [40100/50000 (80%)]  \tLoss:   92.012917\trec:   65.244659\tkl:   26.768261\n",
      "====> Epoch: 575 Average train loss: 90.5605\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.9208\n",
      "Epoch: 576 [  100/50000 ( 0%)]  \tLoss:   88.592018\trec:   61.506561\tkl:   27.085459\n",
      "Epoch: 576 [10100/50000 (20%)]  \tLoss:   91.415039\trec:   64.263374\tkl:   27.151665\n",
      "Epoch: 576 [20100/50000 (40%)]  \tLoss:   92.160797\trec:   65.083252\tkl:   27.077538\n",
      "Epoch: 576 [30100/50000 (60%)]  \tLoss:   89.236977\trec:   62.547398\tkl:   26.689585\n",
      "Epoch: 576 [40100/50000 (80%)]  \tLoss:   93.084259\trec:   65.605080\tkl:   27.479170\n",
      "====> Epoch: 576 Average train loss: 90.5300\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5727\n",
      "Epoch: 577 [  100/50000 ( 0%)]  \tLoss:   85.659630\trec:   59.617176\tkl:   26.042446\n",
      "Epoch: 577 [10100/50000 (20%)]  \tLoss:   92.006134\trec:   64.971222\tkl:   27.034912\n",
      "Epoch: 577 [20100/50000 (40%)]  \tLoss:   84.877510\trec:   59.284451\tkl:   25.593054\n",
      "Epoch: 577 [30100/50000 (60%)]  \tLoss:   92.773163\trec:   65.340355\tkl:   27.432812\n",
      "Epoch: 577 [40100/50000 (80%)]  \tLoss:   91.238335\trec:   64.531227\tkl:   26.707111\n",
      "====> Epoch: 577 Average train loss: 90.5080\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6764\n",
      "Epoch: 578 [  100/50000 ( 0%)]  \tLoss:   88.396278\trec:   62.359200\tkl:   26.037079\n",
      "Epoch: 578 [10100/50000 (20%)]  \tLoss:   95.732208\trec:   66.675056\tkl:   29.057152\n",
      "Epoch: 578 [20100/50000 (40%)]  \tLoss:   92.027107\trec:   64.532028\tkl:   27.495075\n",
      "Epoch: 578 [30100/50000 (60%)]  \tLoss:   88.531288\trec:   62.034199\tkl:   26.497093\n",
      "Epoch: 578 [40100/50000 (80%)]  \tLoss:   92.531166\trec:   65.224213\tkl:   27.306959\n",
      "====> Epoch: 578 Average train loss: 90.5106\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6148\n",
      "Epoch: 579 [  100/50000 ( 0%)]  \tLoss:   90.539528\trec:   64.211723\tkl:   26.327803\n",
      "Epoch: 579 [10100/50000 (20%)]  \tLoss:   89.921661\trec:   63.225079\tkl:   26.696579\n",
      "Epoch: 579 [20100/50000 (40%)]  \tLoss:   90.652946\trec:   63.414051\tkl:   27.238901\n",
      "Epoch: 579 [30100/50000 (60%)]  \tLoss:   93.104591\trec:   66.512466\tkl:   26.592125\n",
      "Epoch: 579 [40100/50000 (80%)]  \tLoss:   92.918037\trec:   65.653954\tkl:   27.264078\n",
      "====> Epoch: 579 Average train loss: 90.5288\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7277\n",
      "Epoch: 580 [  100/50000 ( 0%)]  \tLoss:   91.637146\trec:   63.945332\tkl:   27.691813\n",
      "Epoch: 580 [10100/50000 (20%)]  \tLoss:   95.236580\trec:   67.588051\tkl:   27.648533\n",
      "Epoch: 580 [20100/50000 (40%)]  \tLoss:   94.796547\trec:   67.167526\tkl:   27.629023\n",
      "Epoch: 580 [30100/50000 (60%)]  \tLoss:   91.404785\trec:   64.288536\tkl:   27.116255\n",
      "Epoch: 580 [40100/50000 (80%)]  \tLoss:   88.450783\trec:   61.439522\tkl:   27.011253\n",
      "====> Epoch: 580 Average train loss: 90.5418\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7147\n",
      "Epoch: 581 [  100/50000 ( 0%)]  \tLoss:   88.056580\trec:   61.587357\tkl:   26.469225\n",
      "Epoch: 581 [10100/50000 (20%)]  \tLoss:   90.648727\trec:   63.546021\tkl:   27.102715\n",
      "Epoch: 581 [20100/50000 (40%)]  \tLoss:   90.528687\trec:   63.294228\tkl:   27.234457\n",
      "Epoch: 581 [30100/50000 (60%)]  \tLoss:   88.382423\trec:   62.379883\tkl:   26.002537\n",
      "Epoch: 581 [40100/50000 (80%)]  \tLoss:   88.456383\trec:   61.814270\tkl:   26.642117\n",
      "====> Epoch: 581 Average train loss: 90.5172\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6687\n",
      "Epoch: 582 [  100/50000 ( 0%)]  \tLoss:   88.630600\trec:   61.508778\tkl:   27.121822\n",
      "Epoch: 582 [10100/50000 (20%)]  \tLoss:   91.807892\trec:   64.673447\tkl:   27.134438\n",
      "Epoch: 582 [20100/50000 (40%)]  \tLoss:   91.639236\trec:   64.408943\tkl:   27.230288\n",
      "Epoch: 582 [30100/50000 (60%)]  \tLoss:   92.608765\trec:   64.707840\tkl:   27.900925\n",
      "Epoch: 582 [40100/50000 (80%)]  \tLoss:   87.401207\trec:   61.459999\tkl:   25.941214\n",
      "====> Epoch: 582 Average train loss: 90.5083\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6611\n",
      "Epoch: 583 [  100/50000 ( 0%)]  \tLoss:   96.546753\trec:   69.074760\tkl:   27.471994\n",
      "Epoch: 583 [10100/50000 (20%)]  \tLoss:   87.431061\trec:   61.576366\tkl:   25.854696\n",
      "Epoch: 583 [20100/50000 (40%)]  \tLoss:   89.635239\trec:   63.674969\tkl:   25.960270\n",
      "Epoch: 583 [30100/50000 (60%)]  \tLoss:   90.210838\trec:   63.697430\tkl:   26.513407\n",
      "Epoch: 583 [40100/50000 (80%)]  \tLoss:   92.764923\trec:   65.502579\tkl:   27.262341\n",
      "====> Epoch: 583 Average train loss: 90.5011\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7073\n",
      "Epoch: 584 [  100/50000 ( 0%)]  \tLoss:   88.057510\trec:   62.496201\tkl:   25.561306\n",
      "Epoch: 584 [10100/50000 (20%)]  \tLoss:   87.983818\trec:   61.548252\tkl:   26.435564\n",
      "Epoch: 584 [20100/50000 (40%)]  \tLoss:   90.420914\trec:   64.008034\tkl:   26.412876\n",
      "Epoch: 584 [30100/50000 (60%)]  \tLoss:   86.731255\trec:   60.643177\tkl:   26.088078\n",
      "Epoch: 584 [40100/50000 (80%)]  \tLoss:   91.417702\trec:   64.348915\tkl:   27.068789\n",
      "====> Epoch: 584 Average train loss: 90.4952\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6911\n",
      "Epoch: 585 [  100/50000 ( 0%)]  \tLoss:   91.140038\trec:   64.564430\tkl:   26.575609\n",
      "Epoch: 585 [10100/50000 (20%)]  \tLoss:   91.779541\trec:   64.326706\tkl:   27.452839\n",
      "Epoch: 585 [20100/50000 (40%)]  \tLoss:   90.489365\trec:   63.835606\tkl:   26.653757\n",
      "Epoch: 585 [30100/50000 (60%)]  \tLoss:   91.703308\trec:   65.098854\tkl:   26.604452\n",
      "Epoch: 585 [40100/50000 (80%)]  \tLoss:   87.471794\trec:   61.274746\tkl:   26.197056\n",
      "====> Epoch: 585 Average train loss: 90.5051\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5226\n",
      "Epoch: 586 [  100/50000 ( 0%)]  \tLoss:   89.611610\trec:   62.969505\tkl:   26.642101\n",
      "Epoch: 586 [10100/50000 (20%)]  \tLoss:   85.848198\trec:   60.923901\tkl:   24.924301\n",
      "Epoch: 586 [20100/50000 (40%)]  \tLoss:   90.622780\trec:   63.923611\tkl:   26.699167\n",
      "Epoch: 586 [30100/50000 (60%)]  \tLoss:   89.782326\trec:   62.756405\tkl:   27.025919\n",
      "Epoch: 586 [40100/50000 (80%)]  \tLoss:   93.265564\trec:   65.864914\tkl:   27.400656\n",
      "====> Epoch: 586 Average train loss: 90.5065\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6177\n",
      "Epoch: 587 [  100/50000 ( 0%)]  \tLoss:   87.074829\trec:   60.379433\tkl:   26.695396\n",
      "Epoch: 587 [10100/50000 (20%)]  \tLoss:   89.354233\trec:   63.162090\tkl:   26.192148\n",
      "Epoch: 587 [20100/50000 (40%)]  \tLoss:   90.232811\trec:   63.024605\tkl:   27.208208\n",
      "Epoch: 587 [30100/50000 (60%)]  \tLoss:   91.096687\trec:   63.852753\tkl:   27.243935\n",
      "Epoch: 587 [40100/50000 (80%)]  \tLoss:   91.884521\trec:   64.801712\tkl:   27.082809\n",
      "====> Epoch: 587 Average train loss: 90.4841\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6231\n",
      "Epoch: 588 [  100/50000 ( 0%)]  \tLoss:   92.890053\trec:   65.381355\tkl:   27.508698\n",
      "Epoch: 588 [10100/50000 (20%)]  \tLoss:   91.378456\trec:   63.210918\tkl:   28.167536\n",
      "Epoch: 588 [20100/50000 (40%)]  \tLoss:   89.600830\trec:   63.240612\tkl:   26.360214\n",
      "Epoch: 588 [30100/50000 (60%)]  \tLoss:   94.848770\trec:   66.559258\tkl:   28.289511\n",
      "Epoch: 588 [40100/50000 (80%)]  \tLoss:   90.030800\trec:   62.828430\tkl:   27.202366\n",
      "====> Epoch: 588 Average train loss: 90.4752\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5475\n",
      "Epoch: 589 [  100/50000 ( 0%)]  \tLoss:   95.578285\trec:   67.182175\tkl:   28.396111\n",
      "Epoch: 589 [10100/50000 (20%)]  \tLoss:   85.229706\trec:   59.441406\tkl:   25.788300\n",
      "Epoch: 589 [20100/50000 (40%)]  \tLoss:   92.681007\trec:   65.638016\tkl:   27.042990\n",
      "Epoch: 589 [30100/50000 (60%)]  \tLoss:   89.595268\trec:   62.272221\tkl:   27.323055\n",
      "Epoch: 589 [40100/50000 (80%)]  \tLoss:   89.233917\trec:   63.418373\tkl:   25.815542\n",
      "====> Epoch: 589 Average train loss: 90.4646\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5513\n",
      "Epoch: 590 [  100/50000 ( 0%)]  \tLoss:   89.378845\trec:   63.344719\tkl:   26.034126\n",
      "Epoch: 590 [10100/50000 (20%)]  \tLoss:   90.380836\trec:   63.511318\tkl:   26.869518\n",
      "Epoch: 590 [20100/50000 (40%)]  \tLoss:   88.674042\trec:   62.290100\tkl:   26.383942\n",
      "Epoch: 590 [30100/50000 (60%)]  \tLoss:   91.851250\trec:   64.714043\tkl:   27.137197\n",
      "Epoch: 590 [40100/50000 (80%)]  \tLoss:   88.980553\trec:   62.623669\tkl:   26.356884\n",
      "====> Epoch: 590 Average train loss: 90.4816\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5688\n",
      "Epoch: 591 [  100/50000 ( 0%)]  \tLoss:   98.277832\trec:   70.055840\tkl:   28.221996\n",
      "Epoch: 591 [10100/50000 (20%)]  \tLoss:   94.219589\trec:   66.907974\tkl:   27.311605\n",
      "Epoch: 591 [20100/50000 (40%)]  \tLoss:   88.773064\trec:   62.140285\tkl:   26.632782\n",
      "Epoch: 591 [30100/50000 (60%)]  \tLoss:   92.967583\trec:   65.213814\tkl:   27.753775\n",
      "Epoch: 591 [40100/50000 (80%)]  \tLoss:   87.638123\trec:   61.290741\tkl:   26.347385\n",
      "====> Epoch: 591 Average train loss: 90.4704\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5492\n",
      "Epoch: 592 [  100/50000 ( 0%)]  \tLoss:   87.564705\trec:   61.487156\tkl:   26.077553\n",
      "Epoch: 592 [10100/50000 (20%)]  \tLoss:   91.742638\trec:   64.822594\tkl:   26.920040\n",
      "Epoch: 592 [20100/50000 (40%)]  \tLoss:   92.564011\trec:   65.512657\tkl:   27.051353\n",
      "Epoch: 592 [30100/50000 (60%)]  \tLoss:   90.906166\trec:   64.323280\tkl:   26.582890\n",
      "Epoch: 592 [40100/50000 (80%)]  \tLoss:   90.949020\trec:   64.378555\tkl:   26.570465\n",
      "====> Epoch: 592 Average train loss: 90.4695\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6372\n",
      "Epoch: 593 [  100/50000 ( 0%)]  \tLoss:   89.965332\trec:   62.884380\tkl:   27.080952\n",
      "Epoch: 593 [10100/50000 (20%)]  \tLoss:   88.541801\trec:   62.347610\tkl:   26.194191\n",
      "Epoch: 593 [20100/50000 (40%)]  \tLoss:   89.241226\trec:   62.908543\tkl:   26.332682\n",
      "Epoch: 593 [30100/50000 (60%)]  \tLoss:   91.055000\trec:   63.698154\tkl:   27.356850\n",
      "Epoch: 593 [40100/50000 (80%)]  \tLoss:   89.524605\trec:   62.550526\tkl:   26.974081\n",
      "====> Epoch: 593 Average train loss: 90.4734\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6213\n",
      "Epoch: 594 [  100/50000 ( 0%)]  \tLoss:   84.344704\trec:   58.881531\tkl:   25.463175\n",
      "Epoch: 594 [10100/50000 (20%)]  \tLoss:   93.847862\trec:   66.211174\tkl:   27.636688\n",
      "Epoch: 594 [20100/50000 (40%)]  \tLoss:   86.281303\trec:   60.464287\tkl:   25.817017\n",
      "Epoch: 594 [30100/50000 (60%)]  \tLoss:   92.701294\trec:   65.943314\tkl:   26.757980\n",
      "Epoch: 594 [40100/50000 (80%)]  \tLoss:   95.732399\trec:   68.731873\tkl:   27.000526\n",
      "====> Epoch: 594 Average train loss: 90.4670\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6484\n",
      "Epoch: 595 [  100/50000 ( 0%)]  \tLoss:   88.943260\trec:   62.272694\tkl:   26.670568\n",
      "Epoch: 595 [10100/50000 (20%)]  \tLoss:   92.116310\trec:   65.631340\tkl:   26.484970\n",
      "Epoch: 595 [20100/50000 (40%)]  \tLoss:   89.240639\trec:   63.410496\tkl:   25.830149\n",
      "Epoch: 595 [30100/50000 (60%)]  \tLoss:   89.769272\trec:   63.417515\tkl:   26.351757\n",
      "Epoch: 595 [40100/50000 (80%)]  \tLoss:   90.738228\trec:   64.297485\tkl:   26.440744\n",
      "====> Epoch: 595 Average train loss: 90.4483\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6184\n",
      "Epoch: 596 [  100/50000 ( 0%)]  \tLoss:   88.768700\trec:   62.480278\tkl:   26.288425\n",
      "Epoch: 596 [10100/50000 (20%)]  \tLoss:   87.724792\trec:   60.788303\tkl:   26.936491\n",
      "Epoch: 596 [20100/50000 (40%)]  \tLoss:   96.175308\trec:   68.082428\tkl:   28.092875\n",
      "Epoch: 596 [30100/50000 (60%)]  \tLoss:   92.060455\trec:   65.200020\tkl:   26.860435\n",
      "Epoch: 596 [40100/50000 (80%)]  \tLoss:   90.274391\trec:   63.324360\tkl:   26.950039\n",
      "====> Epoch: 596 Average train loss: 90.4430\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6030\n",
      "Epoch: 597 [  100/50000 ( 0%)]  \tLoss:   87.387733\trec:   61.275574\tkl:   26.112156\n",
      "Epoch: 597 [10100/50000 (20%)]  \tLoss:   92.163025\trec:   64.211060\tkl:   27.951960\n",
      "Epoch: 597 [20100/50000 (40%)]  \tLoss:   92.913864\trec:   66.459511\tkl:   26.454355\n",
      "Epoch: 597 [30100/50000 (60%)]  \tLoss:   89.079445\trec:   62.463711\tkl:   26.615730\n",
      "Epoch: 597 [40100/50000 (80%)]  \tLoss:   90.650887\trec:   64.028633\tkl:   26.622257\n",
      "====> Epoch: 597 Average train loss: 90.4366\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5884\n",
      "Epoch: 598 [  100/50000 ( 0%)]  \tLoss:   90.840332\trec:   63.488510\tkl:   27.351820\n",
      "Epoch: 598 [10100/50000 (20%)]  \tLoss:   95.636017\trec:   67.382866\tkl:   28.253149\n",
      "Epoch: 598 [20100/50000 (40%)]  \tLoss:   89.435303\trec:   62.766132\tkl:   26.669172\n",
      "Epoch: 598 [30100/50000 (60%)]  \tLoss:   88.743729\trec:   61.505779\tkl:   27.237944\n",
      "Epoch: 598 [40100/50000 (80%)]  \tLoss:   98.130653\trec:   69.997520\tkl:   28.133127\n",
      "====> Epoch: 598 Average train loss: 90.4469\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5964\n",
      "Epoch: 599 [  100/50000 ( 0%)]  \tLoss:   94.203026\trec:   66.530479\tkl:   27.672541\n",
      "Epoch: 599 [10100/50000 (20%)]  \tLoss:   87.585258\trec:   61.549824\tkl:   26.035439\n",
      "Epoch: 599 [20100/50000 (40%)]  \tLoss:   90.148056\trec:   63.156689\tkl:   26.991367\n",
      "Epoch: 599 [30100/50000 (60%)]  \tLoss:   93.734825\trec:   65.516930\tkl:   28.217890\n",
      "Epoch: 599 [40100/50000 (80%)]  \tLoss:   90.715927\trec:   63.188427\tkl:   27.527502\n",
      "====> Epoch: 599 Average train loss: 90.4460\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5820\n",
      "Epoch: 600 [  100/50000 ( 0%)]  \tLoss:   85.805542\trec:   60.236511\tkl:   25.569033\n",
      "Epoch: 600 [10100/50000 (20%)]  \tLoss:   87.700592\trec:   60.907959\tkl:   26.792633\n",
      "Epoch: 600 [20100/50000 (40%)]  \tLoss:   91.597771\trec:   65.224762\tkl:   26.373007\n",
      "Epoch: 600 [30100/50000 (60%)]  \tLoss:   91.746811\trec:   65.066727\tkl:   26.680088\n",
      "Epoch: 600 [40100/50000 (80%)]  \tLoss:   91.610878\trec:   63.735241\tkl:   27.875637\n",
      "====> Epoch: 600 Average train loss: 90.4420\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5940\n",
      "Epoch: 601 [  100/50000 ( 0%)]  \tLoss:   92.346870\trec:   65.261681\tkl:   27.085196\n",
      "Epoch: 601 [10100/50000 (20%)]  \tLoss:   89.485054\trec:   62.324085\tkl:   27.160967\n",
      "Epoch: 601 [20100/50000 (40%)]  \tLoss:   91.697495\trec:   64.917458\tkl:   26.780033\n",
      "Epoch: 601 [30100/50000 (60%)]  \tLoss:   95.693512\trec:   66.996040\tkl:   28.697466\n",
      "Epoch: 601 [40100/50000 (80%)]  \tLoss:   94.995224\trec:   67.613213\tkl:   27.382013\n",
      "====> Epoch: 601 Average train loss: 90.4167\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6878\n",
      "Epoch: 602 [  100/50000 ( 0%)]  \tLoss:   88.558670\trec:   62.134140\tkl:   26.424526\n",
      "Epoch: 602 [10100/50000 (20%)]  \tLoss:   91.710007\trec:   64.516129\tkl:   27.193884\n",
      "Epoch: 602 [20100/50000 (40%)]  \tLoss:   90.582321\trec:   63.730671\tkl:   26.851646\n",
      "Epoch: 602 [30100/50000 (60%)]  \tLoss:   86.661476\trec:   61.156921\tkl:   25.504553\n",
      "Epoch: 602 [40100/50000 (80%)]  \tLoss:   91.559975\trec:   64.346588\tkl:   27.213398\n",
      "====> Epoch: 602 Average train loss: 90.4286\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6260\n",
      "Epoch: 603 [  100/50000 ( 0%)]  \tLoss:   90.318298\trec:   63.813759\tkl:   26.504538\n",
      "Epoch: 603 [10100/50000 (20%)]  \tLoss:   88.168266\trec:   62.224033\tkl:   25.944237\n",
      "Epoch: 603 [20100/50000 (40%)]  \tLoss:   86.918335\trec:   61.223534\tkl:   25.694799\n",
      "Epoch: 603 [30100/50000 (60%)]  \tLoss:   92.786415\trec:   64.892334\tkl:   27.894081\n",
      "Epoch: 603 [40100/50000 (80%)]  \tLoss:   87.301239\trec:   61.621830\tkl:   25.679409\n",
      "====> Epoch: 603 Average train loss: 90.4114\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5761\n",
      "Epoch: 604 [  100/50000 ( 0%)]  \tLoss:   86.863083\trec:   60.727409\tkl:   26.135672\n",
      "Epoch: 604 [10100/50000 (20%)]  \tLoss:   93.212746\trec:   66.133926\tkl:   27.078819\n",
      "Epoch: 604 [20100/50000 (40%)]  \tLoss:   91.066994\trec:   63.637451\tkl:   27.429544\n",
      "Epoch: 604 [30100/50000 (60%)]  \tLoss:   90.808357\trec:   64.319695\tkl:   26.488659\n",
      "Epoch: 604 [40100/50000 (80%)]  \tLoss:   93.626999\trec:   66.964134\tkl:   26.662863\n",
      "====> Epoch: 604 Average train loss: 90.4205\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7073\n",
      "Epoch: 605 [  100/50000 ( 0%)]  \tLoss:   85.249283\trec:   59.662968\tkl:   25.586317\n",
      "Epoch: 605 [10100/50000 (20%)]  \tLoss:   91.715195\trec:   64.792969\tkl:   26.922220\n",
      "Epoch: 605 [20100/50000 (40%)]  \tLoss:   88.310226\trec:   61.545345\tkl:   26.764875\n",
      "Epoch: 605 [30100/50000 (60%)]  \tLoss:   89.359406\trec:   62.850456\tkl:   26.508942\n",
      "Epoch: 605 [40100/50000 (80%)]  \tLoss:   90.618149\trec:   63.174221\tkl:   27.443932\n",
      "====> Epoch: 605 Average train loss: 90.4237\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5292\n",
      "Epoch: 606 [  100/50000 ( 0%)]  \tLoss:   91.061752\trec:   64.217476\tkl:   26.844273\n",
      "Epoch: 606 [10100/50000 (20%)]  \tLoss:   89.421753\trec:   62.691311\tkl:   26.730446\n",
      "Epoch: 606 [20100/50000 (40%)]  \tLoss:   89.539146\trec:   61.863358\tkl:   27.675791\n",
      "Epoch: 606 [30100/50000 (60%)]  \tLoss:   90.833649\trec:   63.785515\tkl:   27.048130\n",
      "Epoch: 606 [40100/50000 (80%)]  \tLoss:   92.534370\trec:   64.665520\tkl:   27.868862\n",
      "====> Epoch: 606 Average train loss: 90.3940\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5976\n",
      "Epoch: 607 [  100/50000 ( 0%)]  \tLoss:   90.586845\trec:   63.796551\tkl:   26.790291\n",
      "Epoch: 607 [10100/50000 (20%)]  \tLoss:   90.107956\trec:   63.880009\tkl:   26.227951\n",
      "Epoch: 607 [20100/50000 (40%)]  \tLoss:   88.408943\trec:   62.092915\tkl:   26.316032\n",
      "Epoch: 607 [30100/50000 (60%)]  \tLoss:   92.158081\trec:   64.563301\tkl:   27.594784\n",
      "Epoch: 607 [40100/50000 (80%)]  \tLoss:   91.078903\trec:   64.378922\tkl:   26.699980\n",
      "====> Epoch: 607 Average train loss: 90.4049\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5746\n",
      "Epoch: 608 [  100/50000 ( 0%)]  \tLoss:   85.329605\trec:   59.012997\tkl:   26.316616\n",
      "Epoch: 608 [10100/50000 (20%)]  \tLoss:   93.019127\trec:   66.297775\tkl:   26.721354\n",
      "Epoch: 608 [20100/50000 (40%)]  \tLoss:   94.052437\trec:   66.659874\tkl:   27.392563\n",
      "Epoch: 608 [30100/50000 (60%)]  \tLoss:   91.239540\trec:   64.872498\tkl:   26.367039\n",
      "Epoch: 608 [40100/50000 (80%)]  \tLoss:   90.790016\trec:   63.926624\tkl:   26.863394\n",
      "====> Epoch: 608 Average train loss: 90.4089\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4967\n",
      "Epoch: 609 [  100/50000 ( 0%)]  \tLoss:   90.554352\trec:   63.986580\tkl:   26.567778\n",
      "Epoch: 609 [10100/50000 (20%)]  \tLoss:   92.959877\trec:   65.138130\tkl:   27.821747\n",
      "Epoch: 609 [20100/50000 (40%)]  \tLoss:   93.695007\trec:   66.043327\tkl:   27.651682\n",
      "Epoch: 609 [30100/50000 (60%)]  \tLoss:   89.555679\trec:   62.176281\tkl:   27.379395\n",
      "Epoch: 609 [40100/50000 (80%)]  \tLoss:   91.862450\trec:   65.138214\tkl:   26.724230\n",
      "====> Epoch: 609 Average train loss: 90.4004\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6077\n",
      "Epoch: 610 [  100/50000 ( 0%)]  \tLoss:   88.685356\trec:   61.746113\tkl:   26.939249\n",
      "Epoch: 610 [10100/50000 (20%)]  \tLoss:   85.633110\trec:   59.793560\tkl:   25.839556\n",
      "Epoch: 610 [20100/50000 (40%)]  \tLoss:   92.598495\trec:   65.842667\tkl:   26.755819\n",
      "Epoch: 610 [30100/50000 (60%)]  \tLoss:   93.279434\trec:   66.211349\tkl:   27.068090\n",
      "Epoch: 610 [40100/50000 (80%)]  \tLoss:   93.463921\trec:   66.103455\tkl:   27.360464\n",
      "====> Epoch: 610 Average train loss: 90.3871\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5063\n",
      "Epoch: 611 [  100/50000 ( 0%)]  \tLoss:   90.963966\trec:   63.896912\tkl:   27.067051\n",
      "Epoch: 611 [10100/50000 (20%)]  \tLoss:   90.328178\trec:   63.816696\tkl:   26.511488\n",
      "Epoch: 611 [20100/50000 (40%)]  \tLoss:   86.638359\trec:   61.007549\tkl:   25.630810\n",
      "Epoch: 611 [30100/50000 (60%)]  \tLoss:   92.408768\trec:   65.509224\tkl:   26.899538\n",
      "Epoch: 611 [40100/50000 (80%)]  \tLoss:   90.147301\trec:   62.363300\tkl:   27.784008\n",
      "====> Epoch: 611 Average train loss: 90.3938\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7097\n",
      "Epoch: 612 [  100/50000 ( 0%)]  \tLoss:   89.714531\trec:   62.549206\tkl:   27.165323\n",
      "Epoch: 612 [10100/50000 (20%)]  \tLoss:   91.513046\trec:   63.970348\tkl:   27.542700\n",
      "Epoch: 612 [20100/50000 (40%)]  \tLoss:   91.954933\trec:   64.680275\tkl:   27.274654\n",
      "Epoch: 612 [30100/50000 (60%)]  \tLoss:   96.104958\trec:   68.485382\tkl:   27.619576\n",
      "Epoch: 612 [40100/50000 (80%)]  \tLoss:   85.459732\trec:   60.092197\tkl:   25.367537\n",
      "====> Epoch: 612 Average train loss: 90.3761\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.7310\n",
      "Epoch: 613 [  100/50000 ( 0%)]  \tLoss:   89.202888\trec:   62.886253\tkl:   26.316637\n",
      "Epoch: 613 [10100/50000 (20%)]  \tLoss:   90.953644\trec:   63.849678\tkl:   27.103964\n",
      "Epoch: 613 [20100/50000 (40%)]  \tLoss:   88.819702\trec:   61.610634\tkl:   27.209074\n",
      "Epoch: 613 [30100/50000 (60%)]  \tLoss:   89.906059\trec:   63.171745\tkl:   26.734316\n",
      "Epoch: 613 [40100/50000 (80%)]  \tLoss:   90.475464\trec:   64.398911\tkl:   26.076557\n",
      "====> Epoch: 613 Average train loss: 90.3788\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6509\n",
      "Epoch: 614 [  100/50000 ( 0%)]  \tLoss:   87.603203\trec:   61.496780\tkl:   26.106415\n",
      "Epoch: 614 [10100/50000 (20%)]  \tLoss:   87.322121\trec:   62.079365\tkl:   25.242754\n",
      "Epoch: 614 [20100/50000 (40%)]  \tLoss:   90.208931\trec:   63.656475\tkl:   26.552458\n",
      "Epoch: 614 [30100/50000 (60%)]  \tLoss:   86.594254\trec:   61.266747\tkl:   25.327515\n",
      "Epoch: 614 [40100/50000 (80%)]  \tLoss:   88.385796\trec:   62.548473\tkl:   25.837320\n",
      "====> Epoch: 614 Average train loss: 90.3694\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6100\n",
      "Epoch: 615 [  100/50000 ( 0%)]  \tLoss:   90.387245\trec:   63.932922\tkl:   26.454321\n",
      "Epoch: 615 [10100/50000 (20%)]  \tLoss:   89.925262\trec:   63.165916\tkl:   26.759342\n",
      "Epoch: 615 [20100/50000 (40%)]  \tLoss:   89.083374\trec:   62.437187\tkl:   26.646193\n",
      "Epoch: 615 [30100/50000 (60%)]  \tLoss:   87.990738\trec:   61.360233\tkl:   26.630503\n",
      "Epoch: 615 [40100/50000 (80%)]  \tLoss:   89.475700\trec:   62.702084\tkl:   26.773621\n",
      "====> Epoch: 615 Average train loss: 90.4002\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6089\n",
      "Epoch: 616 [  100/50000 ( 0%)]  \tLoss:   91.494659\trec:   64.156563\tkl:   27.338087\n",
      "Epoch: 616 [10100/50000 (20%)]  \tLoss:   91.765411\trec:   63.997334\tkl:   27.768074\n",
      "Epoch: 616 [20100/50000 (40%)]  \tLoss:   86.384155\trec:   59.886600\tkl:   26.497555\n",
      "Epoch: 616 [30100/50000 (60%)]  \tLoss:   92.586632\trec:   64.879913\tkl:   27.706713\n",
      "Epoch: 616 [40100/50000 (80%)]  \tLoss:   95.749176\trec:   67.604683\tkl:   28.144491\n",
      "====> Epoch: 616 Average train loss: 90.3802\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5257\n",
      "Epoch: 617 [  100/50000 ( 0%)]  \tLoss:   89.422729\trec:   62.251587\tkl:   27.171150\n",
      "Epoch: 617 [10100/50000 (20%)]  \tLoss:   88.874565\trec:   62.424335\tkl:   26.450239\n",
      "Epoch: 617 [20100/50000 (40%)]  \tLoss:   88.834755\trec:   62.770847\tkl:   26.063906\n",
      "Epoch: 617 [30100/50000 (60%)]  \tLoss:   90.432022\trec:   63.171406\tkl:   27.260612\n",
      "Epoch: 617 [40100/50000 (80%)]  \tLoss:   91.498833\trec:   64.820747\tkl:   26.678087\n",
      "====> Epoch: 617 Average train loss: 90.3760\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5787\n",
      "Epoch: 618 [  100/50000 ( 0%)]  \tLoss:   90.937660\trec:   64.159630\tkl:   26.778038\n",
      "Epoch: 618 [10100/50000 (20%)]  \tLoss:   91.031792\trec:   64.088081\tkl:   26.943712\n",
      "Epoch: 618 [20100/50000 (40%)]  \tLoss:   92.813980\trec:   66.705635\tkl:   26.108347\n",
      "Epoch: 618 [30100/50000 (60%)]  \tLoss:   89.400116\trec:   63.038124\tkl:   26.361996\n",
      "Epoch: 618 [40100/50000 (80%)]  \tLoss:   92.464493\trec:   64.995346\tkl:   27.469145\n",
      "====> Epoch: 618 Average train loss: 90.3706\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5942\n",
      "Epoch: 619 [  100/50000 ( 0%)]  \tLoss:   86.008400\trec:   60.024071\tkl:   25.984331\n",
      "Epoch: 619 [10100/50000 (20%)]  \tLoss:   89.802963\trec:   63.309071\tkl:   26.493893\n",
      "Epoch: 619 [20100/50000 (40%)]  \tLoss:   91.409164\trec:   64.264122\tkl:   27.145050\n",
      "Epoch: 619 [30100/50000 (60%)]  \tLoss:   90.899345\trec:   64.424210\tkl:   26.475134\n",
      "Epoch: 619 [40100/50000 (80%)]  \tLoss:   87.088921\trec:   61.638660\tkl:   25.450268\n",
      "====> Epoch: 619 Average train loss: 90.3550\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5346\n",
      "Epoch: 620 [  100/50000 ( 0%)]  \tLoss:   90.399635\trec:   64.110886\tkl:   26.288748\n",
      "Epoch: 620 [10100/50000 (20%)]  \tLoss:   86.312675\trec:   60.067333\tkl:   26.245346\n",
      "Epoch: 620 [20100/50000 (40%)]  \tLoss:   89.195763\trec:   62.634907\tkl:   26.560860\n",
      "Epoch: 620 [30100/50000 (60%)]  \tLoss:   90.689835\trec:   63.695801\tkl:   26.994036\n",
      "Epoch: 620 [40100/50000 (80%)]  \tLoss:   92.549316\trec:   65.655617\tkl:   26.893698\n",
      "====> Epoch: 620 Average train loss: 90.3754\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6555\n",
      "Epoch: 621 [  100/50000 ( 0%)]  \tLoss:   91.974350\trec:   64.636963\tkl:   27.337389\n",
      "Epoch: 621 [10100/50000 (20%)]  \tLoss:   94.282944\trec:   66.554436\tkl:   27.728516\n",
      "Epoch: 621 [20100/50000 (40%)]  \tLoss:   89.680565\trec:   63.132954\tkl:   26.547609\n",
      "Epoch: 621 [30100/50000 (60%)]  \tLoss:   90.488510\trec:   63.407898\tkl:   27.080612\n",
      "Epoch: 621 [40100/50000 (80%)]  \tLoss:   89.116714\trec:   62.394970\tkl:   26.721748\n",
      "====> Epoch: 621 Average train loss: 90.3773\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5490\n",
      "Epoch: 622 [  100/50000 ( 0%)]  \tLoss:   90.094078\trec:   63.944374\tkl:   26.149702\n",
      "Epoch: 622 [10100/50000 (20%)]  \tLoss:   90.548317\trec:   63.254009\tkl:   27.294315\n",
      "Epoch: 622 [20100/50000 (40%)]  \tLoss:   91.407661\trec:   64.483322\tkl:   26.924345\n",
      "Epoch: 622 [30100/50000 (60%)]  \tLoss:   88.253113\trec:   61.688774\tkl:   26.564339\n",
      "Epoch: 622 [40100/50000 (80%)]  \tLoss:   90.135231\trec:   62.684551\tkl:   27.450686\n",
      "====> Epoch: 622 Average train loss: 90.3535\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5544\n",
      "Epoch: 623 [  100/50000 ( 0%)]  \tLoss:   90.354492\trec:   63.861187\tkl:   26.493309\n",
      "Epoch: 623 [10100/50000 (20%)]  \tLoss:   87.596054\trec:   61.041389\tkl:   26.554665\n",
      "Epoch: 623 [20100/50000 (40%)]  \tLoss:   90.426941\trec:   63.765350\tkl:   26.661591\n",
      "Epoch: 623 [30100/50000 (60%)]  \tLoss:   84.292984\trec:   58.945301\tkl:   25.347685\n",
      "Epoch: 623 [40100/50000 (80%)]  \tLoss:   83.652351\trec:   57.929531\tkl:   25.722822\n",
      "====> Epoch: 623 Average train loss: 90.3383\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5558\n",
      "Epoch: 624 [  100/50000 ( 0%)]  \tLoss:   90.994835\trec:   63.359619\tkl:   27.635212\n",
      "Epoch: 624 [10100/50000 (20%)]  \tLoss:   87.381851\trec:   61.288582\tkl:   26.093271\n",
      "Epoch: 624 [20100/50000 (40%)]  \tLoss:   89.255898\trec:   63.439198\tkl:   25.816698\n",
      "Epoch: 624 [30100/50000 (60%)]  \tLoss:   89.743591\trec:   62.701878\tkl:   27.041710\n",
      "Epoch: 624 [40100/50000 (80%)]  \tLoss:   91.615997\trec:   65.209122\tkl:   26.406868\n",
      "====> Epoch: 624 Average train loss: 90.3360\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5864\n",
      "Epoch: 625 [  100/50000 ( 0%)]  \tLoss:   91.896591\trec:   64.933517\tkl:   26.963076\n",
      "Epoch: 625 [10100/50000 (20%)]  \tLoss:   87.261177\trec:   61.098484\tkl:   26.162695\n",
      "Epoch: 625 [20100/50000 (40%)]  \tLoss:   91.724663\trec:   64.547028\tkl:   27.177639\n",
      "Epoch: 625 [30100/50000 (60%)]  \tLoss:   90.439903\trec:   63.821613\tkl:   26.618282\n",
      "Epoch: 625 [40100/50000 (80%)]  \tLoss:   91.089706\trec:   63.851475\tkl:   27.238230\n",
      "====> Epoch: 625 Average train loss: 90.3266\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5552\n",
      "Epoch: 626 [  100/50000 ( 0%)]  \tLoss:   92.367271\trec:   65.250664\tkl:   27.116604\n",
      "Epoch: 626 [10100/50000 (20%)]  \tLoss:   89.856178\trec:   62.914562\tkl:   26.941618\n",
      "Epoch: 626 [20100/50000 (40%)]  \tLoss:   87.274635\trec:   61.253963\tkl:   26.020674\n",
      "Epoch: 626 [30100/50000 (60%)]  \tLoss:   90.572205\trec:   64.421295\tkl:   26.150915\n",
      "Epoch: 626 [40100/50000 (80%)]  \tLoss:   93.119568\trec:   66.485245\tkl:   26.634323\n",
      "====> Epoch: 626 Average train loss: 90.3176\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6298\n",
      "Epoch: 627 [  100/50000 ( 0%)]  \tLoss:   92.294662\trec:   65.634010\tkl:   26.660656\n",
      "Epoch: 627 [10100/50000 (20%)]  \tLoss:   87.828339\trec:   61.637520\tkl:   26.190825\n",
      "Epoch: 627 [20100/50000 (40%)]  \tLoss:   89.996338\trec:   62.433941\tkl:   27.562399\n",
      "Epoch: 627 [30100/50000 (60%)]  \tLoss:   89.083702\trec:   62.792015\tkl:   26.291687\n",
      "Epoch: 627 [40100/50000 (80%)]  \tLoss:   90.606522\trec:   63.488319\tkl:   27.118208\n",
      "====> Epoch: 627 Average train loss: 90.3155\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5820\n",
      "Epoch: 628 [  100/50000 ( 0%)]  \tLoss:   94.105995\trec:   67.662697\tkl:   26.443306\n",
      "Epoch: 628 [10100/50000 (20%)]  \tLoss:   87.156349\trec:   59.954880\tkl:   27.201462\n",
      "Epoch: 628 [20100/50000 (40%)]  \tLoss:   86.596413\trec:   60.489872\tkl:   26.106544\n",
      "Epoch: 628 [30100/50000 (60%)]  \tLoss:   89.232048\trec:   62.737167\tkl:   26.494886\n",
      "Epoch: 628 [40100/50000 (80%)]  \tLoss:   89.261032\trec:   62.388309\tkl:   26.872726\n",
      "====> Epoch: 628 Average train loss: 90.3077\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6040\n",
      "Epoch: 629 [  100/50000 ( 0%)]  \tLoss:   90.609627\trec:   63.244373\tkl:   27.365248\n",
      "Epoch: 629 [10100/50000 (20%)]  \tLoss:   90.636230\trec:   63.484554\tkl:   27.151680\n",
      "Epoch: 629 [20100/50000 (40%)]  \tLoss:   92.746101\trec:   65.205536\tkl:   27.540565\n",
      "Epoch: 629 [30100/50000 (60%)]  \tLoss:   89.744614\trec:   63.402683\tkl:   26.341936\n",
      "Epoch: 629 [40100/50000 (80%)]  \tLoss:   93.814003\trec:   66.901672\tkl:   26.912327\n",
      "====> Epoch: 629 Average train loss: 90.3327\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4887\n",
      "Epoch: 630 [  100/50000 ( 0%)]  \tLoss:   91.771812\trec:   64.646721\tkl:   27.125095\n",
      "Epoch: 630 [10100/50000 (20%)]  \tLoss:   94.259842\trec:   66.434128\tkl:   27.825718\n",
      "Epoch: 630 [20100/50000 (40%)]  \tLoss:   89.753670\trec:   62.313084\tkl:   27.440582\n",
      "Epoch: 630 [30100/50000 (60%)]  \tLoss:   89.880653\trec:   63.589394\tkl:   26.291260\n",
      "Epoch: 630 [40100/50000 (80%)]  \tLoss:   89.913025\trec:   63.147854\tkl:   26.765167\n",
      "====> Epoch: 630 Average train loss: 90.3523\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4137\n",
      "Epoch: 631 [  100/50000 ( 0%)]  \tLoss:   90.465782\trec:   63.019238\tkl:   27.446545\n",
      "Epoch: 631 [10100/50000 (20%)]  \tLoss:   90.836655\trec:   63.411411\tkl:   27.425243\n",
      "Epoch: 631 [20100/50000 (40%)]  \tLoss:   84.826981\trec:   58.596016\tkl:   26.230968\n",
      "Epoch: 631 [30100/50000 (60%)]  \tLoss:   91.934448\trec:   64.683624\tkl:   27.250828\n",
      "Epoch: 631 [40100/50000 (80%)]  \tLoss:   89.290215\trec:   62.241848\tkl:   27.048359\n",
      "====> Epoch: 631 Average train loss: 90.3108\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5726\n",
      "Epoch: 632 [  100/50000 ( 0%)]  \tLoss:   89.953102\trec:   62.941513\tkl:   27.011591\n",
      "Epoch: 632 [10100/50000 (20%)]  \tLoss:   88.946671\trec:   61.943748\tkl:   27.002920\n",
      "Epoch: 632 [20100/50000 (40%)]  \tLoss:   90.632309\trec:   63.853985\tkl:   26.778330\n",
      "Epoch: 632 [30100/50000 (60%)]  \tLoss:   95.334999\trec:   67.573990\tkl:   27.761013\n",
      "Epoch: 632 [40100/50000 (80%)]  \tLoss:   92.623894\trec:   65.275742\tkl:   27.348154\n",
      "====> Epoch: 632 Average train loss: 90.3353\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5557\n",
      "Epoch: 633 [  100/50000 ( 0%)]  \tLoss:   88.218857\trec:   61.604706\tkl:   26.614149\n",
      "Epoch: 633 [10100/50000 (20%)]  \tLoss:   93.544441\trec:   66.292107\tkl:   27.252337\n",
      "Epoch: 633 [20100/50000 (40%)]  \tLoss:   91.192612\trec:   63.142548\tkl:   28.050072\n",
      "Epoch: 633 [30100/50000 (60%)]  \tLoss:   90.241310\trec:   63.026081\tkl:   27.215229\n",
      "Epoch: 633 [40100/50000 (80%)]  \tLoss:   95.012909\trec:   67.081879\tkl:   27.931034\n",
      "====> Epoch: 633 Average train loss: 90.3141\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5331\n",
      "Epoch: 634 [  100/50000 ( 0%)]  \tLoss:   89.989723\trec:   62.835842\tkl:   27.153879\n",
      "Epoch: 634 [10100/50000 (20%)]  \tLoss:   91.631783\trec:   64.310295\tkl:   27.321493\n",
      "Epoch: 634 [20100/50000 (40%)]  \tLoss:   89.416344\trec:   62.026367\tkl:   27.389980\n",
      "Epoch: 634 [30100/50000 (60%)]  \tLoss:   89.981094\trec:   63.048077\tkl:   26.933014\n",
      "Epoch: 634 [40100/50000 (80%)]  \tLoss:   89.693474\trec:   63.895218\tkl:   25.798258\n",
      "====> Epoch: 634 Average train loss: 90.3196\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5228\n",
      "Epoch: 635 [  100/50000 ( 0%)]  \tLoss:   88.482300\trec:   62.652203\tkl:   25.830099\n",
      "Epoch: 635 [10100/50000 (20%)]  \tLoss:   89.348930\trec:   63.058495\tkl:   26.290442\n",
      "Epoch: 635 [20100/50000 (40%)]  \tLoss:   91.553818\trec:   65.062019\tkl:   26.491798\n",
      "Epoch: 635 [30100/50000 (60%)]  \tLoss:   89.022186\trec:   62.782303\tkl:   26.239885\n",
      "Epoch: 635 [40100/50000 (80%)]  \tLoss:   90.740601\trec:   63.174179\tkl:   27.566429\n",
      "====> Epoch: 635 Average train loss: 90.2975\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5142\n",
      "Epoch: 636 [  100/50000 ( 0%)]  \tLoss:   88.378342\trec:   61.553635\tkl:   26.824703\n",
      "Epoch: 636 [10100/50000 (20%)]  \tLoss:   90.423897\trec:   63.686531\tkl:   26.737366\n",
      "Epoch: 636 [20100/50000 (40%)]  \tLoss:   89.649940\trec:   62.539776\tkl:   27.110165\n",
      "Epoch: 636 [30100/50000 (60%)]  \tLoss:   89.732094\trec:   63.266762\tkl:   26.465340\n",
      "Epoch: 636 [40100/50000 (80%)]  \tLoss:   90.990318\trec:   64.478867\tkl:   26.511454\n",
      "====> Epoch: 636 Average train loss: 90.2761\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5282\n",
      "Epoch: 637 [  100/50000 ( 0%)]  \tLoss:   88.453522\trec:   62.730721\tkl:   25.722799\n",
      "Epoch: 637 [10100/50000 (20%)]  \tLoss:   89.158356\trec:   62.507000\tkl:   26.651361\n",
      "Epoch: 637 [20100/50000 (40%)]  \tLoss:   91.728592\trec:   64.087677\tkl:   27.640913\n",
      "Epoch: 637 [30100/50000 (60%)]  \tLoss:   91.428101\trec:   64.606133\tkl:   26.821972\n",
      "Epoch: 637 [40100/50000 (80%)]  \tLoss:   88.672554\trec:   62.537514\tkl:   26.135048\n",
      "====> Epoch: 637 Average train loss: 90.3141\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5219\n",
      "Epoch: 638 [  100/50000 ( 0%)]  \tLoss:   88.981987\trec:   63.095268\tkl:   25.886726\n",
      "Epoch: 638 [10100/50000 (20%)]  \tLoss:   90.223358\trec:   62.192028\tkl:   28.031332\n",
      "Epoch: 638 [20100/50000 (40%)]  \tLoss:   89.491287\trec:   63.568104\tkl:   25.923187\n",
      "Epoch: 638 [30100/50000 (60%)]  \tLoss:   93.489655\trec:   65.617027\tkl:   27.872629\n",
      "Epoch: 638 [40100/50000 (80%)]  \tLoss:   91.878647\trec:   64.460358\tkl:   27.418293\n",
      "====> Epoch: 638 Average train loss: 90.2832\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5871\n",
      "Epoch: 639 [  100/50000 ( 0%)]  \tLoss:   89.500114\trec:   63.183235\tkl:   26.316874\n",
      "Epoch: 639 [10100/50000 (20%)]  \tLoss:   90.408653\trec:   63.731411\tkl:   26.677246\n",
      "Epoch: 639 [20100/50000 (40%)]  \tLoss:   87.271446\trec:   61.214626\tkl:   26.056816\n",
      "Epoch: 639 [30100/50000 (60%)]  \tLoss:   94.238594\trec:   65.955574\tkl:   28.283012\n",
      "Epoch: 639 [40100/50000 (80%)]  \tLoss:   93.784538\trec:   66.144333\tkl:   27.640202\n",
      "====> Epoch: 639 Average train loss: 90.2953\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6091\n",
      "Epoch: 640 [  100/50000 ( 0%)]  \tLoss:   89.422951\trec:   62.741199\tkl:   26.681753\n",
      "Epoch: 640 [10100/50000 (20%)]  \tLoss:   92.095894\trec:   65.033966\tkl:   27.061930\n",
      "Epoch: 640 [20100/50000 (40%)]  \tLoss:   88.825348\trec:   61.548225\tkl:   27.277122\n",
      "Epoch: 640 [30100/50000 (60%)]  \tLoss:   97.635918\trec:   69.127205\tkl:   28.508711\n",
      "Epoch: 640 [40100/50000 (80%)]  \tLoss:   89.824471\trec:   62.979683\tkl:   26.844784\n",
      "====> Epoch: 640 Average train loss: 90.2844\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4921\n",
      "Epoch: 641 [  100/50000 ( 0%)]  \tLoss:   90.219917\trec:   62.498993\tkl:   27.720922\n",
      "Epoch: 641 [10100/50000 (20%)]  \tLoss:   86.064865\trec:   59.866745\tkl:   26.198112\n",
      "Epoch: 641 [20100/50000 (40%)]  \tLoss:   93.220673\trec:   65.550446\tkl:   27.670227\n",
      "Epoch: 641 [30100/50000 (60%)]  \tLoss:   90.175850\trec:   63.616631\tkl:   26.559217\n",
      "Epoch: 641 [40100/50000 (80%)]  \tLoss:   86.733475\trec:   61.006577\tkl:   25.726904\n",
      "====> Epoch: 641 Average train loss: 90.2533\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5330\n",
      "Epoch: 642 [  100/50000 ( 0%)]  \tLoss:   92.076035\trec:   66.273003\tkl:   25.803022\n",
      "Epoch: 642 [10100/50000 (20%)]  \tLoss:   88.168144\trec:   61.261211\tkl:   26.906929\n",
      "Epoch: 642 [20100/50000 (40%)]  \tLoss:   88.627586\trec:   61.580627\tkl:   27.046955\n",
      "Epoch: 642 [30100/50000 (60%)]  \tLoss:   83.185417\trec:   57.625290\tkl:   25.560123\n",
      "Epoch: 642 [40100/50000 (80%)]  \tLoss:   91.524948\trec:   64.889336\tkl:   26.635614\n",
      "====> Epoch: 642 Average train loss: 90.2731\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4503\n",
      "Epoch: 643 [  100/50000 ( 0%)]  \tLoss:   90.263329\trec:   63.730846\tkl:   26.532482\n",
      "Epoch: 643 [10100/50000 (20%)]  \tLoss:   89.313965\trec:   62.721062\tkl:   26.592899\n",
      "Epoch: 643 [20100/50000 (40%)]  \tLoss:   85.161461\trec:   59.905972\tkl:   25.255497\n",
      "Epoch: 643 [30100/50000 (60%)]  \tLoss:   93.972702\trec:   66.124947\tkl:   27.847755\n",
      "Epoch: 643 [40100/50000 (80%)]  \tLoss:   89.517250\trec:   62.419109\tkl:   27.098141\n",
      "====> Epoch: 643 Average train loss: 90.2680\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5836\n",
      "Epoch: 644 [  100/50000 ( 0%)]  \tLoss:   91.669624\trec:   64.125877\tkl:   27.543755\n",
      "Epoch: 644 [10100/50000 (20%)]  \tLoss:   86.903175\trec:   60.568710\tkl:   26.334463\n",
      "Epoch: 644 [20100/50000 (40%)]  \tLoss:   88.948143\trec:   61.540379\tkl:   27.407759\n",
      "Epoch: 644 [30100/50000 (60%)]  \tLoss:   86.468918\trec:   59.291748\tkl:   27.177164\n",
      "Epoch: 644 [40100/50000 (80%)]  \tLoss:   91.408173\trec:   63.850838\tkl:   27.557333\n",
      "====> Epoch: 644 Average train loss: 90.2581\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5297\n",
      "Epoch: 645 [  100/50000 ( 0%)]  \tLoss:   91.512344\trec:   64.498611\tkl:   27.013727\n",
      "Epoch: 645 [10100/50000 (20%)]  \tLoss:   91.841667\trec:   64.784615\tkl:   27.057047\n",
      "Epoch: 645 [20100/50000 (40%)]  \tLoss:   89.410378\trec:   61.337582\tkl:   28.072800\n",
      "Epoch: 645 [30100/50000 (60%)]  \tLoss:   88.900749\trec:   62.954189\tkl:   25.946560\n",
      "Epoch: 645 [40100/50000 (80%)]  \tLoss:   90.983688\trec:   64.553017\tkl:   26.430676\n",
      "====> Epoch: 645 Average train loss: 90.2566\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6140\n",
      "Epoch: 646 [  100/50000 ( 0%)]  \tLoss:   88.962891\trec:   62.323631\tkl:   26.639252\n",
      "Epoch: 646 [10100/50000 (20%)]  \tLoss:   89.318726\trec:   62.286995\tkl:   27.031738\n",
      "Epoch: 646 [20100/50000 (40%)]  \tLoss:   91.170105\trec:   64.144478\tkl:   27.025625\n",
      "Epoch: 646 [30100/50000 (60%)]  \tLoss:   89.137283\trec:   62.293808\tkl:   26.843479\n",
      "Epoch: 646 [40100/50000 (80%)]  \tLoss:   89.034897\trec:   62.619228\tkl:   26.415668\n",
      "====> Epoch: 646 Average train loss: 90.2546\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4898\n",
      "Epoch: 647 [  100/50000 ( 0%)]  \tLoss:   91.766151\trec:   65.193748\tkl:   26.572407\n",
      "Epoch: 647 [10100/50000 (20%)]  \tLoss:   89.931679\trec:   62.843487\tkl:   27.088196\n",
      "Epoch: 647 [20100/50000 (40%)]  \tLoss:   92.327438\trec:   65.123466\tkl:   27.203972\n",
      "Epoch: 647 [30100/50000 (60%)]  \tLoss:   86.346710\trec:   60.278671\tkl:   26.068037\n",
      "Epoch: 647 [40100/50000 (80%)]  \tLoss:   93.319939\trec:   66.170647\tkl:   27.149292\n",
      "====> Epoch: 647 Average train loss: 90.2467\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5045\n",
      "Epoch: 648 [  100/50000 ( 0%)]  \tLoss:   91.013885\trec:   63.408047\tkl:   27.605839\n",
      "Epoch: 648 [10100/50000 (20%)]  \tLoss:   89.322304\trec:   62.838886\tkl:   26.483418\n",
      "Epoch: 648 [20100/50000 (40%)]  \tLoss:   91.596657\trec:   64.067253\tkl:   27.529406\n",
      "Epoch: 648 [30100/50000 (60%)]  \tLoss:   89.127579\trec:   61.334427\tkl:   27.793148\n",
      "Epoch: 648 [40100/50000 (80%)]  \tLoss:   94.929459\trec:   67.036705\tkl:   27.892754\n",
      "====> Epoch: 648 Average train loss: 90.2619\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6761\n",
      "Epoch: 649 [  100/50000 ( 0%)]  \tLoss:   89.788231\trec:   62.282238\tkl:   27.505991\n",
      "Epoch: 649 [10100/50000 (20%)]  \tLoss:   90.968201\trec:   64.144531\tkl:   26.823662\n",
      "Epoch: 649 [20100/50000 (40%)]  \tLoss:   88.516510\trec:   62.544949\tkl:   25.971561\n",
      "Epoch: 649 [30100/50000 (60%)]  \tLoss:   87.980453\trec:   61.335819\tkl:   26.644636\n",
      "Epoch: 649 [40100/50000 (80%)]  \tLoss:   91.420372\trec:   63.740417\tkl:   27.679949\n",
      "====> Epoch: 649 Average train loss: 90.2578\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5710\n",
      "Epoch: 650 [  100/50000 ( 0%)]  \tLoss:   91.808144\trec:   64.036934\tkl:   27.771214\n",
      "Epoch: 650 [10100/50000 (20%)]  \tLoss:   90.118446\trec:   63.745502\tkl:   26.372946\n",
      "Epoch: 650 [20100/50000 (40%)]  \tLoss:   91.152733\trec:   63.239826\tkl:   27.912905\n",
      "Epoch: 650 [30100/50000 (60%)]  \tLoss:   88.218262\trec:   62.635967\tkl:   25.582293\n",
      "Epoch: 650 [40100/50000 (80%)]  \tLoss:   90.330177\trec:   63.113495\tkl:   27.216684\n",
      "====> Epoch: 650 Average train loss: 90.2657\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4319\n",
      "Epoch: 651 [  100/50000 ( 0%)]  \tLoss:   93.023125\trec:   66.319420\tkl:   26.703709\n",
      "Epoch: 651 [10100/50000 (20%)]  \tLoss:   87.787697\trec:   61.463802\tkl:   26.323893\n",
      "Epoch: 651 [20100/50000 (40%)]  \tLoss:   89.467384\trec:   62.550999\tkl:   26.916376\n",
      "Epoch: 651 [30100/50000 (60%)]  \tLoss:   89.519585\trec:   62.459934\tkl:   27.059650\n",
      "Epoch: 651 [40100/50000 (80%)]  \tLoss:   89.439362\trec:   63.220993\tkl:   26.218369\n",
      "====> Epoch: 651 Average train loss: 90.2421\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4816\n",
      "Epoch: 652 [  100/50000 ( 0%)]  \tLoss:   90.963829\trec:   64.262550\tkl:   26.701277\n",
      "Epoch: 652 [10100/50000 (20%)]  \tLoss:   91.364548\trec:   64.073257\tkl:   27.291286\n",
      "Epoch: 652 [20100/50000 (40%)]  \tLoss:   89.752357\trec:   63.185127\tkl:   26.567230\n",
      "Epoch: 652 [30100/50000 (60%)]  \tLoss:   88.595291\trec:   61.216171\tkl:   27.379116\n",
      "Epoch: 652 [40100/50000 (80%)]  \tLoss:   92.810577\trec:   65.419037\tkl:   27.391537\n",
      "====> Epoch: 652 Average train loss: 90.2407\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5035\n",
      "Epoch: 653 [  100/50000 ( 0%)]  \tLoss:   86.745796\trec:   61.323711\tkl:   25.422085\n",
      "Epoch: 653 [10100/50000 (20%)]  \tLoss:   88.811012\trec:   62.326355\tkl:   26.484655\n",
      "Epoch: 653 [20100/50000 (40%)]  \tLoss:   91.744125\trec:   64.639793\tkl:   27.104332\n",
      "Epoch: 653 [30100/50000 (60%)]  \tLoss:   92.101501\trec:   64.467476\tkl:   27.634026\n",
      "Epoch: 653 [40100/50000 (80%)]  \tLoss:   89.089684\trec:   62.395088\tkl:   26.694597\n",
      "====> Epoch: 653 Average train loss: 90.2389\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4455\n",
      "Epoch: 654 [  100/50000 ( 0%)]  \tLoss:   90.547058\trec:   64.033325\tkl:   26.513737\n",
      "Epoch: 654 [10100/50000 (20%)]  \tLoss:   90.706093\trec:   64.382233\tkl:   26.323854\n",
      "Epoch: 654 [20100/50000 (40%)]  \tLoss:   92.203003\trec:   64.922432\tkl:   27.280573\n",
      "Epoch: 654 [30100/50000 (60%)]  \tLoss:   88.878578\trec:   62.008659\tkl:   26.869925\n",
      "Epoch: 654 [40100/50000 (80%)]  \tLoss:   87.495369\trec:   62.331787\tkl:   25.163582\n",
      "====> Epoch: 654 Average train loss: 90.2457\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5890\n",
      "Epoch: 655 [  100/50000 ( 0%)]  \tLoss:   86.221275\trec:   60.007851\tkl:   26.213427\n",
      "Epoch: 655 [10100/50000 (20%)]  \tLoss:   86.590370\trec:   59.829060\tkl:   26.761305\n",
      "Epoch: 655 [20100/50000 (40%)]  \tLoss:   92.604393\trec:   66.102409\tkl:   26.501976\n",
      "Epoch: 655 [30100/50000 (60%)]  \tLoss:   85.591431\trec:   59.499226\tkl:   26.092207\n",
      "Epoch: 655 [40100/50000 (80%)]  \tLoss:   90.946014\trec:   64.089836\tkl:   26.856178\n",
      "====> Epoch: 655 Average train loss: 90.2431\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4968\n",
      "Epoch: 656 [  100/50000 ( 0%)]  \tLoss:   89.569977\trec:   63.360737\tkl:   26.209248\n",
      "Epoch: 656 [10100/50000 (20%)]  \tLoss:   87.237862\trec:   61.984089\tkl:   25.253769\n",
      "Epoch: 656 [20100/50000 (40%)]  \tLoss:   90.275780\trec:   62.985989\tkl:   27.289795\n",
      "Epoch: 656 [30100/50000 (60%)]  \tLoss:   87.205925\trec:   61.046814\tkl:   26.159109\n",
      "Epoch: 656 [40100/50000 (80%)]  \tLoss:   88.578339\trec:   62.527348\tkl:   26.050991\n",
      "====> Epoch: 656 Average train loss: 90.2342\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4783\n",
      "Epoch: 657 [  100/50000 ( 0%)]  \tLoss:   89.475479\trec:   62.481716\tkl:   26.993761\n",
      "Epoch: 657 [10100/50000 (20%)]  \tLoss:   88.618019\trec:   62.811462\tkl:   25.806555\n",
      "Epoch: 657 [20100/50000 (40%)]  \tLoss:   86.653473\trec:   60.732712\tkl:   25.920761\n",
      "Epoch: 657 [30100/50000 (60%)]  \tLoss:   93.467339\trec:   66.073242\tkl:   27.394100\n",
      "Epoch: 657 [40100/50000 (80%)]  \tLoss:   89.900978\trec:   63.842865\tkl:   26.058113\n",
      "====> Epoch: 657 Average train loss: 90.1881\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4811\n",
      "Epoch: 658 [  100/50000 ( 0%)]  \tLoss:   87.812576\trec:   61.266052\tkl:   26.546526\n",
      "Epoch: 658 [10100/50000 (20%)]  \tLoss:   89.509727\trec:   63.368122\tkl:   26.141602\n",
      "Epoch: 658 [20100/50000 (40%)]  \tLoss:   89.683846\trec:   63.457458\tkl:   26.226381\n",
      "Epoch: 658 [30100/50000 (60%)]  \tLoss:   90.615150\trec:   63.132034\tkl:   27.483120\n",
      "Epoch: 658 [40100/50000 (80%)]  \tLoss:   90.347343\trec:   63.839512\tkl:   26.507826\n",
      "====> Epoch: 658 Average train loss: 90.2478\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4872\n",
      "Epoch: 659 [  100/50000 ( 0%)]  \tLoss:   92.094452\trec:   64.947937\tkl:   27.146519\n",
      "Epoch: 659 [10100/50000 (20%)]  \tLoss:   92.982697\trec:   64.666855\tkl:   28.315834\n",
      "Epoch: 659 [20100/50000 (40%)]  \tLoss:   85.615417\trec:   60.843761\tkl:   24.771658\n",
      "Epoch: 659 [30100/50000 (60%)]  \tLoss:   91.401993\trec:   64.404167\tkl:   26.997822\n",
      "Epoch: 659 [40100/50000 (80%)]  \tLoss:   90.016754\trec:   63.321522\tkl:   26.695236\n",
      "====> Epoch: 659 Average train loss: 90.2110\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3717\n",
      "Epoch: 660 [  100/50000 ( 0%)]  \tLoss:   89.191658\trec:   62.492458\tkl:   26.699196\n",
      "Epoch: 660 [10100/50000 (20%)]  \tLoss:   90.313026\trec:   63.509533\tkl:   26.803490\n",
      "Epoch: 660 [20100/50000 (40%)]  \tLoss:   92.379189\trec:   65.183701\tkl:   27.195490\n",
      "Epoch: 660 [30100/50000 (60%)]  \tLoss:   90.145424\trec:   62.895832\tkl:   27.249590\n",
      "Epoch: 660 [40100/50000 (80%)]  \tLoss:   88.922264\trec:   62.552013\tkl:   26.370253\n",
      "====> Epoch: 660 Average train loss: 90.2066\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5327\n",
      "Epoch: 661 [  100/50000 ( 0%)]  \tLoss:   88.423149\trec:   62.454887\tkl:   25.968269\n",
      "Epoch: 661 [10100/50000 (20%)]  \tLoss:   91.598419\trec:   63.964680\tkl:   27.633736\n",
      "Epoch: 661 [20100/50000 (40%)]  \tLoss:   91.386337\trec:   64.827904\tkl:   26.558432\n",
      "Epoch: 661 [30100/50000 (60%)]  \tLoss:   89.689941\trec:   63.019150\tkl:   26.670795\n",
      "Epoch: 661 [40100/50000 (80%)]  \tLoss:   90.494873\trec:   63.673447\tkl:   26.821428\n",
      "====> Epoch: 661 Average train loss: 90.2074\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4311\n",
      "Epoch: 662 [  100/50000 ( 0%)]  \tLoss:   91.783493\trec:   64.112946\tkl:   27.670555\n",
      "Epoch: 662 [10100/50000 (20%)]  \tLoss:   86.075546\trec:   59.807762\tkl:   26.267780\n",
      "Epoch: 662 [20100/50000 (40%)]  \tLoss:   91.179970\trec:   63.446346\tkl:   27.733620\n",
      "Epoch: 662 [30100/50000 (60%)]  \tLoss:   88.808159\trec:   61.400291\tkl:   27.407871\n",
      "Epoch: 662 [40100/50000 (80%)]  \tLoss:   88.127129\trec:   61.160557\tkl:   26.966576\n",
      "====> Epoch: 662 Average train loss: 90.2226\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5592\n",
      "Epoch: 663 [  100/50000 ( 0%)]  \tLoss:   92.493355\trec:   65.858078\tkl:   26.635288\n",
      "Epoch: 663 [10100/50000 (20%)]  \tLoss:   89.704117\trec:   62.628063\tkl:   27.076050\n",
      "Epoch: 663 [20100/50000 (40%)]  \tLoss:   89.308731\trec:   62.115147\tkl:   27.193579\n",
      "Epoch: 663 [30100/50000 (60%)]  \tLoss:   92.798988\trec:   65.788033\tkl:   27.010960\n",
      "Epoch: 663 [40100/50000 (80%)]  \tLoss:   87.249901\trec:   61.013073\tkl:   26.236830\n",
      "====> Epoch: 663 Average train loss: 90.1912\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4873\n",
      "Epoch: 664 [  100/50000 ( 0%)]  \tLoss:   88.343491\trec:   60.696758\tkl:   27.646732\n",
      "Epoch: 664 [10100/50000 (20%)]  \tLoss:   88.516403\trec:   61.746227\tkl:   26.770180\n",
      "Epoch: 664 [20100/50000 (40%)]  \tLoss:   90.537994\trec:   63.602558\tkl:   26.935436\n",
      "Epoch: 664 [30100/50000 (60%)]  \tLoss:   92.723114\trec:   65.433311\tkl:   27.289806\n",
      "Epoch: 664 [40100/50000 (80%)]  \tLoss:   91.843307\trec:   64.904541\tkl:   26.938766\n",
      "====> Epoch: 664 Average train loss: 90.1653\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4374\n",
      "Epoch: 665 [  100/50000 ( 0%)]  \tLoss:   89.631012\trec:   61.968582\tkl:   27.662426\n",
      "Epoch: 665 [10100/50000 (20%)]  \tLoss:   88.579979\trec:   62.587742\tkl:   25.992231\n",
      "Epoch: 665 [20100/50000 (40%)]  \tLoss:   89.129059\trec:   62.935638\tkl:   26.193424\n",
      "Epoch: 665 [30100/50000 (60%)]  \tLoss:   90.198280\trec:   63.402527\tkl:   26.795752\n",
      "Epoch: 665 [40100/50000 (80%)]  \tLoss:   90.020737\trec:   62.609402\tkl:   27.411343\n",
      "====> Epoch: 665 Average train loss: 90.1956\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5218\n",
      "Epoch: 666 [  100/50000 ( 0%)]  \tLoss:   90.340584\trec:   63.319897\tkl:   27.020693\n",
      "Epoch: 666 [10100/50000 (20%)]  \tLoss:   89.912155\trec:   63.629108\tkl:   26.283049\n",
      "Epoch: 666 [20100/50000 (40%)]  \tLoss:   86.674919\trec:   59.732986\tkl:   26.941931\n",
      "Epoch: 666 [30100/50000 (60%)]  \tLoss:   92.419220\trec:   65.400642\tkl:   27.018578\n",
      "Epoch: 666 [40100/50000 (80%)]  \tLoss:   93.443726\trec:   65.685982\tkl:   27.757748\n",
      "====> Epoch: 666 Average train loss: 90.2087\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5535\n",
      "Epoch: 667 [  100/50000 ( 0%)]  \tLoss:   93.332695\trec:   66.412628\tkl:   26.920057\n",
      "Epoch: 667 [10100/50000 (20%)]  \tLoss:   93.510574\trec:   66.302971\tkl:   27.207602\n",
      "Epoch: 667 [20100/50000 (40%)]  \tLoss:   89.848267\trec:   63.142509\tkl:   26.705761\n",
      "Epoch: 667 [30100/50000 (60%)]  \tLoss:   90.282341\trec:   63.923691\tkl:   26.358650\n",
      "Epoch: 667 [40100/50000 (80%)]  \tLoss:   94.878807\trec:   66.741768\tkl:   28.137041\n",
      "====> Epoch: 667 Average train loss: 90.1803\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4333\n",
      "Epoch: 668 [  100/50000 ( 0%)]  \tLoss:   90.869995\trec:   63.596149\tkl:   27.273844\n",
      "Epoch: 668 [10100/50000 (20%)]  \tLoss:   92.243752\trec:   65.009018\tkl:   27.234728\n",
      "Epoch: 668 [20100/50000 (40%)]  \tLoss:   90.492371\trec:   64.588463\tkl:   25.903908\n",
      "Epoch: 668 [30100/50000 (60%)]  \tLoss:   86.072510\trec:   60.065575\tkl:   26.006935\n",
      "Epoch: 668 [40100/50000 (80%)]  \tLoss:   93.422905\trec:   65.757500\tkl:   27.665407\n",
      "====> Epoch: 668 Average train loss: 90.1718\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5721\n",
      "Epoch: 669 [  100/50000 ( 0%)]  \tLoss:   94.620522\trec:   66.615807\tkl:   28.004726\n",
      "Epoch: 669 [10100/50000 (20%)]  \tLoss:   90.164185\trec:   63.394695\tkl:   26.769491\n",
      "Epoch: 669 [20100/50000 (40%)]  \tLoss:   91.168785\trec:   63.677704\tkl:   27.491089\n",
      "Epoch: 669 [30100/50000 (60%)]  \tLoss:   89.210724\trec:   62.524441\tkl:   26.686275\n",
      "Epoch: 669 [40100/50000 (80%)]  \tLoss:   89.970367\trec:   63.148602\tkl:   26.821762\n",
      "====> Epoch: 669 Average train loss: 90.1788\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4431\n",
      "Epoch: 670 [  100/50000 ( 0%)]  \tLoss:   86.642334\trec:   60.478848\tkl:   26.163486\n",
      "Epoch: 670 [10100/50000 (20%)]  \tLoss:   92.904526\trec:   65.388550\tkl:   27.515982\n",
      "Epoch: 670 [20100/50000 (40%)]  \tLoss:   91.104218\trec:   64.905464\tkl:   26.198753\n",
      "Epoch: 670 [30100/50000 (60%)]  \tLoss:   91.969780\trec:   63.948818\tkl:   28.020964\n",
      "Epoch: 670 [40100/50000 (80%)]  \tLoss:   89.949188\trec:   63.079414\tkl:   26.869772\n",
      "====> Epoch: 670 Average train loss: 90.1688\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4750\n",
      "Epoch: 671 [  100/50000 ( 0%)]  \tLoss:   94.410263\trec:   67.074341\tkl:   27.335915\n",
      "Epoch: 671 [10100/50000 (20%)]  \tLoss:   85.943573\trec:   59.001221\tkl:   26.942356\n",
      "Epoch: 671 [20100/50000 (40%)]  \tLoss:   92.207207\trec:   64.918365\tkl:   27.288845\n",
      "Epoch: 671 [30100/50000 (60%)]  \tLoss:   90.217201\trec:   63.692299\tkl:   26.524910\n",
      "Epoch: 671 [40100/50000 (80%)]  \tLoss:   90.113869\trec:   63.747849\tkl:   26.366020\n",
      "====> Epoch: 671 Average train loss: 90.1649\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5364\n",
      "Epoch: 672 [  100/50000 ( 0%)]  \tLoss:   91.355545\trec:   65.280029\tkl:   26.075518\n",
      "Epoch: 672 [10100/50000 (20%)]  \tLoss:   93.120331\trec:   65.118164\tkl:   28.002165\n",
      "Epoch: 672 [20100/50000 (40%)]  \tLoss:   85.358749\trec:   59.439037\tkl:   25.919716\n",
      "Epoch: 672 [30100/50000 (60%)]  \tLoss:   89.006538\trec:   62.122326\tkl:   26.884216\n",
      "Epoch: 672 [40100/50000 (80%)]  \tLoss:   90.177162\trec:   63.243587\tkl:   26.933578\n",
      "====> Epoch: 672 Average train loss: 90.1819\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5345\n",
      "Epoch: 673 [  100/50000 ( 0%)]  \tLoss:   89.950424\trec:   63.507378\tkl:   26.443050\n",
      "Epoch: 673 [10100/50000 (20%)]  \tLoss:   93.768631\trec:   65.986168\tkl:   27.782463\n",
      "Epoch: 673 [20100/50000 (40%)]  \tLoss:   90.304840\trec:   63.646599\tkl:   26.658247\n",
      "Epoch: 673 [30100/50000 (60%)]  \tLoss:   90.878769\trec:   64.459770\tkl:   26.418999\n",
      "Epoch: 673 [40100/50000 (80%)]  \tLoss:   88.527008\trec:   61.995399\tkl:   26.531612\n",
      "====> Epoch: 673 Average train loss: 90.1464\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4388\n",
      "Epoch: 674 [  100/50000 ( 0%)]  \tLoss:   90.132607\trec:   62.623013\tkl:   27.509594\n",
      "Epoch: 674 [10100/50000 (20%)]  \tLoss:   90.598045\trec:   63.444572\tkl:   27.153469\n",
      "Epoch: 674 [20100/50000 (40%)]  \tLoss:   89.843094\trec:   62.595917\tkl:   27.247175\n",
      "Epoch: 674 [30100/50000 (60%)]  \tLoss:   91.100151\trec:   64.337540\tkl:   26.762617\n",
      "Epoch: 674 [40100/50000 (80%)]  \tLoss:   92.007256\trec:   65.001251\tkl:   27.006002\n",
      "====> Epoch: 674 Average train loss: 90.1629\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4599\n",
      "Epoch: 675 [  100/50000 ( 0%)]  \tLoss:   85.994019\trec:   59.675503\tkl:   26.318516\n",
      "Epoch: 675 [10100/50000 (20%)]  \tLoss:   90.772690\trec:   64.842506\tkl:   25.930193\n",
      "Epoch: 675 [20100/50000 (40%)]  \tLoss:   88.666473\trec:   62.135883\tkl:   26.530590\n",
      "Epoch: 675 [30100/50000 (60%)]  \tLoss:   90.746010\trec:   64.004578\tkl:   26.741434\n",
      "Epoch: 675 [40100/50000 (80%)]  \tLoss:   89.955086\trec:   63.321674\tkl:   26.633410\n",
      "====> Epoch: 675 Average train loss: 90.1744\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4634\n",
      "Epoch: 676 [  100/50000 ( 0%)]  \tLoss:   88.618027\trec:   62.500732\tkl:   26.117300\n",
      "Epoch: 676 [10100/50000 (20%)]  \tLoss:   88.906677\trec:   62.415905\tkl:   26.490776\n",
      "Epoch: 676 [20100/50000 (40%)]  \tLoss:   92.369873\trec:   64.435104\tkl:   27.934767\n",
      "Epoch: 676 [30100/50000 (60%)]  \tLoss:   90.754684\trec:   63.961601\tkl:   26.793085\n",
      "Epoch: 676 [40100/50000 (80%)]  \tLoss:   89.464058\trec:   62.495781\tkl:   26.968279\n",
      "====> Epoch: 676 Average train loss: 90.1767\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5723\n",
      "Epoch: 677 [  100/50000 ( 0%)]  \tLoss:   93.497940\trec:   66.499146\tkl:   26.998796\n",
      "Epoch: 677 [10100/50000 (20%)]  \tLoss:   90.061638\trec:   62.836502\tkl:   27.225132\n",
      "Epoch: 677 [20100/50000 (40%)]  \tLoss:   92.352417\trec:   64.796349\tkl:   27.556068\n",
      "Epoch: 677 [30100/50000 (60%)]  \tLoss:   90.326134\trec:   63.027901\tkl:   27.298235\n",
      "Epoch: 677 [40100/50000 (80%)]  \tLoss:   94.978416\trec:   67.943855\tkl:   27.034565\n",
      "====> Epoch: 677 Average train loss: 90.1773\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5238\n",
      "Epoch: 678 [  100/50000 ( 0%)]  \tLoss:   88.783546\trec:   62.310959\tkl:   26.472582\n",
      "Epoch: 678 [10100/50000 (20%)]  \tLoss:   88.817497\trec:   61.862617\tkl:   26.954885\n",
      "Epoch: 678 [20100/50000 (40%)]  \tLoss:   91.946648\trec:   64.292778\tkl:   27.653870\n",
      "Epoch: 678 [30100/50000 (60%)]  \tLoss:   93.967110\trec:   66.482117\tkl:   27.484995\n",
      "Epoch: 678 [40100/50000 (80%)]  \tLoss:   90.298759\trec:   63.312057\tkl:   26.986702\n",
      "====> Epoch: 678 Average train loss: 90.1609\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3386\n",
      "Epoch: 679 [  100/50000 ( 0%)]  \tLoss:   87.284370\trec:   60.561192\tkl:   26.723185\n",
      "Epoch: 679 [10100/50000 (20%)]  \tLoss:   88.185036\trec:   61.371433\tkl:   26.813601\n",
      "Epoch: 679 [20100/50000 (40%)]  \tLoss:   88.458038\trec:   61.352654\tkl:   27.105383\n",
      "Epoch: 679 [30100/50000 (60%)]  \tLoss:   92.027351\trec:   64.115265\tkl:   27.912086\n",
      "Epoch: 679 [40100/50000 (80%)]  \tLoss:   94.187706\trec:   66.500343\tkl:   27.687355\n",
      "====> Epoch: 679 Average train loss: 90.1386\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4485\n",
      "Epoch: 680 [  100/50000 ( 0%)]  \tLoss:   92.028374\trec:   64.389839\tkl:   27.638529\n",
      "Epoch: 680 [10100/50000 (20%)]  \tLoss:   91.509392\trec:   63.369518\tkl:   28.139870\n",
      "Epoch: 680 [20100/50000 (40%)]  \tLoss:   90.342056\trec:   64.229469\tkl:   26.112587\n",
      "Epoch: 680 [30100/50000 (60%)]  \tLoss:   91.781219\trec:   64.639854\tkl:   27.141367\n",
      "Epoch: 680 [40100/50000 (80%)]  \tLoss:   92.319489\trec:   65.648956\tkl:   26.670528\n",
      "====> Epoch: 680 Average train loss: 90.1562\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4868\n",
      "Epoch: 681 [  100/50000 ( 0%)]  \tLoss:   88.018898\trec:   62.146126\tkl:   25.872768\n",
      "Epoch: 681 [10100/50000 (20%)]  \tLoss:   88.134689\trec:   61.491741\tkl:   26.642948\n",
      "Epoch: 681 [20100/50000 (40%)]  \tLoss:   94.412079\trec:   66.662544\tkl:   27.749531\n",
      "Epoch: 681 [30100/50000 (60%)]  \tLoss:   90.234428\trec:   62.902634\tkl:   27.331800\n",
      "Epoch: 681 [40100/50000 (80%)]  \tLoss:   91.271736\trec:   64.538055\tkl:   26.733677\n",
      "====> Epoch: 681 Average train loss: 90.1084\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4712\n",
      "Epoch: 682 [  100/50000 ( 0%)]  \tLoss:   89.418770\trec:   62.750679\tkl:   26.668085\n",
      "Epoch: 682 [10100/50000 (20%)]  \tLoss:   89.139542\trec:   62.545849\tkl:   26.593691\n",
      "Epoch: 682 [20100/50000 (40%)]  \tLoss:   91.900543\trec:   64.975922\tkl:   26.924618\n",
      "Epoch: 682 [30100/50000 (60%)]  \tLoss:   90.747498\trec:   62.500683\tkl:   28.246817\n",
      "Epoch: 682 [40100/50000 (80%)]  \tLoss:   85.533615\trec:   58.525101\tkl:   27.008511\n",
      "====> Epoch: 682 Average train loss: 90.1666\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3926\n",
      "Epoch: 683 [  100/50000 ( 0%)]  \tLoss:   90.818497\trec:   64.279419\tkl:   26.539072\n",
      "Epoch: 683 [10100/50000 (20%)]  \tLoss:   90.736710\trec:   64.036217\tkl:   26.700485\n",
      "Epoch: 683 [20100/50000 (40%)]  \tLoss:   86.301979\trec:   60.528713\tkl:   25.773264\n",
      "Epoch: 683 [30100/50000 (60%)]  \tLoss:   86.508980\trec:   60.468143\tkl:   26.040836\n",
      "Epoch: 683 [40100/50000 (80%)]  \tLoss:   88.614059\trec:   61.666824\tkl:   26.947231\n",
      "====> Epoch: 683 Average train loss: 90.1335\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4607\n",
      "Epoch: 684 [  100/50000 ( 0%)]  \tLoss:   91.196709\trec:   64.636612\tkl:   26.560095\n",
      "Epoch: 684 [10100/50000 (20%)]  \tLoss:   89.029312\trec:   62.889072\tkl:   26.140242\n",
      "Epoch: 684 [20100/50000 (40%)]  \tLoss:   89.939713\trec:   63.107773\tkl:   26.831940\n",
      "Epoch: 684 [30100/50000 (60%)]  \tLoss:   91.507156\trec:   64.561348\tkl:   26.945812\n",
      "Epoch: 684 [40100/50000 (80%)]  \tLoss:   86.006226\trec:   59.822884\tkl:   26.183346\n",
      "====> Epoch: 684 Average train loss: 90.1453\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4202\n",
      "Epoch: 685 [  100/50000 ( 0%)]  \tLoss:   92.790787\trec:   66.053467\tkl:   26.737324\n",
      "Epoch: 685 [10100/50000 (20%)]  \tLoss:   92.948204\trec:   65.444824\tkl:   27.503384\n",
      "Epoch: 685 [20100/50000 (40%)]  \tLoss:   94.453697\trec:   66.835373\tkl:   27.618326\n",
      "Epoch: 685 [30100/50000 (60%)]  \tLoss:   91.036568\trec:   64.068634\tkl:   26.967941\n",
      "Epoch: 685 [40100/50000 (80%)]  \tLoss:   87.310303\trec:   60.763710\tkl:   26.546589\n",
      "====> Epoch: 685 Average train loss: 90.1327\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4543\n",
      "Epoch: 686 [  100/50000 ( 0%)]  \tLoss:   92.577263\trec:   64.845757\tkl:   27.731503\n",
      "Epoch: 686 [10100/50000 (20%)]  \tLoss:   93.073990\trec:   64.971954\tkl:   28.102037\n",
      "Epoch: 686 [20100/50000 (40%)]  \tLoss:   87.057304\trec:   61.216244\tkl:   25.841057\n",
      "Epoch: 686 [30100/50000 (60%)]  \tLoss:   90.626053\trec:   63.378963\tkl:   27.247084\n",
      "Epoch: 686 [40100/50000 (80%)]  \tLoss:   91.891953\trec:   65.225243\tkl:   26.666704\n",
      "====> Epoch: 686 Average train loss: 90.1163\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4755\n",
      "Epoch: 687 [  100/50000 ( 0%)]  \tLoss:   87.296700\trec:   60.970253\tkl:   26.326450\n",
      "Epoch: 687 [10100/50000 (20%)]  \tLoss:   85.220787\trec:   59.431629\tkl:   25.789160\n",
      "Epoch: 687 [20100/50000 (40%)]  \tLoss:   91.233971\trec:   63.483898\tkl:   27.750074\n",
      "Epoch: 687 [30100/50000 (60%)]  \tLoss:   89.646797\trec:   63.061443\tkl:   26.585350\n",
      "Epoch: 687 [40100/50000 (80%)]  \tLoss:   91.169716\trec:   64.154594\tkl:   27.015116\n",
      "====> Epoch: 687 Average train loss: 90.1152\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5850\n",
      "Epoch: 688 [  100/50000 ( 0%)]  \tLoss:   89.223648\trec:   62.978935\tkl:   26.244717\n",
      "Epoch: 688 [10100/50000 (20%)]  \tLoss:   90.217720\trec:   63.487976\tkl:   26.729748\n",
      "Epoch: 688 [20100/50000 (40%)]  \tLoss:   92.078110\trec:   65.034386\tkl:   27.043728\n",
      "Epoch: 688 [30100/50000 (60%)]  \tLoss:   88.908562\trec:   62.292782\tkl:   26.615780\n",
      "Epoch: 688 [40100/50000 (80%)]  \tLoss:   88.413139\trec:   62.617313\tkl:   25.795828\n",
      "====> Epoch: 688 Average train loss: 90.1288\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3586\n",
      "Epoch: 689 [  100/50000 ( 0%)]  \tLoss:   89.453964\trec:   63.144489\tkl:   26.309469\n",
      "Epoch: 689 [10100/50000 (20%)]  \tLoss:   88.211128\trec:   61.559189\tkl:   26.651937\n",
      "Epoch: 689 [20100/50000 (40%)]  \tLoss:   84.913277\trec:   58.632240\tkl:   26.281036\n",
      "Epoch: 689 [30100/50000 (60%)]  \tLoss:   90.498100\trec:   63.626511\tkl:   26.871593\n",
      "Epoch: 689 [40100/50000 (80%)]  \tLoss:   88.809669\trec:   62.236580\tkl:   26.573086\n",
      "====> Epoch: 689 Average train loss: 90.1040\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4161\n",
      "Epoch: 690 [  100/50000 ( 0%)]  \tLoss:   92.059883\trec:   64.042549\tkl:   28.017328\n",
      "Epoch: 690 [10100/50000 (20%)]  \tLoss:   91.770721\trec:   64.578514\tkl:   27.192211\n",
      "Epoch: 690 [20100/50000 (40%)]  \tLoss:   88.402481\trec:   62.211514\tkl:   26.190962\n",
      "Epoch: 690 [30100/50000 (60%)]  \tLoss:   91.364044\trec:   64.450783\tkl:   26.913261\n",
      "Epoch: 690 [40100/50000 (80%)]  \tLoss:   88.785194\trec:   62.415409\tkl:   26.369787\n",
      "====> Epoch: 690 Average train loss: 90.1054\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4891\n",
      "Epoch: 691 [  100/50000 ( 0%)]  \tLoss:   91.548744\trec:   64.347603\tkl:   27.201149\n",
      "Epoch: 691 [10100/50000 (20%)]  \tLoss:   89.921532\trec:   62.926758\tkl:   26.994774\n",
      "Epoch: 691 [20100/50000 (40%)]  \tLoss:   91.735245\trec:   64.440331\tkl:   27.294912\n",
      "Epoch: 691 [30100/50000 (60%)]  \tLoss:   90.024117\trec:   62.857094\tkl:   27.167021\n",
      "Epoch: 691 [40100/50000 (80%)]  \tLoss:   92.679581\trec:   65.968987\tkl:   26.710588\n",
      "====> Epoch: 691 Average train loss: 90.1203\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5442\n",
      "Epoch: 692 [  100/50000 ( 0%)]  \tLoss:   87.389908\trec:   61.056747\tkl:   26.333164\n",
      "Epoch: 692 [10100/50000 (20%)]  \tLoss:   85.401962\trec:   59.406208\tkl:   25.995750\n",
      "Epoch: 692 [20100/50000 (40%)]  \tLoss:   93.172974\trec:   66.082253\tkl:   27.090729\n",
      "Epoch: 692 [30100/50000 (60%)]  \tLoss:   92.605659\trec:   65.096092\tkl:   27.509573\n",
      "Epoch: 692 [40100/50000 (80%)]  \tLoss:   89.842873\trec:   62.314499\tkl:   27.528374\n",
      "====> Epoch: 692 Average train loss: 90.0986\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4044\n",
      "Epoch: 693 [  100/50000 ( 0%)]  \tLoss:   89.056656\trec:   62.354267\tkl:   26.702396\n",
      "Epoch: 693 [10100/50000 (20%)]  \tLoss:   92.435738\trec:   64.460236\tkl:   27.975504\n",
      "Epoch: 693 [20100/50000 (40%)]  \tLoss:   91.594620\trec:   64.341003\tkl:   27.253614\n",
      "Epoch: 693 [30100/50000 (60%)]  \tLoss:   88.551765\trec:   62.105751\tkl:   26.446014\n",
      "Epoch: 693 [40100/50000 (80%)]  \tLoss:   88.645584\trec:   61.622669\tkl:   27.022915\n",
      "====> Epoch: 693 Average train loss: 90.1052\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3840\n",
      "Epoch: 694 [  100/50000 ( 0%)]  \tLoss:   89.109093\trec:   62.745281\tkl:   26.363808\n",
      "Epoch: 694 [10100/50000 (20%)]  \tLoss:   89.906837\trec:   63.246975\tkl:   26.659863\n",
      "Epoch: 694 [20100/50000 (40%)]  \tLoss:   96.265396\trec:   67.777779\tkl:   28.487619\n",
      "Epoch: 694 [30100/50000 (60%)]  \tLoss:   89.359253\trec:   62.817856\tkl:   26.541399\n",
      "Epoch: 694 [40100/50000 (80%)]  \tLoss:   93.031303\trec:   65.362740\tkl:   27.668564\n",
      "====> Epoch: 694 Average train loss: 90.1062\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4460\n",
      "Epoch: 695 [  100/50000 ( 0%)]  \tLoss:   92.634842\trec:   65.954659\tkl:   26.680183\n",
      "Epoch: 695 [10100/50000 (20%)]  \tLoss:   87.921913\trec:   60.900497\tkl:   27.021418\n",
      "Epoch: 695 [20100/50000 (40%)]  \tLoss:   90.415283\trec:   63.402370\tkl:   27.012907\n",
      "Epoch: 695 [30100/50000 (60%)]  \tLoss:   91.455116\trec:   64.089249\tkl:   27.365866\n",
      "Epoch: 695 [40100/50000 (80%)]  \tLoss:   89.338364\trec:   62.554287\tkl:   26.784079\n",
      "====> Epoch: 695 Average train loss: 90.1059\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4204\n",
      "Epoch: 696 [  100/50000 ( 0%)]  \tLoss:   91.514206\trec:   64.366508\tkl:   27.147692\n",
      "Epoch: 696 [10100/50000 (20%)]  \tLoss:   92.347496\trec:   65.798943\tkl:   26.548553\n",
      "Epoch: 696 [20100/50000 (40%)]  \tLoss:   91.518532\trec:   64.253128\tkl:   27.265409\n",
      "Epoch: 696 [30100/50000 (60%)]  \tLoss:   87.718224\trec:   61.148613\tkl:   26.569607\n",
      "Epoch: 696 [40100/50000 (80%)]  \tLoss:   84.372360\trec:   58.759949\tkl:   25.612413\n",
      "====> Epoch: 696 Average train loss: 90.0961\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4705\n",
      "Epoch: 697 [  100/50000 ( 0%)]  \tLoss:   90.545586\trec:   63.712547\tkl:   26.833042\n",
      "Epoch: 697 [10100/50000 (20%)]  \tLoss:   90.866638\trec:   64.364716\tkl:   26.501921\n",
      "Epoch: 697 [20100/50000 (40%)]  \tLoss:   89.355034\trec:   62.940193\tkl:   26.414843\n",
      "Epoch: 697 [30100/50000 (60%)]  \tLoss:   92.437080\trec:   65.388718\tkl:   27.048361\n",
      "Epoch: 697 [40100/50000 (80%)]  \tLoss:   93.167686\trec:   65.266594\tkl:   27.901089\n",
      "====> Epoch: 697 Average train loss: 90.0630\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5150\n",
      "Epoch: 698 [  100/50000 ( 0%)]  \tLoss:   86.539627\trec:   60.240990\tkl:   26.298632\n",
      "Epoch: 698 [10100/50000 (20%)]  \tLoss:   90.805527\trec:   63.101673\tkl:   27.703852\n",
      "Epoch: 698 [20100/50000 (40%)]  \tLoss:   87.193710\trec:   60.289726\tkl:   26.903984\n",
      "Epoch: 698 [30100/50000 (60%)]  \tLoss:   92.483047\trec:   64.884384\tkl:   27.598660\n",
      "Epoch: 698 [40100/50000 (80%)]  \tLoss:   91.346725\trec:   63.589863\tkl:   27.756865\n",
      "====> Epoch: 698 Average train loss: 90.0676\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4867\n",
      "Epoch: 699 [  100/50000 ( 0%)]  \tLoss:   93.146851\trec:   65.878265\tkl:   27.268585\n",
      "Epoch: 699 [10100/50000 (20%)]  \tLoss:   89.375557\trec:   62.416409\tkl:   26.959146\n",
      "Epoch: 699 [20100/50000 (40%)]  \tLoss:   85.628960\trec:   59.970360\tkl:   25.658604\n",
      "Epoch: 699 [30100/50000 (60%)]  \tLoss:   89.753563\trec:   63.156181\tkl:   26.597382\n",
      "Epoch: 699 [40100/50000 (80%)]  \tLoss:   91.371635\trec:   64.353386\tkl:   27.018246\n",
      "====> Epoch: 699 Average train loss: 90.0861\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4862\n",
      "Epoch: 700 [  100/50000 ( 0%)]  \tLoss:   94.429527\trec:   67.026299\tkl:   27.403236\n",
      "Epoch: 700 [10100/50000 (20%)]  \tLoss:   88.612694\trec:   61.933792\tkl:   26.678904\n",
      "Epoch: 700 [20100/50000 (40%)]  \tLoss:   89.076622\trec:   62.247288\tkl:   26.829330\n",
      "Epoch: 700 [30100/50000 (60%)]  \tLoss:   87.755760\trec:   61.952305\tkl:   25.803453\n",
      "Epoch: 700 [40100/50000 (80%)]  \tLoss:   91.167007\trec:   63.831463\tkl:   27.335550\n",
      "====> Epoch: 700 Average train loss: 90.0773\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4538\n",
      "Epoch: 701 [  100/50000 ( 0%)]  \tLoss:   95.423378\trec:   67.340347\tkl:   28.083027\n",
      "Epoch: 701 [10100/50000 (20%)]  \tLoss:   86.946266\trec:   60.003437\tkl:   26.942831\n",
      "Epoch: 701 [20100/50000 (40%)]  \tLoss:   88.841911\trec:   62.313885\tkl:   26.528023\n",
      "Epoch: 701 [30100/50000 (60%)]  \tLoss:   87.415741\trec:   61.268764\tkl:   26.146975\n",
      "Epoch: 701 [40100/50000 (80%)]  \tLoss:   92.199539\trec:   65.247864\tkl:   26.951674\n",
      "====> Epoch: 701 Average train loss: 90.0768\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4238\n",
      "Epoch: 702 [  100/50000 ( 0%)]  \tLoss:   90.283340\trec:   64.028870\tkl:   26.254463\n",
      "Epoch: 702 [10100/50000 (20%)]  \tLoss:   86.576012\trec:   60.994877\tkl:   25.581135\n",
      "Epoch: 702 [20100/50000 (40%)]  \tLoss:   92.822937\trec:   65.562378\tkl:   27.260553\n",
      "Epoch: 702 [30100/50000 (60%)]  \tLoss:   88.054352\trec:   60.857422\tkl:   27.196936\n",
      "Epoch: 702 [40100/50000 (80%)]  \tLoss:   91.157242\trec:   63.579002\tkl:   27.578241\n",
      "====> Epoch: 702 Average train loss: 90.0719\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4993\n",
      "Epoch: 703 [  100/50000 ( 0%)]  \tLoss:   91.597351\trec:   65.171143\tkl:   26.426208\n",
      "Epoch: 703 [10100/50000 (20%)]  \tLoss:   92.411064\trec:   65.151100\tkl:   27.259962\n",
      "Epoch: 703 [20100/50000 (40%)]  \tLoss:   92.017441\trec:   64.955086\tkl:   27.062347\n",
      "Epoch: 703 [30100/50000 (60%)]  \tLoss:   91.017471\trec:   63.949394\tkl:   27.068075\n",
      "Epoch: 703 [40100/50000 (80%)]  \tLoss:   88.419724\trec:   62.459583\tkl:   25.960144\n",
      "====> Epoch: 703 Average train loss: 90.0660\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.6355\n",
      "Epoch: 704 [  100/50000 ( 0%)]  \tLoss:   91.291344\trec:   64.298668\tkl:   26.992672\n",
      "Epoch: 704 [10100/50000 (20%)]  \tLoss:   89.652252\trec:   62.080776\tkl:   27.571482\n",
      "Epoch: 704 [20100/50000 (40%)]  \tLoss:   91.435143\trec:   65.368340\tkl:   26.066803\n",
      "Epoch: 704 [30100/50000 (60%)]  \tLoss:   87.600998\trec:   61.047401\tkl:   26.553589\n",
      "Epoch: 704 [40100/50000 (80%)]  \tLoss:   89.713135\trec:   62.117874\tkl:   27.595259\n",
      "====> Epoch: 704 Average train loss: 90.0770\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4716\n",
      "Epoch: 705 [  100/50000 ( 0%)]  \tLoss:   90.716873\trec:   63.496288\tkl:   27.220581\n",
      "Epoch: 705 [10100/50000 (20%)]  \tLoss:   89.885406\trec:   63.033817\tkl:   26.851591\n",
      "Epoch: 705 [20100/50000 (40%)]  \tLoss:   84.100113\trec:   59.131958\tkl:   24.968159\n",
      "Epoch: 705 [30100/50000 (60%)]  \tLoss:   86.433121\trec:   60.681122\tkl:   25.751997\n",
      "Epoch: 705 [40100/50000 (80%)]  \tLoss:   89.041542\trec:   62.691921\tkl:   26.349617\n",
      "====> Epoch: 705 Average train loss: 90.0714\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4071\n",
      "Epoch: 706 [  100/50000 ( 0%)]  \tLoss:   92.221939\trec:   65.619240\tkl:   26.602701\n",
      "Epoch: 706 [10100/50000 (20%)]  \tLoss:   89.353188\trec:   62.681404\tkl:   26.671787\n",
      "Epoch: 706 [20100/50000 (40%)]  \tLoss:   87.602066\trec:   60.724957\tkl:   26.877110\n",
      "Epoch: 706 [30100/50000 (60%)]  \tLoss:   86.010849\trec:   59.499287\tkl:   26.511562\n",
      "Epoch: 706 [40100/50000 (80%)]  \tLoss:   90.951660\trec:   63.502209\tkl:   27.449446\n",
      "====> Epoch: 706 Average train loss: 90.0651\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4490\n",
      "Epoch: 707 [  100/50000 ( 0%)]  \tLoss:   89.127068\trec:   62.338398\tkl:   26.788673\n",
      "Epoch: 707 [10100/50000 (20%)]  \tLoss:   91.189354\trec:   63.124958\tkl:   28.064392\n",
      "Epoch: 707 [20100/50000 (40%)]  \tLoss:   89.502075\trec:   62.708172\tkl:   26.793903\n",
      "Epoch: 707 [30100/50000 (60%)]  \tLoss:   93.253983\trec:   64.634605\tkl:   28.619371\n",
      "Epoch: 707 [40100/50000 (80%)]  \tLoss:   93.313164\trec:   66.252319\tkl:   27.060839\n",
      "====> Epoch: 707 Average train loss: 90.0562\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4856\n",
      "Epoch: 708 [  100/50000 ( 0%)]  \tLoss:   91.251183\trec:   64.338394\tkl:   26.912785\n",
      "Epoch: 708 [10100/50000 (20%)]  \tLoss:   90.376778\trec:   64.049080\tkl:   26.327692\n",
      "Epoch: 708 [20100/50000 (40%)]  \tLoss:   91.774216\trec:   64.081902\tkl:   27.692314\n",
      "Epoch: 708 [30100/50000 (60%)]  \tLoss:   89.922729\trec:   63.253586\tkl:   26.669147\n",
      "Epoch: 708 [40100/50000 (80%)]  \tLoss:   90.108139\trec:   63.473633\tkl:   26.634512\n",
      "====> Epoch: 708 Average train loss: 90.0656\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5210\n",
      "Epoch: 709 [  100/50000 ( 0%)]  \tLoss:   90.336601\trec:   63.623116\tkl:   26.713484\n",
      "Epoch: 709 [10100/50000 (20%)]  \tLoss:   91.340584\trec:   64.240501\tkl:   27.100082\n",
      "Epoch: 709 [20100/50000 (40%)]  \tLoss:   93.873795\trec:   67.044777\tkl:   26.829016\n",
      "Epoch: 709 [30100/50000 (60%)]  \tLoss:   92.253494\trec:   65.823799\tkl:   26.429688\n",
      "Epoch: 709 [40100/50000 (80%)]  \tLoss:   91.454117\trec:   64.973732\tkl:   26.480383\n",
      "====> Epoch: 709 Average train loss: 90.0481\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4727\n",
      "Epoch: 710 [  100/50000 ( 0%)]  \tLoss:   93.496040\trec:   65.124489\tkl:   28.371550\n",
      "Epoch: 710 [10100/50000 (20%)]  \tLoss:   88.357384\trec:   61.487274\tkl:   26.870104\n",
      "Epoch: 710 [20100/50000 (40%)]  \tLoss:   88.798141\trec:   62.446888\tkl:   26.351259\n",
      "Epoch: 710 [30100/50000 (60%)]  \tLoss:   88.927521\trec:   63.244213\tkl:   25.683308\n",
      "Epoch: 710 [40100/50000 (80%)]  \tLoss:   91.693436\trec:   64.028458\tkl:   27.664986\n",
      "====> Epoch: 710 Average train loss: 90.0413\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4926\n",
      "Epoch: 711 [  100/50000 ( 0%)]  \tLoss:   91.091583\trec:   63.342655\tkl:   27.748922\n",
      "Epoch: 711 [10100/50000 (20%)]  \tLoss:   89.445053\trec:   62.282177\tkl:   27.162878\n",
      "Epoch: 711 [20100/50000 (40%)]  \tLoss:   92.959206\trec:   65.458855\tkl:   27.500349\n",
      "Epoch: 711 [30100/50000 (60%)]  \tLoss:   89.944412\trec:   63.067337\tkl:   26.877071\n",
      "Epoch: 711 [40100/50000 (80%)]  \tLoss:   89.627129\trec:   62.472576\tkl:   27.154545\n",
      "====> Epoch: 711 Average train loss: 90.0402\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4708\n",
      "Epoch: 712 [  100/50000 ( 0%)]  \tLoss:   90.570084\trec:   63.526302\tkl:   27.043783\n",
      "Epoch: 712 [10100/50000 (20%)]  \tLoss:   84.546715\trec:   58.643955\tkl:   25.902765\n",
      "Epoch: 712 [20100/50000 (40%)]  \tLoss:   86.326797\trec:   60.628109\tkl:   25.698689\n",
      "Epoch: 712 [30100/50000 (60%)]  \tLoss:   88.344757\trec:   61.552185\tkl:   26.792568\n",
      "Epoch: 712 [40100/50000 (80%)]  \tLoss:   91.783173\trec:   63.709614\tkl:   28.073559\n",
      "====> Epoch: 712 Average train loss: 90.0463\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5603\n",
      "Epoch: 713 [  100/50000 ( 0%)]  \tLoss:   90.424072\trec:   63.516014\tkl:   26.908056\n",
      "Epoch: 713 [10100/50000 (20%)]  \tLoss:   89.183807\trec:   62.233871\tkl:   26.949930\n",
      "Epoch: 713 [20100/50000 (40%)]  \tLoss:   90.166817\trec:   61.768467\tkl:   28.398344\n",
      "Epoch: 713 [30100/50000 (60%)]  \tLoss:   86.175430\trec:   59.347546\tkl:   26.827881\n",
      "Epoch: 713 [40100/50000 (80%)]  \tLoss:   91.140312\trec:   64.536667\tkl:   26.603640\n",
      "====> Epoch: 713 Average train loss: 90.0267\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3885\n",
      "Epoch: 714 [  100/50000 ( 0%)]  \tLoss:   88.523766\trec:   61.072979\tkl:   27.450792\n",
      "Epoch: 714 [10100/50000 (20%)]  \tLoss:   90.884171\trec:   64.113419\tkl:   26.770748\n",
      "Epoch: 714 [20100/50000 (40%)]  \tLoss:   88.979599\trec:   61.838299\tkl:   27.141300\n",
      "Epoch: 714 [30100/50000 (60%)]  \tLoss:   91.730240\trec:   64.224098\tkl:   27.506147\n",
      "Epoch: 714 [40100/50000 (80%)]  \tLoss:   92.421043\trec:   64.762947\tkl:   27.658098\n",
      "====> Epoch: 714 Average train loss: 90.0248\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4145\n",
      "Epoch: 715 [  100/50000 ( 0%)]  \tLoss:   89.136284\trec:   62.718636\tkl:   26.417654\n",
      "Epoch: 715 [10100/50000 (20%)]  \tLoss:   86.170792\trec:   58.861172\tkl:   27.309618\n",
      "Epoch: 715 [20100/50000 (40%)]  \tLoss:   88.846596\trec:   62.806416\tkl:   26.040190\n",
      "Epoch: 715 [30100/50000 (60%)]  \tLoss:   90.310242\trec:   63.075386\tkl:   27.234858\n",
      "Epoch: 715 [40100/50000 (80%)]  \tLoss:   91.425957\trec:   63.670124\tkl:   27.755835\n",
      "====> Epoch: 715 Average train loss: 89.9978\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4914\n",
      "Epoch: 716 [  100/50000 ( 0%)]  \tLoss:   90.148689\trec:   63.138660\tkl:   27.010033\n",
      "Epoch: 716 [10100/50000 (20%)]  \tLoss:   93.599930\trec:   66.252556\tkl:   27.347372\n",
      "Epoch: 716 [20100/50000 (40%)]  \tLoss:   87.696159\trec:   60.858784\tkl:   26.837376\n",
      "Epoch: 716 [30100/50000 (60%)]  \tLoss:   88.953972\trec:   62.127888\tkl:   26.826086\n",
      "Epoch: 716 [40100/50000 (80%)]  \tLoss:   92.371353\trec:   65.028053\tkl:   27.343307\n",
      "====> Epoch: 716 Average train loss: 90.0306\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4533\n",
      "Epoch: 717 [  100/50000 ( 0%)]  \tLoss:   87.244041\trec:   60.897839\tkl:   26.346201\n",
      "Epoch: 717 [10100/50000 (20%)]  \tLoss:   88.815308\trec:   61.414993\tkl:   27.400322\n",
      "Epoch: 717 [20100/50000 (40%)]  \tLoss:   90.483788\trec:   63.883701\tkl:   26.600084\n",
      "Epoch: 717 [30100/50000 (60%)]  \tLoss:   90.723618\trec:   64.434616\tkl:   26.289003\n",
      "Epoch: 717 [40100/50000 (80%)]  \tLoss:   91.037598\trec:   63.689167\tkl:   27.348423\n",
      "====> Epoch: 717 Average train loss: 90.0431\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4391\n",
      "Epoch: 718 [  100/50000 ( 0%)]  \tLoss:   93.267860\trec:   65.390556\tkl:   27.877302\n",
      "Epoch: 718 [10100/50000 (20%)]  \tLoss:   87.586784\trec:   61.189236\tkl:   26.397551\n",
      "Epoch: 718 [20100/50000 (40%)]  \tLoss:   92.894691\trec:   65.043243\tkl:   27.851454\n",
      "Epoch: 718 [30100/50000 (60%)]  \tLoss:   91.204399\trec:   63.884766\tkl:   27.319641\n",
      "Epoch: 718 [40100/50000 (80%)]  \tLoss:   86.524872\trec:   60.433182\tkl:   26.091686\n",
      "====> Epoch: 718 Average train loss: 90.0179\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5340\n",
      "Epoch: 719 [  100/50000 ( 0%)]  \tLoss:   89.163239\trec:   62.328457\tkl:   26.834785\n",
      "Epoch: 719 [10100/50000 (20%)]  \tLoss:   90.456657\trec:   63.310432\tkl:   27.146219\n",
      "Epoch: 719 [20100/50000 (40%)]  \tLoss:   95.159698\trec:   66.702362\tkl:   28.457336\n",
      "Epoch: 719 [30100/50000 (60%)]  \tLoss:   87.204704\trec:   60.961895\tkl:   26.242817\n",
      "Epoch: 719 [40100/50000 (80%)]  \tLoss:   89.838699\trec:   62.096912\tkl:   27.741783\n",
      "====> Epoch: 719 Average train loss: 90.0222\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4915\n",
      "Epoch: 720 [  100/50000 ( 0%)]  \tLoss:   91.254829\trec:   63.746433\tkl:   27.508396\n",
      "Epoch: 720 [10100/50000 (20%)]  \tLoss:   83.690529\trec:   59.016270\tkl:   24.674252\n",
      "Epoch: 720 [20100/50000 (40%)]  \tLoss:   88.002441\trec:   61.193291\tkl:   26.809153\n",
      "Epoch: 720 [30100/50000 (60%)]  \tLoss:   86.142517\trec:   60.572433\tkl:   25.570080\n",
      "Epoch: 720 [40100/50000 (80%)]  \tLoss:   87.898163\trec:   61.020996\tkl:   26.877167\n",
      "====> Epoch: 720 Average train loss: 90.0231\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3979\n",
      "Epoch: 721 [  100/50000 ( 0%)]  \tLoss:   88.640450\trec:   62.129475\tkl:   26.510965\n",
      "Epoch: 721 [10100/50000 (20%)]  \tLoss:   91.517799\trec:   64.162682\tkl:   27.355122\n",
      "Epoch: 721 [20100/50000 (40%)]  \tLoss:   90.326324\trec:   62.980515\tkl:   27.345812\n",
      "Epoch: 721 [30100/50000 (60%)]  \tLoss:   85.160385\trec:   60.076385\tkl:   25.084002\n",
      "Epoch: 721 [40100/50000 (80%)]  \tLoss:   91.271698\trec:   63.925106\tkl:   27.346596\n",
      "====> Epoch: 721 Average train loss: 89.9971\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4579\n",
      "Epoch: 722 [  100/50000 ( 0%)]  \tLoss:   92.102974\trec:   65.088966\tkl:   27.014013\n",
      "Epoch: 722 [10100/50000 (20%)]  \tLoss:   90.450996\trec:   63.420692\tkl:   27.030308\n",
      "Epoch: 722 [20100/50000 (40%)]  \tLoss:   91.158707\trec:   63.932171\tkl:   27.226542\n",
      "Epoch: 722 [30100/50000 (60%)]  \tLoss:   88.308189\trec:   61.655735\tkl:   26.652458\n",
      "Epoch: 722 [40100/50000 (80%)]  \tLoss:   92.770981\trec:   65.355957\tkl:   27.415030\n",
      "====> Epoch: 722 Average train loss: 89.9941\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4662\n",
      "Epoch: 723 [  100/50000 ( 0%)]  \tLoss:   89.327301\trec:   62.265827\tkl:   27.061480\n",
      "Epoch: 723 [10100/50000 (20%)]  \tLoss:   90.812782\trec:   63.414345\tkl:   27.398434\n",
      "Epoch: 723 [20100/50000 (40%)]  \tLoss:   90.530602\trec:   63.573250\tkl:   26.957354\n",
      "Epoch: 723 [30100/50000 (60%)]  \tLoss:   91.759796\trec:   64.269920\tkl:   27.489872\n",
      "Epoch: 723 [40100/50000 (80%)]  \tLoss:   87.828545\trec:   61.521832\tkl:   26.306711\n",
      "====> Epoch: 723 Average train loss: 90.0095\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3590\n",
      "Epoch: 724 [  100/50000 ( 0%)]  \tLoss:   86.812111\trec:   60.988018\tkl:   25.824091\n",
      "Epoch: 724 [10100/50000 (20%)]  \tLoss:   88.207222\trec:   61.051830\tkl:   27.155392\n",
      "Epoch: 724 [20100/50000 (40%)]  \tLoss:   91.286041\trec:   64.781860\tkl:   26.504181\n",
      "Epoch: 724 [30100/50000 (60%)]  \tLoss:   91.427040\trec:   63.964443\tkl:   27.462599\n",
      "Epoch: 724 [40100/50000 (80%)]  \tLoss:   93.967072\trec:   66.764030\tkl:   27.203032\n",
      "====> Epoch: 724 Average train loss: 90.0249\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4884\n",
      "Epoch: 725 [  100/50000 ( 0%)]  \tLoss:   89.515182\trec:   61.582893\tkl:   27.932291\n",
      "Epoch: 725 [10100/50000 (20%)]  \tLoss:   89.069946\trec:   62.079765\tkl:   26.990183\n",
      "Epoch: 725 [20100/50000 (40%)]  \tLoss:   89.566582\trec:   62.204319\tkl:   27.362255\n",
      "Epoch: 725 [30100/50000 (60%)]  \tLoss:   89.691071\trec:   63.415085\tkl:   26.275980\n",
      "Epoch: 725 [40100/50000 (80%)]  \tLoss:   87.503876\trec:   60.822178\tkl:   26.681702\n",
      "====> Epoch: 725 Average train loss: 89.9958\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4193\n",
      "Epoch: 726 [  100/50000 ( 0%)]  \tLoss:   88.786247\trec:   62.580261\tkl:   26.205988\n",
      "Epoch: 726 [10100/50000 (20%)]  \tLoss:   90.782440\trec:   64.437431\tkl:   26.345007\n",
      "Epoch: 726 [20100/50000 (40%)]  \tLoss:   92.341599\trec:   64.833656\tkl:   27.507944\n",
      "Epoch: 726 [30100/50000 (60%)]  \tLoss:   89.635040\trec:   62.466702\tkl:   27.168329\n",
      "Epoch: 726 [40100/50000 (80%)]  \tLoss:   91.310760\trec:   64.921028\tkl:   26.389732\n",
      "====> Epoch: 726 Average train loss: 90.0220\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4609\n",
      "Epoch: 727 [  100/50000 ( 0%)]  \tLoss:   90.449234\trec:   63.871990\tkl:   26.577250\n",
      "Epoch: 727 [10100/50000 (20%)]  \tLoss:   87.236786\trec:   61.381626\tkl:   25.855158\n",
      "Epoch: 727 [20100/50000 (40%)]  \tLoss:   89.894875\trec:   62.811638\tkl:   27.083235\n",
      "Epoch: 727 [30100/50000 (60%)]  \tLoss:   81.961655\trec:   56.707554\tkl:   25.254101\n",
      "Epoch: 727 [40100/50000 (80%)]  \tLoss:   91.672279\trec:   64.643875\tkl:   27.028408\n",
      "====> Epoch: 727 Average train loss: 89.9883\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4199\n",
      "Epoch: 728 [  100/50000 ( 0%)]  \tLoss:   88.115860\trec:   61.763855\tkl:   26.352003\n",
      "Epoch: 728 [10100/50000 (20%)]  \tLoss:   86.454941\trec:   60.682625\tkl:   25.772318\n",
      "Epoch: 728 [20100/50000 (40%)]  \tLoss:   86.400291\trec:   59.968884\tkl:   26.431404\n",
      "Epoch: 728 [30100/50000 (60%)]  \tLoss:   88.388618\trec:   61.276081\tkl:   27.112539\n",
      "Epoch: 728 [40100/50000 (80%)]  \tLoss:   91.495476\trec:   63.323593\tkl:   28.171883\n",
      "====> Epoch: 728 Average train loss: 90.0031\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3132\n",
      "Epoch: 729 [  100/50000 ( 0%)]  \tLoss:   85.078400\trec:   59.043354\tkl:   26.035038\n",
      "Epoch: 729 [10100/50000 (20%)]  \tLoss:   91.422890\trec:   64.169128\tkl:   27.253757\n",
      "Epoch: 729 [20100/50000 (40%)]  \tLoss:   91.150246\trec:   64.655479\tkl:   26.494762\n",
      "Epoch: 729 [30100/50000 (60%)]  \tLoss:   87.463623\trec:   61.302341\tkl:   26.161278\n",
      "Epoch: 729 [40100/50000 (80%)]  \tLoss:   91.908409\trec:   64.540421\tkl:   27.367985\n",
      "====> Epoch: 729 Average train loss: 89.9960\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4613\n",
      "Epoch: 730 [  100/50000 ( 0%)]  \tLoss:   90.493393\trec:   63.222626\tkl:   27.270771\n",
      "Epoch: 730 [10100/50000 (20%)]  \tLoss:   90.377441\trec:   62.804577\tkl:   27.572855\n",
      "Epoch: 730 [20100/50000 (40%)]  \tLoss:   91.120232\trec:   64.466362\tkl:   26.653862\n",
      "Epoch: 730 [30100/50000 (60%)]  \tLoss:   90.898926\trec:   63.698910\tkl:   27.200020\n",
      "Epoch: 730 [40100/50000 (80%)]  \tLoss:   89.657455\trec:   63.483162\tkl:   26.174294\n",
      "====> Epoch: 730 Average train loss: 89.9884\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4404\n",
      "Epoch: 731 [  100/50000 ( 0%)]  \tLoss:   86.331139\trec:   59.920124\tkl:   26.411015\n",
      "Epoch: 731 [10100/50000 (20%)]  \tLoss:   91.756950\trec:   64.796501\tkl:   26.960449\n",
      "Epoch: 731 [20100/50000 (40%)]  \tLoss:   92.515976\trec:   65.576218\tkl:   26.939758\n",
      "Epoch: 731 [30100/50000 (60%)]  \tLoss:   91.854233\trec:   65.068619\tkl:   26.785622\n",
      "Epoch: 731 [40100/50000 (80%)]  \tLoss:   93.385056\trec:   66.083809\tkl:   27.301239\n",
      "====> Epoch: 731 Average train loss: 89.9822\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3146\n",
      "Epoch: 732 [  100/50000 ( 0%)]  \tLoss:   92.740021\trec:   65.112564\tkl:   27.627445\n",
      "Epoch: 732 [10100/50000 (20%)]  \tLoss:   86.098846\trec:   60.464504\tkl:   25.634346\n",
      "Epoch: 732 [20100/50000 (40%)]  \tLoss:   92.894394\trec:   65.190140\tkl:   27.704247\n",
      "Epoch: 732 [30100/50000 (60%)]  \tLoss:   93.179161\trec:   64.878998\tkl:   28.300165\n",
      "Epoch: 732 [40100/50000 (80%)]  \tLoss:   89.847511\trec:   62.453838\tkl:   27.393671\n",
      "====> Epoch: 732 Average train loss: 89.9751\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4333\n",
      "Epoch: 733 [  100/50000 ( 0%)]  \tLoss:   86.532906\trec:   60.209873\tkl:   26.323038\n",
      "Epoch: 733 [10100/50000 (20%)]  \tLoss:   86.388199\trec:   60.405762\tkl:   25.982441\n",
      "Epoch: 733 [20100/50000 (40%)]  \tLoss:   91.038162\trec:   63.360291\tkl:   27.677870\n",
      "Epoch: 733 [30100/50000 (60%)]  \tLoss:   86.446190\trec:   60.625420\tkl:   25.820774\n",
      "Epoch: 733 [40100/50000 (80%)]  \tLoss:   91.129898\trec:   64.407600\tkl:   26.722298\n",
      "====> Epoch: 733 Average train loss: 89.9640\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4481\n",
      "Epoch: 734 [  100/50000 ( 0%)]  \tLoss:   89.716736\trec:   62.758846\tkl:   26.957893\n",
      "Epoch: 734 [10100/50000 (20%)]  \tLoss:   89.721443\trec:   62.483444\tkl:   27.238003\n",
      "Epoch: 734 [20100/50000 (40%)]  \tLoss:   90.731041\trec:   64.038765\tkl:   26.692278\n",
      "Epoch: 734 [30100/50000 (60%)]  \tLoss:   89.409233\trec:   62.214725\tkl:   27.194510\n",
      "Epoch: 734 [40100/50000 (80%)]  \tLoss:   90.251022\trec:   63.281425\tkl:   26.969597\n",
      "====> Epoch: 734 Average train loss: 89.9589\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3981\n",
      "Epoch: 735 [  100/50000 ( 0%)]  \tLoss:   86.959541\trec:   60.175240\tkl:   26.784302\n",
      "Epoch: 735 [10100/50000 (20%)]  \tLoss:   86.939316\trec:   60.256367\tkl:   26.682951\n",
      "Epoch: 735 [20100/50000 (40%)]  \tLoss:   88.686676\trec:   62.191887\tkl:   26.494791\n",
      "Epoch: 735 [30100/50000 (60%)]  \tLoss:   91.384186\trec:   63.809490\tkl:   27.574699\n",
      "Epoch: 735 [40100/50000 (80%)]  \tLoss:   88.055641\trec:   61.352673\tkl:   26.702969\n",
      "====> Epoch: 735 Average train loss: 89.9774\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4799\n",
      "Epoch: 736 [  100/50000 ( 0%)]  \tLoss:   88.011055\trec:   61.157234\tkl:   26.853813\n",
      "Epoch: 736 [10100/50000 (20%)]  \tLoss:   90.375397\trec:   63.498325\tkl:   26.877075\n",
      "Epoch: 736 [20100/50000 (40%)]  \tLoss:   87.758041\trec:   61.084393\tkl:   26.673655\n",
      "Epoch: 736 [30100/50000 (60%)]  \tLoss:   89.316582\trec:   62.319805\tkl:   26.996775\n",
      "Epoch: 736 [40100/50000 (80%)]  \tLoss:   88.831055\trec:   61.278332\tkl:   27.552719\n",
      "====> Epoch: 736 Average train loss: 89.9843\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3363\n",
      "Epoch: 737 [  100/50000 ( 0%)]  \tLoss:   85.447105\trec:   59.628788\tkl:   25.818317\n",
      "Epoch: 737 [10100/50000 (20%)]  \tLoss:   88.963280\trec:   62.127480\tkl:   26.835798\n",
      "Epoch: 737 [20100/50000 (40%)]  \tLoss:   89.326073\trec:   61.561615\tkl:   27.764460\n",
      "Epoch: 737 [30100/50000 (60%)]  \tLoss:   90.776695\trec:   63.783623\tkl:   26.993071\n",
      "Epoch: 737 [40100/50000 (80%)]  \tLoss:   92.144585\trec:   64.377029\tkl:   27.767561\n",
      "====> Epoch: 737 Average train loss: 89.9664\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4956\n",
      "Epoch: 738 [  100/50000 ( 0%)]  \tLoss:   87.371490\trec:   61.353821\tkl:   26.017668\n",
      "Epoch: 738 [10100/50000 (20%)]  \tLoss:   90.760468\trec:   63.294281\tkl:   27.466181\n",
      "Epoch: 738 [20100/50000 (40%)]  \tLoss:   87.592400\trec:   61.516582\tkl:   26.075815\n",
      "Epoch: 738 [30100/50000 (60%)]  \tLoss:   91.206421\trec:   64.679390\tkl:   26.527035\n",
      "Epoch: 738 [40100/50000 (80%)]  \tLoss:   88.601929\trec:   61.632587\tkl:   26.969345\n",
      "====> Epoch: 738 Average train loss: 89.9529\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4705\n",
      "Epoch: 739 [  100/50000 ( 0%)]  \tLoss:   89.938911\trec:   62.766628\tkl:   27.172281\n",
      "Epoch: 739 [10100/50000 (20%)]  \tLoss:   90.460388\trec:   63.174187\tkl:   27.286203\n",
      "Epoch: 739 [20100/50000 (40%)]  \tLoss:   92.889015\trec:   65.326408\tkl:   27.562605\n",
      "Epoch: 739 [30100/50000 (60%)]  \tLoss:   95.270935\trec:   67.461754\tkl:   27.809177\n",
      "Epoch: 739 [40100/50000 (80%)]  \tLoss:   90.757431\trec:   62.753075\tkl:   28.004353\n",
      "====> Epoch: 739 Average train loss: 89.9555\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4222\n",
      "Epoch: 740 [  100/50000 ( 0%)]  \tLoss:   90.829956\trec:   63.509438\tkl:   27.320524\n",
      "Epoch: 740 [10100/50000 (20%)]  \tLoss:   85.885529\trec:   59.735897\tkl:   26.149626\n",
      "Epoch: 740 [20100/50000 (40%)]  \tLoss:   86.648766\trec:   60.384609\tkl:   26.264160\n",
      "Epoch: 740 [30100/50000 (60%)]  \tLoss:   92.035370\trec:   65.316292\tkl:   26.719072\n",
      "Epoch: 740 [40100/50000 (80%)]  \tLoss:   89.110756\trec:   62.120953\tkl:   26.989807\n",
      "====> Epoch: 740 Average train loss: 89.9637\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4786\n",
      "Epoch: 741 [  100/50000 ( 0%)]  \tLoss:   88.804520\trec:   62.283379\tkl:   26.521139\n",
      "Epoch: 741 [10100/50000 (20%)]  \tLoss:   90.261017\trec:   62.778362\tkl:   27.482656\n",
      "Epoch: 741 [20100/50000 (40%)]  \tLoss:   94.311150\trec:   67.227737\tkl:   27.083410\n",
      "Epoch: 741 [30100/50000 (60%)]  \tLoss:   88.575760\trec:   62.139263\tkl:   26.436499\n",
      "Epoch: 741 [40100/50000 (80%)]  \tLoss:   88.111679\trec:   61.768562\tkl:   26.343113\n",
      "====> Epoch: 741 Average train loss: 89.9547\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3337\n",
      "Epoch: 742 [  100/50000 ( 0%)]  \tLoss:   92.698883\trec:   65.773239\tkl:   26.925638\n",
      "Epoch: 742 [10100/50000 (20%)]  \tLoss:   85.480453\trec:   58.859188\tkl:   26.621271\n",
      "Epoch: 742 [20100/50000 (40%)]  \tLoss:   90.821564\trec:   63.744419\tkl:   27.077148\n",
      "Epoch: 742 [30100/50000 (60%)]  \tLoss:   89.810448\trec:   63.450272\tkl:   26.360172\n",
      "Epoch: 742 [40100/50000 (80%)]  \tLoss:   90.000473\trec:   62.978172\tkl:   27.022306\n",
      "====> Epoch: 742 Average train loss: 89.9438\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4347\n",
      "Epoch: 743 [  100/50000 ( 0%)]  \tLoss:   92.265739\trec:   64.060211\tkl:   28.205523\n",
      "Epoch: 743 [10100/50000 (20%)]  \tLoss:   89.728218\trec:   62.577156\tkl:   27.151062\n",
      "Epoch: 743 [20100/50000 (40%)]  \tLoss:   91.857368\trec:   64.259666\tkl:   27.597708\n",
      "Epoch: 743 [30100/50000 (60%)]  \tLoss:   91.119904\trec:   64.367752\tkl:   26.752144\n",
      "Epoch: 743 [40100/50000 (80%)]  \tLoss:   86.921036\trec:   60.099255\tkl:   26.821779\n",
      "====> Epoch: 743 Average train loss: 89.9520\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3805\n",
      "Epoch: 744 [  100/50000 ( 0%)]  \tLoss:   89.852394\trec:   63.151600\tkl:   26.700792\n",
      "Epoch: 744 [10100/50000 (20%)]  \tLoss:   91.919685\trec:   64.820847\tkl:   27.098837\n",
      "Epoch: 744 [20100/50000 (40%)]  \tLoss:   88.641464\trec:   60.951702\tkl:   27.689760\n",
      "Epoch: 744 [30100/50000 (60%)]  \tLoss:   91.151001\trec:   64.005249\tkl:   27.145754\n",
      "Epoch: 744 [40100/50000 (80%)]  \tLoss:   88.375137\trec:   62.724922\tkl:   25.650217\n",
      "====> Epoch: 744 Average train loss: 89.9564\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3766\n",
      "Epoch: 745 [  100/50000 ( 0%)]  \tLoss:   88.911148\trec:   62.079853\tkl:   26.831301\n",
      "Epoch: 745 [10100/50000 (20%)]  \tLoss:   87.659813\trec:   61.021965\tkl:   26.637846\n",
      "Epoch: 745 [20100/50000 (40%)]  \tLoss:   88.885544\trec:   62.398769\tkl:   26.486778\n",
      "Epoch: 745 [30100/50000 (60%)]  \tLoss:   91.070541\trec:   63.282993\tkl:   27.787554\n",
      "Epoch: 745 [40100/50000 (80%)]  \tLoss:   88.924995\trec:   61.803398\tkl:   27.121601\n",
      "====> Epoch: 745 Average train loss: 89.9452\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4326\n",
      "Epoch: 746 [  100/50000 ( 0%)]  \tLoss:   90.481766\trec:   63.561306\tkl:   26.920462\n",
      "Epoch: 746 [10100/50000 (20%)]  \tLoss:   92.433289\trec:   65.157135\tkl:   27.276150\n",
      "Epoch: 746 [20100/50000 (40%)]  \tLoss:   89.963921\trec:   62.681335\tkl:   27.282587\n",
      "Epoch: 746 [30100/50000 (60%)]  \tLoss:   94.296249\trec:   67.686661\tkl:   26.609587\n",
      "Epoch: 746 [40100/50000 (80%)]  \tLoss:   89.113785\trec:   62.756405\tkl:   26.357380\n",
      "====> Epoch: 746 Average train loss: 89.9374\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4258\n",
      "Epoch: 747 [  100/50000 ( 0%)]  \tLoss:   88.651573\trec:   61.598511\tkl:   27.053059\n",
      "Epoch: 747 [10100/50000 (20%)]  \tLoss:   90.221245\trec:   62.639771\tkl:   27.581478\n",
      "Epoch: 747 [20100/50000 (40%)]  \tLoss:   86.971329\trec:   60.854061\tkl:   26.117262\n",
      "Epoch: 747 [30100/50000 (60%)]  \tLoss:   89.021599\trec:   62.691475\tkl:   26.330122\n",
      "Epoch: 747 [40100/50000 (80%)]  \tLoss:   88.780502\trec:   61.374237\tkl:   27.406263\n",
      "====> Epoch: 747 Average train loss: 89.9268\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5124\n",
      "Epoch: 748 [  100/50000 ( 0%)]  \tLoss:   90.451447\trec:   63.065971\tkl:   27.385468\n",
      "Epoch: 748 [10100/50000 (20%)]  \tLoss:   90.580597\trec:   63.271618\tkl:   27.308973\n",
      "Epoch: 748 [20100/50000 (40%)]  \tLoss:   92.453751\trec:   64.622910\tkl:   27.830839\n",
      "Epoch: 748 [30100/50000 (60%)]  \tLoss:   91.540497\trec:   65.060684\tkl:   26.479809\n",
      "Epoch: 748 [40100/50000 (80%)]  \tLoss:   90.567421\trec:   63.255108\tkl:   27.312313\n",
      "====> Epoch: 748 Average train loss: 89.9234\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3818\n",
      "Epoch: 749 [  100/50000 ( 0%)]  \tLoss:   87.946877\trec:   61.289436\tkl:   26.657440\n",
      "Epoch: 749 [10100/50000 (20%)]  \tLoss:   89.649292\trec:   62.574608\tkl:   27.074684\n",
      "Epoch: 749 [20100/50000 (40%)]  \tLoss:   91.253258\trec:   64.961807\tkl:   26.291451\n",
      "Epoch: 749 [30100/50000 (60%)]  \tLoss:   94.504707\trec:   66.539757\tkl:   27.964949\n",
      "Epoch: 749 [40100/50000 (80%)]  \tLoss:   88.837662\trec:   62.037479\tkl:   26.800182\n",
      "====> Epoch: 749 Average train loss: 89.9302\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3534\n",
      "Epoch: 750 [  100/50000 ( 0%)]  \tLoss:   92.480125\trec:   65.146072\tkl:   27.334053\n",
      "Epoch: 750 [10100/50000 (20%)]  \tLoss:   92.243408\trec:   65.170311\tkl:   27.073097\n",
      "Epoch: 750 [20100/50000 (40%)]  \tLoss:   90.438477\trec:   63.345554\tkl:   27.092916\n",
      "Epoch: 750 [30100/50000 (60%)]  \tLoss:   87.851303\trec:   60.815662\tkl:   27.035646\n",
      "Epoch: 750 [40100/50000 (80%)]  \tLoss:   89.109100\trec:   61.925816\tkl:   27.183283\n",
      "====> Epoch: 750 Average train loss: 89.9091\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3714\n",
      "Epoch: 751 [  100/50000 ( 0%)]  \tLoss:   90.031128\trec:   63.163738\tkl:   26.867393\n",
      "Epoch: 751 [10100/50000 (20%)]  \tLoss:   85.503006\trec:   58.869061\tkl:   26.633945\n",
      "Epoch: 751 [20100/50000 (40%)]  \tLoss:   89.901588\trec:   62.795654\tkl:   27.105936\n",
      "Epoch: 751 [30100/50000 (60%)]  \tLoss:   89.696220\trec:   63.013538\tkl:   26.682680\n",
      "Epoch: 751 [40100/50000 (80%)]  \tLoss:   87.379761\trec:   60.788303\tkl:   26.591461\n",
      "====> Epoch: 751 Average train loss: 89.9373\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3752\n",
      "Epoch: 752 [  100/50000 ( 0%)]  \tLoss:   88.709587\trec:   62.389336\tkl:   26.320248\n",
      "Epoch: 752 [10100/50000 (20%)]  \tLoss:   91.349709\trec:   64.156258\tkl:   27.193441\n",
      "Epoch: 752 [20100/50000 (40%)]  \tLoss:   91.017578\trec:   63.476082\tkl:   27.541498\n",
      "Epoch: 752 [30100/50000 (60%)]  \tLoss:   87.819763\trec:   62.022953\tkl:   25.796816\n",
      "Epoch: 752 [40100/50000 (80%)]  \tLoss:   87.434158\trec:   61.847694\tkl:   25.586470\n",
      "====> Epoch: 752 Average train loss: 89.9122\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4744\n",
      "Epoch: 753 [  100/50000 ( 0%)]  \tLoss:   90.346230\trec:   62.758797\tkl:   27.587431\n",
      "Epoch: 753 [10100/50000 (20%)]  \tLoss:   87.954033\trec:   60.874306\tkl:   27.079729\n",
      "Epoch: 753 [20100/50000 (40%)]  \tLoss:   91.558708\trec:   64.121796\tkl:   27.436913\n",
      "Epoch: 753 [30100/50000 (60%)]  \tLoss:   90.208992\trec:   63.407131\tkl:   26.801859\n",
      "Epoch: 753 [40100/50000 (80%)]  \tLoss:   90.373932\trec:   63.587360\tkl:   26.786573\n",
      "====> Epoch: 753 Average train loss: 89.9128\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3274\n",
      "Epoch: 754 [  100/50000 ( 0%)]  \tLoss:   86.361763\trec:   59.681866\tkl:   26.679901\n",
      "Epoch: 754 [10100/50000 (20%)]  \tLoss:   87.950417\trec:   61.037685\tkl:   26.912733\n",
      "Epoch: 754 [20100/50000 (40%)]  \tLoss:   93.277641\trec:   66.182327\tkl:   27.095320\n",
      "Epoch: 754 [30100/50000 (60%)]  \tLoss:   87.029236\trec:   59.757236\tkl:   27.271996\n",
      "Epoch: 754 [40100/50000 (80%)]  \tLoss:   89.947182\trec:   63.285618\tkl:   26.661572\n",
      "====> Epoch: 754 Average train loss: 89.9430\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5078\n",
      "Epoch: 755 [  100/50000 ( 0%)]  \tLoss:   89.758415\trec:   62.376144\tkl:   27.382271\n",
      "Epoch: 755 [10100/50000 (20%)]  \tLoss:   85.901611\trec:   60.213299\tkl:   25.688311\n",
      "Epoch: 755 [20100/50000 (40%)]  \tLoss:   90.412910\trec:   63.099686\tkl:   27.313225\n",
      "Epoch: 755 [30100/50000 (60%)]  \tLoss:   87.695251\trec:   60.870686\tkl:   26.824568\n",
      "Epoch: 755 [40100/50000 (80%)]  \tLoss:   81.624634\trec:   56.764812\tkl:   24.859819\n",
      "====> Epoch: 755 Average train loss: 89.9036\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.5092\n",
      "Epoch: 756 [  100/50000 ( 0%)]  \tLoss:   92.836823\trec:   64.881210\tkl:   27.955612\n",
      "Epoch: 756 [10100/50000 (20%)]  \tLoss:   95.421280\trec:   66.586006\tkl:   28.835276\n",
      "Epoch: 756 [20100/50000 (40%)]  \tLoss:   90.150177\trec:   64.097023\tkl:   26.053148\n",
      "Epoch: 756 [30100/50000 (60%)]  \tLoss:   90.704185\trec:   63.644516\tkl:   27.059669\n",
      "Epoch: 756 [40100/50000 (80%)]  \tLoss:   90.788376\trec:   63.179157\tkl:   27.609213\n",
      "====> Epoch: 756 Average train loss: 89.9217\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3811\n",
      "Epoch: 757 [  100/50000 ( 0%)]  \tLoss:   86.165466\trec:   60.646679\tkl:   25.518784\n",
      "Epoch: 757 [10100/50000 (20%)]  \tLoss:   90.593666\trec:   63.858154\tkl:   26.735518\n",
      "Epoch: 757 [20100/50000 (40%)]  \tLoss:   89.864357\trec:   63.274014\tkl:   26.590343\n",
      "Epoch: 757 [30100/50000 (60%)]  \tLoss:   89.651932\trec:   62.107811\tkl:   27.544121\n",
      "Epoch: 757 [40100/50000 (80%)]  \tLoss:   86.903374\trec:   61.018608\tkl:   25.884775\n",
      "====> Epoch: 757 Average train loss: 89.8958\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3520\n",
      "Epoch: 758 [  100/50000 ( 0%)]  \tLoss:   87.983437\trec:   61.898418\tkl:   26.085018\n",
      "Epoch: 758 [10100/50000 (20%)]  \tLoss:   92.612106\trec:   65.695274\tkl:   26.916830\n",
      "Epoch: 758 [20100/50000 (40%)]  \tLoss:   93.456345\trec:   65.919731\tkl:   27.536612\n",
      "Epoch: 758 [30100/50000 (60%)]  \tLoss:   92.639854\trec:   65.560448\tkl:   27.079403\n",
      "Epoch: 758 [40100/50000 (80%)]  \tLoss:   92.197807\trec:   65.241127\tkl:   26.956688\n",
      "====> Epoch: 758 Average train loss: 89.8952\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4532\n",
      "Epoch: 759 [  100/50000 ( 0%)]  \tLoss:   87.212364\trec:   59.983982\tkl:   27.228378\n",
      "Epoch: 759 [10100/50000 (20%)]  \tLoss:   91.115524\trec:   64.506203\tkl:   26.609331\n",
      "Epoch: 759 [20100/50000 (40%)]  \tLoss:   91.293404\trec:   64.524483\tkl:   26.768923\n",
      "Epoch: 759 [30100/50000 (60%)]  \tLoss:   89.735229\trec:   63.202438\tkl:   26.532793\n",
      "Epoch: 759 [40100/50000 (80%)]  \tLoss:   89.671951\trec:   62.723747\tkl:   26.948202\n",
      "====> Epoch: 759 Average train loss: 89.9343\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3423\n",
      "Epoch: 760 [  100/50000 ( 0%)]  \tLoss:   89.069946\trec:   61.931557\tkl:   27.138393\n",
      "Epoch: 760 [10100/50000 (20%)]  \tLoss:   87.200768\trec:   60.623104\tkl:   26.577665\n",
      "Epoch: 760 [20100/50000 (40%)]  \tLoss:   89.740143\trec:   62.281082\tkl:   27.459063\n",
      "Epoch: 760 [30100/50000 (60%)]  \tLoss:   88.765953\trec:   62.171474\tkl:   26.594486\n",
      "Epoch: 760 [40100/50000 (80%)]  \tLoss:   90.359726\trec:   62.618690\tkl:   27.741030\n",
      "====> Epoch: 760 Average train loss: 89.8851\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4502\n",
      "Epoch: 761 [  100/50000 ( 0%)]  \tLoss:   89.618736\trec:   62.704891\tkl:   26.913845\n",
      "Epoch: 761 [10100/50000 (20%)]  \tLoss:   91.974380\trec:   64.528923\tkl:   27.445463\n",
      "Epoch: 761 [20100/50000 (40%)]  \tLoss:   88.836922\trec:   62.257038\tkl:   26.579882\n",
      "Epoch: 761 [30100/50000 (60%)]  \tLoss:   94.355270\trec:   67.219551\tkl:   27.135721\n",
      "Epoch: 761 [40100/50000 (80%)]  \tLoss:   90.875259\trec:   63.828327\tkl:   27.046930\n",
      "====> Epoch: 761 Average train loss: 89.8912\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3296\n",
      "Epoch: 762 [  100/50000 ( 0%)]  \tLoss:   90.522926\trec:   63.500263\tkl:   27.022671\n",
      "Epoch: 762 [10100/50000 (20%)]  \tLoss:   89.745079\trec:   62.633133\tkl:   27.111938\n",
      "Epoch: 762 [20100/50000 (40%)]  \tLoss:   87.293503\trec:   60.146042\tkl:   27.147461\n",
      "Epoch: 762 [30100/50000 (60%)]  \tLoss:   86.818993\trec:   59.495258\tkl:   27.323734\n",
      "Epoch: 762 [40100/50000 (80%)]  \tLoss:   90.310349\trec:   64.152863\tkl:   26.157490\n",
      "====> Epoch: 762 Average train loss: 89.8883\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4390\n",
      "Epoch: 763 [  100/50000 ( 0%)]  \tLoss:   89.987442\trec:   62.764236\tkl:   27.223202\n",
      "Epoch: 763 [10100/50000 (20%)]  \tLoss:   90.298126\trec:   63.106194\tkl:   27.191925\n",
      "Epoch: 763 [20100/50000 (40%)]  \tLoss:   90.160622\trec:   63.559170\tkl:   26.601460\n",
      "Epoch: 763 [30100/50000 (60%)]  \tLoss:   91.810699\trec:   64.508324\tkl:   27.302372\n",
      "Epoch: 763 [40100/50000 (80%)]  \tLoss:   94.303291\trec:   66.331123\tkl:   27.972164\n",
      "====> Epoch: 763 Average train loss: 89.9043\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4392\n",
      "Epoch: 764 [  100/50000 ( 0%)]  \tLoss:   92.347839\trec:   64.051758\tkl:   28.296082\n",
      "Epoch: 764 [10100/50000 (20%)]  \tLoss:   89.165436\trec:   61.790329\tkl:   27.375107\n",
      "Epoch: 764 [20100/50000 (40%)]  \tLoss:   88.364288\trec:   61.515881\tkl:   26.848400\n",
      "Epoch: 764 [30100/50000 (60%)]  \tLoss:   88.631798\trec:   61.737530\tkl:   26.894270\n",
      "Epoch: 764 [40100/50000 (80%)]  \tLoss:   90.600487\trec:   63.715946\tkl:   26.884541\n",
      "====> Epoch: 764 Average train loss: 89.8911\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2414\n",
      "Epoch: 765 [  100/50000 ( 0%)]  \tLoss:   92.015388\trec:   64.215927\tkl:   27.799458\n",
      "Epoch: 765 [10100/50000 (20%)]  \tLoss:   93.800972\trec:   65.682693\tkl:   28.118282\n",
      "Epoch: 765 [20100/50000 (40%)]  \tLoss:   94.784676\trec:   67.852905\tkl:   26.931770\n",
      "Epoch: 765 [30100/50000 (60%)]  \tLoss:   92.587097\trec:   64.511787\tkl:   28.075314\n",
      "Epoch: 765 [40100/50000 (80%)]  \tLoss:   91.642441\trec:   64.200752\tkl:   27.441685\n",
      "====> Epoch: 765 Average train loss: 89.8985\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4195\n",
      "Epoch: 766 [  100/50000 ( 0%)]  \tLoss:   92.620232\trec:   65.529793\tkl:   27.090441\n",
      "Epoch: 766 [10100/50000 (20%)]  \tLoss:   90.090263\trec:   62.828247\tkl:   27.262018\n",
      "Epoch: 766 [20100/50000 (40%)]  \tLoss:   90.114532\trec:   63.570454\tkl:   26.544075\n",
      "Epoch: 766 [30100/50000 (60%)]  \tLoss:   86.014099\trec:   59.804726\tkl:   26.209372\n",
      "Epoch: 766 [40100/50000 (80%)]  \tLoss:   91.228394\trec:   64.003227\tkl:   27.225168\n",
      "====> Epoch: 766 Average train loss: 89.8616\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3950\n",
      "Epoch: 767 [  100/50000 ( 0%)]  \tLoss:   86.419968\trec:   60.379002\tkl:   26.040966\n",
      "Epoch: 767 [10100/50000 (20%)]  \tLoss:   89.760338\trec:   62.376743\tkl:   27.383600\n",
      "Epoch: 767 [20100/50000 (40%)]  \tLoss:   88.293701\trec:   61.478691\tkl:   26.815006\n",
      "Epoch: 767 [30100/50000 (60%)]  \tLoss:   86.793922\trec:   60.189556\tkl:   26.604374\n",
      "Epoch: 767 [40100/50000 (80%)]  \tLoss:   89.564606\trec:   63.096203\tkl:   26.468397\n",
      "====> Epoch: 767 Average train loss: 89.8880\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3788\n",
      "Epoch: 768 [  100/50000 ( 0%)]  \tLoss:   88.221542\trec:   61.779686\tkl:   26.441860\n",
      "Epoch: 768 [10100/50000 (20%)]  \tLoss:   86.728394\trec:   60.326355\tkl:   26.402046\n",
      "Epoch: 768 [20100/50000 (40%)]  \tLoss:   93.902245\trec:   65.864647\tkl:   28.037600\n",
      "Epoch: 768 [30100/50000 (60%)]  \tLoss:   90.858505\trec:   63.913387\tkl:   26.945116\n",
      "Epoch: 768 [40100/50000 (80%)]  \tLoss:   86.845856\trec:   60.051403\tkl:   26.794456\n",
      "====> Epoch: 768 Average train loss: 89.8664\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3945\n",
      "Epoch: 769 [  100/50000 ( 0%)]  \tLoss:   85.482735\trec:   59.243847\tkl:   26.238888\n",
      "Epoch: 769 [10100/50000 (20%)]  \tLoss:   91.516930\trec:   64.194122\tkl:   27.322815\n",
      "Epoch: 769 [20100/50000 (40%)]  \tLoss:   87.155724\trec:   60.834797\tkl:   26.320923\n",
      "Epoch: 769 [30100/50000 (60%)]  \tLoss:   91.284218\trec:   63.733929\tkl:   27.550293\n",
      "Epoch: 769 [40100/50000 (80%)]  \tLoss:   90.492538\trec:   63.927971\tkl:   26.564562\n",
      "====> Epoch: 769 Average train loss: 89.8928\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3928\n",
      "Epoch: 770 [  100/50000 ( 0%)]  \tLoss:   90.824257\trec:   63.248230\tkl:   27.576027\n",
      "Epoch: 770 [10100/50000 (20%)]  \tLoss:   90.314079\trec:   63.377975\tkl:   26.936098\n",
      "Epoch: 770 [20100/50000 (40%)]  \tLoss:   91.238716\trec:   63.145508\tkl:   28.093212\n",
      "Epoch: 770 [30100/50000 (60%)]  \tLoss:   88.282639\trec:   62.266048\tkl:   26.016586\n",
      "Epoch: 770 [40100/50000 (80%)]  \tLoss:   93.133522\trec:   66.481148\tkl:   26.652372\n",
      "====> Epoch: 770 Average train loss: 89.8596\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3852\n",
      "Epoch: 771 [  100/50000 ( 0%)]  \tLoss:   87.457504\trec:   61.197144\tkl:   26.260366\n",
      "Epoch: 771 [10100/50000 (20%)]  \tLoss:   88.552208\trec:   61.655785\tkl:   26.896416\n",
      "Epoch: 771 [20100/50000 (40%)]  \tLoss:   91.059746\trec:   63.312778\tkl:   27.746965\n",
      "Epoch: 771 [30100/50000 (60%)]  \tLoss:   91.635445\trec:   64.647392\tkl:   26.988054\n",
      "Epoch: 771 [40100/50000 (80%)]  \tLoss:   88.357254\trec:   62.014771\tkl:   26.342487\n",
      "====> Epoch: 771 Average train loss: 89.8588\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3628\n",
      "Epoch: 772 [  100/50000 ( 0%)]  \tLoss:   92.330666\trec:   65.579430\tkl:   26.751234\n",
      "Epoch: 772 [10100/50000 (20%)]  \tLoss:   87.878906\trec:   60.836792\tkl:   27.042116\n",
      "Epoch: 772 [20100/50000 (40%)]  \tLoss:   89.882370\trec:   62.759354\tkl:   27.123014\n",
      "Epoch: 772 [30100/50000 (60%)]  \tLoss:   93.120674\trec:   65.530624\tkl:   27.590046\n",
      "Epoch: 772 [40100/50000 (80%)]  \tLoss:   86.719025\trec:   59.812057\tkl:   26.906967\n",
      "====> Epoch: 772 Average train loss: 89.8624\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3008\n",
      "Epoch: 773 [  100/50000 ( 0%)]  \tLoss:   91.300758\trec:   63.838020\tkl:   27.462742\n",
      "Epoch: 773 [10100/50000 (20%)]  \tLoss:   92.866852\trec:   65.555122\tkl:   27.311729\n",
      "Epoch: 773 [20100/50000 (40%)]  \tLoss:   91.114761\trec:   63.713524\tkl:   27.401239\n",
      "Epoch: 773 [30100/50000 (60%)]  \tLoss:   91.516502\trec:   64.281372\tkl:   27.235134\n",
      "Epoch: 773 [40100/50000 (80%)]  \tLoss:   89.523621\trec:   61.114124\tkl:   28.409498\n",
      "====> Epoch: 773 Average train loss: 89.8583\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3567\n",
      "Epoch: 774 [  100/50000 ( 0%)]  \tLoss:   89.736824\trec:   62.798321\tkl:   26.938507\n",
      "Epoch: 774 [10100/50000 (20%)]  \tLoss:   93.801369\trec:   66.293350\tkl:   27.508013\n",
      "Epoch: 774 [20100/50000 (40%)]  \tLoss:   86.096893\trec:   60.689541\tkl:   25.407358\n",
      "Epoch: 774 [30100/50000 (60%)]  \tLoss:   91.647476\trec:   63.757412\tkl:   27.890072\n",
      "Epoch: 774 [40100/50000 (80%)]  \tLoss:   90.952469\trec:   63.718212\tkl:   27.234261\n",
      "====> Epoch: 774 Average train loss: 89.8558\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4010\n",
      "Epoch: 775 [  100/50000 ( 0%)]  \tLoss:   91.466896\trec:   64.641739\tkl:   26.825155\n",
      "Epoch: 775 [10100/50000 (20%)]  \tLoss:   89.964615\trec:   63.317917\tkl:   26.646696\n",
      "Epoch: 775 [20100/50000 (40%)]  \tLoss:   92.362770\trec:   64.915421\tkl:   27.447350\n",
      "Epoch: 775 [30100/50000 (60%)]  \tLoss:   91.199814\trec:   63.840942\tkl:   27.358871\n",
      "Epoch: 775 [40100/50000 (80%)]  \tLoss:   88.485352\trec:   62.046169\tkl:   26.439182\n",
      "====> Epoch: 775 Average train loss: 89.8497\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4058\n",
      "Epoch: 776 [  100/50000 ( 0%)]  \tLoss:   91.581619\trec:   64.452095\tkl:   27.129526\n",
      "Epoch: 776 [10100/50000 (20%)]  \tLoss:   92.214722\trec:   63.815701\tkl:   28.399027\n",
      "Epoch: 776 [20100/50000 (40%)]  \tLoss:   87.164894\trec:   60.774834\tkl:   26.390055\n",
      "Epoch: 776 [30100/50000 (60%)]  \tLoss:   88.353035\trec:   61.497322\tkl:   26.855715\n",
      "Epoch: 776 [40100/50000 (80%)]  \tLoss:   89.412315\trec:   62.531250\tkl:   26.881062\n",
      "====> Epoch: 776 Average train loss: 89.8690\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4647\n",
      "Epoch: 777 [  100/50000 ( 0%)]  \tLoss:   89.459976\trec:   62.872890\tkl:   26.587088\n",
      "Epoch: 777 [10100/50000 (20%)]  \tLoss:   90.471542\trec:   63.054188\tkl:   27.417349\n",
      "Epoch: 777 [20100/50000 (40%)]  \tLoss:   88.616508\trec:   62.379040\tkl:   26.237471\n",
      "Epoch: 777 [30100/50000 (60%)]  \tLoss:   93.905914\trec:   66.052223\tkl:   27.853689\n",
      "Epoch: 777 [40100/50000 (80%)]  \tLoss:   84.451401\trec:   58.192734\tkl:   26.258667\n",
      "====> Epoch: 777 Average train loss: 89.8831\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4093\n",
      "Epoch: 778 [  100/50000 ( 0%)]  \tLoss:   89.318985\trec:   62.443035\tkl:   26.875950\n",
      "Epoch: 778 [10100/50000 (20%)]  \tLoss:   90.714119\trec:   62.965839\tkl:   27.748276\n",
      "Epoch: 778 [20100/50000 (40%)]  \tLoss:   88.193184\trec:   61.686394\tkl:   26.506792\n",
      "Epoch: 778 [30100/50000 (60%)]  \tLoss:   91.624367\trec:   65.011208\tkl:   26.613155\n",
      "Epoch: 778 [40100/50000 (80%)]  \tLoss:   92.737793\trec:   65.222221\tkl:   27.515568\n",
      "====> Epoch: 778 Average train loss: 89.8652\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4023\n",
      "Epoch: 779 [  100/50000 ( 0%)]  \tLoss:   89.167580\trec:   61.922680\tkl:   27.244902\n",
      "Epoch: 779 [10100/50000 (20%)]  \tLoss:   87.816093\trec:   61.237988\tkl:   26.578102\n",
      "Epoch: 779 [20100/50000 (40%)]  \tLoss:   92.286430\trec:   64.452339\tkl:   27.834093\n",
      "Epoch: 779 [30100/50000 (60%)]  \tLoss:   88.188248\trec:   61.051888\tkl:   27.136360\n",
      "Epoch: 779 [40100/50000 (80%)]  \tLoss:   90.677612\trec:   64.700005\tkl:   25.977612\n",
      "====> Epoch: 779 Average train loss: 89.8442\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3848\n",
      "Epoch: 780 [  100/50000 ( 0%)]  \tLoss:   90.619606\trec:   63.678299\tkl:   26.941313\n",
      "Epoch: 780 [10100/50000 (20%)]  \tLoss:   92.801125\trec:   64.847198\tkl:   27.953922\n",
      "Epoch: 780 [20100/50000 (40%)]  \tLoss:   89.602890\trec:   63.249565\tkl:   26.353327\n",
      "Epoch: 780 [30100/50000 (60%)]  \tLoss:   91.070122\trec:   64.131065\tkl:   26.939064\n",
      "Epoch: 780 [40100/50000 (80%)]  \tLoss:   89.157379\trec:   62.361382\tkl:   26.795996\n",
      "====> Epoch: 780 Average train loss: 89.8537\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4466\n",
      "Epoch: 781 [  100/50000 ( 0%)]  \tLoss:   88.879585\trec:   62.521179\tkl:   26.358408\n",
      "Epoch: 781 [10100/50000 (20%)]  \tLoss:   93.277626\trec:   65.098953\tkl:   28.178669\n",
      "Epoch: 781 [20100/50000 (40%)]  \tLoss:   87.331284\trec:   60.447044\tkl:   26.884247\n",
      "Epoch: 781 [30100/50000 (60%)]  \tLoss:   89.405815\trec:   63.023972\tkl:   26.381849\n",
      "Epoch: 781 [40100/50000 (80%)]  \tLoss:   90.354073\trec:   63.253983\tkl:   27.100084\n",
      "====> Epoch: 781 Average train loss: 89.8474\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4382\n",
      "Epoch: 782 [  100/50000 ( 0%)]  \tLoss:   89.131752\trec:   61.740898\tkl:   27.390860\n",
      "Epoch: 782 [10100/50000 (20%)]  \tLoss:   89.347916\trec:   62.066845\tkl:   27.281076\n",
      "Epoch: 782 [20100/50000 (40%)]  \tLoss:   86.623962\trec:   59.939129\tkl:   26.684834\n",
      "Epoch: 782 [30100/50000 (60%)]  \tLoss:   92.702087\trec:   65.124138\tkl:   27.577955\n",
      "Epoch: 782 [40100/50000 (80%)]  \tLoss:   87.683159\trec:   61.513927\tkl:   26.169228\n",
      "====> Epoch: 782 Average train loss: 89.8101\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4295\n",
      "Epoch: 783 [  100/50000 ( 0%)]  \tLoss:   90.672821\trec:   63.540806\tkl:   27.132013\n",
      "Epoch: 783 [10100/50000 (20%)]  \tLoss:   93.740875\trec:   65.736549\tkl:   28.004330\n",
      "Epoch: 783 [20100/50000 (40%)]  \tLoss:   95.171227\trec:   67.605309\tkl:   27.565912\n",
      "Epoch: 783 [30100/50000 (60%)]  \tLoss:   91.529297\trec:   64.274170\tkl:   27.255121\n",
      "Epoch: 783 [40100/50000 (80%)]  \tLoss:   90.249702\trec:   63.558208\tkl:   26.691496\n",
      "====> Epoch: 783 Average train loss: 89.8262\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3545\n",
      "Epoch: 784 [  100/50000 ( 0%)]  \tLoss:   86.758438\trec:   60.363640\tkl:   26.394789\n",
      "Epoch: 784 [10100/50000 (20%)]  \tLoss:   88.678886\trec:   62.625801\tkl:   26.053080\n",
      "Epoch: 784 [20100/50000 (40%)]  \tLoss:   90.298988\trec:   63.368359\tkl:   26.930637\n",
      "Epoch: 784 [30100/50000 (60%)]  \tLoss:   89.865974\trec:   63.006439\tkl:   26.859533\n",
      "Epoch: 784 [40100/50000 (80%)]  \tLoss:   91.620880\trec:   64.584404\tkl:   27.036476\n",
      "====> Epoch: 784 Average train loss: 89.8224\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2977\n",
      "Epoch: 785 [  100/50000 ( 0%)]  \tLoss:   89.478287\trec:   62.893944\tkl:   26.584343\n",
      "Epoch: 785 [10100/50000 (20%)]  \tLoss:   90.774239\trec:   63.082588\tkl:   27.691654\n",
      "Epoch: 785 [20100/50000 (40%)]  \tLoss:   92.495644\trec:   64.691734\tkl:   27.803905\n",
      "Epoch: 785 [30100/50000 (60%)]  \tLoss:   94.104980\trec:   65.446602\tkl:   28.658381\n",
      "Epoch: 785 [40100/50000 (80%)]  \tLoss:   89.006599\trec:   61.581863\tkl:   27.424734\n",
      "====> Epoch: 785 Average train loss: 89.8101\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3212\n",
      "Epoch: 786 [  100/50000 ( 0%)]  \tLoss:   85.747284\trec:   59.028816\tkl:   26.718466\n",
      "Epoch: 786 [10100/50000 (20%)]  \tLoss:   91.846230\trec:   64.378952\tkl:   27.467285\n",
      "Epoch: 786 [20100/50000 (40%)]  \tLoss:   90.468895\trec:   63.040989\tkl:   27.427902\n",
      "Epoch: 786 [30100/50000 (60%)]  \tLoss:   88.381798\trec:   61.023525\tkl:   27.358271\n",
      "Epoch: 786 [40100/50000 (80%)]  \tLoss:   90.513847\trec:   64.621719\tkl:   25.892120\n",
      "====> Epoch: 786 Average train loss: 89.8484\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4191\n",
      "Epoch: 787 [  100/50000 ( 0%)]  \tLoss:   88.543663\trec:   61.824200\tkl:   26.719465\n",
      "Epoch: 787 [10100/50000 (20%)]  \tLoss:   86.031036\trec:   60.881618\tkl:   25.149408\n",
      "Epoch: 787 [20100/50000 (40%)]  \tLoss:   86.543457\trec:   59.638069\tkl:   26.905390\n",
      "Epoch: 787 [30100/50000 (60%)]  \tLoss:   92.205956\trec:   64.609665\tkl:   27.596296\n",
      "Epoch: 787 [40100/50000 (80%)]  \tLoss:   90.742538\trec:   63.477642\tkl:   27.264894\n",
      "====> Epoch: 787 Average train loss: 89.8334\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3868\n",
      "Epoch: 788 [  100/50000 ( 0%)]  \tLoss:   89.540993\trec:   61.953407\tkl:   27.587593\n",
      "Epoch: 788 [10100/50000 (20%)]  \tLoss:   90.070312\trec:   62.463245\tkl:   27.607065\n",
      "Epoch: 788 [20100/50000 (40%)]  \tLoss:   88.658279\trec:   61.360077\tkl:   27.298201\n",
      "Epoch: 788 [30100/50000 (60%)]  \tLoss:   92.199318\trec:   65.022873\tkl:   27.176447\n",
      "Epoch: 788 [40100/50000 (80%)]  \tLoss:   89.688103\trec:   61.944126\tkl:   27.743977\n",
      "====> Epoch: 788 Average train loss: 89.8347\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4104\n",
      "Epoch: 789 [  100/50000 ( 0%)]  \tLoss:   88.138809\trec:   61.075291\tkl:   27.063517\n",
      "Epoch: 789 [10100/50000 (20%)]  \tLoss:   94.206093\trec:   66.256096\tkl:   27.949991\n",
      "Epoch: 789 [20100/50000 (40%)]  \tLoss:   89.124695\trec:   61.829945\tkl:   27.294748\n",
      "Epoch: 789 [30100/50000 (60%)]  \tLoss:   86.797653\trec:   60.655071\tkl:   26.142578\n",
      "Epoch: 789 [40100/50000 (80%)]  \tLoss:   91.227829\trec:   64.423309\tkl:   26.804516\n",
      "====> Epoch: 789 Average train loss: 89.8250\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3579\n",
      "Epoch: 790 [  100/50000 ( 0%)]  \tLoss:   86.616013\trec:   60.313824\tkl:   26.302187\n",
      "Epoch: 790 [10100/50000 (20%)]  \tLoss:   87.991348\trec:   61.715195\tkl:   26.276154\n",
      "Epoch: 790 [20100/50000 (40%)]  \tLoss:   89.465340\trec:   61.911800\tkl:   27.553537\n",
      "Epoch: 790 [30100/50000 (60%)]  \tLoss:   89.640282\trec:   62.363895\tkl:   27.276384\n",
      "Epoch: 790 [40100/50000 (80%)]  \tLoss:   92.112289\trec:   64.352623\tkl:   27.759672\n",
      "====> Epoch: 790 Average train loss: 89.8256\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3950\n",
      "Epoch: 791 [  100/50000 ( 0%)]  \tLoss:   89.836357\trec:   62.368881\tkl:   27.467476\n",
      "Epoch: 791 [10100/50000 (20%)]  \tLoss:   91.059921\trec:   63.413803\tkl:   27.646114\n",
      "Epoch: 791 [20100/50000 (40%)]  \tLoss:   91.294136\trec:   64.218369\tkl:   27.075766\n",
      "Epoch: 791 [30100/50000 (60%)]  \tLoss:   89.173737\trec:   62.438637\tkl:   26.735100\n",
      "Epoch: 791 [40100/50000 (80%)]  \tLoss:   86.484627\trec:   60.844028\tkl:   25.640600\n",
      "====> Epoch: 791 Average train loss: 89.8062\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3338\n",
      "Epoch: 792 [  100/50000 ( 0%)]  \tLoss:   91.141747\trec:   63.996716\tkl:   27.145029\n",
      "Epoch: 792 [10100/50000 (20%)]  \tLoss:   87.726112\trec:   60.507362\tkl:   27.218752\n",
      "Epoch: 792 [20100/50000 (40%)]  \tLoss:   87.757637\trec:   60.569118\tkl:   27.188511\n",
      "Epoch: 792 [30100/50000 (60%)]  \tLoss:   88.227325\trec:   61.823750\tkl:   26.403568\n",
      "Epoch: 792 [40100/50000 (80%)]  \tLoss:   90.843361\trec:   63.908760\tkl:   26.934597\n",
      "====> Epoch: 792 Average train loss: 89.8341\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2623\n",
      "Epoch: 793 [  100/50000 ( 0%)]  \tLoss:   91.318634\trec:   63.770130\tkl:   27.548500\n",
      "Epoch: 793 [10100/50000 (20%)]  \tLoss:   91.450821\trec:   63.076435\tkl:   28.374390\n",
      "Epoch: 793 [20100/50000 (40%)]  \tLoss:   92.084419\trec:   64.491211\tkl:   27.593210\n",
      "Epoch: 793 [30100/50000 (60%)]  \tLoss:   86.976074\trec:   60.548187\tkl:   26.427891\n",
      "Epoch: 793 [40100/50000 (80%)]  \tLoss:   92.154022\trec:   64.160362\tkl:   27.993662\n",
      "====> Epoch: 793 Average train loss: 89.8322\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4139\n",
      "Epoch: 794 [  100/50000 ( 0%)]  \tLoss:   91.049347\trec:   63.657673\tkl:   27.391666\n",
      "Epoch: 794 [10100/50000 (20%)]  \tLoss:   91.155846\trec:   64.609245\tkl:   26.546608\n",
      "Epoch: 794 [20100/50000 (40%)]  \tLoss:   88.526337\trec:   62.454403\tkl:   26.071936\n",
      "Epoch: 794 [30100/50000 (60%)]  \tLoss:   86.453995\trec:   60.565067\tkl:   25.888924\n",
      "Epoch: 794 [40100/50000 (80%)]  \tLoss:   87.367149\trec:   60.441570\tkl:   26.925571\n",
      "====> Epoch: 794 Average train loss: 89.7784\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4452\n",
      "Epoch: 795 [  100/50000 ( 0%)]  \tLoss:   87.015312\trec:   60.357967\tkl:   26.657345\n",
      "Epoch: 795 [10100/50000 (20%)]  \tLoss:   93.143105\trec:   65.132172\tkl:   28.010929\n",
      "Epoch: 795 [20100/50000 (40%)]  \tLoss:   89.688210\trec:   63.360264\tkl:   26.327951\n",
      "Epoch: 795 [30100/50000 (60%)]  \tLoss:   87.121101\trec:   60.765476\tkl:   26.355621\n",
      "Epoch: 795 [40100/50000 (80%)]  \tLoss:   86.077187\trec:   60.197197\tkl:   25.879984\n",
      "====> Epoch: 795 Average train loss: 89.8185\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3929\n",
      "Epoch: 796 [  100/50000 ( 0%)]  \tLoss:   93.379257\trec:   65.368629\tkl:   28.010630\n",
      "Epoch: 796 [10100/50000 (20%)]  \tLoss:   85.042412\trec:   58.595810\tkl:   26.446602\n",
      "Epoch: 796 [20100/50000 (40%)]  \tLoss:   90.145439\trec:   63.367729\tkl:   26.777706\n",
      "Epoch: 796 [30100/50000 (60%)]  \tLoss:   92.979050\trec:   65.966400\tkl:   27.012648\n",
      "Epoch: 796 [40100/50000 (80%)]  \tLoss:   95.382324\trec:   67.526909\tkl:   27.855408\n",
      "====> Epoch: 796 Average train loss: 89.7997\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4475\n",
      "Epoch: 797 [  100/50000 ( 0%)]  \tLoss:   91.805397\trec:   64.950066\tkl:   26.855333\n",
      "Epoch: 797 [10100/50000 (20%)]  \tLoss:   91.563210\trec:   64.108444\tkl:   27.454767\n",
      "Epoch: 797 [20100/50000 (40%)]  \tLoss:   90.861961\trec:   64.866989\tkl:   25.994978\n",
      "Epoch: 797 [30100/50000 (60%)]  \tLoss:   90.567871\trec:   63.477058\tkl:   27.090811\n",
      "Epoch: 797 [40100/50000 (80%)]  \tLoss:   87.546776\trec:   61.203438\tkl:   26.343344\n",
      "====> Epoch: 797 Average train loss: 89.8243\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3448\n",
      "Epoch: 798 [  100/50000 ( 0%)]  \tLoss:   86.139931\trec:   59.407211\tkl:   26.732721\n",
      "Epoch: 798 [10100/50000 (20%)]  \tLoss:   91.733185\trec:   64.364677\tkl:   27.368498\n",
      "Epoch: 798 [20100/50000 (40%)]  \tLoss:   90.323143\trec:   62.732403\tkl:   27.590742\n",
      "Epoch: 798 [30100/50000 (60%)]  \tLoss:   86.109299\trec:   59.964001\tkl:   26.145290\n",
      "Epoch: 798 [40100/50000 (80%)]  \tLoss:   92.842125\trec:   65.414734\tkl:   27.427395\n",
      "====> Epoch: 798 Average train loss: 89.7847\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3435\n",
      "Epoch: 799 [  100/50000 ( 0%)]  \tLoss:   91.942497\trec:   64.315163\tkl:   27.627331\n",
      "Epoch: 799 [10100/50000 (20%)]  \tLoss:   93.291328\trec:   65.437469\tkl:   27.853851\n",
      "Epoch: 799 [20100/50000 (40%)]  \tLoss:   86.825066\trec:   61.180107\tkl:   25.644958\n",
      "Epoch: 799 [30100/50000 (60%)]  \tLoss:   89.293053\trec:   62.484226\tkl:   26.808828\n",
      "Epoch: 799 [40100/50000 (80%)]  \tLoss:   87.862762\trec:   61.392460\tkl:   26.470303\n",
      "====> Epoch: 799 Average train loss: 89.7787\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3946\n",
      "Epoch: 800 [  100/50000 ( 0%)]  \tLoss:   88.926056\trec:   61.554417\tkl:   27.371637\n",
      "Epoch: 800 [10100/50000 (20%)]  \tLoss:   91.299736\trec:   63.813435\tkl:   27.486301\n",
      "Epoch: 800 [20100/50000 (40%)]  \tLoss:   90.078018\trec:   62.936123\tkl:   27.141893\n",
      "Epoch: 800 [30100/50000 (60%)]  \tLoss:   91.338837\trec:   64.599991\tkl:   26.738844\n",
      "Epoch: 800 [40100/50000 (80%)]  \tLoss:   90.428650\trec:   63.366184\tkl:   27.062460\n",
      "====> Epoch: 800 Average train loss: 89.7811\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3340\n",
      "Epoch: 801 [  100/50000 ( 0%)]  \tLoss:   87.588867\trec:   60.492264\tkl:   27.096603\n",
      "Epoch: 801 [10100/50000 (20%)]  \tLoss:   90.721680\trec:   63.304676\tkl:   27.417006\n",
      "Epoch: 801 [20100/50000 (40%)]  \tLoss:   86.316139\trec:   58.966644\tkl:   27.349497\n",
      "Epoch: 801 [30100/50000 (60%)]  \tLoss:   84.287979\trec:   58.080448\tkl:   26.207527\n",
      "Epoch: 801 [40100/50000 (80%)]  \tLoss:   88.493393\trec:   61.592209\tkl:   26.901180\n",
      "====> Epoch: 801 Average train loss: 89.7970\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3313\n",
      "Epoch: 802 [  100/50000 ( 0%)]  \tLoss:   87.270378\trec:   61.059185\tkl:   26.211199\n",
      "Epoch: 802 [10100/50000 (20%)]  \tLoss:   87.173927\trec:   60.974834\tkl:   26.199091\n",
      "Epoch: 802 [20100/50000 (40%)]  \tLoss:   90.189629\trec:   61.585674\tkl:   28.603952\n",
      "Epoch: 802 [30100/50000 (60%)]  \tLoss:   91.294571\trec:   63.902508\tkl:   27.392056\n",
      "Epoch: 802 [40100/50000 (80%)]  \tLoss:   92.166237\trec:   65.194931\tkl:   26.971306\n",
      "====> Epoch: 802 Average train loss: 89.7824\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3216\n",
      "Epoch: 803 [  100/50000 ( 0%)]  \tLoss:   85.788971\trec:   59.625389\tkl:   26.163582\n",
      "Epoch: 803 [10100/50000 (20%)]  \tLoss:   86.850761\trec:   60.051971\tkl:   26.798788\n",
      "Epoch: 803 [20100/50000 (40%)]  \tLoss:   86.950592\trec:   60.166355\tkl:   26.784241\n",
      "Epoch: 803 [30100/50000 (60%)]  \tLoss:   90.607040\trec:   63.051521\tkl:   27.555517\n",
      "Epoch: 803 [40100/50000 (80%)]  \tLoss:   87.303688\trec:   60.812485\tkl:   26.491201\n",
      "====> Epoch: 803 Average train loss: 89.7724\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4235\n",
      "Epoch: 804 [  100/50000 ( 0%)]  \tLoss:   89.353813\trec:   61.793537\tkl:   27.560280\n",
      "Epoch: 804 [10100/50000 (20%)]  \tLoss:   93.368942\trec:   64.735611\tkl:   28.633324\n",
      "Epoch: 804 [20100/50000 (40%)]  \tLoss:   94.486443\trec:   67.378174\tkl:   27.108269\n",
      "Epoch: 804 [30100/50000 (60%)]  \tLoss:   89.814125\trec:   63.123066\tkl:   26.691063\n",
      "Epoch: 804 [40100/50000 (80%)]  \tLoss:   84.903084\trec:   58.520000\tkl:   26.383080\n",
      "====> Epoch: 804 Average train loss: 89.7575\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2937\n",
      "Epoch: 805 [  100/50000 ( 0%)]  \tLoss:   88.700241\trec:   61.903694\tkl:   26.796545\n",
      "Epoch: 805 [10100/50000 (20%)]  \tLoss:   87.420128\trec:   60.236397\tkl:   27.183729\n",
      "Epoch: 805 [20100/50000 (40%)]  \tLoss:   89.519196\trec:   62.746017\tkl:   26.773184\n",
      "Epoch: 805 [30100/50000 (60%)]  \tLoss:   86.435059\trec:   60.188389\tkl:   26.246670\n",
      "Epoch: 805 [40100/50000 (80%)]  \tLoss:   90.082253\trec:   63.332382\tkl:   26.749870\n",
      "====> Epoch: 805 Average train loss: 89.7759\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4602\n",
      "Epoch: 806 [  100/50000 ( 0%)]  \tLoss:   88.304558\trec:   62.089470\tkl:   26.215090\n",
      "Epoch: 806 [10100/50000 (20%)]  \tLoss:   92.026627\trec:   64.236572\tkl:   27.790060\n",
      "Epoch: 806 [20100/50000 (40%)]  \tLoss:   92.525848\trec:   65.287964\tkl:   27.237881\n",
      "Epoch: 806 [30100/50000 (60%)]  \tLoss:   91.051270\trec:   63.911865\tkl:   27.139406\n",
      "Epoch: 806 [40100/50000 (80%)]  \tLoss:   89.844765\trec:   63.085213\tkl:   26.759550\n",
      "====> Epoch: 806 Average train loss: 89.7903\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4424\n",
      "Epoch: 807 [  100/50000 ( 0%)]  \tLoss:   87.969421\trec:   60.907410\tkl:   27.062012\n",
      "Epoch: 807 [10100/50000 (20%)]  \tLoss:   92.631805\trec:   65.209343\tkl:   27.422462\n",
      "Epoch: 807 [20100/50000 (40%)]  \tLoss:   87.016953\trec:   60.485477\tkl:   26.531473\n",
      "Epoch: 807 [30100/50000 (60%)]  \tLoss:   94.132294\trec:   65.713669\tkl:   28.418627\n",
      "Epoch: 807 [40100/50000 (80%)]  \tLoss:   92.545799\trec:   64.384689\tkl:   28.161118\n",
      "====> Epoch: 807 Average train loss: 89.7686\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4054\n",
      "Epoch: 808 [  100/50000 ( 0%)]  \tLoss:   92.106667\trec:   63.841419\tkl:   28.265251\n",
      "Epoch: 808 [10100/50000 (20%)]  \tLoss:   89.818985\trec:   62.998543\tkl:   26.820435\n",
      "Epoch: 808 [20100/50000 (40%)]  \tLoss:   88.939980\trec:   61.855526\tkl:   27.084452\n",
      "Epoch: 808 [30100/50000 (60%)]  \tLoss:   89.305321\trec:   62.307671\tkl:   26.997648\n",
      "Epoch: 808 [40100/50000 (80%)]  \tLoss:   87.844841\trec:   62.272518\tkl:   25.572327\n",
      "====> Epoch: 808 Average train loss: 89.7780\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2953\n",
      "Epoch: 809 [  100/50000 ( 0%)]  \tLoss:   89.232147\trec:   62.154297\tkl:   27.077854\n",
      "Epoch: 809 [10100/50000 (20%)]  \tLoss:   88.084129\trec:   61.649075\tkl:   26.435053\n",
      "Epoch: 809 [20100/50000 (40%)]  \tLoss:   89.859802\trec:   63.631123\tkl:   26.228676\n",
      "Epoch: 809 [30100/50000 (60%)]  \tLoss:   90.516342\trec:   62.899975\tkl:   27.616371\n",
      "Epoch: 809 [40100/50000 (80%)]  \tLoss:   90.508995\trec:   63.480507\tkl:   27.028486\n",
      "====> Epoch: 809 Average train loss: 89.7685\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3510\n",
      "Epoch: 810 [  100/50000 ( 0%)]  \tLoss:   89.028336\trec:   62.052280\tkl:   26.976053\n",
      "Epoch: 810 [10100/50000 (20%)]  \tLoss:   93.528336\trec:   66.191574\tkl:   27.336769\n",
      "Epoch: 810 [20100/50000 (40%)]  \tLoss:   90.859413\trec:   63.695889\tkl:   27.163530\n",
      "Epoch: 810 [30100/50000 (60%)]  \tLoss:   90.472969\trec:   62.857899\tkl:   27.615065\n",
      "Epoch: 810 [40100/50000 (80%)]  \tLoss:   87.787384\trec:   61.288338\tkl:   26.499043\n",
      "====> Epoch: 810 Average train loss: 89.7605\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3838\n",
      "Epoch: 811 [  100/50000 ( 0%)]  \tLoss:   90.227379\trec:   62.077000\tkl:   28.150383\n",
      "Epoch: 811 [10100/50000 (20%)]  \tLoss:   90.723488\trec:   64.416519\tkl:   26.306967\n",
      "Epoch: 811 [20100/50000 (40%)]  \tLoss:   91.465935\trec:   64.338737\tkl:   27.127201\n",
      "Epoch: 811 [30100/50000 (60%)]  \tLoss:   90.548103\trec:   63.130905\tkl:   27.417192\n",
      "Epoch: 811 [40100/50000 (80%)]  \tLoss:   91.534508\trec:   64.878082\tkl:   26.656420\n",
      "====> Epoch: 811 Average train loss: 89.7678\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3417\n",
      "Epoch: 812 [  100/50000 ( 0%)]  \tLoss:   91.291084\trec:   63.718777\tkl:   27.572306\n",
      "Epoch: 812 [10100/50000 (20%)]  \tLoss:   90.546684\trec:   64.369820\tkl:   26.176867\n",
      "Epoch: 812 [20100/50000 (40%)]  \tLoss:   90.148453\trec:   62.802059\tkl:   27.346401\n",
      "Epoch: 812 [30100/50000 (60%)]  \tLoss:   88.076172\trec:   60.418282\tkl:   27.657885\n",
      "Epoch: 812 [40100/50000 (80%)]  \tLoss:   88.787910\trec:   62.101101\tkl:   26.686808\n",
      "====> Epoch: 812 Average train loss: 89.7377\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4673\n",
      "Epoch: 813 [  100/50000 ( 0%)]  \tLoss:   89.614311\trec:   62.360275\tkl:   27.254042\n",
      "Epoch: 813 [10100/50000 (20%)]  \tLoss:   91.987808\trec:   64.709770\tkl:   27.278044\n",
      "Epoch: 813 [20100/50000 (40%)]  \tLoss:   90.148125\trec:   63.845310\tkl:   26.302807\n",
      "Epoch: 813 [30100/50000 (60%)]  \tLoss:   88.637009\trec:   61.397881\tkl:   27.239136\n",
      "Epoch: 813 [40100/50000 (80%)]  \tLoss:   86.712227\trec:   60.699413\tkl:   26.012812\n",
      "====> Epoch: 813 Average train loss: 89.7542\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3316\n",
      "Epoch: 814 [  100/50000 ( 0%)]  \tLoss:   86.906120\trec:   60.013069\tkl:   26.893049\n",
      "Epoch: 814 [10100/50000 (20%)]  \tLoss:   93.378181\trec:   66.835220\tkl:   26.542953\n",
      "Epoch: 814 [20100/50000 (40%)]  \tLoss:   84.632866\trec:   58.928082\tkl:   25.704786\n",
      "Epoch: 814 [30100/50000 (60%)]  \tLoss:   88.233795\trec:   61.926460\tkl:   26.307341\n",
      "Epoch: 814 [40100/50000 (80%)]  \tLoss:   87.966721\trec:   61.738781\tkl:   26.227936\n",
      "====> Epoch: 814 Average train loss: 89.7527\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3961\n",
      "Epoch: 815 [  100/50000 ( 0%)]  \tLoss:   86.266991\trec:   61.018845\tkl:   25.248148\n",
      "Epoch: 815 [10100/50000 (20%)]  \tLoss:   88.489120\trec:   61.657635\tkl:   26.831486\n",
      "Epoch: 815 [20100/50000 (40%)]  \tLoss:   91.626015\trec:   64.257355\tkl:   27.368654\n",
      "Epoch: 815 [30100/50000 (60%)]  \tLoss:   89.224052\trec:   62.277309\tkl:   26.946743\n",
      "Epoch: 815 [40100/50000 (80%)]  \tLoss:   91.667595\trec:   63.750599\tkl:   27.916996\n",
      "====> Epoch: 815 Average train loss: 89.7652\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3399\n",
      "Epoch: 816 [  100/50000 ( 0%)]  \tLoss:   91.603874\trec:   64.449883\tkl:   27.153994\n",
      "Epoch: 816 [10100/50000 (20%)]  \tLoss:   93.005959\trec:   65.421997\tkl:   27.583954\n",
      "Epoch: 816 [20100/50000 (40%)]  \tLoss:   92.480240\trec:   64.483299\tkl:   27.996943\n",
      "Epoch: 816 [30100/50000 (60%)]  \tLoss:   87.646034\trec:   60.489033\tkl:   27.157001\n",
      "Epoch: 816 [40100/50000 (80%)]  \tLoss:   87.389656\trec:   60.901108\tkl:   26.488548\n",
      "====> Epoch: 816 Average train loss: 89.7396\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3018\n",
      "Epoch: 817 [  100/50000 ( 0%)]  \tLoss:   90.440086\trec:   63.811169\tkl:   26.628914\n",
      "Epoch: 817 [10100/50000 (20%)]  \tLoss:   90.092926\trec:   63.373299\tkl:   26.719631\n",
      "Epoch: 817 [20100/50000 (40%)]  \tLoss:   92.161903\trec:   64.915184\tkl:   27.246716\n",
      "Epoch: 817 [30100/50000 (60%)]  \tLoss:   88.948463\trec:   61.922661\tkl:   27.025803\n",
      "Epoch: 817 [40100/50000 (80%)]  \tLoss:   85.504547\trec:   59.372627\tkl:   26.131927\n",
      "====> Epoch: 817 Average train loss: 89.7473\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3871\n",
      "Epoch: 818 [  100/50000 ( 0%)]  \tLoss:   88.574272\trec:   62.258640\tkl:   26.315632\n",
      "Epoch: 818 [10100/50000 (20%)]  \tLoss:   87.390312\trec:   60.954784\tkl:   26.435526\n",
      "Epoch: 818 [20100/50000 (40%)]  \tLoss:   88.576561\trec:   61.490368\tkl:   27.086189\n",
      "Epoch: 818 [30100/50000 (60%)]  \tLoss:   90.976044\trec:   64.781387\tkl:   26.194654\n",
      "Epoch: 818 [40100/50000 (80%)]  \tLoss:   89.133560\trec:   62.325909\tkl:   26.807657\n",
      "====> Epoch: 818 Average train loss: 89.7725\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2252\n",
      "Epoch: 819 [  100/50000 ( 0%)]  \tLoss:   90.610191\trec:   63.592148\tkl:   27.018042\n",
      "Epoch: 819 [10100/50000 (20%)]  \tLoss:   86.828888\trec:   60.029366\tkl:   26.799517\n",
      "Epoch: 819 [20100/50000 (40%)]  \tLoss:   91.144760\trec:   63.937801\tkl:   27.206961\n",
      "Epoch: 819 [30100/50000 (60%)]  \tLoss:   87.117416\trec:   60.838242\tkl:   26.279175\n",
      "Epoch: 819 [40100/50000 (80%)]  \tLoss:   88.123238\trec:   60.991833\tkl:   27.131405\n",
      "====> Epoch: 819 Average train loss: 89.7280\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3013\n",
      "Epoch: 820 [  100/50000 ( 0%)]  \tLoss:   92.349373\trec:   65.552711\tkl:   26.796661\n",
      "Epoch: 820 [10100/50000 (20%)]  \tLoss:   91.776474\trec:   64.950890\tkl:   26.825583\n",
      "Epoch: 820 [20100/50000 (40%)]  \tLoss:   88.381851\trec:   61.090767\tkl:   27.291094\n",
      "Epoch: 820 [30100/50000 (60%)]  \tLoss:   91.728806\trec:   64.570503\tkl:   27.158310\n",
      "Epoch: 820 [40100/50000 (80%)]  \tLoss:   89.185532\trec:   62.827663\tkl:   26.357868\n",
      "====> Epoch: 820 Average train loss: 89.7767\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2820\n",
      "Epoch: 821 [  100/50000 ( 0%)]  \tLoss:   95.554100\trec:   66.645805\tkl:   28.908287\n",
      "Epoch: 821 [10100/50000 (20%)]  \tLoss:   88.123619\trec:   62.117821\tkl:   26.005802\n",
      "Epoch: 821 [20100/50000 (40%)]  \tLoss:   88.560631\trec:   62.188923\tkl:   26.371706\n",
      "Epoch: 821 [30100/50000 (60%)]  \tLoss:   93.767090\trec:   65.522697\tkl:   28.244398\n",
      "Epoch: 821 [40100/50000 (80%)]  \tLoss:   91.546776\trec:   64.381180\tkl:   27.165600\n",
      "====> Epoch: 821 Average train loss: 89.7322\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3829\n",
      "Epoch: 822 [  100/50000 ( 0%)]  \tLoss:   91.436226\trec:   64.131104\tkl:   27.305124\n",
      "Epoch: 822 [10100/50000 (20%)]  \tLoss:   92.078331\trec:   63.921082\tkl:   28.157244\n",
      "Epoch: 822 [20100/50000 (40%)]  \tLoss:   91.045799\trec:   64.304512\tkl:   26.741285\n",
      "Epoch: 822 [30100/50000 (60%)]  \tLoss:   85.898911\trec:   59.952187\tkl:   25.946728\n",
      "Epoch: 822 [40100/50000 (80%)]  \tLoss:   86.834068\trec:   60.531590\tkl:   26.302481\n",
      "====> Epoch: 822 Average train loss: 89.7345\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3479\n",
      "Epoch: 823 [  100/50000 ( 0%)]  \tLoss:   90.683533\trec:   63.714657\tkl:   26.968882\n",
      "Epoch: 823 [10100/50000 (20%)]  \tLoss:   87.153549\trec:   60.429913\tkl:   26.723637\n",
      "Epoch: 823 [20100/50000 (40%)]  \tLoss:   95.775642\trec:   67.811348\tkl:   27.964294\n",
      "Epoch: 823 [30100/50000 (60%)]  \tLoss:   86.388611\trec:   60.270653\tkl:   26.117954\n",
      "Epoch: 823 [40100/50000 (80%)]  \tLoss:   88.726006\trec:   61.332821\tkl:   27.393183\n",
      "====> Epoch: 823 Average train loss: 89.7010\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3050\n",
      "Epoch: 824 [  100/50000 ( 0%)]  \tLoss:   91.530899\trec:   65.001503\tkl:   26.529388\n",
      "Epoch: 824 [10100/50000 (20%)]  \tLoss:   85.717468\trec:   59.399891\tkl:   26.317577\n",
      "Epoch: 824 [20100/50000 (40%)]  \tLoss:   91.666313\trec:   65.523483\tkl:   26.142832\n",
      "Epoch: 824 [30100/50000 (60%)]  \tLoss:   91.763809\trec:   64.200119\tkl:   27.563696\n",
      "Epoch: 824 [40100/50000 (80%)]  \tLoss:   88.995033\trec:   62.481121\tkl:   26.513910\n",
      "====> Epoch: 824 Average train loss: 89.7386\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3964\n",
      "Epoch: 825 [  100/50000 ( 0%)]  \tLoss:   91.318535\trec:   63.581806\tkl:   27.736725\n",
      "Epoch: 825 [10100/50000 (20%)]  \tLoss:   90.438652\trec:   64.040627\tkl:   26.398027\n",
      "Epoch: 825 [20100/50000 (40%)]  \tLoss:   89.648415\trec:   61.642147\tkl:   28.006271\n",
      "Epoch: 825 [30100/50000 (60%)]  \tLoss:   89.916222\trec:   62.950901\tkl:   26.965315\n",
      "Epoch: 825 [40100/50000 (80%)]  \tLoss:   90.245972\trec:   63.715820\tkl:   26.530155\n",
      "====> Epoch: 825 Average train loss: 89.7117\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3483\n",
      "Epoch: 826 [  100/50000 ( 0%)]  \tLoss:   90.167976\trec:   62.585228\tkl:   27.582750\n",
      "Epoch: 826 [10100/50000 (20%)]  \tLoss:   87.243690\trec:   61.103752\tkl:   26.139940\n",
      "Epoch: 826 [20100/50000 (40%)]  \tLoss:   86.998535\trec:   61.481682\tkl:   25.516853\n",
      "Epoch: 826 [30100/50000 (60%)]  \tLoss:   87.886818\trec:   61.271587\tkl:   26.615225\n",
      "Epoch: 826 [40100/50000 (80%)]  \tLoss:   87.181152\trec:   60.830906\tkl:   26.350248\n",
      "====> Epoch: 826 Average train loss: 89.7243\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3208\n",
      "Epoch: 827 [  100/50000 ( 0%)]  \tLoss:   90.033165\trec:   62.826511\tkl:   27.206652\n",
      "Epoch: 827 [10100/50000 (20%)]  \tLoss:   89.409500\trec:   62.720917\tkl:   26.688581\n",
      "Epoch: 827 [20100/50000 (40%)]  \tLoss:   89.906540\trec:   63.233723\tkl:   26.672817\n",
      "Epoch: 827 [30100/50000 (60%)]  \tLoss:   90.360008\trec:   63.241245\tkl:   27.118761\n",
      "Epoch: 827 [40100/50000 (80%)]  \tLoss:   89.865486\trec:   62.568428\tkl:   27.297066\n",
      "====> Epoch: 827 Average train loss: 89.7234\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3287\n",
      "Epoch: 828 [  100/50000 ( 0%)]  \tLoss:   93.125786\trec:   65.464172\tkl:   27.661615\n",
      "Epoch: 828 [10100/50000 (20%)]  \tLoss:   90.354919\trec:   63.205242\tkl:   27.149675\n",
      "Epoch: 828 [20100/50000 (40%)]  \tLoss:   91.483620\trec:   63.524902\tkl:   27.958719\n",
      "Epoch: 828 [30100/50000 (60%)]  \tLoss:   87.566559\trec:   60.644535\tkl:   26.922020\n",
      "Epoch: 828 [40100/50000 (80%)]  \tLoss:   94.630768\trec:   65.521271\tkl:   29.109499\n",
      "====> Epoch: 828 Average train loss: 89.7279\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3534\n",
      "Epoch: 829 [  100/50000 ( 0%)]  \tLoss:   89.298248\trec:   62.718304\tkl:   26.579948\n",
      "Epoch: 829 [10100/50000 (20%)]  \tLoss:   89.469238\trec:   62.931622\tkl:   26.537617\n",
      "Epoch: 829 [20100/50000 (40%)]  \tLoss:   91.425026\trec:   62.962444\tkl:   28.462585\n",
      "Epoch: 829 [30100/50000 (60%)]  \tLoss:   91.582909\trec:   63.882519\tkl:   27.700386\n",
      "Epoch: 829 [40100/50000 (80%)]  \tLoss:   90.386841\trec:   63.515488\tkl:   26.871355\n",
      "====> Epoch: 829 Average train loss: 89.7166\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4100\n",
      "Epoch: 830 [  100/50000 ( 0%)]  \tLoss:   90.252495\trec:   62.852276\tkl:   27.400225\n",
      "Epoch: 830 [10100/50000 (20%)]  \tLoss:   89.360527\trec:   63.155975\tkl:   26.204546\n",
      "Epoch: 830 [20100/50000 (40%)]  \tLoss:   88.979561\trec:   62.912434\tkl:   26.067122\n",
      "Epoch: 830 [30100/50000 (60%)]  \tLoss:   89.053467\trec:   61.758579\tkl:   27.294889\n",
      "Epoch: 830 [40100/50000 (80%)]  \tLoss:   91.726845\trec:   64.169914\tkl:   27.556932\n",
      "====> Epoch: 830 Average train loss: 89.7121\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3269\n",
      "Epoch: 831 [  100/50000 ( 0%)]  \tLoss:   91.760513\trec:   64.779381\tkl:   26.981133\n",
      "Epoch: 831 [10100/50000 (20%)]  \tLoss:   92.762169\trec:   64.582146\tkl:   28.180023\n",
      "Epoch: 831 [20100/50000 (40%)]  \tLoss:   85.712227\trec:   59.148273\tkl:   26.563944\n",
      "Epoch: 831 [30100/50000 (60%)]  \tLoss:   93.244537\trec:   65.071503\tkl:   28.173035\n",
      "Epoch: 831 [40100/50000 (80%)]  \tLoss:   89.758583\trec:   62.857918\tkl:   26.900661\n",
      "====> Epoch: 831 Average train loss: 89.7078\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3184\n",
      "Epoch: 832 [  100/50000 ( 0%)]  \tLoss:   89.239799\trec:   62.507900\tkl:   26.731909\n",
      "Epoch: 832 [10100/50000 (20%)]  \tLoss:   91.854843\trec:   64.710518\tkl:   27.144323\n",
      "Epoch: 832 [20100/50000 (40%)]  \tLoss:   89.362198\trec:   62.921894\tkl:   26.440302\n",
      "Epoch: 832 [30100/50000 (60%)]  \tLoss:   92.446968\trec:   64.909416\tkl:   27.537556\n",
      "Epoch: 832 [40100/50000 (80%)]  \tLoss:   94.188278\trec:   66.349228\tkl:   27.839054\n",
      "====> Epoch: 832 Average train loss: 89.7174\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2864\n",
      "Epoch: 833 [  100/50000 ( 0%)]  \tLoss:   92.076141\trec:   64.498833\tkl:   27.577307\n",
      "Epoch: 833 [10100/50000 (20%)]  \tLoss:   89.519630\trec:   61.657822\tkl:   27.861811\n",
      "Epoch: 833 [20100/50000 (40%)]  \tLoss:   87.775955\trec:   61.384735\tkl:   26.391226\n",
      "Epoch: 833 [30100/50000 (60%)]  \tLoss:   83.962852\trec:   58.389462\tkl:   25.573389\n",
      "Epoch: 833 [40100/50000 (80%)]  \tLoss:   90.443192\trec:   63.467674\tkl:   26.975515\n",
      "====> Epoch: 833 Average train loss: 89.7108\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3709\n",
      "Epoch: 834 [  100/50000 ( 0%)]  \tLoss:   91.201950\trec:   63.209118\tkl:   27.992826\n",
      "Epoch: 834 [10100/50000 (20%)]  \tLoss:   89.658279\trec:   63.312634\tkl:   26.345642\n",
      "Epoch: 834 [20100/50000 (40%)]  \tLoss:   91.308563\trec:   64.489380\tkl:   26.819176\n",
      "Epoch: 834 [30100/50000 (60%)]  \tLoss:   89.427017\trec:   62.776966\tkl:   26.650051\n",
      "Epoch: 834 [40100/50000 (80%)]  \tLoss:   91.212654\trec:   63.500320\tkl:   27.712337\n",
      "====> Epoch: 834 Average train loss: 89.7218\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3769\n",
      "Epoch: 835 [  100/50000 ( 0%)]  \tLoss:   87.122169\trec:   61.141102\tkl:   25.981070\n",
      "Epoch: 835 [10100/50000 (20%)]  \tLoss:   89.174545\trec:   62.435894\tkl:   26.738657\n",
      "Epoch: 835 [20100/50000 (40%)]  \tLoss:   84.910988\trec:   59.267269\tkl:   25.643715\n",
      "Epoch: 835 [30100/50000 (60%)]  \tLoss:   91.663452\trec:   64.198006\tkl:   27.465445\n",
      "Epoch: 835 [40100/50000 (80%)]  \tLoss:   91.085068\trec:   63.355751\tkl:   27.729313\n",
      "====> Epoch: 835 Average train loss: 89.7185\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3299\n",
      "Epoch: 836 [  100/50000 ( 0%)]  \tLoss:   93.045509\trec:   65.630020\tkl:   27.415480\n",
      "Epoch: 836 [10100/50000 (20%)]  \tLoss:   90.840065\trec:   62.819023\tkl:   28.021048\n",
      "Epoch: 836 [20100/50000 (40%)]  \tLoss:   91.828773\trec:   63.504211\tkl:   28.324562\n",
      "Epoch: 836 [30100/50000 (60%)]  \tLoss:   87.526718\trec:   61.297470\tkl:   26.229244\n",
      "Epoch: 836 [40100/50000 (80%)]  \tLoss:   90.922615\trec:   63.886684\tkl:   27.035934\n",
      "====> Epoch: 836 Average train loss: 89.7066\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3296\n",
      "Epoch: 837 [  100/50000 ( 0%)]  \tLoss:   93.267853\trec:   66.308342\tkl:   26.959511\n",
      "Epoch: 837 [10100/50000 (20%)]  \tLoss:   87.417107\trec:   60.876865\tkl:   26.540239\n",
      "Epoch: 837 [20100/50000 (40%)]  \tLoss:   88.964233\trec:   61.738281\tkl:   27.225954\n",
      "Epoch: 837 [30100/50000 (60%)]  \tLoss:   91.270767\trec:   64.128487\tkl:   27.142286\n",
      "Epoch: 837 [40100/50000 (80%)]  \tLoss:   93.198830\trec:   65.468971\tkl:   27.729849\n",
      "====> Epoch: 837 Average train loss: 89.7087\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3721\n",
      "Epoch: 838 [  100/50000 ( 0%)]  \tLoss:   88.982109\trec:   62.385323\tkl:   26.596785\n",
      "Epoch: 838 [10100/50000 (20%)]  \tLoss:   88.177711\trec:   61.396030\tkl:   26.781685\n",
      "Epoch: 838 [20100/50000 (40%)]  \tLoss:   93.405975\trec:   65.714073\tkl:   27.691898\n",
      "Epoch: 838 [30100/50000 (60%)]  \tLoss:   89.363251\trec:   62.397480\tkl:   26.965769\n",
      "Epoch: 838 [40100/50000 (80%)]  \tLoss:   89.237373\trec:   63.671024\tkl:   25.566347\n",
      "====> Epoch: 838 Average train loss: 89.7001\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4147\n",
      "Epoch: 839 [  100/50000 ( 0%)]  \tLoss:   90.174477\trec:   63.561111\tkl:   26.613371\n",
      "Epoch: 839 [10100/50000 (20%)]  \tLoss:   88.919060\trec:   61.539375\tkl:   27.379690\n",
      "Epoch: 839 [20100/50000 (40%)]  \tLoss:   91.181854\trec:   63.126858\tkl:   28.054989\n",
      "Epoch: 839 [30100/50000 (60%)]  \tLoss:   89.334526\trec:   62.140713\tkl:   27.193813\n",
      "Epoch: 839 [40100/50000 (80%)]  \tLoss:   85.934052\trec:   59.084412\tkl:   26.849636\n",
      "====> Epoch: 839 Average train loss: 89.6851\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3270\n",
      "Epoch: 840 [  100/50000 ( 0%)]  \tLoss:   89.767624\trec:   62.084709\tkl:   27.682915\n",
      "Epoch: 840 [10100/50000 (20%)]  \tLoss:   93.098610\trec:   66.168854\tkl:   26.929760\n",
      "Epoch: 840 [20100/50000 (40%)]  \tLoss:   88.962875\trec:   62.813572\tkl:   26.149303\n",
      "Epoch: 840 [30100/50000 (60%)]  \tLoss:   87.380447\trec:   60.673698\tkl:   26.706745\n",
      "Epoch: 840 [40100/50000 (80%)]  \tLoss:   88.374580\trec:   60.970852\tkl:   27.403723\n",
      "====> Epoch: 840 Average train loss: 89.7001\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4590\n",
      "Epoch: 841 [  100/50000 ( 0%)]  \tLoss:   90.669220\trec:   64.023605\tkl:   26.645609\n",
      "Epoch: 841 [10100/50000 (20%)]  \tLoss:   84.569725\trec:   57.982754\tkl:   26.586977\n",
      "Epoch: 841 [20100/50000 (40%)]  \tLoss:   92.065392\trec:   64.793335\tkl:   27.272060\n",
      "Epoch: 841 [30100/50000 (60%)]  \tLoss:   86.692429\trec:   60.264736\tkl:   26.427694\n",
      "Epoch: 841 [40100/50000 (80%)]  \tLoss:   94.811218\trec:   67.160683\tkl:   27.650539\n",
      "====> Epoch: 841 Average train loss: 89.6983\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3829\n",
      "Epoch: 842 [  100/50000 ( 0%)]  \tLoss:   91.675934\trec:   64.029480\tkl:   27.646452\n",
      "Epoch: 842 [10100/50000 (20%)]  \tLoss:   86.553864\trec:   60.887978\tkl:   25.665884\n",
      "Epoch: 842 [20100/50000 (40%)]  \tLoss:   92.364883\trec:   64.940826\tkl:   27.424057\n",
      "Epoch: 842 [30100/50000 (60%)]  \tLoss:   93.280502\trec:   65.250679\tkl:   28.029823\n",
      "Epoch: 842 [40100/50000 (80%)]  \tLoss:   91.413902\trec:   63.912983\tkl:   27.500925\n",
      "====> Epoch: 842 Average train loss: 89.6482\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3410\n",
      "Epoch: 843 [  100/50000 ( 0%)]  \tLoss:   89.651199\trec:   62.293682\tkl:   27.357521\n",
      "Epoch: 843 [10100/50000 (20%)]  \tLoss:   90.287811\trec:   62.667995\tkl:   27.619814\n",
      "Epoch: 843 [20100/50000 (40%)]  \tLoss:   88.249489\trec:   61.762299\tkl:   26.487190\n",
      "Epoch: 843 [30100/50000 (60%)]  \tLoss:   90.421112\trec:   62.576694\tkl:   27.844416\n",
      "Epoch: 843 [40100/50000 (80%)]  \tLoss:   91.427330\trec:   63.771553\tkl:   27.655783\n",
      "====> Epoch: 843 Average train loss: 89.6975\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2659\n",
      "Epoch: 844 [  100/50000 ( 0%)]  \tLoss:   89.805389\trec:   62.797432\tkl:   27.007961\n",
      "Epoch: 844 [10100/50000 (20%)]  \tLoss:   92.578232\trec:   65.258469\tkl:   27.319767\n",
      "Epoch: 844 [20100/50000 (40%)]  \tLoss:   88.121964\trec:   61.382885\tkl:   26.739075\n",
      "Epoch: 844 [30100/50000 (60%)]  \tLoss:   83.829842\trec:   57.974892\tkl:   25.854954\n",
      "Epoch: 844 [40100/50000 (80%)]  \tLoss:   84.152695\trec:   58.669338\tkl:   25.483355\n",
      "====> Epoch: 844 Average train loss: 89.6841\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3029\n",
      "Epoch: 845 [  100/50000 ( 0%)]  \tLoss:   87.088516\trec:   60.558952\tkl:   26.529558\n",
      "Epoch: 845 [10100/50000 (20%)]  \tLoss:   92.312637\trec:   64.770454\tkl:   27.542183\n",
      "Epoch: 845 [20100/50000 (40%)]  \tLoss:   90.819862\trec:   63.377346\tkl:   27.442514\n",
      "Epoch: 845 [30100/50000 (60%)]  \tLoss:   91.302490\trec:   63.633327\tkl:   27.669163\n",
      "Epoch: 845 [40100/50000 (80%)]  \tLoss:   91.800812\trec:   65.145576\tkl:   26.655233\n",
      "====> Epoch: 845 Average train loss: 89.6715\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2705\n",
      "Epoch: 846 [  100/50000 ( 0%)]  \tLoss:   88.792877\trec:   61.274120\tkl:   27.518759\n",
      "Epoch: 846 [10100/50000 (20%)]  \tLoss:   87.568756\trec:   61.525360\tkl:   26.043400\n",
      "Epoch: 846 [20100/50000 (40%)]  \tLoss:   87.768730\trec:   61.813034\tkl:   25.955690\n",
      "Epoch: 846 [30100/50000 (60%)]  \tLoss:   89.262993\trec:   62.661659\tkl:   26.601339\n",
      "Epoch: 846 [40100/50000 (80%)]  \tLoss:   91.494370\trec:   65.146950\tkl:   26.347420\n",
      "====> Epoch: 846 Average train loss: 89.6641\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3417\n",
      "Epoch: 847 [  100/50000 ( 0%)]  \tLoss:   87.967941\trec:   60.517010\tkl:   27.450930\n",
      "Epoch: 847 [10100/50000 (20%)]  \tLoss:   89.679512\trec:   62.388123\tkl:   27.291382\n",
      "Epoch: 847 [20100/50000 (40%)]  \tLoss:   93.073586\trec:   65.511749\tkl:   27.561831\n",
      "Epoch: 847 [30100/50000 (60%)]  \tLoss:   87.887047\trec:   62.055077\tkl:   25.831974\n",
      "Epoch: 847 [40100/50000 (80%)]  \tLoss:   91.900620\trec:   64.283630\tkl:   27.616987\n",
      "====> Epoch: 847 Average train loss: 89.6635\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3211\n",
      "Epoch: 848 [  100/50000 ( 0%)]  \tLoss:   89.665543\trec:   62.714481\tkl:   26.951069\n",
      "Epoch: 848 [10100/50000 (20%)]  \tLoss:   87.827133\trec:   60.965977\tkl:   26.861162\n",
      "Epoch: 848 [20100/50000 (40%)]  \tLoss:   90.571236\trec:   62.535629\tkl:   28.035610\n",
      "Epoch: 848 [30100/50000 (60%)]  \tLoss:   87.393867\trec:   60.377861\tkl:   27.016001\n",
      "Epoch: 848 [40100/50000 (80%)]  \tLoss:   89.042664\trec:   62.452038\tkl:   26.590624\n",
      "====> Epoch: 848 Average train loss: 89.6770\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2885\n",
      "Epoch: 849 [  100/50000 ( 0%)]  \tLoss:   91.348091\trec:   64.092018\tkl:   27.256073\n",
      "Epoch: 849 [10100/50000 (20%)]  \tLoss:   91.936638\trec:   64.606941\tkl:   27.329695\n",
      "Epoch: 849 [20100/50000 (40%)]  \tLoss:   89.574852\trec:   62.537811\tkl:   27.037043\n",
      "Epoch: 849 [30100/50000 (60%)]  \tLoss:   89.388092\trec:   61.379421\tkl:   28.008669\n",
      "Epoch: 849 [40100/50000 (80%)]  \tLoss:   91.640068\trec:   64.072563\tkl:   27.567499\n",
      "====> Epoch: 849 Average train loss: 89.6875\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3775\n",
      "Epoch: 850 [  100/50000 ( 0%)]  \tLoss:   88.985252\trec:   62.518631\tkl:   26.466625\n",
      "Epoch: 850 [10100/50000 (20%)]  \tLoss:   90.106270\trec:   62.386787\tkl:   27.719477\n",
      "Epoch: 850 [20100/50000 (40%)]  \tLoss:   91.495934\trec:   63.077236\tkl:   28.418695\n",
      "Epoch: 850 [30100/50000 (60%)]  \tLoss:   85.212761\trec:   59.375488\tkl:   25.837276\n",
      "Epoch: 850 [40100/50000 (80%)]  \tLoss:   86.811073\trec:   60.368500\tkl:   26.442577\n",
      "====> Epoch: 850 Average train loss: 89.6595\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3934\n",
      "Epoch: 851 [  100/50000 ( 0%)]  \tLoss:   89.093552\trec:   61.958652\tkl:   27.134901\n",
      "Epoch: 851 [10100/50000 (20%)]  \tLoss:   92.926636\trec:   65.664833\tkl:   27.261810\n",
      "Epoch: 851 [20100/50000 (40%)]  \tLoss:   94.718422\trec:   66.332741\tkl:   28.385691\n",
      "Epoch: 851 [30100/50000 (60%)]  \tLoss:   91.677963\trec:   64.797401\tkl:   26.880562\n",
      "Epoch: 851 [40100/50000 (80%)]  \tLoss:   91.136246\trec:   64.196457\tkl:   26.939793\n",
      "====> Epoch: 851 Average train loss: 89.6728\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2586\n",
      "Epoch: 852 [  100/50000 ( 0%)]  \tLoss:   92.377731\trec:   64.788712\tkl:   27.589020\n",
      "Epoch: 852 [10100/50000 (20%)]  \tLoss:   86.754738\trec:   61.169014\tkl:   25.585720\n",
      "Epoch: 852 [20100/50000 (40%)]  \tLoss:   90.167343\trec:   63.079472\tkl:   27.087870\n",
      "Epoch: 852 [30100/50000 (60%)]  \tLoss:   88.089615\trec:   61.015182\tkl:   27.074436\n",
      "Epoch: 852 [40100/50000 (80%)]  \tLoss:   88.186768\trec:   62.261162\tkl:   25.925606\n",
      "====> Epoch: 852 Average train loss: 89.6681\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3018\n",
      "Epoch: 853 [  100/50000 ( 0%)]  \tLoss:   87.935173\trec:   61.618176\tkl:   26.317001\n",
      "Epoch: 853 [10100/50000 (20%)]  \tLoss:   90.300270\trec:   63.870380\tkl:   26.429890\n",
      "Epoch: 853 [20100/50000 (40%)]  \tLoss:   93.199615\trec:   65.394981\tkl:   27.804638\n",
      "Epoch: 853 [30100/50000 (60%)]  \tLoss:   87.325958\trec:   60.799561\tkl:   26.526394\n",
      "Epoch: 853 [40100/50000 (80%)]  \tLoss:   89.581825\trec:   63.258865\tkl:   26.322958\n",
      "====> Epoch: 853 Average train loss: 89.6402\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3360\n",
      "Epoch: 854 [  100/50000 ( 0%)]  \tLoss:   87.747658\trec:   61.572636\tkl:   26.175016\n",
      "Epoch: 854 [10100/50000 (20%)]  \tLoss:   88.461639\trec:   61.815937\tkl:   26.645699\n",
      "Epoch: 854 [20100/50000 (40%)]  \tLoss:   91.596222\trec:   64.613457\tkl:   26.982765\n",
      "Epoch: 854 [30100/50000 (60%)]  \tLoss:   92.812202\trec:   65.291672\tkl:   27.520535\n",
      "Epoch: 854 [40100/50000 (80%)]  \tLoss:   91.060806\trec:   63.132469\tkl:   27.928339\n",
      "====> Epoch: 854 Average train loss: 89.6539\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3135\n",
      "Epoch: 855 [  100/50000 ( 0%)]  \tLoss:   91.130699\trec:   63.658367\tkl:   27.472328\n",
      "Epoch: 855 [10100/50000 (20%)]  \tLoss:   90.340485\trec:   62.299793\tkl:   28.040689\n",
      "Epoch: 855 [20100/50000 (40%)]  \tLoss:   91.247597\trec:   64.363976\tkl:   26.883627\n",
      "Epoch: 855 [30100/50000 (60%)]  \tLoss:   88.725548\trec:   62.502338\tkl:   26.223213\n",
      "Epoch: 855 [40100/50000 (80%)]  \tLoss:   86.535675\trec:   59.755413\tkl:   26.780256\n",
      "====> Epoch: 855 Average train loss: 89.6423\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3146\n",
      "Epoch: 856 [  100/50000 ( 0%)]  \tLoss:   86.280914\trec:   59.337318\tkl:   26.943604\n",
      "Epoch: 856 [10100/50000 (20%)]  \tLoss:   94.320572\trec:   66.496696\tkl:   27.823875\n",
      "Epoch: 856 [20100/50000 (40%)]  \tLoss:   90.354332\trec:   63.925213\tkl:   26.429123\n",
      "Epoch: 856 [30100/50000 (60%)]  \tLoss:   88.567627\trec:   62.290012\tkl:   26.277611\n",
      "Epoch: 856 [40100/50000 (80%)]  \tLoss:   89.655190\trec:   62.568291\tkl:   27.086908\n",
      "====> Epoch: 856 Average train loss: 89.6621\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4506\n",
      "Epoch: 857 [  100/50000 ( 0%)]  \tLoss:   89.501404\trec:   62.196159\tkl:   27.305248\n",
      "Epoch: 857 [10100/50000 (20%)]  \tLoss:   90.249840\trec:   62.590946\tkl:   27.658892\n",
      "Epoch: 857 [20100/50000 (40%)]  \tLoss:   88.454514\trec:   63.032055\tkl:   25.422455\n",
      "Epoch: 857 [30100/50000 (60%)]  \tLoss:   89.196266\trec:   62.987926\tkl:   26.208334\n",
      "Epoch: 857 [40100/50000 (80%)]  \tLoss:   89.633980\trec:   63.262077\tkl:   26.371904\n",
      "====> Epoch: 857 Average train loss: 89.6496\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2448\n",
      "Epoch: 858 [  100/50000 ( 0%)]  \tLoss:   87.153183\trec:   60.247524\tkl:   26.905653\n",
      "Epoch: 858 [10100/50000 (20%)]  \tLoss:   90.158119\trec:   63.620174\tkl:   26.537954\n",
      "Epoch: 858 [20100/50000 (40%)]  \tLoss:   89.537918\trec:   63.152958\tkl:   26.384958\n",
      "Epoch: 858 [30100/50000 (60%)]  \tLoss:   89.509682\trec:   62.344028\tkl:   27.165653\n",
      "Epoch: 858 [40100/50000 (80%)]  \tLoss:   86.157814\trec:   59.956768\tkl:   26.201044\n",
      "====> Epoch: 858 Average train loss: 89.6415\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2789\n",
      "Epoch: 859 [  100/50000 ( 0%)]  \tLoss:   92.043259\trec:   64.673912\tkl:   27.369354\n",
      "Epoch: 859 [10100/50000 (20%)]  \tLoss:   86.920250\trec:   59.724319\tkl:   27.195930\n",
      "Epoch: 859 [20100/50000 (40%)]  \tLoss:   89.225349\trec:   61.760605\tkl:   27.464743\n",
      "Epoch: 859 [30100/50000 (60%)]  \tLoss:   87.660294\trec:   60.864059\tkl:   26.796227\n",
      "Epoch: 859 [40100/50000 (80%)]  \tLoss:   93.043007\trec:   65.601349\tkl:   27.441660\n",
      "====> Epoch: 859 Average train loss: 89.6613\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3764\n",
      "Epoch: 860 [  100/50000 ( 0%)]  \tLoss:   91.957108\trec:   64.922760\tkl:   27.034348\n",
      "Epoch: 860 [10100/50000 (20%)]  \tLoss:   91.072441\trec:   62.915215\tkl:   28.157230\n",
      "Epoch: 860 [20100/50000 (40%)]  \tLoss:   90.132126\trec:   63.361904\tkl:   26.770220\n",
      "Epoch: 860 [30100/50000 (60%)]  \tLoss:   88.460762\trec:   61.260746\tkl:   27.200014\n",
      "Epoch: 860 [40100/50000 (80%)]  \tLoss:   87.640884\trec:   61.672493\tkl:   25.968393\n",
      "====> Epoch: 860 Average train loss: 89.6392\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3398\n",
      "Epoch: 861 [  100/50000 ( 0%)]  \tLoss:   86.951591\trec:   60.424393\tkl:   26.527197\n",
      "Epoch: 861 [10100/50000 (20%)]  \tLoss:   88.005112\trec:   60.913857\tkl:   27.091257\n",
      "Epoch: 861 [20100/50000 (40%)]  \tLoss:   91.247314\trec:   63.404041\tkl:   27.843271\n",
      "Epoch: 861 [30100/50000 (60%)]  \tLoss:   90.301773\trec:   62.910645\tkl:   27.391136\n",
      "Epoch: 861 [40100/50000 (80%)]  \tLoss:   86.861916\trec:   60.408737\tkl:   26.453178\n",
      "====> Epoch: 861 Average train loss: 89.6340\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4214\n",
      "Epoch: 862 [  100/50000 ( 0%)]  \tLoss:   89.769958\trec:   63.582352\tkl:   26.187613\n",
      "Epoch: 862 [10100/50000 (20%)]  \tLoss:   94.441788\trec:   65.828140\tkl:   28.613640\n",
      "Epoch: 862 [20100/50000 (40%)]  \tLoss:   88.355507\trec:   62.102592\tkl:   26.252916\n",
      "Epoch: 862 [30100/50000 (60%)]  \tLoss:   87.429626\trec:   60.755409\tkl:   26.674221\n",
      "Epoch: 862 [40100/50000 (80%)]  \tLoss:   89.232925\trec:   62.028931\tkl:   27.203993\n",
      "====> Epoch: 862 Average train loss: 89.6490\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2742\n",
      "Epoch: 863 [  100/50000 ( 0%)]  \tLoss:   92.204987\trec:   64.550369\tkl:   27.654621\n",
      "Epoch: 863 [10100/50000 (20%)]  \tLoss:   90.161308\trec:   63.401501\tkl:   26.759806\n",
      "Epoch: 863 [20100/50000 (40%)]  \tLoss:   93.484726\trec:   65.621056\tkl:   27.863668\n",
      "Epoch: 863 [30100/50000 (60%)]  \tLoss:   87.235428\trec:   60.413788\tkl:   26.821646\n",
      "Epoch: 863 [40100/50000 (80%)]  \tLoss:   90.230537\trec:   62.009434\tkl:   28.221104\n",
      "====> Epoch: 863 Average train loss: 89.6191\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3907\n",
      "Epoch: 864 [  100/50000 ( 0%)]  \tLoss:   87.448189\trec:   62.095119\tkl:   25.353067\n",
      "Epoch: 864 [10100/50000 (20%)]  \tLoss:   89.895477\trec:   62.820911\tkl:   27.074562\n",
      "Epoch: 864 [20100/50000 (40%)]  \tLoss:   88.989113\trec:   61.688148\tkl:   27.300964\n",
      "Epoch: 864 [30100/50000 (60%)]  \tLoss:   93.810059\trec:   65.324860\tkl:   28.485197\n",
      "Epoch: 864 [40100/50000 (80%)]  \tLoss:   91.181740\trec:   63.022968\tkl:   28.158764\n",
      "====> Epoch: 864 Average train loss: 89.6351\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2945\n",
      "Epoch: 865 [  100/50000 ( 0%)]  \tLoss:   91.636650\trec:   63.546444\tkl:   28.090204\n",
      "Epoch: 865 [10100/50000 (20%)]  \tLoss:   92.263130\trec:   64.686096\tkl:   27.577034\n",
      "Epoch: 865 [20100/50000 (40%)]  \tLoss:   90.506180\trec:   63.640774\tkl:   26.865408\n",
      "Epoch: 865 [30100/50000 (60%)]  \tLoss:   88.968102\trec:   61.568123\tkl:   27.399982\n",
      "Epoch: 865 [40100/50000 (80%)]  \tLoss:   91.658951\trec:   63.934006\tkl:   27.724943\n",
      "====> Epoch: 865 Average train loss: 89.6185\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4167\n",
      "Epoch: 866 [  100/50000 ( 0%)]  \tLoss:   89.269783\trec:   61.846069\tkl:   27.423717\n",
      "Epoch: 866 [10100/50000 (20%)]  \tLoss:   89.074120\trec:   62.531025\tkl:   26.543100\n",
      "Epoch: 866 [20100/50000 (40%)]  \tLoss:   86.047989\trec:   59.849346\tkl:   26.198647\n",
      "Epoch: 866 [30100/50000 (60%)]  \tLoss:   89.335083\trec:   62.878269\tkl:   26.456818\n",
      "Epoch: 866 [40100/50000 (80%)]  \tLoss:   94.020157\trec:   66.145065\tkl:   27.875084\n",
      "====> Epoch: 866 Average train loss: 89.6468\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3399\n",
      "Epoch: 867 [  100/50000 ( 0%)]  \tLoss:   89.787148\trec:   62.745888\tkl:   27.041254\n",
      "Epoch: 867 [10100/50000 (20%)]  \tLoss:   89.325714\trec:   62.350330\tkl:   26.975382\n",
      "Epoch: 867 [20100/50000 (40%)]  \tLoss:   89.885391\trec:   61.949883\tkl:   27.935503\n",
      "Epoch: 867 [30100/50000 (60%)]  \tLoss:   89.127861\trec:   62.091702\tkl:   27.036156\n",
      "Epoch: 867 [40100/50000 (80%)]  \tLoss:   91.336166\trec:   63.518509\tkl:   27.817661\n",
      "====> Epoch: 867 Average train loss: 89.6452\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4214\n",
      "Epoch: 868 [  100/50000 ( 0%)]  \tLoss:   90.711754\trec:   63.163311\tkl:   27.548452\n",
      "Epoch: 868 [10100/50000 (20%)]  \tLoss:   86.655487\trec:   60.840183\tkl:   25.815302\n",
      "Epoch: 868 [20100/50000 (40%)]  \tLoss:   90.367752\trec:   62.939579\tkl:   27.428179\n",
      "Epoch: 868 [30100/50000 (60%)]  \tLoss:   90.811844\trec:   63.478291\tkl:   27.333557\n",
      "Epoch: 868 [40100/50000 (80%)]  \tLoss:   91.236641\trec:   64.179306\tkl:   27.057335\n",
      "====> Epoch: 868 Average train loss: 89.6245\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2802\n",
      "Epoch: 869 [  100/50000 ( 0%)]  \tLoss:   91.521584\trec:   64.216087\tkl:   27.305494\n",
      "Epoch: 869 [10100/50000 (20%)]  \tLoss:   86.948593\trec:   60.505016\tkl:   26.443569\n",
      "Epoch: 869 [20100/50000 (40%)]  \tLoss:   91.937363\trec:   63.523430\tkl:   28.413935\n",
      "Epoch: 869 [30100/50000 (60%)]  \tLoss:   86.223366\trec:   60.176525\tkl:   26.046841\n",
      "Epoch: 869 [40100/50000 (80%)]  \tLoss:   85.701759\trec:   59.369900\tkl:   26.331850\n",
      "====> Epoch: 869 Average train loss: 89.6288\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.1954\n",
      "Epoch: 870 [  100/50000 ( 0%)]  \tLoss:   90.063347\trec:   62.992973\tkl:   27.070374\n",
      "Epoch: 870 [10100/50000 (20%)]  \tLoss:   87.652573\trec:   60.810745\tkl:   26.841831\n",
      "Epoch: 870 [20100/50000 (40%)]  \tLoss:   93.259216\trec:   65.568306\tkl:   27.690908\n",
      "Epoch: 870 [30100/50000 (60%)]  \tLoss:   87.244537\trec:   61.173222\tkl:   26.071318\n",
      "Epoch: 870 [40100/50000 (80%)]  \tLoss:   89.006538\trec:   61.957733\tkl:   27.048813\n",
      "====> Epoch: 870 Average train loss: 89.6394\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3196\n",
      "Epoch: 871 [  100/50000 ( 0%)]  \tLoss:   85.164963\trec:   59.305542\tkl:   25.859421\n",
      "Epoch: 871 [10100/50000 (20%)]  \tLoss:   92.964706\trec:   64.775635\tkl:   28.189072\n",
      "Epoch: 871 [20100/50000 (40%)]  \tLoss:   89.467628\trec:   63.347412\tkl:   26.120213\n",
      "Epoch: 871 [30100/50000 (60%)]  \tLoss:   88.751259\trec:   63.318798\tkl:   25.432463\n",
      "Epoch: 871 [40100/50000 (80%)]  \tLoss:   89.591171\trec:   62.877464\tkl:   26.713701\n",
      "====> Epoch: 871 Average train loss: 89.6259\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2719\n",
      "Epoch: 872 [  100/50000 ( 0%)]  \tLoss:   92.404762\trec:   64.547798\tkl:   27.856956\n",
      "Epoch: 872 [10100/50000 (20%)]  \tLoss:   87.426132\trec:   60.887299\tkl:   26.538832\n",
      "Epoch: 872 [20100/50000 (40%)]  \tLoss:   89.874763\trec:   62.561581\tkl:   27.313185\n",
      "Epoch: 872 [30100/50000 (60%)]  \tLoss:   87.647514\trec:   61.216503\tkl:   26.431009\n",
      "Epoch: 872 [40100/50000 (80%)]  \tLoss:   88.972031\trec:   61.939705\tkl:   27.032328\n",
      "====> Epoch: 872 Average train loss: 89.6178\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3158\n",
      "Epoch: 873 [  100/50000 ( 0%)]  \tLoss:   92.819679\trec:   66.035118\tkl:   26.784563\n",
      "Epoch: 873 [10100/50000 (20%)]  \tLoss:   93.041656\trec:   65.167290\tkl:   27.874359\n",
      "Epoch: 873 [20100/50000 (40%)]  \tLoss:   90.327354\trec:   63.323864\tkl:   27.003487\n",
      "Epoch: 873 [30100/50000 (60%)]  \tLoss:   84.634132\trec:   58.376377\tkl:   26.257753\n",
      "Epoch: 873 [40100/50000 (80%)]  \tLoss:   89.423836\trec:   62.361126\tkl:   27.062712\n",
      "====> Epoch: 873 Average train loss: 89.6187\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3461\n",
      "Epoch: 874 [  100/50000 ( 0%)]  \tLoss:   90.932732\trec:   64.216545\tkl:   26.716187\n",
      "Epoch: 874 [10100/50000 (20%)]  \tLoss:   89.588425\trec:   63.423115\tkl:   26.165310\n",
      "Epoch: 874 [20100/50000 (40%)]  \tLoss:   89.093269\trec:   61.840458\tkl:   27.252815\n",
      "Epoch: 874 [30100/50000 (60%)]  \tLoss:   90.671791\trec:   62.960155\tkl:   27.711639\n",
      "Epoch: 874 [40100/50000 (80%)]  \tLoss:   87.992722\trec:   61.859726\tkl:   26.132996\n",
      "====> Epoch: 874 Average train loss: 89.6465\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4291\n",
      "Epoch: 875 [  100/50000 ( 0%)]  \tLoss:   84.487022\trec:   58.970341\tkl:   25.516676\n",
      "Epoch: 875 [10100/50000 (20%)]  \tLoss:   84.250313\trec:   57.846104\tkl:   26.404205\n",
      "Epoch: 875 [20100/50000 (40%)]  \tLoss:   90.298065\trec:   62.822216\tkl:   27.475851\n",
      "Epoch: 875 [30100/50000 (60%)]  \tLoss:   89.996460\trec:   62.683613\tkl:   27.312849\n",
      "Epoch: 875 [40100/50000 (80%)]  \tLoss:   89.512886\trec:   62.610428\tkl:   26.902458\n",
      "====> Epoch: 875 Average train loss: 89.6165\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2917\n",
      "Epoch: 876 [  100/50000 ( 0%)]  \tLoss:   88.665627\trec:   61.835972\tkl:   26.829653\n",
      "Epoch: 876 [10100/50000 (20%)]  \tLoss:   92.163651\trec:   64.386383\tkl:   27.777260\n",
      "Epoch: 876 [20100/50000 (40%)]  \tLoss:   90.882599\trec:   63.977192\tkl:   26.905405\n",
      "Epoch: 876 [30100/50000 (60%)]  \tLoss:   89.138306\trec:   62.258251\tkl:   26.880056\n",
      "Epoch: 876 [40100/50000 (80%)]  \tLoss:   92.853767\trec:   64.667496\tkl:   28.186270\n",
      "====> Epoch: 876 Average train loss: 89.5909\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3698\n",
      "Epoch: 877 [  100/50000 ( 0%)]  \tLoss:   88.584663\trec:   62.111362\tkl:   26.473310\n",
      "Epoch: 877 [10100/50000 (20%)]  \tLoss:   88.725456\trec:   62.436371\tkl:   26.289085\n",
      "Epoch: 877 [20100/50000 (40%)]  \tLoss:   89.632263\trec:   62.825264\tkl:   26.807001\n",
      "Epoch: 877 [30100/50000 (60%)]  \tLoss:   88.694481\trec:   61.498180\tkl:   27.196299\n",
      "Epoch: 877 [40100/50000 (80%)]  \tLoss:   89.931290\trec:   63.203369\tkl:   26.727917\n",
      "====> Epoch: 877 Average train loss: 89.6067\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2090\n",
      "Epoch: 878 [  100/50000 ( 0%)]  \tLoss:   89.900185\trec:   62.660690\tkl:   27.239492\n",
      "Epoch: 878 [10100/50000 (20%)]  \tLoss:   94.964455\trec:   67.316055\tkl:   27.648397\n",
      "Epoch: 878 [20100/50000 (40%)]  \tLoss:   93.299179\trec:   65.738235\tkl:   27.560944\n",
      "Epoch: 878 [30100/50000 (60%)]  \tLoss:   91.819862\trec:   64.525826\tkl:   27.294035\n",
      "Epoch: 878 [40100/50000 (80%)]  \tLoss:   92.053131\trec:   64.367767\tkl:   27.685368\n",
      "====> Epoch: 878 Average train loss: 89.6063\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3399\n",
      "Epoch: 879 [  100/50000 ( 0%)]  \tLoss:   87.406502\trec:   61.050068\tkl:   26.356432\n",
      "Epoch: 879 [10100/50000 (20%)]  \tLoss:   88.359848\trec:   61.587292\tkl:   26.772560\n",
      "Epoch: 879 [20100/50000 (40%)]  \tLoss:   87.781853\trec:   60.585949\tkl:   27.195900\n",
      "Epoch: 879 [30100/50000 (60%)]  \tLoss:   89.204453\trec:   62.351006\tkl:   26.853451\n",
      "Epoch: 879 [40100/50000 (80%)]  \tLoss:   90.147926\trec:   63.183708\tkl:   26.964224\n",
      "====> Epoch: 879 Average train loss: 89.6045\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4030\n",
      "Epoch: 880 [  100/50000 ( 0%)]  \tLoss:   87.239395\trec:   60.988010\tkl:   26.251383\n",
      "Epoch: 880 [10100/50000 (20%)]  \tLoss:   90.442413\trec:   63.276482\tkl:   27.165930\n",
      "Epoch: 880 [20100/50000 (40%)]  \tLoss:   91.737892\trec:   64.336685\tkl:   27.401196\n",
      "Epoch: 880 [30100/50000 (60%)]  \tLoss:   90.291557\trec:   62.333908\tkl:   27.957645\n",
      "Epoch: 880 [40100/50000 (80%)]  \tLoss:   93.944679\trec:   66.410454\tkl:   27.534220\n",
      "====> Epoch: 880 Average train loss: 89.6106\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3479\n",
      "Epoch: 881 [  100/50000 ( 0%)]  \tLoss:   94.688377\trec:   65.857910\tkl:   28.830465\n",
      "Epoch: 881 [10100/50000 (20%)]  \tLoss:   94.562195\trec:   67.303413\tkl:   27.258785\n",
      "Epoch: 881 [20100/50000 (40%)]  \tLoss:   90.952042\trec:   63.964607\tkl:   26.987431\n",
      "Epoch: 881 [30100/50000 (60%)]  \tLoss:   91.205681\trec:   64.325798\tkl:   26.879885\n",
      "Epoch: 881 [40100/50000 (80%)]  \tLoss:   90.783829\trec:   63.845383\tkl:   26.938446\n",
      "====> Epoch: 881 Average train loss: 89.5942\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3536\n",
      "Epoch: 882 [  100/50000 ( 0%)]  \tLoss:   89.309372\trec:   62.979824\tkl:   26.329548\n",
      "Epoch: 882 [10100/50000 (20%)]  \tLoss:   84.303398\trec:   58.230408\tkl:   26.072983\n",
      "Epoch: 882 [20100/50000 (40%)]  \tLoss:   89.572350\trec:   62.046814\tkl:   27.525534\n",
      "Epoch: 882 [30100/50000 (60%)]  \tLoss:   90.824150\trec:   63.306805\tkl:   27.517345\n",
      "Epoch: 882 [40100/50000 (80%)]  \tLoss:   88.978249\trec:   61.565086\tkl:   27.413166\n",
      "====> Epoch: 882 Average train loss: 89.6018\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4039\n",
      "Epoch: 883 [  100/50000 ( 0%)]  \tLoss:   88.359009\trec:   61.655773\tkl:   26.703234\n",
      "Epoch: 883 [10100/50000 (20%)]  \tLoss:   89.991386\trec:   62.961529\tkl:   27.029854\n",
      "Epoch: 883 [20100/50000 (40%)]  \tLoss:   87.484825\trec:   61.093819\tkl:   26.391005\n",
      "Epoch: 883 [30100/50000 (60%)]  \tLoss:   89.339607\trec:   62.617561\tkl:   26.722046\n",
      "Epoch: 883 [40100/50000 (80%)]  \tLoss:   89.949303\trec:   62.545681\tkl:   27.403622\n",
      "====> Epoch: 883 Average train loss: 89.6056\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2979\n",
      "Epoch: 884 [  100/50000 ( 0%)]  \tLoss:   90.111923\trec:   63.652210\tkl:   26.459709\n",
      "Epoch: 884 [10100/50000 (20%)]  \tLoss:   88.374878\trec:   61.188709\tkl:   27.186169\n",
      "Epoch: 884 [20100/50000 (40%)]  \tLoss:   93.468513\trec:   65.771141\tkl:   27.697374\n",
      "Epoch: 884 [30100/50000 (60%)]  \tLoss:   89.688202\trec:   61.817566\tkl:   27.870628\n",
      "Epoch: 884 [40100/50000 (80%)]  \tLoss:   86.972946\trec:   60.022186\tkl:   26.950764\n",
      "====> Epoch: 884 Average train loss: 89.5736\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3210\n",
      "Epoch: 885 [  100/50000 ( 0%)]  \tLoss:   93.235397\trec:   65.078857\tkl:   28.156546\n",
      "Epoch: 885 [10100/50000 (20%)]  \tLoss:   86.028915\trec:   59.066738\tkl:   26.962179\n",
      "Epoch: 885 [20100/50000 (40%)]  \tLoss:   86.336021\trec:   60.146679\tkl:   26.189344\n",
      "Epoch: 885 [30100/50000 (60%)]  \tLoss:   90.116577\trec:   63.545155\tkl:   26.571430\n",
      "Epoch: 885 [40100/50000 (80%)]  \tLoss:   84.608116\trec:   58.790642\tkl:   25.817472\n",
      "====> Epoch: 885 Average train loss: 89.5969\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3986\n",
      "Epoch: 886 [  100/50000 ( 0%)]  \tLoss:   90.894295\trec:   63.772717\tkl:   27.121582\n",
      "Epoch: 886 [10100/50000 (20%)]  \tLoss:   90.640076\trec:   63.204887\tkl:   27.435192\n",
      "Epoch: 886 [20100/50000 (40%)]  \tLoss:   89.298454\trec:   61.274227\tkl:   28.024233\n",
      "Epoch: 886 [30100/50000 (60%)]  \tLoss:   89.905457\trec:   63.220520\tkl:   26.684937\n",
      "Epoch: 886 [40100/50000 (80%)]  \tLoss:   88.674507\trec:   60.721470\tkl:   27.953047\n",
      "====> Epoch: 886 Average train loss: 89.5747\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2431\n",
      "Epoch: 887 [  100/50000 ( 0%)]  \tLoss:   91.527946\trec:   63.875248\tkl:   27.652704\n",
      "Epoch: 887 [10100/50000 (20%)]  \tLoss:   88.618370\trec:   62.059540\tkl:   26.558830\n",
      "Epoch: 887 [20100/50000 (40%)]  \tLoss:   90.903397\trec:   63.402569\tkl:   27.500824\n",
      "Epoch: 887 [30100/50000 (60%)]  \tLoss:   89.323746\trec:   62.965847\tkl:   26.357895\n",
      "Epoch: 887 [40100/50000 (80%)]  \tLoss:   89.405060\trec:   62.731525\tkl:   26.673532\n",
      "====> Epoch: 887 Average train loss: 89.5968\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2736\n",
      "Epoch: 888 [  100/50000 ( 0%)]  \tLoss:   87.784317\trec:   61.085438\tkl:   26.698874\n",
      "Epoch: 888 [10100/50000 (20%)]  \tLoss:   95.792130\trec:   68.030235\tkl:   27.761889\n",
      "Epoch: 888 [20100/50000 (40%)]  \tLoss:   93.675659\trec:   65.729103\tkl:   27.946552\n",
      "Epoch: 888 [30100/50000 (60%)]  \tLoss:   89.748100\trec:   62.661629\tkl:   27.086470\n",
      "Epoch: 888 [40100/50000 (80%)]  \tLoss:   92.200317\trec:   65.083702\tkl:   27.116613\n",
      "====> Epoch: 888 Average train loss: 89.5751\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3671\n",
      "Epoch: 889 [  100/50000 ( 0%)]  \tLoss:   88.727341\trec:   61.160965\tkl:   27.566376\n",
      "Epoch: 889 [10100/50000 (20%)]  \tLoss:   89.597511\trec:   62.779877\tkl:   26.817635\n",
      "Epoch: 889 [20100/50000 (40%)]  \tLoss:   93.498413\trec:   64.871918\tkl:   28.626493\n",
      "Epoch: 889 [30100/50000 (60%)]  \tLoss:   88.361084\trec:   61.352951\tkl:   27.008127\n",
      "Epoch: 889 [40100/50000 (80%)]  \tLoss:   90.799118\trec:   62.922886\tkl:   27.876234\n",
      "====> Epoch: 889 Average train loss: 89.5909\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3069\n",
      "Epoch: 890 [  100/50000 ( 0%)]  \tLoss:   85.607147\trec:   59.383572\tkl:   26.223576\n",
      "Epoch: 890 [10100/50000 (20%)]  \tLoss:   87.954918\trec:   61.474590\tkl:   26.480330\n",
      "Epoch: 890 [20100/50000 (40%)]  \tLoss:   88.835670\trec:   61.513603\tkl:   27.322069\n",
      "Epoch: 890 [30100/50000 (60%)]  \tLoss:   90.378731\trec:   64.397247\tkl:   25.981482\n",
      "Epoch: 890 [40100/50000 (80%)]  \tLoss:   88.812576\trec:   61.131279\tkl:   27.681303\n",
      "====> Epoch: 890 Average train loss: 89.5759\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2911\n",
      "Epoch: 891 [  100/50000 ( 0%)]  \tLoss:   89.398842\trec:   62.135769\tkl:   27.263081\n",
      "Epoch: 891 [10100/50000 (20%)]  \tLoss:   89.661530\trec:   62.248493\tkl:   27.413034\n",
      "Epoch: 891 [20100/50000 (40%)]  \tLoss:   88.833290\trec:   62.024368\tkl:   26.808922\n",
      "Epoch: 891 [30100/50000 (60%)]  \tLoss:   88.380547\trec:   61.302063\tkl:   27.078476\n",
      "Epoch: 891 [40100/50000 (80%)]  \tLoss:   87.477242\trec:   60.530396\tkl:   26.946846\n",
      "====> Epoch: 891 Average train loss: 89.5823\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3454\n",
      "Epoch: 892 [  100/50000 ( 0%)]  \tLoss:   92.695953\trec:   64.987579\tkl:   27.708382\n",
      "Epoch: 892 [10100/50000 (20%)]  \tLoss:   83.659569\trec:   57.235043\tkl:   26.424520\n",
      "Epoch: 892 [20100/50000 (40%)]  \tLoss:   88.553711\trec:   62.312695\tkl:   26.241020\n",
      "Epoch: 892 [30100/50000 (60%)]  \tLoss:   85.310822\trec:   60.457645\tkl:   24.853170\n",
      "Epoch: 892 [40100/50000 (80%)]  \tLoss:   87.681030\trec:   61.356743\tkl:   26.324287\n",
      "====> Epoch: 892 Average train loss: 89.5839\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3071\n",
      "Epoch: 893 [  100/50000 ( 0%)]  \tLoss:   87.933769\trec:   60.173485\tkl:   27.760284\n",
      "Epoch: 893 [10100/50000 (20%)]  \tLoss:   87.617119\trec:   60.691082\tkl:   26.926037\n",
      "Epoch: 893 [20100/50000 (40%)]  \tLoss:   86.779579\trec:   59.940342\tkl:   26.839235\n",
      "Epoch: 893 [30100/50000 (60%)]  \tLoss:   86.220329\trec:   59.991684\tkl:   26.228645\n",
      "Epoch: 893 [40100/50000 (80%)]  \tLoss:   88.028549\trec:   60.565422\tkl:   27.463131\n",
      "====> Epoch: 893 Average train loss: 89.5561\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2586\n",
      "Epoch: 894 [  100/50000 ( 0%)]  \tLoss:   89.682198\trec:   62.636581\tkl:   27.045614\n",
      "Epoch: 894 [10100/50000 (20%)]  \tLoss:   90.362167\trec:   62.949741\tkl:   27.412432\n",
      "Epoch: 894 [20100/50000 (40%)]  \tLoss:   90.231834\trec:   63.769466\tkl:   26.462370\n",
      "Epoch: 894 [30100/50000 (60%)]  \tLoss:   89.499870\trec:   62.126839\tkl:   27.373030\n",
      "Epoch: 894 [40100/50000 (80%)]  \tLoss:   90.978516\trec:   63.818798\tkl:   27.159719\n",
      "====> Epoch: 894 Average train loss: 89.5712\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4642\n",
      "Epoch: 895 [  100/50000 ( 0%)]  \tLoss:   90.080994\trec:   63.031853\tkl:   27.049145\n",
      "Epoch: 895 [10100/50000 (20%)]  \tLoss:   90.499489\trec:   63.434998\tkl:   27.064497\n",
      "Epoch: 895 [20100/50000 (40%)]  \tLoss:   88.397598\trec:   61.695595\tkl:   26.702000\n",
      "Epoch: 895 [30100/50000 (60%)]  \tLoss:   87.627274\trec:   60.962132\tkl:   26.665144\n",
      "Epoch: 895 [40100/50000 (80%)]  \tLoss:   89.133995\trec:   63.040302\tkl:   26.093691\n",
      "====> Epoch: 895 Average train loss: 89.5716\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3233\n",
      "Epoch: 896 [  100/50000 ( 0%)]  \tLoss:   91.028984\trec:   63.110237\tkl:   27.918739\n",
      "Epoch: 896 [10100/50000 (20%)]  \tLoss:   90.308632\trec:   63.130829\tkl:   27.177803\n",
      "Epoch: 896 [20100/50000 (40%)]  \tLoss:   92.274429\trec:   64.548241\tkl:   27.726191\n",
      "Epoch: 896 [30100/50000 (60%)]  \tLoss:   86.954414\trec:   59.983978\tkl:   26.970436\n",
      "Epoch: 896 [40100/50000 (80%)]  \tLoss:   91.078682\trec:   63.346680\tkl:   27.731998\n",
      "====> Epoch: 896 Average train loss: 89.5551\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2879\n",
      "Epoch: 897 [  100/50000 ( 0%)]  \tLoss:   87.665154\trec:   60.470242\tkl:   27.194916\n",
      "Epoch: 897 [10100/50000 (20%)]  \tLoss:   89.541504\trec:   62.491680\tkl:   27.049826\n",
      "Epoch: 897 [20100/50000 (40%)]  \tLoss:   90.138641\trec:   62.984501\tkl:   27.154137\n",
      "Epoch: 897 [30100/50000 (60%)]  \tLoss:   87.986755\trec:   61.865925\tkl:   26.120829\n",
      "Epoch: 897 [40100/50000 (80%)]  \tLoss:   82.722389\trec:   57.306454\tkl:   25.415934\n",
      "====> Epoch: 897 Average train loss: 89.5491\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3357\n",
      "Epoch: 898 [  100/50000 ( 0%)]  \tLoss:   86.027130\trec:   59.752171\tkl:   26.274958\n",
      "Epoch: 898 [10100/50000 (20%)]  \tLoss:   91.990204\trec:   64.304039\tkl:   27.686161\n",
      "Epoch: 898 [20100/50000 (40%)]  \tLoss:   89.663803\trec:   62.268978\tkl:   27.394831\n",
      "Epoch: 898 [30100/50000 (60%)]  \tLoss:   88.081245\trec:   61.460739\tkl:   26.620512\n",
      "Epoch: 898 [40100/50000 (80%)]  \tLoss:   87.114334\trec:   60.365078\tkl:   26.749258\n",
      "====> Epoch: 898 Average train loss: 89.5805\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2809\n",
      "Epoch: 899 [  100/50000 ( 0%)]  \tLoss:   90.923096\trec:   63.015720\tkl:   27.907372\n",
      "Epoch: 899 [10100/50000 (20%)]  \tLoss:   90.823746\trec:   64.450890\tkl:   26.372860\n",
      "Epoch: 899 [20100/50000 (40%)]  \tLoss:   88.471092\trec:   61.994095\tkl:   26.477001\n",
      "Epoch: 899 [30100/50000 (60%)]  \tLoss:   91.230400\trec:   63.772694\tkl:   27.457706\n",
      "Epoch: 899 [40100/50000 (80%)]  \tLoss:   89.895859\trec:   62.447247\tkl:   27.448618\n",
      "====> Epoch: 899 Average train loss: 89.5758\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3268\n",
      "Epoch: 900 [  100/50000 ( 0%)]  \tLoss:   89.501183\trec:   62.320019\tkl:   27.181162\n",
      "Epoch: 900 [10100/50000 (20%)]  \tLoss:   88.868843\trec:   61.616055\tkl:   27.252787\n",
      "Epoch: 900 [20100/50000 (40%)]  \tLoss:   89.878769\trec:   62.275253\tkl:   27.603518\n",
      "Epoch: 900 [30100/50000 (60%)]  \tLoss:   88.644394\trec:   61.695538\tkl:   26.948856\n",
      "Epoch: 900 [40100/50000 (80%)]  \tLoss:   95.347183\trec:   66.216286\tkl:   29.130905\n",
      "====> Epoch: 900 Average train loss: 89.5517\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3897\n",
      "Epoch: 901 [  100/50000 ( 0%)]  \tLoss:   92.485664\trec:   65.974266\tkl:   26.511400\n",
      "Epoch: 901 [10100/50000 (20%)]  \tLoss:   89.742165\trec:   62.101112\tkl:   27.641054\n",
      "Epoch: 901 [20100/50000 (40%)]  \tLoss:   90.910065\trec:   64.086472\tkl:   26.823593\n",
      "Epoch: 901 [30100/50000 (60%)]  \tLoss:   89.806091\trec:   62.370594\tkl:   27.435503\n",
      "Epoch: 901 [40100/50000 (80%)]  \tLoss:   86.961189\trec:   60.363495\tkl:   26.597700\n",
      "====> Epoch: 901 Average train loss: 89.5495\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2510\n",
      "Epoch: 902 [  100/50000 ( 0%)]  \tLoss:   89.151031\trec:   62.219440\tkl:   26.931593\n",
      "Epoch: 902 [10100/50000 (20%)]  \tLoss:   90.595253\trec:   63.019470\tkl:   27.575781\n",
      "Epoch: 902 [20100/50000 (40%)]  \tLoss:   88.830391\trec:   62.045174\tkl:   26.785210\n",
      "Epoch: 902 [30100/50000 (60%)]  \tLoss:   90.662994\trec:   63.188625\tkl:   27.474373\n",
      "Epoch: 902 [40100/50000 (80%)]  \tLoss:   86.569756\trec:   59.433899\tkl:   27.135851\n",
      "====> Epoch: 902 Average train loss: 89.5505\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2259\n",
      "Epoch: 903 [  100/50000 ( 0%)]  \tLoss:   90.851326\trec:   63.603691\tkl:   27.247641\n",
      "Epoch: 903 [10100/50000 (20%)]  \tLoss:   90.342026\trec:   63.812546\tkl:   26.529484\n",
      "Epoch: 903 [20100/50000 (40%)]  \tLoss:   90.651154\trec:   63.134411\tkl:   27.516733\n",
      "Epoch: 903 [30100/50000 (60%)]  \tLoss:   90.147507\trec:   63.783661\tkl:   26.363848\n",
      "Epoch: 903 [40100/50000 (80%)]  \tLoss:   87.480827\trec:   61.027088\tkl:   26.453739\n",
      "====> Epoch: 903 Average train loss: 89.5465\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3472\n",
      "Epoch: 904 [  100/50000 ( 0%)]  \tLoss:   87.757050\trec:   61.259041\tkl:   26.498007\n",
      "Epoch: 904 [10100/50000 (20%)]  \tLoss:   88.346756\trec:   61.703602\tkl:   26.643154\n",
      "Epoch: 904 [20100/50000 (40%)]  \tLoss:   90.094048\trec:   63.417099\tkl:   26.676952\n",
      "Epoch: 904 [30100/50000 (60%)]  \tLoss:   90.296753\trec:   63.972469\tkl:   26.324289\n",
      "Epoch: 904 [40100/50000 (80%)]  \tLoss:   94.363396\trec:   66.164551\tkl:   28.198843\n",
      "====> Epoch: 904 Average train loss: 89.5572\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3321\n",
      "Epoch: 905 [  100/50000 ( 0%)]  \tLoss:   90.305496\trec:   62.912842\tkl:   27.392658\n",
      "Epoch: 905 [10100/50000 (20%)]  \tLoss:   91.535172\trec:   64.184189\tkl:   27.350988\n",
      "Epoch: 905 [20100/50000 (40%)]  \tLoss:   94.244598\trec:   65.763664\tkl:   28.480936\n",
      "Epoch: 905 [30100/50000 (60%)]  \tLoss:   89.019547\trec:   63.046150\tkl:   25.973396\n",
      "Epoch: 905 [40100/50000 (80%)]  \tLoss:   92.806168\trec:   64.518661\tkl:   28.287510\n",
      "====> Epoch: 905 Average train loss: 89.5419\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2876\n",
      "Epoch: 906 [  100/50000 ( 0%)]  \tLoss:   87.254845\trec:   59.767864\tkl:   27.486973\n",
      "Epoch: 906 [10100/50000 (20%)]  \tLoss:   91.962593\trec:   64.458984\tkl:   27.503614\n",
      "Epoch: 906 [20100/50000 (40%)]  \tLoss:   89.793037\trec:   62.167568\tkl:   27.625467\n",
      "Epoch: 906 [30100/50000 (60%)]  \tLoss:   89.191200\trec:   62.062328\tkl:   27.128872\n",
      "Epoch: 906 [40100/50000 (80%)]  \tLoss:   92.352928\trec:   64.772957\tkl:   27.579967\n",
      "====> Epoch: 906 Average train loss: 89.5537\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4126\n",
      "Epoch: 907 [  100/50000 ( 0%)]  \tLoss:   89.290115\trec:   61.715656\tkl:   27.574453\n",
      "Epoch: 907 [10100/50000 (20%)]  \tLoss:   88.087242\trec:   60.982769\tkl:   27.104477\n",
      "Epoch: 907 [20100/50000 (40%)]  \tLoss:   89.836792\trec:   62.382858\tkl:   27.453932\n",
      "Epoch: 907 [30100/50000 (60%)]  \tLoss:   90.490387\trec:   63.163952\tkl:   27.326433\n",
      "Epoch: 907 [40100/50000 (80%)]  \tLoss:   91.852905\trec:   64.228081\tkl:   27.624825\n",
      "====> Epoch: 907 Average train loss: 89.5357\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3135\n",
      "Epoch: 908 [  100/50000 ( 0%)]  \tLoss:   88.418556\trec:   60.839993\tkl:   27.578562\n",
      "Epoch: 908 [10100/50000 (20%)]  \tLoss:   91.554840\trec:   64.472000\tkl:   27.082836\n",
      "Epoch: 908 [20100/50000 (40%)]  \tLoss:   91.634766\trec:   63.667515\tkl:   27.967255\n",
      "Epoch: 908 [30100/50000 (60%)]  \tLoss:   87.471443\trec:   60.559364\tkl:   26.912075\n",
      "Epoch: 908 [40100/50000 (80%)]  \tLoss:   90.317635\trec:   63.290974\tkl:   27.026655\n",
      "====> Epoch: 908 Average train loss: 89.5505\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3489\n",
      "Epoch: 909 [  100/50000 ( 0%)]  \tLoss:   85.053825\trec:   59.145233\tkl:   25.908588\n",
      "Epoch: 909 [10100/50000 (20%)]  \tLoss:   87.439919\trec:   61.804272\tkl:   25.635649\n",
      "Epoch: 909 [20100/50000 (40%)]  \tLoss:   87.108223\trec:   60.815964\tkl:   26.292257\n",
      "Epoch: 909 [30100/50000 (60%)]  \tLoss:   87.967216\trec:   61.489365\tkl:   26.477848\n",
      "Epoch: 909 [40100/50000 (80%)]  \tLoss:   87.907303\trec:   61.171326\tkl:   26.735979\n",
      "====> Epoch: 909 Average train loss: 89.5207\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2164\n",
      "Epoch: 910 [  100/50000 ( 0%)]  \tLoss:   90.796432\trec:   62.617996\tkl:   28.178434\n",
      "Epoch: 910 [10100/50000 (20%)]  \tLoss:   84.248955\trec:   57.867004\tkl:   26.381948\n",
      "Epoch: 910 [20100/50000 (40%)]  \tLoss:   90.815834\trec:   63.147034\tkl:   27.668806\n",
      "Epoch: 910 [30100/50000 (60%)]  \tLoss:   90.279625\trec:   63.139286\tkl:   27.140347\n",
      "Epoch: 910 [40100/50000 (80%)]  \tLoss:   87.492905\trec:   61.037430\tkl:   26.455475\n",
      "====> Epoch: 910 Average train loss: 89.5222\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3495\n",
      "Epoch: 911 [  100/50000 ( 0%)]  \tLoss:   86.266396\trec:   60.305595\tkl:   25.960802\n",
      "Epoch: 911 [10100/50000 (20%)]  \tLoss:   86.861557\trec:   60.425304\tkl:   26.436258\n",
      "Epoch: 911 [20100/50000 (40%)]  \tLoss:   91.939995\trec:   64.252838\tkl:   27.687153\n",
      "Epoch: 911 [30100/50000 (60%)]  \tLoss:   89.059448\trec:   62.222759\tkl:   26.836697\n",
      "Epoch: 911 [40100/50000 (80%)]  \tLoss:   90.996544\trec:   64.052544\tkl:   26.943991\n",
      "====> Epoch: 911 Average train loss: 89.5268\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4226\n",
      "Epoch: 912 [  100/50000 ( 0%)]  \tLoss:   84.926315\trec:   58.527893\tkl:   26.398420\n",
      "Epoch: 912 [10100/50000 (20%)]  \tLoss:   87.506485\trec:   60.519226\tkl:   26.987253\n",
      "Epoch: 912 [20100/50000 (40%)]  \tLoss:   90.996910\trec:   63.453793\tkl:   27.543118\n",
      "Epoch: 912 [30100/50000 (60%)]  \tLoss:   90.258598\trec:   63.004749\tkl:   27.253849\n",
      "Epoch: 912 [40100/50000 (80%)]  \tLoss:   89.462303\trec:   62.352085\tkl:   27.110222\n",
      "====> Epoch: 912 Average train loss: 89.5263\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3551\n",
      "Epoch: 913 [  100/50000 ( 0%)]  \tLoss:   88.349403\trec:   62.067772\tkl:   26.281633\n",
      "Epoch: 913 [10100/50000 (20%)]  \tLoss:   86.129120\trec:   59.820210\tkl:   26.308907\n",
      "Epoch: 913 [20100/50000 (40%)]  \tLoss:   91.865456\trec:   63.858887\tkl:   28.006571\n",
      "Epoch: 913 [30100/50000 (60%)]  \tLoss:   88.972275\trec:   62.198055\tkl:   26.774218\n",
      "Epoch: 913 [40100/50000 (80%)]  \tLoss:   89.554306\trec:   62.610859\tkl:   26.943447\n",
      "====> Epoch: 913 Average train loss: 89.5289\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2571\n",
      "Epoch: 914 [  100/50000 ( 0%)]  \tLoss:   89.194725\trec:   62.029793\tkl:   27.164928\n",
      "Epoch: 914 [10100/50000 (20%)]  \tLoss:   91.700737\trec:   64.052200\tkl:   27.648539\n",
      "Epoch: 914 [20100/50000 (40%)]  \tLoss:   90.750046\trec:   63.082382\tkl:   27.667667\n",
      "Epoch: 914 [30100/50000 (60%)]  \tLoss:   91.348244\trec:   63.489380\tkl:   27.858856\n",
      "Epoch: 914 [40100/50000 (80%)]  \tLoss:   87.100006\trec:   60.346504\tkl:   26.753502\n",
      "====> Epoch: 914 Average train loss: 89.5146\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3150\n",
      "Epoch: 915 [  100/50000 ( 0%)]  \tLoss:   92.233620\trec:   64.894058\tkl:   27.339558\n",
      "Epoch: 915 [10100/50000 (20%)]  \tLoss:   92.457306\trec:   64.364655\tkl:   28.092651\n",
      "Epoch: 915 [20100/50000 (40%)]  \tLoss:   91.607468\trec:   63.859558\tkl:   27.747910\n",
      "Epoch: 915 [30100/50000 (60%)]  \tLoss:   90.324890\trec:   62.407207\tkl:   27.917685\n",
      "Epoch: 915 [40100/50000 (80%)]  \tLoss:   87.419975\trec:   61.152084\tkl:   26.267897\n",
      "====> Epoch: 915 Average train loss: 89.5180\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3344\n",
      "Epoch: 916 [  100/50000 ( 0%)]  \tLoss:   89.569946\trec:   62.907410\tkl:   26.662537\n",
      "Epoch: 916 [10100/50000 (20%)]  \tLoss:   92.216713\trec:   64.504120\tkl:   27.712603\n",
      "Epoch: 916 [20100/50000 (40%)]  \tLoss:   91.709648\trec:   64.079041\tkl:   27.630606\n",
      "Epoch: 916 [30100/50000 (60%)]  \tLoss:   89.540810\trec:   62.800896\tkl:   26.739912\n",
      "Epoch: 916 [40100/50000 (80%)]  \tLoss:   88.818474\trec:   62.422035\tkl:   26.396435\n",
      "====> Epoch: 916 Average train loss: 89.5426\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3941\n",
      "Epoch: 917 [  100/50000 ( 0%)]  \tLoss:   91.542336\trec:   64.144257\tkl:   27.398079\n",
      "Epoch: 917 [10100/50000 (20%)]  \tLoss:   86.574059\trec:   59.668945\tkl:   26.905111\n",
      "Epoch: 917 [20100/50000 (40%)]  \tLoss:   93.462593\trec:   65.292351\tkl:   28.170238\n",
      "Epoch: 917 [30100/50000 (60%)]  \tLoss:   87.339294\trec:   61.473213\tkl:   25.866089\n",
      "Epoch: 917 [40100/50000 (80%)]  \tLoss:   93.282829\trec:   65.772499\tkl:   27.510328\n",
      "====> Epoch: 917 Average train loss: 89.5131\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3321\n",
      "Epoch: 918 [  100/50000 ( 0%)]  \tLoss:   90.072563\trec:   63.197372\tkl:   26.875195\n",
      "Epoch: 918 [10100/50000 (20%)]  \tLoss:   92.235191\trec:   64.802040\tkl:   27.433151\n",
      "Epoch: 918 [20100/50000 (40%)]  \tLoss:   91.082619\trec:   63.422741\tkl:   27.659872\n",
      "Epoch: 918 [30100/50000 (60%)]  \tLoss:   88.621460\trec:   62.331406\tkl:   26.290062\n",
      "Epoch: 918 [40100/50000 (80%)]  \tLoss:   88.423660\trec:   61.153465\tkl:   27.270193\n",
      "====> Epoch: 918 Average train loss: 89.4715\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3621\n",
      "Epoch: 919 [  100/50000 ( 0%)]  \tLoss:   93.427521\trec:   65.243027\tkl:   28.184496\n",
      "Epoch: 919 [10100/50000 (20%)]  \tLoss:   92.560860\trec:   65.669044\tkl:   26.891811\n",
      "Epoch: 919 [20100/50000 (40%)]  \tLoss:   91.128845\trec:   63.792206\tkl:   27.336639\n",
      "Epoch: 919 [30100/50000 (60%)]  \tLoss:   85.439308\trec:   59.769592\tkl:   25.669712\n",
      "Epoch: 919 [40100/50000 (80%)]  \tLoss:   92.365738\trec:   65.119675\tkl:   27.246059\n",
      "====> Epoch: 919 Average train loss: 89.5217\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2927\n",
      "Epoch: 920 [  100/50000 ( 0%)]  \tLoss:   86.846855\trec:   60.288376\tkl:   26.558472\n",
      "Epoch: 920 [10100/50000 (20%)]  \tLoss:   90.379471\trec:   62.288166\tkl:   28.091305\n",
      "Epoch: 920 [20100/50000 (40%)]  \tLoss:   88.444214\trec:   62.197868\tkl:   26.246349\n",
      "Epoch: 920 [30100/50000 (60%)]  \tLoss:   93.011909\trec:   65.628784\tkl:   27.383125\n",
      "Epoch: 920 [40100/50000 (80%)]  \tLoss:   89.832848\trec:   63.077332\tkl:   26.755512\n",
      "====> Epoch: 920 Average train loss: 89.5168\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2385\n",
      "Epoch: 921 [  100/50000 ( 0%)]  \tLoss:   88.443192\trec:   61.855923\tkl:   26.587273\n",
      "Epoch: 921 [10100/50000 (20%)]  \tLoss:   88.153305\trec:   61.556580\tkl:   26.596727\n",
      "Epoch: 921 [20100/50000 (40%)]  \tLoss:   90.611343\trec:   62.939632\tkl:   27.671715\n",
      "Epoch: 921 [30100/50000 (60%)]  \tLoss:   89.832222\trec:   62.894447\tkl:   26.937782\n",
      "Epoch: 921 [40100/50000 (80%)]  \tLoss:   93.296402\trec:   65.743889\tkl:   27.552517\n",
      "====> Epoch: 921 Average train loss: 89.5039\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3050\n",
      "Epoch: 922 [  100/50000 ( 0%)]  \tLoss:   91.978767\trec:   64.390732\tkl:   27.588036\n",
      "Epoch: 922 [10100/50000 (20%)]  \tLoss:   87.883522\trec:   61.291855\tkl:   26.591669\n",
      "Epoch: 922 [20100/50000 (40%)]  \tLoss:   88.258301\trec:   61.220879\tkl:   27.037426\n",
      "Epoch: 922 [30100/50000 (60%)]  \tLoss:   90.850777\trec:   63.687519\tkl:   27.163261\n",
      "Epoch: 922 [40100/50000 (80%)]  \tLoss:   88.614540\trec:   62.077377\tkl:   26.537165\n",
      "====> Epoch: 922 Average train loss: 89.4976\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3247\n",
      "Epoch: 923 [  100/50000 ( 0%)]  \tLoss:   88.664452\trec:   62.127846\tkl:   26.536612\n",
      "Epoch: 923 [10100/50000 (20%)]  \tLoss:   91.943657\trec:   64.293068\tkl:   27.650595\n",
      "Epoch: 923 [20100/50000 (40%)]  \tLoss:   89.575645\trec:   61.609470\tkl:   27.966169\n",
      "Epoch: 923 [30100/50000 (60%)]  \tLoss:   86.627266\trec:   58.997108\tkl:   27.630150\n",
      "Epoch: 923 [40100/50000 (80%)]  \tLoss:   87.796738\trec:   61.131969\tkl:   26.664768\n",
      "====> Epoch: 923 Average train loss: 89.5114\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2910\n",
      "Epoch: 924 [  100/50000 ( 0%)]  \tLoss:   89.232361\trec:   61.951038\tkl:   27.281328\n",
      "Epoch: 924 [10100/50000 (20%)]  \tLoss:   86.967705\trec:   60.458271\tkl:   26.509430\n",
      "Epoch: 924 [20100/50000 (40%)]  \tLoss:   88.589088\trec:   61.464256\tkl:   27.124834\n",
      "Epoch: 924 [30100/50000 (60%)]  \tLoss:   89.182182\trec:   62.510086\tkl:   26.672104\n",
      "Epoch: 924 [40100/50000 (80%)]  \tLoss:   90.719177\trec:   64.007202\tkl:   26.711967\n",
      "====> Epoch: 924 Average train loss: 89.5131\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3029\n",
      "Epoch: 925 [  100/50000 ( 0%)]  \tLoss:   88.114555\trec:   60.745281\tkl:   27.369276\n",
      "Epoch: 925 [10100/50000 (20%)]  \tLoss:   84.197235\trec:   58.030319\tkl:   26.166914\n",
      "Epoch: 925 [20100/50000 (40%)]  \tLoss:   91.247147\trec:   64.236603\tkl:   27.010542\n",
      "Epoch: 925 [30100/50000 (60%)]  \tLoss:   91.484772\trec:   64.157623\tkl:   27.327148\n",
      "Epoch: 925 [40100/50000 (80%)]  \tLoss:   86.489754\trec:   60.288662\tkl:   26.201094\n",
      "====> Epoch: 925 Average train loss: 89.4963\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3256\n",
      "Epoch: 926 [  100/50000 ( 0%)]  \tLoss:   89.586044\trec:   62.530361\tkl:   27.055683\n",
      "Epoch: 926 [10100/50000 (20%)]  \tLoss:   89.763954\trec:   62.457909\tkl:   27.306047\n",
      "Epoch: 926 [20100/50000 (40%)]  \tLoss:   89.907959\trec:   62.416142\tkl:   27.491817\n",
      "Epoch: 926 [30100/50000 (60%)]  \tLoss:   89.058289\trec:   62.030491\tkl:   27.027800\n",
      "Epoch: 926 [40100/50000 (80%)]  \tLoss:   90.113930\trec:   63.851494\tkl:   26.262442\n",
      "====> Epoch: 926 Average train loss: 89.4958\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2876\n",
      "Epoch: 927 [  100/50000 ( 0%)]  \tLoss:   91.259575\trec:   64.209328\tkl:   27.050245\n",
      "Epoch: 927 [10100/50000 (20%)]  \tLoss:   89.838524\trec:   63.216015\tkl:   26.622507\n",
      "Epoch: 927 [20100/50000 (40%)]  \tLoss:   87.278908\trec:   60.818455\tkl:   26.460445\n",
      "Epoch: 927 [30100/50000 (60%)]  \tLoss:   86.802963\trec:   60.392159\tkl:   26.410807\n",
      "Epoch: 927 [40100/50000 (80%)]  \tLoss:   85.886719\trec:   59.682011\tkl:   26.204712\n",
      "====> Epoch: 927 Average train loss: 89.5072\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2393\n",
      "Epoch: 928 [  100/50000 ( 0%)]  \tLoss:   91.434235\trec:   63.664143\tkl:   27.770088\n",
      "Epoch: 928 [10100/50000 (20%)]  \tLoss:   90.280174\trec:   62.989853\tkl:   27.290323\n",
      "Epoch: 928 [20100/50000 (40%)]  \tLoss:   88.107033\trec:   62.098846\tkl:   26.008188\n",
      "Epoch: 928 [30100/50000 (60%)]  \tLoss:   90.355919\trec:   63.142548\tkl:   27.213369\n",
      "Epoch: 928 [40100/50000 (80%)]  \tLoss:   90.015884\trec:   62.704819\tkl:   27.311071\n",
      "====> Epoch: 928 Average train loss: 89.5012\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3205\n",
      "Epoch: 929 [  100/50000 ( 0%)]  \tLoss:   91.960258\trec:   64.383583\tkl:   27.576679\n",
      "Epoch: 929 [10100/50000 (20%)]  \tLoss:   92.496216\trec:   65.108223\tkl:   27.387995\n",
      "Epoch: 929 [20100/50000 (40%)]  \tLoss:   90.330788\trec:   62.817486\tkl:   27.513308\n",
      "Epoch: 929 [30100/50000 (60%)]  \tLoss:   92.113808\trec:   64.306046\tkl:   27.807753\n",
      "Epoch: 929 [40100/50000 (80%)]  \tLoss:   89.296219\trec:   62.135067\tkl:   27.161152\n",
      "====> Epoch: 929 Average train loss: 89.4960\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2100\n",
      "Epoch: 930 [  100/50000 ( 0%)]  \tLoss:   87.072540\trec:   61.109905\tkl:   25.962631\n",
      "Epoch: 930 [10100/50000 (20%)]  \tLoss:   90.884468\trec:   63.684734\tkl:   27.199741\n",
      "Epoch: 930 [20100/50000 (40%)]  \tLoss:   90.303291\trec:   63.690189\tkl:   26.613100\n",
      "Epoch: 930 [30100/50000 (60%)]  \tLoss:   87.531319\trec:   61.115124\tkl:   26.416189\n",
      "Epoch: 930 [40100/50000 (80%)]  \tLoss:   91.444122\trec:   64.430832\tkl:   27.013292\n",
      "====> Epoch: 930 Average train loss: 89.5061\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2802\n",
      "Epoch: 931 [  100/50000 ( 0%)]  \tLoss:   89.181519\trec:   61.811481\tkl:   27.370043\n",
      "Epoch: 931 [10100/50000 (20%)]  \tLoss:   84.270264\trec:   58.099266\tkl:   26.170996\n",
      "Epoch: 931 [20100/50000 (40%)]  \tLoss:   92.046974\trec:   64.199211\tkl:   27.847757\n",
      "Epoch: 931 [30100/50000 (60%)]  \tLoss:   91.953667\trec:   65.294090\tkl:   26.659575\n",
      "Epoch: 931 [40100/50000 (80%)]  \tLoss:   92.273071\trec:   65.046982\tkl:   27.226091\n",
      "====> Epoch: 931 Average train loss: 89.4796\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3452\n",
      "Epoch: 932 [  100/50000 ( 0%)]  \tLoss:   86.112869\trec:   59.252464\tkl:   26.860403\n",
      "Epoch: 932 [10100/50000 (20%)]  \tLoss:   89.115906\trec:   62.022099\tkl:   27.093807\n",
      "Epoch: 932 [20100/50000 (40%)]  \tLoss:   85.309433\trec:   59.198799\tkl:   26.110634\n",
      "Epoch: 932 [30100/50000 (60%)]  \tLoss:   90.250854\trec:   62.226894\tkl:   28.023966\n",
      "Epoch: 932 [40100/50000 (80%)]  \tLoss:   87.305611\trec:   60.383667\tkl:   26.921947\n",
      "====> Epoch: 932 Average train loss: 89.4897\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2798\n",
      "Epoch: 933 [  100/50000 ( 0%)]  \tLoss:   87.284393\trec:   60.250858\tkl:   27.033539\n",
      "Epoch: 933 [10100/50000 (20%)]  \tLoss:   89.939354\trec:   63.525429\tkl:   26.413925\n",
      "Epoch: 933 [20100/50000 (40%)]  \tLoss:   94.463181\trec:   66.352203\tkl:   28.110979\n",
      "Epoch: 933 [30100/50000 (60%)]  \tLoss:   87.619003\trec:   60.857529\tkl:   26.761477\n",
      "Epoch: 933 [40100/50000 (80%)]  \tLoss:   90.783867\trec:   63.388432\tkl:   27.395439\n",
      "====> Epoch: 933 Average train loss: 89.4934\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2085\n",
      "Epoch: 934 [  100/50000 ( 0%)]  \tLoss:   89.806183\trec:   63.090946\tkl:   26.715237\n",
      "Epoch: 934 [10100/50000 (20%)]  \tLoss:   90.098389\trec:   62.843681\tkl:   27.254707\n",
      "Epoch: 934 [20100/50000 (40%)]  \tLoss:   88.782204\trec:   61.248581\tkl:   27.533621\n",
      "Epoch: 934 [30100/50000 (60%)]  \tLoss:   90.808136\trec:   63.808594\tkl:   26.999538\n",
      "Epoch: 934 [40100/50000 (80%)]  \tLoss:   85.501503\trec:   59.093975\tkl:   26.407534\n",
      "====> Epoch: 934 Average train loss: 89.4834\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3310\n",
      "Epoch: 935 [  100/50000 ( 0%)]  \tLoss:   92.078804\trec:   64.103882\tkl:   27.974924\n",
      "Epoch: 935 [10100/50000 (20%)]  \tLoss:   87.558037\trec:   60.650742\tkl:   26.907295\n",
      "Epoch: 935 [20100/50000 (40%)]  \tLoss:   88.751404\trec:   61.383007\tkl:   27.368401\n",
      "Epoch: 935 [30100/50000 (60%)]  \tLoss:   90.612495\trec:   64.027695\tkl:   26.584806\n",
      "Epoch: 935 [40100/50000 (80%)]  \tLoss:   90.459122\trec:   63.071640\tkl:   27.387478\n",
      "====> Epoch: 935 Average train loss: 89.4894\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4292\n",
      "Epoch: 936 [  100/50000 ( 0%)]  \tLoss:   91.104774\trec:   63.408642\tkl:   27.696135\n",
      "Epoch: 936 [10100/50000 (20%)]  \tLoss:   87.291084\trec:   60.348724\tkl:   26.942356\n",
      "Epoch: 936 [20100/50000 (40%)]  \tLoss:   91.934059\trec:   64.784637\tkl:   27.149426\n",
      "Epoch: 936 [30100/50000 (60%)]  \tLoss:   88.467773\trec:   61.484383\tkl:   26.983383\n",
      "Epoch: 936 [40100/50000 (80%)]  \tLoss:   88.845047\trec:   61.466747\tkl:   27.378304\n",
      "====> Epoch: 936 Average train loss: 89.5216\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3645\n",
      "Epoch: 937 [  100/50000 ( 0%)]  \tLoss:   94.071259\trec:   65.260185\tkl:   28.811066\n",
      "Epoch: 937 [10100/50000 (20%)]  \tLoss:   88.284561\trec:   61.103104\tkl:   27.181454\n",
      "Epoch: 937 [20100/50000 (40%)]  \tLoss:   87.299820\trec:   60.285072\tkl:   27.014746\n",
      "Epoch: 937 [30100/50000 (60%)]  \tLoss:   91.072098\trec:   64.479576\tkl:   26.592520\n",
      "Epoch: 937 [40100/50000 (80%)]  \tLoss:   88.939873\trec:   61.419144\tkl:   27.520727\n",
      "====> Epoch: 937 Average train loss: 89.4556\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3441\n",
      "Epoch: 938 [  100/50000 ( 0%)]  \tLoss:   86.660698\trec:   60.190865\tkl:   26.469843\n",
      "Epoch: 938 [10100/50000 (20%)]  \tLoss:   88.618790\trec:   61.588577\tkl:   27.030214\n",
      "Epoch: 938 [20100/50000 (40%)]  \tLoss:   90.650795\trec:   63.649574\tkl:   27.001225\n",
      "Epoch: 938 [30100/50000 (60%)]  \tLoss:   88.537712\trec:   61.085110\tkl:   27.452599\n",
      "Epoch: 938 [40100/50000 (80%)]  \tLoss:   88.218628\trec:   61.652660\tkl:   26.565973\n",
      "====> Epoch: 938 Average train loss: 89.4836\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2858\n",
      "Epoch: 939 [  100/50000 ( 0%)]  \tLoss:   88.924614\trec:   61.447548\tkl:   27.477072\n",
      "Epoch: 939 [10100/50000 (20%)]  \tLoss:   91.659882\trec:   64.368492\tkl:   27.291384\n",
      "Epoch: 939 [20100/50000 (40%)]  \tLoss:   85.811249\trec:   59.571365\tkl:   26.239882\n",
      "Epoch: 939 [30100/50000 (60%)]  \tLoss:   86.990242\trec:   60.209080\tkl:   26.781164\n",
      "Epoch: 939 [40100/50000 (80%)]  \tLoss:   88.401329\trec:   61.715496\tkl:   26.685831\n",
      "====> Epoch: 939 Average train loss: 89.4477\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2758\n",
      "Epoch: 940 [  100/50000 ( 0%)]  \tLoss:   90.967560\trec:   63.578777\tkl:   27.388773\n",
      "Epoch: 940 [10100/50000 (20%)]  \tLoss:   89.960815\trec:   62.332474\tkl:   27.628342\n",
      "Epoch: 940 [20100/50000 (40%)]  \tLoss:   90.070610\trec:   62.583710\tkl:   27.486906\n",
      "Epoch: 940 [30100/50000 (60%)]  \tLoss:   90.478745\trec:   63.771065\tkl:   26.707687\n",
      "Epoch: 940 [40100/50000 (80%)]  \tLoss:   89.980766\trec:   62.201599\tkl:   27.779171\n",
      "====> Epoch: 940 Average train loss: 89.4661\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2896\n",
      "Epoch: 941 [  100/50000 ( 0%)]  \tLoss:   89.889275\trec:   62.355000\tkl:   27.534277\n",
      "Epoch: 941 [10100/50000 (20%)]  \tLoss:   94.063438\trec:   65.811150\tkl:   28.252281\n",
      "Epoch: 941 [20100/50000 (40%)]  \tLoss:   93.120201\trec:   65.440910\tkl:   27.679298\n",
      "Epoch: 941 [30100/50000 (60%)]  \tLoss:   95.115730\trec:   66.938210\tkl:   28.177521\n",
      "Epoch: 941 [40100/50000 (80%)]  \tLoss:   91.224785\trec:   63.877586\tkl:   27.347195\n",
      "====> Epoch: 941 Average train loss: 89.4847\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3046\n",
      "Epoch: 942 [  100/50000 ( 0%)]  \tLoss:   86.696091\trec:   60.277451\tkl:   26.418636\n",
      "Epoch: 942 [10100/50000 (20%)]  \tLoss:   86.889168\trec:   61.813881\tkl:   25.075291\n",
      "Epoch: 942 [20100/50000 (40%)]  \tLoss:   91.951271\trec:   64.487106\tkl:   27.464163\n",
      "Epoch: 942 [30100/50000 (60%)]  \tLoss:   88.173180\trec:   62.316151\tkl:   25.857035\n",
      "Epoch: 942 [40100/50000 (80%)]  \tLoss:   94.428078\trec:   66.184975\tkl:   28.243097\n",
      "====> Epoch: 942 Average train loss: 89.4685\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2325\n",
      "Epoch: 943 [  100/50000 ( 0%)]  \tLoss:   91.484238\trec:   64.169571\tkl:   27.314667\n",
      "Epoch: 943 [10100/50000 (20%)]  \tLoss:   91.040138\trec:   63.535652\tkl:   27.504484\n",
      "Epoch: 943 [20100/50000 (40%)]  \tLoss:   90.494179\trec:   63.805420\tkl:   26.688759\n",
      "Epoch: 943 [30100/50000 (60%)]  \tLoss:   86.737381\trec:   59.524094\tkl:   27.213285\n",
      "Epoch: 943 [40100/50000 (80%)]  \tLoss:   87.794647\trec:   60.603771\tkl:   27.190868\n",
      "====> Epoch: 943 Average train loss: 89.4571\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2216\n",
      "Epoch: 944 [  100/50000 ( 0%)]  \tLoss:   90.072632\trec:   62.945057\tkl:   27.127581\n",
      "Epoch: 944 [10100/50000 (20%)]  \tLoss:   89.388748\trec:   62.652489\tkl:   26.736258\n",
      "Epoch: 944 [20100/50000 (40%)]  \tLoss:   90.251122\trec:   63.473083\tkl:   26.778034\n",
      "Epoch: 944 [30100/50000 (60%)]  \tLoss:   85.331207\trec:   58.851059\tkl:   26.480148\n",
      "Epoch: 944 [40100/50000 (80%)]  \tLoss:   91.186676\trec:   64.109879\tkl:   27.076796\n",
      "====> Epoch: 944 Average train loss: 89.4715\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2250\n",
      "Epoch: 945 [  100/50000 ( 0%)]  \tLoss:   87.521042\trec:   61.045235\tkl:   26.475809\n",
      "Epoch: 945 [10100/50000 (20%)]  \tLoss:   93.774590\trec:   64.891350\tkl:   28.883240\n",
      "Epoch: 945 [20100/50000 (40%)]  \tLoss:   88.909561\trec:   61.739109\tkl:   27.170446\n",
      "Epoch: 945 [30100/50000 (60%)]  \tLoss:   89.636871\trec:   62.069981\tkl:   27.566889\n",
      "Epoch: 945 [40100/50000 (80%)]  \tLoss:   90.195938\trec:   63.214245\tkl:   26.981686\n",
      "====> Epoch: 945 Average train loss: 89.4574\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3562\n",
      "Epoch: 946 [  100/50000 ( 0%)]  \tLoss:   90.199135\trec:   62.368000\tkl:   27.831142\n",
      "Epoch: 946 [10100/50000 (20%)]  \tLoss:   92.008575\trec:   64.135468\tkl:   27.873104\n",
      "Epoch: 946 [20100/50000 (40%)]  \tLoss:   89.590157\trec:   62.824314\tkl:   26.765844\n",
      "Epoch: 946 [30100/50000 (60%)]  \tLoss:   89.114861\trec:   61.933910\tkl:   27.180948\n",
      "Epoch: 946 [40100/50000 (80%)]  \tLoss:   85.629128\trec:   59.812000\tkl:   25.817129\n",
      "====> Epoch: 946 Average train loss: 89.4568\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2741\n",
      "Epoch: 947 [  100/50000 ( 0%)]  \tLoss:   89.303726\trec:   61.976185\tkl:   27.327545\n",
      "Epoch: 947 [10100/50000 (20%)]  \tLoss:   84.746033\trec:   58.668182\tkl:   26.077850\n",
      "Epoch: 947 [20100/50000 (40%)]  \tLoss:   88.349960\trec:   61.191730\tkl:   27.158226\n",
      "Epoch: 947 [30100/50000 (60%)]  \tLoss:   92.287361\trec:   64.022530\tkl:   28.264826\n",
      "Epoch: 947 [40100/50000 (80%)]  \tLoss:   91.016396\trec:   63.494221\tkl:   27.522169\n",
      "====> Epoch: 947 Average train loss: 89.4467\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3779\n",
      "Epoch: 948 [  100/50000 ( 0%)]  \tLoss:   84.828545\trec:   58.919647\tkl:   25.908895\n",
      "Epoch: 948 [10100/50000 (20%)]  \tLoss:   86.858368\trec:   60.224354\tkl:   26.634010\n",
      "Epoch: 948 [20100/50000 (40%)]  \tLoss:   87.387215\trec:   60.433006\tkl:   26.954206\n",
      "Epoch: 948 [30100/50000 (60%)]  \tLoss:   90.505203\trec:   62.746941\tkl:   27.758261\n",
      "Epoch: 948 [40100/50000 (80%)]  \tLoss:   91.687302\trec:   63.187901\tkl:   28.499399\n",
      "====> Epoch: 948 Average train loss: 89.4704\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3167\n",
      "Epoch: 949 [  100/50000 ( 0%)]  \tLoss:   87.238747\trec:   61.171867\tkl:   26.066879\n",
      "Epoch: 949 [10100/50000 (20%)]  \tLoss:   85.035477\trec:   58.996311\tkl:   26.039162\n",
      "Epoch: 949 [20100/50000 (40%)]  \tLoss:   93.521851\trec:   66.205574\tkl:   27.316278\n",
      "Epoch: 949 [30100/50000 (60%)]  \tLoss:   86.021065\trec:   59.209843\tkl:   26.811220\n",
      "Epoch: 949 [40100/50000 (80%)]  \tLoss:   89.482147\trec:   62.192715\tkl:   27.289433\n",
      "====> Epoch: 949 Average train loss: 89.4571\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2577\n",
      "Epoch: 950 [  100/50000 ( 0%)]  \tLoss:   90.332436\trec:   63.323498\tkl:   27.008938\n",
      "Epoch: 950 [10100/50000 (20%)]  \tLoss:   85.064507\trec:   59.177769\tkl:   25.886747\n",
      "Epoch: 950 [20100/50000 (40%)]  \tLoss:   88.729370\trec:   61.902195\tkl:   26.827183\n",
      "Epoch: 950 [30100/50000 (60%)]  \tLoss:   86.922325\trec:   60.920437\tkl:   26.001888\n",
      "Epoch: 950 [40100/50000 (80%)]  \tLoss:   87.866600\trec:   60.761311\tkl:   27.105289\n",
      "====> Epoch: 950 Average train loss: 89.4514\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2492\n",
      "Epoch: 951 [  100/50000 ( 0%)]  \tLoss:   89.212830\trec:   61.732006\tkl:   27.480822\n",
      "Epoch: 951 [10100/50000 (20%)]  \tLoss:   87.664780\trec:   60.913376\tkl:   26.751408\n",
      "Epoch: 951 [20100/50000 (40%)]  \tLoss:   92.600075\trec:   65.124107\tkl:   27.475973\n",
      "Epoch: 951 [30100/50000 (60%)]  \tLoss:   90.490791\trec:   62.738564\tkl:   27.752228\n",
      "Epoch: 951 [40100/50000 (80%)]  \tLoss:   91.679977\trec:   63.984997\tkl:   27.694983\n",
      "====> Epoch: 951 Average train loss: 89.4312\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2709\n",
      "Epoch: 952 [  100/50000 ( 0%)]  \tLoss:   88.992401\trec:   61.903511\tkl:   27.088886\n",
      "Epoch: 952 [10100/50000 (20%)]  \tLoss:   89.946274\trec:   62.949089\tkl:   26.997189\n",
      "Epoch: 952 [20100/50000 (40%)]  \tLoss:   92.005600\trec:   64.356659\tkl:   27.648943\n",
      "Epoch: 952 [30100/50000 (60%)]  \tLoss:   90.754112\trec:   63.135471\tkl:   27.618637\n",
      "Epoch: 952 [40100/50000 (80%)]  \tLoss:   93.324226\trec:   65.861923\tkl:   27.462301\n",
      "====> Epoch: 952 Average train loss: 89.4366\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2273\n",
      "Epoch: 953 [  100/50000 ( 0%)]  \tLoss:   89.988884\trec:   62.820545\tkl:   27.168339\n",
      "Epoch: 953 [10100/50000 (20%)]  \tLoss:   89.843452\trec:   62.342030\tkl:   27.501427\n",
      "Epoch: 953 [20100/50000 (40%)]  \tLoss:   93.744614\trec:   65.760834\tkl:   27.983782\n",
      "Epoch: 953 [30100/50000 (60%)]  \tLoss:   88.269745\trec:   61.738903\tkl:   26.530842\n",
      "Epoch: 953 [40100/50000 (80%)]  \tLoss:   90.083725\trec:   63.302986\tkl:   26.780741\n",
      "====> Epoch: 953 Average train loss: 89.4243\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3596\n",
      "Epoch: 954 [  100/50000 ( 0%)]  \tLoss:   87.310158\trec:   60.716805\tkl:   26.593353\n",
      "Epoch: 954 [10100/50000 (20%)]  \tLoss:   89.528580\trec:   62.619724\tkl:   26.908857\n",
      "Epoch: 954 [20100/50000 (40%)]  \tLoss:   88.304962\trec:   61.552917\tkl:   26.752035\n",
      "Epoch: 954 [30100/50000 (60%)]  \tLoss:   87.829384\trec:   61.585140\tkl:   26.244242\n",
      "Epoch: 954 [40100/50000 (80%)]  \tLoss:   91.526970\trec:   63.865322\tkl:   27.661648\n",
      "====> Epoch: 954 Average train loss: 89.4328\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2902\n",
      "Epoch: 955 [  100/50000 ( 0%)]  \tLoss:   89.968765\trec:   62.958347\tkl:   27.010414\n",
      "Epoch: 955 [10100/50000 (20%)]  \tLoss:   87.080177\trec:   60.047947\tkl:   27.032227\n",
      "Epoch: 955 [20100/50000 (40%)]  \tLoss:   89.036530\trec:   62.278156\tkl:   26.758371\n",
      "Epoch: 955 [30100/50000 (60%)]  \tLoss:   89.454567\trec:   63.066830\tkl:   26.387733\n",
      "Epoch: 955 [40100/50000 (80%)]  \tLoss:   88.207932\trec:   61.680752\tkl:   26.527182\n",
      "====> Epoch: 955 Average train loss: 89.4263\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3150\n",
      "Epoch: 956 [  100/50000 ( 0%)]  \tLoss:   88.932579\trec:   62.531364\tkl:   26.401207\n",
      "Epoch: 956 [10100/50000 (20%)]  \tLoss:   91.942314\trec:   64.470230\tkl:   27.472082\n",
      "Epoch: 956 [20100/50000 (40%)]  \tLoss:   89.989540\trec:   62.257282\tkl:   27.732256\n",
      "Epoch: 956 [30100/50000 (60%)]  \tLoss:   88.851501\trec:   62.040375\tkl:   26.811132\n",
      "Epoch: 956 [40100/50000 (80%)]  \tLoss:   87.776405\trec:   61.251133\tkl:   26.525270\n",
      "====> Epoch: 956 Average train loss: 89.4458\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3440\n",
      "Epoch: 957 [  100/50000 ( 0%)]  \tLoss:   91.362534\trec:   63.444260\tkl:   27.918278\n",
      "Epoch: 957 [10100/50000 (20%)]  \tLoss:   91.343689\trec:   63.685150\tkl:   27.658537\n",
      "Epoch: 957 [20100/50000 (40%)]  \tLoss:   90.425468\trec:   62.680897\tkl:   27.744564\n",
      "Epoch: 957 [30100/50000 (60%)]  \tLoss:   94.252060\trec:   65.600098\tkl:   28.651964\n",
      "Epoch: 957 [40100/50000 (80%)]  \tLoss:   89.265106\trec:   62.333813\tkl:   26.931292\n",
      "====> Epoch: 957 Average train loss: 89.4419\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2590\n",
      "Epoch: 958 [  100/50000 ( 0%)]  \tLoss:   88.965645\trec:   62.587307\tkl:   26.378336\n",
      "Epoch: 958 [10100/50000 (20%)]  \tLoss:   93.809334\trec:   65.749268\tkl:   28.060066\n",
      "Epoch: 958 [20100/50000 (40%)]  \tLoss:   91.240753\trec:   63.754711\tkl:   27.486036\n",
      "Epoch: 958 [30100/50000 (60%)]  \tLoss:   90.048721\trec:   62.253925\tkl:   27.794794\n",
      "Epoch: 958 [40100/50000 (80%)]  \tLoss:   90.244537\trec:   63.386620\tkl:   26.857920\n",
      "====> Epoch: 958 Average train loss: 89.3949\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2885\n",
      "Epoch: 959 [  100/50000 ( 0%)]  \tLoss:   89.631660\trec:   62.324951\tkl:   27.306709\n",
      "Epoch: 959 [10100/50000 (20%)]  \tLoss:   91.078194\trec:   63.386864\tkl:   27.691330\n",
      "Epoch: 959 [20100/50000 (40%)]  \tLoss:   86.361725\trec:   60.048645\tkl:   26.313080\n",
      "Epoch: 959 [30100/50000 (60%)]  \tLoss:   90.920532\trec:   63.164570\tkl:   27.755966\n",
      "Epoch: 959 [40100/50000 (80%)]  \tLoss:   86.351807\trec:   59.517475\tkl:   26.834333\n",
      "====> Epoch: 959 Average train loss: 89.4266\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.1647\n",
      "Epoch: 960 [  100/50000 ( 0%)]  \tLoss:   90.151276\trec:   62.837849\tkl:   27.313427\n",
      "Epoch: 960 [10100/50000 (20%)]  \tLoss:   92.344452\trec:   63.821903\tkl:   28.522543\n",
      "Epoch: 960 [20100/50000 (40%)]  \tLoss:   87.492538\trec:   60.166023\tkl:   27.326517\n",
      "Epoch: 960 [30100/50000 (60%)]  \tLoss:   92.445900\trec:   65.016037\tkl:   27.429861\n",
      "Epoch: 960 [40100/50000 (80%)]  \tLoss:   91.114044\trec:   64.031975\tkl:   27.082067\n",
      "====> Epoch: 960 Average train loss: 89.4414\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2329\n",
      "Epoch: 961 [  100/50000 ( 0%)]  \tLoss:   91.541130\trec:   63.777966\tkl:   27.763166\n",
      "Epoch: 961 [10100/50000 (20%)]  \tLoss:   89.181030\trec:   62.317028\tkl:   26.864000\n",
      "Epoch: 961 [20100/50000 (40%)]  \tLoss:   88.810707\trec:   61.602985\tkl:   27.207722\n",
      "Epoch: 961 [30100/50000 (60%)]  \tLoss:   88.301109\trec:   60.567146\tkl:   27.733965\n",
      "Epoch: 961 [40100/50000 (80%)]  \tLoss:   89.647636\trec:   62.547070\tkl:   27.100563\n",
      "====> Epoch: 961 Average train loss: 89.4274\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3029\n",
      "Epoch: 962 [  100/50000 ( 0%)]  \tLoss:   86.951416\trec:   60.151157\tkl:   26.800259\n",
      "Epoch: 962 [10100/50000 (20%)]  \tLoss:   89.290245\trec:   62.087006\tkl:   27.203234\n",
      "Epoch: 962 [20100/50000 (40%)]  \tLoss:   89.819496\trec:   62.392303\tkl:   27.427195\n",
      "Epoch: 962 [30100/50000 (60%)]  \tLoss:   86.429169\trec:   60.560738\tkl:   25.868429\n",
      "Epoch: 962 [40100/50000 (80%)]  \tLoss:   89.818787\trec:   61.855827\tkl:   27.962963\n",
      "====> Epoch: 962 Average train loss: 89.4002\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3261\n",
      "Epoch: 963 [  100/50000 ( 0%)]  \tLoss:   88.654053\trec:   61.185539\tkl:   27.468508\n",
      "Epoch: 963 [10100/50000 (20%)]  \tLoss:   91.792587\trec:   64.319946\tkl:   27.472639\n",
      "Epoch: 963 [20100/50000 (40%)]  \tLoss:   86.933052\trec:   60.641270\tkl:   26.291786\n",
      "Epoch: 963 [30100/50000 (60%)]  \tLoss:   88.192696\trec:   60.913574\tkl:   27.279118\n",
      "Epoch: 963 [40100/50000 (80%)]  \tLoss:   89.184479\trec:   61.585194\tkl:   27.599289\n",
      "====> Epoch: 963 Average train loss: 89.4455\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4214\n",
      "Epoch: 964 [  100/50000 ( 0%)]  \tLoss:   89.450714\trec:   62.613972\tkl:   26.836740\n",
      "Epoch: 964 [10100/50000 (20%)]  \tLoss:   85.858627\trec:   58.765701\tkl:   27.092926\n",
      "Epoch: 964 [20100/50000 (40%)]  \tLoss:   88.285347\trec:   61.384785\tkl:   26.900560\n",
      "Epoch: 964 [30100/50000 (60%)]  \tLoss:   88.959160\trec:   62.163967\tkl:   26.795193\n",
      "Epoch: 964 [40100/50000 (80%)]  \tLoss:   87.248512\trec:   60.633984\tkl:   26.614525\n",
      "====> Epoch: 964 Average train loss: 89.4365\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.1821\n",
      "Epoch: 965 [  100/50000 ( 0%)]  \tLoss:   87.271835\trec:   59.936069\tkl:   27.335772\n",
      "Epoch: 965 [10100/50000 (20%)]  \tLoss:   90.232719\trec:   63.142380\tkl:   27.090340\n",
      "Epoch: 965 [20100/50000 (40%)]  \tLoss:   90.800613\trec:   63.331390\tkl:   27.469221\n",
      "Epoch: 965 [30100/50000 (60%)]  \tLoss:   91.790665\trec:   64.254478\tkl:   27.536180\n",
      "Epoch: 965 [40100/50000 (80%)]  \tLoss:   88.810547\trec:   62.225540\tkl:   26.585005\n",
      "====> Epoch: 965 Average train loss: 89.4042\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2485\n",
      "Epoch: 966 [  100/50000 ( 0%)]  \tLoss:   90.567558\trec:   62.703011\tkl:   27.864542\n",
      "Epoch: 966 [10100/50000 (20%)]  \tLoss:   91.272949\trec:   64.312241\tkl:   26.960705\n",
      "Epoch: 966 [20100/50000 (40%)]  \tLoss:   88.724373\trec:   61.673367\tkl:   27.051004\n",
      "Epoch: 966 [30100/50000 (60%)]  \tLoss:   87.037048\trec:   60.364990\tkl:   26.672066\n",
      "Epoch: 966 [40100/50000 (80%)]  \tLoss:   90.993217\trec:   63.138672\tkl:   27.854548\n",
      "====> Epoch: 966 Average train loss: 89.4330\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2839\n",
      "Epoch: 967 [  100/50000 ( 0%)]  \tLoss:   89.205429\trec:   61.715443\tkl:   27.489986\n",
      "Epoch: 967 [10100/50000 (20%)]  \tLoss:   88.293663\trec:   61.634407\tkl:   26.659252\n",
      "Epoch: 967 [20100/50000 (40%)]  \tLoss:   91.310577\trec:   63.920570\tkl:   27.390001\n",
      "Epoch: 967 [30100/50000 (60%)]  \tLoss:   90.951180\trec:   64.232948\tkl:   26.718229\n",
      "Epoch: 967 [40100/50000 (80%)]  \tLoss:   90.229980\trec:   63.310234\tkl:   26.919741\n",
      "====> Epoch: 967 Average train loss: 89.4430\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3477\n",
      "Epoch: 968 [  100/50000 ( 0%)]  \tLoss:   88.181931\trec:   61.351555\tkl:   26.830376\n",
      "Epoch: 968 [10100/50000 (20%)]  \tLoss:   86.365601\trec:   59.637478\tkl:   26.728127\n",
      "Epoch: 968 [20100/50000 (40%)]  \tLoss:   91.405037\trec:   64.942589\tkl:   26.462442\n",
      "Epoch: 968 [30100/50000 (60%)]  \tLoss:   85.860474\trec:   59.844070\tkl:   26.016403\n",
      "Epoch: 968 [40100/50000 (80%)]  \tLoss:   94.297951\trec:   66.221939\tkl:   28.076006\n",
      "====> Epoch: 968 Average train loss: 89.4248\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2970\n",
      "Epoch: 969 [  100/50000 ( 0%)]  \tLoss:   85.039688\trec:   58.467224\tkl:   26.572460\n",
      "Epoch: 969 [10100/50000 (20%)]  \tLoss:   87.409470\trec:   60.877998\tkl:   26.531477\n",
      "Epoch: 969 [20100/50000 (40%)]  \tLoss:   82.568375\trec:   56.596783\tkl:   25.971598\n",
      "Epoch: 969 [30100/50000 (60%)]  \tLoss:   91.912964\trec:   64.746353\tkl:   27.166611\n",
      "Epoch: 969 [40100/50000 (80%)]  \tLoss:   87.951401\trec:   61.397995\tkl:   26.553406\n",
      "====> Epoch: 969 Average train loss: 89.4122\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2855\n",
      "Epoch: 970 [  100/50000 ( 0%)]  \tLoss:   90.323975\trec:   62.730625\tkl:   27.593346\n",
      "Epoch: 970 [10100/50000 (20%)]  \tLoss:   89.651955\trec:   62.833603\tkl:   26.818354\n",
      "Epoch: 970 [20100/50000 (40%)]  \tLoss:   90.046700\trec:   62.628426\tkl:   27.418268\n",
      "Epoch: 970 [30100/50000 (60%)]  \tLoss:   87.955200\trec:   61.753742\tkl:   26.201462\n",
      "Epoch: 970 [40100/50000 (80%)]  \tLoss:   88.604958\trec:   62.080769\tkl:   26.524189\n",
      "====> Epoch: 970 Average train loss: 89.4166\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3815\n",
      "Epoch: 971 [  100/50000 ( 0%)]  \tLoss:   86.416374\trec:   59.384995\tkl:   27.031382\n",
      "Epoch: 971 [10100/50000 (20%)]  \tLoss:   91.517059\trec:   63.807606\tkl:   27.709455\n",
      "Epoch: 971 [20100/50000 (40%)]  \tLoss:   91.612274\trec:   63.636692\tkl:   27.975578\n",
      "Epoch: 971 [30100/50000 (60%)]  \tLoss:   89.076874\trec:   62.319912\tkl:   26.756958\n",
      "Epoch: 971 [40100/50000 (80%)]  \tLoss:   86.297119\trec:   58.972790\tkl:   27.324327\n",
      "====> Epoch: 971 Average train loss: 89.4170\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3718\n",
      "Epoch: 972 [  100/50000 ( 0%)]  \tLoss:   88.985626\trec:   62.081932\tkl:   26.903688\n",
      "Epoch: 972 [10100/50000 (20%)]  \tLoss:   89.391899\trec:   61.657742\tkl:   27.734161\n",
      "Epoch: 972 [20100/50000 (40%)]  \tLoss:   88.345200\trec:   61.002197\tkl:   27.343008\n",
      "Epoch: 972 [30100/50000 (60%)]  \tLoss:   91.219460\trec:   63.455181\tkl:   27.764282\n",
      "Epoch: 972 [40100/50000 (80%)]  \tLoss:   91.920937\trec:   64.547737\tkl:   27.373196\n",
      "====> Epoch: 972 Average train loss: 89.4121\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3225\n",
      "Epoch: 973 [  100/50000 ( 0%)]  \tLoss:   92.634079\trec:   65.057076\tkl:   27.576996\n",
      "Epoch: 973 [10100/50000 (20%)]  \tLoss:   87.146317\trec:   61.004406\tkl:   26.141911\n",
      "Epoch: 973 [20100/50000 (40%)]  \tLoss:   87.028648\trec:   61.192120\tkl:   25.836531\n",
      "Epoch: 973 [30100/50000 (60%)]  \tLoss:   88.731140\trec:   60.857040\tkl:   27.874102\n",
      "Epoch: 973 [40100/50000 (80%)]  \tLoss:   89.090927\trec:   62.029442\tkl:   27.061481\n",
      "====> Epoch: 973 Average train loss: 89.4039\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3195\n",
      "Epoch: 974 [  100/50000 ( 0%)]  \tLoss:   85.453926\trec:   59.316643\tkl:   26.137278\n",
      "Epoch: 974 [10100/50000 (20%)]  \tLoss:   91.953003\trec:   65.321861\tkl:   26.631147\n",
      "Epoch: 974 [20100/50000 (40%)]  \tLoss:   90.160027\trec:   62.860252\tkl:   27.299774\n",
      "Epoch: 974 [30100/50000 (60%)]  \tLoss:   91.240105\trec:   64.010460\tkl:   27.229649\n",
      "Epoch: 974 [40100/50000 (80%)]  \tLoss:   91.630371\trec:   64.135818\tkl:   27.494547\n",
      "====> Epoch: 974 Average train loss: 89.3951\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3025\n",
      "Epoch: 975 [  100/50000 ( 0%)]  \tLoss:   91.759674\trec:   64.379784\tkl:   27.379892\n",
      "Epoch: 975 [10100/50000 (20%)]  \tLoss:   83.829079\trec:   56.974197\tkl:   26.854877\n",
      "Epoch: 975 [20100/50000 (40%)]  \tLoss:   89.556969\trec:   62.553856\tkl:   27.003120\n",
      "Epoch: 975 [30100/50000 (60%)]  \tLoss:   87.056381\trec:   60.360359\tkl:   26.696024\n",
      "Epoch: 975 [40100/50000 (80%)]  \tLoss:   91.486725\trec:   63.530468\tkl:   27.956257\n",
      "====> Epoch: 975 Average train loss: 89.4074\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3095\n",
      "Epoch: 976 [  100/50000 ( 0%)]  \tLoss:   91.151070\trec:   63.086658\tkl:   28.064409\n",
      "Epoch: 976 [10100/50000 (20%)]  \tLoss:   88.350838\trec:   61.374187\tkl:   26.976652\n",
      "Epoch: 976 [20100/50000 (40%)]  \tLoss:   91.722519\trec:   64.668716\tkl:   27.053795\n",
      "Epoch: 976 [30100/50000 (60%)]  \tLoss:   89.823570\trec:   63.121960\tkl:   26.701607\n",
      "Epoch: 976 [40100/50000 (80%)]  \tLoss:   93.211082\trec:   64.917984\tkl:   28.293093\n",
      "====> Epoch: 976 Average train loss: 89.4082\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2002\n",
      "Epoch: 977 [  100/50000 ( 0%)]  \tLoss:   89.608208\trec:   62.503323\tkl:   27.104885\n",
      "Epoch: 977 [10100/50000 (20%)]  \tLoss:   89.380592\trec:   62.791981\tkl:   26.588614\n",
      "Epoch: 977 [20100/50000 (40%)]  \tLoss:   90.024796\trec:   63.805508\tkl:   26.219284\n",
      "Epoch: 977 [30100/50000 (60%)]  \tLoss:   90.097145\trec:   62.950241\tkl:   27.146906\n",
      "Epoch: 977 [40100/50000 (80%)]  \tLoss:   87.367020\trec:   59.589050\tkl:   27.777967\n",
      "====> Epoch: 977 Average train loss: 89.4024\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4088\n",
      "Epoch: 978 [  100/50000 ( 0%)]  \tLoss:   85.974091\trec:   59.584850\tkl:   26.389238\n",
      "Epoch: 978 [10100/50000 (20%)]  \tLoss:   91.030144\trec:   63.025303\tkl:   28.004845\n",
      "Epoch: 978 [20100/50000 (40%)]  \tLoss:   89.774155\trec:   63.066006\tkl:   26.708153\n",
      "Epoch: 978 [30100/50000 (60%)]  \tLoss:   88.428482\trec:   61.883865\tkl:   26.544619\n",
      "Epoch: 978 [40100/50000 (80%)]  \tLoss:   88.459480\trec:   61.516853\tkl:   26.942629\n",
      "====> Epoch: 978 Average train loss: 89.4030\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2534\n",
      "Epoch: 979 [  100/50000 ( 0%)]  \tLoss:   90.154648\trec:   63.387836\tkl:   26.766811\n",
      "Epoch: 979 [10100/50000 (20%)]  \tLoss:   89.772644\trec:   61.723511\tkl:   28.049133\n",
      "Epoch: 979 [20100/50000 (40%)]  \tLoss:   91.839195\trec:   64.541412\tkl:   27.297785\n",
      "Epoch: 979 [30100/50000 (60%)]  \tLoss:   87.378693\trec:   61.114189\tkl:   26.264496\n",
      "Epoch: 979 [40100/50000 (80%)]  \tLoss:   86.280151\trec:   59.781651\tkl:   26.498503\n",
      "====> Epoch: 979 Average train loss: 89.3797\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2803\n",
      "Epoch: 980 [  100/50000 ( 0%)]  \tLoss:   89.454559\trec:   63.070473\tkl:   26.384087\n",
      "Epoch: 980 [10100/50000 (20%)]  \tLoss:   91.274040\trec:   64.242477\tkl:   27.031563\n",
      "Epoch: 980 [20100/50000 (40%)]  \tLoss:   91.021904\trec:   64.835106\tkl:   26.186796\n",
      "Epoch: 980 [30100/50000 (60%)]  \tLoss:   89.494576\trec:   62.368591\tkl:   27.125984\n",
      "Epoch: 980 [40100/50000 (80%)]  \tLoss:   87.037460\trec:   60.295212\tkl:   26.742249\n",
      "====> Epoch: 980 Average train loss: 89.3921\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4007\n",
      "Epoch: 981 [  100/50000 ( 0%)]  \tLoss:   87.552773\trec:   60.516079\tkl:   27.036695\n",
      "Epoch: 981 [10100/50000 (20%)]  \tLoss:   88.585709\trec:   61.548454\tkl:   27.037258\n",
      "Epoch: 981 [20100/50000 (40%)]  \tLoss:   88.977936\trec:   62.683472\tkl:   26.294464\n",
      "Epoch: 981 [30100/50000 (60%)]  \tLoss:   90.691559\trec:   63.003708\tkl:   27.687851\n",
      "Epoch: 981 [40100/50000 (80%)]  \tLoss:   88.884674\trec:   61.449814\tkl:   27.434866\n",
      "====> Epoch: 981 Average train loss: 89.3730\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2756\n",
      "Epoch: 982 [  100/50000 ( 0%)]  \tLoss:   90.519913\trec:   62.842575\tkl:   27.677336\n",
      "Epoch: 982 [10100/50000 (20%)]  \tLoss:   89.877846\trec:   62.029198\tkl:   27.848656\n",
      "Epoch: 982 [20100/50000 (40%)]  \tLoss:   90.174324\trec:   63.133934\tkl:   27.040392\n",
      "Epoch: 982 [30100/50000 (60%)]  \tLoss:   92.292442\trec:   64.843987\tkl:   27.448454\n",
      "Epoch: 982 [40100/50000 (80%)]  \tLoss:   86.540016\trec:   60.114838\tkl:   26.425179\n",
      "====> Epoch: 982 Average train loss: 89.3754\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3231\n",
      "Epoch: 983 [  100/50000 ( 0%)]  \tLoss:   87.642647\trec:   60.991688\tkl:   26.650957\n",
      "Epoch: 983 [10100/50000 (20%)]  \tLoss:   88.139275\trec:   60.850311\tkl:   27.288967\n",
      "Epoch: 983 [20100/50000 (40%)]  \tLoss:   87.491135\trec:   59.795410\tkl:   27.695726\n",
      "Epoch: 983 [30100/50000 (60%)]  \tLoss:   89.054939\trec:   62.497368\tkl:   26.557570\n",
      "Epoch: 983 [40100/50000 (80%)]  \tLoss:   87.553322\trec:   60.201721\tkl:   27.351599\n",
      "====> Epoch: 983 Average train loss: 89.3939\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3311\n",
      "Epoch: 984 [  100/50000 ( 0%)]  \tLoss:   94.784744\trec:   66.187790\tkl:   28.596947\n",
      "Epoch: 984 [10100/50000 (20%)]  \tLoss:   90.270805\trec:   63.278988\tkl:   26.991821\n",
      "Epoch: 984 [20100/50000 (40%)]  \tLoss:   87.920845\trec:   61.007694\tkl:   26.913157\n",
      "Epoch: 984 [30100/50000 (60%)]  \tLoss:   87.998123\trec:   61.712811\tkl:   26.285315\n",
      "Epoch: 984 [40100/50000 (80%)]  \tLoss:   86.501053\trec:   59.943886\tkl:   26.557165\n",
      "====> Epoch: 984 Average train loss: 89.3763\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2816\n",
      "Epoch: 985 [  100/50000 ( 0%)]  \tLoss:   85.660172\trec:   59.555775\tkl:   26.104399\n",
      "Epoch: 985 [10100/50000 (20%)]  \tLoss:   87.815132\trec:   61.117859\tkl:   26.697273\n",
      "Epoch: 985 [20100/50000 (40%)]  \tLoss:   89.590332\trec:   61.876492\tkl:   27.713833\n",
      "Epoch: 985 [30100/50000 (60%)]  \tLoss:   85.258575\trec:   58.936890\tkl:   26.321690\n",
      "Epoch: 985 [40100/50000 (80%)]  \tLoss:   84.773750\trec:   59.203514\tkl:   25.570236\n",
      "====> Epoch: 985 Average train loss: 89.3982\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4062\n",
      "Epoch: 986 [  100/50000 ( 0%)]  \tLoss:   93.047226\trec:   66.173164\tkl:   26.874063\n",
      "Epoch: 986 [10100/50000 (20%)]  \tLoss:   89.430092\trec:   62.891010\tkl:   26.539085\n",
      "Epoch: 986 [20100/50000 (40%)]  \tLoss:   90.330086\trec:   63.060303\tkl:   27.269787\n",
      "Epoch: 986 [30100/50000 (60%)]  \tLoss:   90.123672\trec:   63.136467\tkl:   26.987204\n",
      "Epoch: 986 [40100/50000 (80%)]  \tLoss:   89.757866\trec:   62.481033\tkl:   27.276833\n",
      "====> Epoch: 986 Average train loss: 89.4042\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2770\n",
      "Epoch: 987 [  100/50000 ( 0%)]  \tLoss:   90.292633\trec:   63.776424\tkl:   26.516209\n",
      "Epoch: 987 [10100/50000 (20%)]  \tLoss:   86.468246\trec:   60.488022\tkl:   25.980232\n",
      "Epoch: 987 [20100/50000 (40%)]  \tLoss:   92.936996\trec:   65.505211\tkl:   27.431791\n",
      "Epoch: 987 [30100/50000 (60%)]  \tLoss:   87.561783\trec:   61.348728\tkl:   26.213058\n",
      "Epoch: 987 [40100/50000 (80%)]  \tLoss:   89.491959\trec:   62.878384\tkl:   26.613579\n",
      "====> Epoch: 987 Average train loss: 89.3903\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3708\n",
      "Epoch: 988 [  100/50000 ( 0%)]  \tLoss:   89.890381\trec:   62.335350\tkl:   27.555029\n",
      "Epoch: 988 [10100/50000 (20%)]  \tLoss:   91.163155\trec:   63.928574\tkl:   27.234577\n",
      "Epoch: 988 [20100/50000 (40%)]  \tLoss:   89.432167\trec:   62.973278\tkl:   26.458889\n",
      "Epoch: 988 [30100/50000 (60%)]  \tLoss:   91.867607\trec:   64.180519\tkl:   27.687092\n",
      "Epoch: 988 [40100/50000 (80%)]  \tLoss:   88.883965\trec:   61.321747\tkl:   27.562218\n",
      "====> Epoch: 988 Average train loss: 89.3753\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2975\n",
      "Epoch: 989 [  100/50000 ( 0%)]  \tLoss:   89.174667\trec:   61.677658\tkl:   27.497002\n",
      "Epoch: 989 [10100/50000 (20%)]  \tLoss:   91.359962\trec:   63.447189\tkl:   27.912767\n",
      "Epoch: 989 [20100/50000 (40%)]  \tLoss:   88.214859\trec:   60.992161\tkl:   27.222702\n",
      "Epoch: 989 [30100/50000 (60%)]  \tLoss:   84.977066\trec:   58.552303\tkl:   26.424770\n",
      "Epoch: 989 [40100/50000 (80%)]  \tLoss:   89.101143\trec:   62.611259\tkl:   26.489880\n",
      "====> Epoch: 989 Average train loss: 89.3584\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3981\n",
      "Epoch: 990 [  100/50000 ( 0%)]  \tLoss:   90.053581\trec:   63.807209\tkl:   26.246374\n",
      "Epoch: 990 [10100/50000 (20%)]  \tLoss:   91.831123\trec:   63.637405\tkl:   28.193716\n",
      "Epoch: 990 [20100/50000 (40%)]  \tLoss:   90.454704\trec:   62.681797\tkl:   27.772905\n",
      "Epoch: 990 [30100/50000 (60%)]  \tLoss:   87.855789\trec:   60.936787\tkl:   26.919001\n",
      "Epoch: 990 [40100/50000 (80%)]  \tLoss:   88.483719\trec:   61.617451\tkl:   26.866272\n",
      "====> Epoch: 990 Average train loss: 89.3747\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2726\n",
      "Epoch: 991 [  100/50000 ( 0%)]  \tLoss:   88.514473\trec:   61.767200\tkl:   26.747271\n",
      "Epoch: 991 [10100/50000 (20%)]  \tLoss:   90.202927\trec:   62.966557\tkl:   27.236372\n",
      "Epoch: 991 [20100/50000 (40%)]  \tLoss:   86.145348\trec:   59.258133\tkl:   26.887211\n",
      "Epoch: 991 [30100/50000 (60%)]  \tLoss:   91.160835\trec:   64.099396\tkl:   27.061436\n",
      "Epoch: 991 [40100/50000 (80%)]  \tLoss:   86.756248\trec:   59.887207\tkl:   26.869047\n",
      "====> Epoch: 991 Average train loss: 89.3576\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2312\n",
      "Epoch: 992 [  100/50000 ( 0%)]  \tLoss:   88.058784\trec:   62.064854\tkl:   25.993931\n",
      "Epoch: 992 [10100/50000 (20%)]  \tLoss:   87.807144\trec:   60.334793\tkl:   27.472353\n",
      "Epoch: 992 [20100/50000 (40%)]  \tLoss:   86.308029\trec:   60.803436\tkl:   25.504585\n",
      "Epoch: 992 [30100/50000 (60%)]  \tLoss:   86.057091\trec:   59.537033\tkl:   26.520050\n",
      "Epoch: 992 [40100/50000 (80%)]  \tLoss:   88.808113\trec:   62.036243\tkl:   26.771872\n",
      "====> Epoch: 992 Average train loss: 89.3625\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3014\n",
      "Epoch: 993 [  100/50000 ( 0%)]  \tLoss:   87.576378\trec:   60.493191\tkl:   27.083185\n",
      "Epoch: 993 [10100/50000 (20%)]  \tLoss:   86.824272\trec:   60.363445\tkl:   26.460833\n",
      "Epoch: 993 [20100/50000 (40%)]  \tLoss:   87.902077\trec:   61.703445\tkl:   26.198635\n",
      "Epoch: 993 [30100/50000 (60%)]  \tLoss:   83.367577\trec:   57.676765\tkl:   25.690813\n",
      "Epoch: 993 [40100/50000 (80%)]  \tLoss:   87.679855\trec:   61.036732\tkl:   26.643118\n",
      "====> Epoch: 993 Average train loss: 89.3751\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3322\n",
      "Epoch: 994 [  100/50000 ( 0%)]  \tLoss:   92.083954\trec:   65.710670\tkl:   26.373285\n",
      "Epoch: 994 [10100/50000 (20%)]  \tLoss:   88.741638\trec:   61.914230\tkl:   26.827402\n",
      "Epoch: 994 [20100/50000 (40%)]  \tLoss:   92.522331\trec:   64.682228\tkl:   27.840107\n",
      "Epoch: 994 [30100/50000 (60%)]  \tLoss:   94.122849\trec:   66.210114\tkl:   27.912737\n",
      "Epoch: 994 [40100/50000 (80%)]  \tLoss:   87.620270\trec:   60.779900\tkl:   26.840366\n",
      "====> Epoch: 994 Average train loss: 89.3577\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.4243\n",
      "Epoch: 995 [  100/50000 ( 0%)]  \tLoss:   87.875938\trec:   60.724739\tkl:   27.151194\n",
      "Epoch: 995 [10100/50000 (20%)]  \tLoss:   89.838608\trec:   62.960918\tkl:   26.877693\n",
      "Epoch: 995 [20100/50000 (40%)]  \tLoss:   87.274139\trec:   60.223461\tkl:   27.050682\n",
      "Epoch: 995 [30100/50000 (60%)]  \tLoss:   87.922050\trec:   59.922722\tkl:   27.999325\n",
      "Epoch: 995 [40100/50000 (80%)]  \tLoss:   93.233025\trec:   64.470215\tkl:   28.762812\n",
      "====> Epoch: 995 Average train loss: 89.3510\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2941\n",
      "Epoch: 996 [  100/50000 ( 0%)]  \tLoss:   90.122154\trec:   62.437729\tkl:   27.684431\n",
      "Epoch: 996 [10100/50000 (20%)]  \tLoss:   93.609146\trec:   65.354988\tkl:   28.254158\n",
      "Epoch: 996 [20100/50000 (40%)]  \tLoss:   86.416168\trec:   59.591579\tkl:   26.824587\n",
      "Epoch: 996 [30100/50000 (60%)]  \tLoss:   89.677696\trec:   63.109482\tkl:   26.568207\n",
      "Epoch: 996 [40100/50000 (80%)]  \tLoss:   89.540718\trec:   61.962578\tkl:   27.578142\n",
      "====> Epoch: 996 Average train loss: 89.3665\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2585\n",
      "Epoch: 997 [  100/50000 ( 0%)]  \tLoss:   89.772369\trec:   62.500751\tkl:   27.271624\n",
      "Epoch: 997 [10100/50000 (20%)]  \tLoss:   88.006592\trec:   60.611755\tkl:   27.394836\n",
      "Epoch: 997 [20100/50000 (40%)]  \tLoss:   88.873436\trec:   61.928173\tkl:   26.945265\n",
      "Epoch: 997 [30100/50000 (60%)]  \tLoss:   87.212013\trec:   60.934036\tkl:   26.277967\n",
      "Epoch: 997 [40100/50000 (80%)]  \tLoss:   90.342773\trec:   63.032261\tkl:   27.310507\n",
      "====> Epoch: 997 Average train loss: 89.3384\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2327\n",
      "Epoch: 998 [  100/50000 ( 0%)]  \tLoss:   88.118263\trec:   61.237949\tkl:   26.880310\n",
      "Epoch: 998 [10100/50000 (20%)]  \tLoss:   85.190750\trec:   58.973671\tkl:   26.217077\n",
      "Epoch: 998 [20100/50000 (40%)]  \tLoss:   87.239716\trec:   60.522587\tkl:   26.717129\n",
      "Epoch: 998 [30100/50000 (60%)]  \tLoss:   88.321320\trec:   61.524960\tkl:   26.796356\n",
      "Epoch: 998 [40100/50000 (80%)]  \tLoss:   88.375328\trec:   61.715527\tkl:   26.659803\n",
      "====> Epoch: 998 Average train loss: 89.3414\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2520\n",
      "Epoch: 999 [  100/50000 ( 0%)]  \tLoss:   87.040680\trec:   59.994999\tkl:   27.045677\n",
      "Epoch: 999 [10100/50000 (20%)]  \tLoss:   85.099503\trec:   59.243053\tkl:   25.856445\n",
      "Epoch: 999 [20100/50000 (40%)]  \tLoss:   85.372902\trec:   58.916111\tkl:   26.456789\n",
      "Epoch: 999 [30100/50000 (60%)]  \tLoss:   91.957527\trec:   63.802200\tkl:   28.155325\n",
      "Epoch: 999 [40100/50000 (80%)]  \tLoss:   91.701324\trec:   63.766624\tkl:   27.934704\n",
      "====> Epoch: 999 Average train loss: 89.3524\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.2144\n",
      "Epoch: 1000 [  100/50000 ( 0%)]  \tLoss:   92.788223\trec:   64.949501\tkl:   27.838724\n",
      "Epoch: 1000 [10100/50000 (20%)]  \tLoss:   90.589020\trec:   62.937000\tkl:   27.652018\n",
      "Epoch: 1000 [20100/50000 (40%)]  \tLoss:   91.815758\trec:   63.654617\tkl:   28.161146\n",
      "Epoch: 1000 [30100/50000 (60%)]  \tLoss:   88.698387\trec:   62.031727\tkl:   26.666662\n",
      "Epoch: 1000 [40100/50000 (80%)]  \tLoss:   90.208839\trec:   62.949852\tkl:   27.258982\n",
      "====> Epoch: 1000 Average train loss: 89.3676\n",
      "beta = 1.0000\n",
      "====> Validation set loss: 93.3038\n",
      "====> Validation set loss: 93.3069\n",
      "Computing log-likelihood on test set\n",
      "Progress: 0.00%\n",
      "Progress: 10.00%\n",
      "Progress: 20.00%\n",
      "Progress: 30.00%\n",
      "Progress: 40.00%\n",
      "Progress: 50.00%\n",
      "Progress: 60.00%\n",
      "Progress: 70.00%\n",
      "Progress: 80.00%\n",
      "Progress: 90.00%\n",
      "====> Test set loss: 92.4638\n",
      "====> Test set log-likelihood: 88.2532\n"
     ]
    }
   ],
   "source": [
    "%run main_experiment_VAE.py -nf 20"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPizVzHf/iuwhUBoi5QxJCG",
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
