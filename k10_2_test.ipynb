{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYOsPTMQTY4s",
    "outputId": "2f12a5b1-814e-4159-dcc5-e2a6803fb168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e-i9vkB6Tk_q",
    "outputId": "2c64413a-ea55-4b09-8998-a3da39a4a4ca"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZRcA9r0T06B",
    "outputId": "4f3e1617-a510-4868-abe4-94836a743091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "cd \"/content/drive/MyDrive/Colab Notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onIxJvv7WU88",
    "outputId": "a4bba373-828c-44f5-b793-5c94dbba2d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch: 378 [  100/50000 ( 0%)]  \tLoss:   89.336876\trec:   63.095573\tkl:   26.241302\n",
      "Epoch: 378 [10100/50000 (20%)]  \tLoss:   88.749557\trec:   63.645008\tkl:   25.104548\n",
      "Epoch: 378 [20100/50000 (40%)]  \tLoss:   98.224960\trec:   71.994644\tkl:   26.230312\n",
      "Epoch: 378 [30100/50000 (60%)]  \tLoss:   95.463425\trec:   69.524841\tkl:   25.938583\n",
      "Epoch: 378 [40100/50000 (80%)]  \tLoss:   89.499763\trec:   64.508865\tkl:   24.990902\n",
      "====> Epoch: 378 Average train loss: 92.3569\n",
      "====> Validation set loss: 95.2068\n",
      "====> Validation set kl: 25.8446\n",
      "Epoch: 379 [  100/50000 ( 0%)]  \tLoss:   89.772926\trec:   64.467644\tkl:   25.305285\n",
      "Epoch: 379 [10100/50000 (20%)]  \tLoss:   90.168594\trec:   64.894539\tkl:   25.274057\n",
      "Epoch: 379 [20100/50000 (40%)]  \tLoss:   89.944878\trec:   64.664719\tkl:   25.280155\n",
      "Epoch: 379 [30100/50000 (60%)]  \tLoss:   89.499252\trec:   64.430786\tkl:   25.068472\n",
      "Epoch: 379 [40100/50000 (80%)]  \tLoss:   91.526245\trec:   66.254738\tkl:   25.271519\n",
      "====> Epoch: 379 Average train loss: 92.3531\n",
      "====> Validation set loss: 95.2333\n",
      "====> Validation set kl: 25.7994\n",
      "Epoch: 380 [  100/50000 ( 0%)]  \tLoss:   95.175034\trec:   68.436134\tkl:   26.738909\n",
      "Epoch: 380 [10100/50000 (20%)]  \tLoss:   95.670021\trec:   68.939011\tkl:   26.731005\n",
      "Epoch: 380 [20100/50000 (40%)]  \tLoss:   93.925117\trec:   67.941940\tkl:   25.983177\n",
      "Epoch: 380 [30100/50000 (60%)]  \tLoss:   96.212486\trec:   68.859695\tkl:   27.352795\n",
      "Epoch: 380 [40100/50000 (80%)]  \tLoss:   88.353981\trec:   62.957333\tkl:   25.396654\n",
      "====> Epoch: 380 Average train loss: 92.3443\n",
      "====> Validation set loss: 95.1483\n",
      "====> Validation set kl: 25.7978\n",
      "Epoch: 381 [  100/50000 ( 0%)]  \tLoss:   93.682549\trec:   66.967628\tkl:   26.714918\n",
      "Epoch: 381 [10100/50000 (20%)]  \tLoss:   93.974976\trec:   67.338699\tkl:   26.636284\n",
      "Epoch: 381 [20100/50000 (40%)]  \tLoss:   91.488976\trec:   65.322235\tkl:   26.166740\n",
      "Epoch: 381 [30100/50000 (60%)]  \tLoss:   92.872070\trec:   66.577744\tkl:   26.294329\n",
      "Epoch: 381 [40100/50000 (80%)]  \tLoss:   93.477203\trec:   67.048965\tkl:   26.428244\n",
      "====> Epoch: 381 Average train loss: 92.3096\n",
      "====> Validation set loss: 95.1490\n",
      "====> Validation set kl: 25.9418\n",
      "Epoch: 382 [  100/50000 ( 0%)]  \tLoss:   96.549957\trec:   69.720032\tkl:   26.829929\n",
      "Epoch: 382 [10100/50000 (20%)]  \tLoss:   93.211716\trec:   67.719460\tkl:   25.492256\n",
      "Epoch: 382 [20100/50000 (40%)]  \tLoss:   96.058472\trec:   68.636772\tkl:   27.421703\n",
      "Epoch: 382 [30100/50000 (60%)]  \tLoss:   90.497398\trec:   64.541222\tkl:   25.956177\n",
      "Epoch: 382 [40100/50000 (80%)]  \tLoss:   95.297104\trec:   69.321945\tkl:   25.975155\n",
      "====> Epoch: 382 Average train loss: 92.3141\n",
      "====> Validation set loss: 95.0764\n",
      "====> Validation set kl: 25.7568\n",
      "Epoch: 383 [  100/50000 ( 0%)]  \tLoss:   96.876579\trec:   70.640175\tkl:   26.236408\n",
      "Epoch: 383 [10100/50000 (20%)]  \tLoss:   89.689781\trec:   64.660835\tkl:   25.028944\n",
      "Epoch: 383 [20100/50000 (40%)]  \tLoss:   92.994453\trec:   67.117485\tkl:   25.876961\n",
      "Epoch: 383 [30100/50000 (60%)]  \tLoss:   89.529274\trec:   64.611847\tkl:   24.917433\n",
      "Epoch: 383 [40100/50000 (80%)]  \tLoss:   86.122749\trec:   61.185467\tkl:   24.937283\n",
      "====> Epoch: 383 Average train loss: 92.2935\n",
      "====> Validation set loss: 95.2376\n",
      "====> Validation set kl: 25.9418\n",
      "Epoch: 384 [  100/50000 ( 0%)]  \tLoss:   89.516464\trec:   63.942783\tkl:   25.573679\n",
      "Epoch: 384 [10100/50000 (20%)]  \tLoss:   92.900597\trec:   66.900803\tkl:   25.999792\n",
      "Epoch: 384 [20100/50000 (40%)]  \tLoss:   94.850426\trec:   68.851028\tkl:   25.999393\n",
      "Epoch: 384 [30100/50000 (60%)]  \tLoss:   95.072052\trec:   68.625504\tkl:   26.446552\n",
      "Epoch: 384 [40100/50000 (80%)]  \tLoss:   91.507545\trec:   65.411003\tkl:   26.096542\n",
      "====> Epoch: 384 Average train loss: 92.3241\n",
      "====> Validation set loss: 95.1013\n",
      "====> Validation set kl: 25.9959\n",
      "Epoch: 385 [  100/50000 ( 0%)]  \tLoss:   91.921783\trec:   65.689972\tkl:   26.231813\n",
      "Epoch: 385 [10100/50000 (20%)]  \tLoss:   94.647720\trec:   67.834152\tkl:   26.813572\n",
      "Epoch: 385 [20100/50000 (40%)]  \tLoss:   92.812157\trec:   67.049225\tkl:   25.762932\n",
      "Epoch: 385 [30100/50000 (60%)]  \tLoss:   91.098824\trec:   65.875038\tkl:   25.223785\n",
      "Epoch: 385 [40100/50000 (80%)]  \tLoss:   89.367325\trec:   64.107346\tkl:   25.259977\n",
      "====> Epoch: 385 Average train loss: 92.2968\n",
      "====> Validation set loss: 95.1771\n",
      "====> Validation set kl: 25.8689\n",
      "Epoch: 386 [  100/50000 ( 0%)]  \tLoss:   87.795242\trec:   62.810894\tkl:   24.984350\n",
      "Epoch: 386 [10100/50000 (20%)]  \tLoss:   93.978806\trec:   67.611382\tkl:   26.367428\n",
      "Epoch: 386 [20100/50000 (40%)]  \tLoss:   94.553268\trec:   68.078629\tkl:   26.474638\n",
      "Epoch: 386 [30100/50000 (60%)]  \tLoss:   93.622223\trec:   68.014816\tkl:   25.607405\n",
      "Epoch: 386 [40100/50000 (80%)]  \tLoss:   89.222946\trec:   63.810917\tkl:   25.412027\n",
      "====> Epoch: 386 Average train loss: 92.2923\n",
      "====> Validation set loss: 95.2030\n",
      "====> Validation set kl: 25.8884\n",
      "Epoch: 387 [  100/50000 ( 0%)]  \tLoss:   89.217651\trec:   63.865948\tkl:   25.351711\n",
      "Epoch: 387 [10100/50000 (20%)]  \tLoss:   90.969780\trec:   66.144562\tkl:   24.825222\n",
      "Epoch: 387 [20100/50000 (40%)]  \tLoss:   93.944351\trec:   67.829887\tkl:   26.114462\n",
      "Epoch: 387 [30100/50000 (60%)]  \tLoss:   93.214882\trec:   67.122955\tkl:   26.091919\n",
      "Epoch: 387 [40100/50000 (80%)]  \tLoss:   94.884224\trec:   68.886505\tkl:   25.997719\n",
      "====> Epoch: 387 Average train loss: 92.2928\n",
      "====> Validation set loss: 95.1936\n",
      "====> Validation set kl: 25.9682\n",
      "Epoch: 388 [  100/50000 ( 0%)]  \tLoss:   93.483818\trec:   67.173607\tkl:   26.310207\n",
      "Epoch: 388 [10100/50000 (20%)]  \tLoss:   90.708847\trec:   65.564682\tkl:   25.144165\n",
      "Epoch: 388 [20100/50000 (40%)]  \tLoss:   90.567177\trec:   65.762665\tkl:   24.804512\n",
      "Epoch: 388 [30100/50000 (60%)]  \tLoss:   93.308990\trec:   67.799400\tkl:   25.509592\n",
      "Epoch: 388 [40100/50000 (80%)]  \tLoss:   91.037712\trec:   65.357605\tkl:   25.680109\n",
      "====> Epoch: 388 Average train loss: 92.2610\n",
      "====> Validation set loss: 95.1194\n",
      "====> Validation set kl: 25.7895\n",
      "Epoch: 389 [  100/50000 ( 0%)]  \tLoss:   90.262856\trec:   64.713539\tkl:   25.549320\n",
      "Epoch: 389 [10100/50000 (20%)]  \tLoss:   90.907822\trec:   65.151314\tkl:   25.756504\n",
      "Epoch: 389 [20100/50000 (40%)]  \tLoss:   94.439362\trec:   67.826286\tkl:   26.613073\n",
      "Epoch: 389 [30100/50000 (60%)]  \tLoss:   94.092743\trec:   68.315926\tkl:   25.776815\n",
      "Epoch: 389 [40100/50000 (80%)]  \tLoss:   91.053604\trec:   64.871124\tkl:   26.182478\n",
      "====> Epoch: 389 Average train loss: 92.2791\n",
      "====> Validation set loss: 95.0910\n",
      "====> Validation set kl: 25.9037\n",
      "Epoch: 390 [  100/50000 ( 0%)]  \tLoss:   98.549477\trec:   72.117340\tkl:   26.432138\n",
      "Epoch: 390 [10100/50000 (20%)]  \tLoss:   88.800117\trec:   63.102642\tkl:   25.697470\n",
      "Epoch: 390 [20100/50000 (40%)]  \tLoss:   91.254433\trec:   66.455612\tkl:   24.798813\n",
      "Epoch: 390 [30100/50000 (60%)]  \tLoss:   89.573608\trec:   63.951000\tkl:   25.622610\n",
      "Epoch: 390 [40100/50000 (80%)]  \tLoss:   90.848320\trec:   64.813942\tkl:   26.034372\n",
      "====> Epoch: 390 Average train loss: 92.2340\n",
      "====> Validation set loss: 95.1134\n",
      "====> Validation set kl: 25.7150\n",
      "Epoch: 391 [  100/50000 ( 0%)]  \tLoss:   90.795044\trec:   65.858513\tkl:   24.936535\n",
      "Epoch: 391 [10100/50000 (20%)]  \tLoss:   92.659370\trec:   66.620232\tkl:   26.039137\n",
      "Epoch: 391 [20100/50000 (40%)]  \tLoss:   94.582680\trec:   68.481346\tkl:   26.101339\n",
      "Epoch: 391 [30100/50000 (60%)]  \tLoss:   89.590340\trec:   63.350136\tkl:   26.240208\n",
      "Epoch: 391 [40100/50000 (80%)]  \tLoss:   92.981911\trec:   67.037544\tkl:   25.944370\n",
      "====> Epoch: 391 Average train loss: 92.2848\n",
      "====> Validation set loss: 95.0497\n",
      "====> Validation set kl: 25.8712\n",
      "Epoch: 392 [  100/50000 ( 0%)]  \tLoss:   92.781075\trec:   65.784256\tkl:   26.996813\n",
      "Epoch: 392 [10100/50000 (20%)]  \tLoss:   94.035751\trec:   67.865135\tkl:   26.170616\n",
      "Epoch: 392 [20100/50000 (40%)]  \tLoss:   91.572807\trec:   66.080055\tkl:   25.492748\n",
      "Epoch: 392 [30100/50000 (60%)]  \tLoss:   92.106476\trec:   66.556641\tkl:   25.549835\n",
      "Epoch: 392 [40100/50000 (80%)]  \tLoss:   95.868866\trec:   69.176926\tkl:   26.691942\n",
      "====> Epoch: 392 Average train loss: 92.2566\n",
      "====> Validation set loss: 95.0851\n",
      "====> Validation set kl: 25.9486\n",
      "Epoch: 393 [  100/50000 ( 0%)]  \tLoss:   94.553574\trec:   68.312538\tkl:   26.241030\n",
      "Epoch: 393 [10100/50000 (20%)]  \tLoss:   94.599586\trec:   67.397942\tkl:   27.201653\n",
      "Epoch: 393 [20100/50000 (40%)]  \tLoss:   94.273483\trec:   67.814774\tkl:   26.458714\n",
      "Epoch: 393 [30100/50000 (60%)]  \tLoss:   92.832420\trec:   66.391876\tkl:   26.440552\n",
      "Epoch: 393 [40100/50000 (80%)]  \tLoss:   88.353973\trec:   63.200039\tkl:   25.153938\n",
      "====> Epoch: 393 Average train loss: 92.2348\n",
      "====> Validation set loss: 95.1797\n",
      "====> Validation set kl: 25.9631\n",
      "Epoch: 394 [  100/50000 ( 0%)]  \tLoss:   92.271111\trec:   66.283096\tkl:   25.988014\n",
      "Epoch: 394 [10100/50000 (20%)]  \tLoss:   89.188477\trec:   63.514637\tkl:   25.673838\n",
      "Epoch: 394 [20100/50000 (40%)]  \tLoss:   92.710121\trec:   65.657639\tkl:   27.052492\n",
      "Epoch: 394 [30100/50000 (60%)]  \tLoss:   96.017136\trec:   70.362785\tkl:   25.654352\n",
      "Epoch: 394 [40100/50000 (80%)]  \tLoss:   95.190620\trec:   68.802971\tkl:   26.387655\n",
      "====> Epoch: 394 Average train loss: 92.2454\n",
      "====> Validation set loss: 95.0715\n",
      "====> Validation set kl: 25.7324\n",
      "Epoch: 395 [  100/50000 ( 0%)]  \tLoss:   96.643867\trec:   69.956421\tkl:   26.687443\n",
      "Epoch: 395 [10100/50000 (20%)]  \tLoss:   93.195038\trec:   67.141464\tkl:   26.053572\n",
      "Epoch: 395 [20100/50000 (40%)]  \tLoss:   92.024498\trec:   66.511223\tkl:   25.513281\n",
      "Epoch: 395 [30100/50000 (60%)]  \tLoss:   88.937653\trec:   64.177017\tkl:   24.760645\n",
      "Epoch: 395 [40100/50000 (80%)]  \tLoss:   96.885056\trec:   70.129730\tkl:   26.755327\n",
      "====> Epoch: 395 Average train loss: 92.2330\n",
      "====> Validation set loss: 95.0934\n",
      "====> Validation set kl: 25.8113\n",
      "Epoch: 396 [  100/50000 ( 0%)]  \tLoss:   93.323746\trec:   67.251534\tkl:   26.072222\n",
      "Epoch: 396 [10100/50000 (20%)]  \tLoss:   90.268394\trec:   64.788849\tkl:   25.479548\n",
      "Epoch: 396 [20100/50000 (40%)]  \tLoss:   94.550499\trec:   68.454781\tkl:   26.095711\n",
      "Epoch: 396 [30100/50000 (60%)]  \tLoss:   91.559830\trec:   65.898117\tkl:   25.661720\n",
      "Epoch: 396 [40100/50000 (80%)]  \tLoss:   89.299332\trec:   63.895527\tkl:   25.403811\n",
      "====> Epoch: 396 Average train loss: 92.2172\n",
      "====> Validation set loss: 95.1804\n",
      "====> Validation set kl: 25.5894\n",
      "Epoch: 397 [  100/50000 ( 0%)]  \tLoss:   91.648796\trec:   65.654900\tkl:   25.993896\n",
      "Epoch: 397 [10100/50000 (20%)]  \tLoss:   95.257889\trec:   68.977715\tkl:   26.280180\n",
      "Epoch: 397 [20100/50000 (40%)]  \tLoss:   89.420975\trec:   63.715324\tkl:   25.705647\n",
      "Epoch: 397 [30100/50000 (60%)]  \tLoss:   94.214294\trec:   68.252762\tkl:   25.961535\n",
      "Epoch: 397 [40100/50000 (80%)]  \tLoss:   92.895164\trec:   66.653183\tkl:   26.241982\n",
      "====> Epoch: 397 Average train loss: 92.2106\n",
      "====> Validation set loss: 95.1184\n",
      "====> Validation set kl: 25.8225\n",
      "Epoch: 398 [  100/50000 ( 0%)]  \tLoss:   91.802231\trec:   66.446503\tkl:   25.355730\n",
      "Epoch: 398 [10100/50000 (20%)]  \tLoss:   93.823555\trec:   67.949821\tkl:   25.873726\n",
      "Epoch: 398 [20100/50000 (40%)]  \tLoss:   93.526161\trec:   67.722168\tkl:   25.803995\n",
      "Epoch: 398 [30100/50000 (60%)]  \tLoss:   87.115952\trec:   62.436195\tkl:   24.679760\n",
      "Epoch: 398 [40100/50000 (80%)]  \tLoss:   93.377754\trec:   67.271729\tkl:   26.106020\n",
      "====> Epoch: 398 Average train loss: 92.2190\n",
      "====> Validation set loss: 95.1573\n",
      "====> Validation set kl: 25.9283\n",
      "Epoch: 399 [  100/50000 ( 0%)]  \tLoss:   85.244942\trec:   60.001431\tkl:   25.243515\n",
      "Epoch: 399 [10100/50000 (20%)]  \tLoss:   91.690994\trec:   65.997421\tkl:   25.693579\n",
      "Epoch: 399 [20100/50000 (40%)]  \tLoss:   90.568634\trec:   64.962959\tkl:   25.605673\n",
      "Epoch: 399 [30100/50000 (60%)]  \tLoss:   90.183983\trec:   63.910290\tkl:   26.273695\n",
      "Epoch: 399 [40100/50000 (80%)]  \tLoss:   93.098740\trec:   66.697121\tkl:   26.401619\n",
      "====> Epoch: 399 Average train loss: 92.2205\n",
      "====> Validation set loss: 95.0184\n",
      "====> Validation set kl: 25.7532\n",
      "Epoch: 400 [  100/50000 ( 0%)]  \tLoss:   91.327080\trec:   65.081451\tkl:   26.245626\n",
      "Epoch: 400 [10100/50000 (20%)]  \tLoss:   97.338600\trec:   70.909050\tkl:   26.429550\n",
      "Epoch: 400 [20100/50000 (40%)]  \tLoss:   91.178955\trec:   65.773170\tkl:   25.405781\n",
      "Epoch: 400 [30100/50000 (60%)]  \tLoss:   94.967705\trec:   68.206169\tkl:   26.761530\n",
      "Epoch: 400 [40100/50000 (80%)]  \tLoss:   90.615196\trec:   65.189545\tkl:   25.425650\n",
      "====> Epoch: 400 Average train loss: 92.2214\n",
      "====> Validation set loss: 95.0752\n",
      "====> Validation set kl: 26.0021\n",
      "Epoch: 401 [  100/50000 ( 0%)]  \tLoss:   88.471275\trec:   62.936722\tkl:   25.534554\n",
      "Epoch: 401 [10100/50000 (20%)]  \tLoss:   95.593475\trec:   69.294800\tkl:   26.298666\n",
      "Epoch: 401 [20100/50000 (40%)]  \tLoss:   93.068054\trec:   66.973434\tkl:   26.094618\n",
      "Epoch: 401 [30100/50000 (60%)]  \tLoss:   88.670586\trec:   63.346336\tkl:   25.324244\n",
      "Epoch: 401 [40100/50000 (80%)]  \tLoss:   90.948174\trec:   65.572800\tkl:   25.375368\n",
      "====> Epoch: 401 Average train loss: 92.1975\n",
      "====> Validation set loss: 95.0496\n",
      "====> Validation set kl: 25.8093\n",
      "Epoch: 402 [  100/50000 ( 0%)]  \tLoss:   91.338356\trec:   65.747292\tkl:   25.591072\n",
      "Epoch: 402 [10100/50000 (20%)]  \tLoss:   95.660858\trec:   69.528374\tkl:   26.132490\n",
      "Epoch: 402 [20100/50000 (40%)]  \tLoss:   90.863823\trec:   64.880089\tkl:   25.983744\n",
      "Epoch: 402 [30100/50000 (60%)]  \tLoss:   90.839165\trec:   64.981094\tkl:   25.858076\n",
      "Epoch: 402 [40100/50000 (80%)]  \tLoss:   91.100830\trec:   65.881874\tkl:   25.218954\n",
      "====> Epoch: 402 Average train loss: 92.1721\n",
      "====> Validation set loss: 95.1012\n",
      "====> Validation set kl: 25.8157\n",
      "Epoch: 403 [  100/50000 ( 0%)]  \tLoss:   94.364487\trec:   68.743996\tkl:   25.620495\n",
      "Epoch: 403 [10100/50000 (20%)]  \tLoss:   90.581284\trec:   65.618736\tkl:   24.962551\n",
      "Epoch: 403 [20100/50000 (40%)]  \tLoss:   93.577332\trec:   67.645409\tkl:   25.931921\n",
      "Epoch: 403 [30100/50000 (60%)]  \tLoss:   93.482773\trec:   66.859352\tkl:   26.623413\n",
      "Epoch: 403 [40100/50000 (80%)]  \tLoss:   92.363800\trec:   66.904778\tkl:   25.459021\n",
      "====> Epoch: 403 Average train loss: 92.1851\n",
      "====> Validation set loss: 95.1195\n",
      "====> Validation set kl: 25.9580\n",
      "Epoch: 404 [  100/50000 ( 0%)]  \tLoss:   89.573784\trec:   63.615917\tkl:   25.957876\n",
      "Epoch: 404 [10100/50000 (20%)]  \tLoss:   92.746544\trec:   66.764458\tkl:   25.982075\n",
      "Epoch: 404 [20100/50000 (40%)]  \tLoss:   92.980026\trec:   67.368614\tkl:   25.611416\n",
      "Epoch: 404 [30100/50000 (60%)]  \tLoss:   89.329117\trec:   63.921074\tkl:   25.408041\n",
      "Epoch: 404 [40100/50000 (80%)]  \tLoss:   92.841660\trec:   66.920631\tkl:   25.921034\n",
      "====> Epoch: 404 Average train loss: 92.1808\n",
      "====> Validation set loss: 95.0546\n",
      "====> Validation set kl: 25.7444\n",
      "Epoch: 405 [  100/50000 ( 0%)]  \tLoss:   91.400818\trec:   66.195915\tkl:   25.204906\n",
      "Epoch: 405 [10100/50000 (20%)]  \tLoss:   93.578583\trec:   67.194679\tkl:   26.383907\n",
      "Epoch: 405 [20100/50000 (40%)]  \tLoss:   91.556503\trec:   66.019890\tkl:   25.536613\n",
      "Epoch: 405 [30100/50000 (60%)]  \tLoss:   92.162949\trec:   67.432861\tkl:   24.730082\n",
      "Epoch: 405 [40100/50000 (80%)]  \tLoss:   91.827606\trec:   66.638840\tkl:   25.188761\n",
      "====> Epoch: 405 Average train loss: 92.1574\n",
      "====> Validation set loss: 95.1116\n",
      "====> Validation set kl: 25.9195\n",
      "Epoch: 406 [  100/50000 ( 0%)]  \tLoss:   89.754295\trec:   64.375679\tkl:   25.378614\n",
      "Epoch: 406 [10100/50000 (20%)]  \tLoss:   93.592361\trec:   67.515602\tkl:   26.076761\n",
      "Epoch: 406 [20100/50000 (40%)]  \tLoss:   91.911362\trec:   65.532364\tkl:   26.378996\n",
      "Epoch: 406 [30100/50000 (60%)]  \tLoss:   93.993080\trec:   67.472481\tkl:   26.520597\n",
      "Epoch: 406 [40100/50000 (80%)]  \tLoss:   90.519333\trec:   64.891861\tkl:   25.627470\n",
      "====> Epoch: 406 Average train loss: 92.1716\n",
      "====> Validation set loss: 95.1373\n",
      "====> Validation set kl: 25.9182\n",
      "Epoch: 407 [  100/50000 ( 0%)]  \tLoss:   94.487030\trec:   68.743980\tkl:   25.743046\n",
      "Epoch: 407 [10100/50000 (20%)]  \tLoss:   96.711235\trec:   70.270020\tkl:   26.441219\n",
      "Epoch: 407 [20100/50000 (40%)]  \tLoss:   88.947800\trec:   63.129795\tkl:   25.818010\n",
      "Epoch: 407 [30100/50000 (60%)]  \tLoss:   96.087242\trec:   70.428970\tkl:   25.658266\n",
      "Epoch: 407 [40100/50000 (80%)]  \tLoss:   87.861290\trec:   62.820625\tkl:   25.040659\n",
      "====> Epoch: 407 Average train loss: 92.1553\n",
      "====> Validation set loss: 95.1157\n",
      "====> Validation set kl: 25.7450\n",
      "Epoch: 408 [  100/50000 ( 0%)]  \tLoss:   93.527863\trec:   67.425087\tkl:   26.102770\n",
      "Epoch: 408 [10100/50000 (20%)]  \tLoss:   90.162178\trec:   65.759933\tkl:   24.402246\n",
      "Epoch: 408 [20100/50000 (40%)]  \tLoss:   93.668358\trec:   66.708954\tkl:   26.959408\n",
      "Epoch: 408 [30100/50000 (60%)]  \tLoss:   91.393524\trec:   65.769547\tkl:   25.623974\n",
      "Epoch: 408 [40100/50000 (80%)]  \tLoss:   92.056343\trec:   66.489212\tkl:   25.567129\n",
      "====> Epoch: 408 Average train loss: 92.1389\n",
      "====> Validation set loss: 95.1259\n",
      "====> Validation set kl: 25.9683\n",
      "Epoch: 409 [  100/50000 ( 0%)]  \tLoss:   87.601013\trec:   62.996552\tkl:   24.604462\n",
      "Epoch: 409 [10100/50000 (20%)]  \tLoss:   92.246346\trec:   66.216187\tkl:   26.030155\n",
      "Epoch: 409 [20100/50000 (40%)]  \tLoss:   94.444267\trec:   67.804077\tkl:   26.640188\n",
      "Epoch: 409 [30100/50000 (60%)]  \tLoss:   93.711189\trec:   67.104980\tkl:   26.606211\n",
      "Epoch: 409 [40100/50000 (80%)]  \tLoss:   91.940094\trec:   67.224358\tkl:   24.715736\n",
      "====> Epoch: 409 Average train loss: 92.1330\n",
      "====> Validation set loss: 95.0617\n",
      "====> Validation set kl: 25.7684\n",
      "Epoch: 410 [  100/50000 ( 0%)]  \tLoss:   92.918633\trec:   66.824982\tkl:   26.093655\n",
      "Epoch: 410 [10100/50000 (20%)]  \tLoss:   92.574707\trec:   66.365913\tkl:   26.208784\n",
      "Epoch: 410 [20100/50000 (40%)]  \tLoss:   92.988892\trec:   67.631287\tkl:   25.357607\n",
      "Epoch: 410 [30100/50000 (60%)]  \tLoss:   93.522301\trec:   68.030388\tkl:   25.491919\n",
      "Epoch: 410 [40100/50000 (80%)]  \tLoss:   88.990112\trec:   64.607780\tkl:   24.382343\n",
      "====> Epoch: 410 Average train loss: 92.1459\n",
      "====> Validation set loss: 95.0983\n",
      "====> Validation set kl: 25.9821\n",
      "Epoch: 411 [  100/50000 ( 0%)]  \tLoss:   90.713799\trec:   65.683517\tkl:   25.030283\n",
      "Epoch: 411 [10100/50000 (20%)]  \tLoss:   92.017593\trec:   66.141701\tkl:   25.875900\n",
      "Epoch: 411 [20100/50000 (40%)]  \tLoss:   92.788826\trec:   66.190826\tkl:   26.598001\n",
      "Epoch: 411 [30100/50000 (60%)]  \tLoss:   89.525604\trec:   63.333569\tkl:   26.192034\n",
      "Epoch: 411 [40100/50000 (80%)]  \tLoss:   97.498810\trec:   69.587646\tkl:   27.911167\n",
      "====> Epoch: 411 Average train loss: 92.1447\n",
      "====> Validation set loss: 95.0877\n",
      "====> Validation set kl: 25.6948\n",
      "Epoch: 412 [  100/50000 ( 0%)]  \tLoss:   92.838219\trec:   67.397911\tkl:   25.440308\n",
      "Epoch: 412 [10100/50000 (20%)]  \tLoss:   90.128044\trec:   64.686226\tkl:   25.441818\n",
      "Epoch: 412 [20100/50000 (40%)]  \tLoss:   95.756821\trec:   68.885109\tkl:   26.871712\n",
      "Epoch: 412 [30100/50000 (60%)]  \tLoss:   94.063194\trec:   67.386375\tkl:   26.676815\n",
      "Epoch: 412 [40100/50000 (80%)]  \tLoss:   91.819824\trec:   65.705582\tkl:   26.114237\n",
      "====> Epoch: 412 Average train loss: 92.1343\n",
      "====> Validation set loss: 94.9345\n",
      "====> Validation set kl: 25.7766\n",
      "Epoch: 413 [  100/50000 ( 0%)]  \tLoss:   93.806305\trec:   67.103134\tkl:   26.703171\n",
      "Epoch: 413 [10100/50000 (20%)]  \tLoss:   88.645996\trec:   62.999325\tkl:   25.646671\n",
      "Epoch: 413 [20100/50000 (40%)]  \tLoss:   90.731560\trec:   64.998993\tkl:   25.732567\n",
      "Epoch: 413 [30100/50000 (60%)]  \tLoss:   95.421173\trec:   68.790314\tkl:   26.630863\n",
      "Epoch: 413 [40100/50000 (80%)]  \tLoss:   90.881844\trec:   65.038834\tkl:   25.843008\n",
      "====> Epoch: 413 Average train loss: 92.1079\n",
      "====> Validation set loss: 95.1117\n",
      "====> Validation set kl: 26.0246\n",
      "Epoch: 414 [  100/50000 ( 0%)]  \tLoss:   91.869644\trec:   65.938904\tkl:   25.930737\n",
      "Epoch: 414 [10100/50000 (20%)]  \tLoss:   94.116280\trec:   68.049835\tkl:   26.066446\n",
      "Epoch: 414 [20100/50000 (40%)]  \tLoss:   90.946953\trec:   64.713371\tkl:   26.233582\n",
      "Epoch: 414 [30100/50000 (60%)]  \tLoss:   89.673210\trec:   63.163383\tkl:   26.509830\n",
      "Epoch: 414 [40100/50000 (80%)]  \tLoss:   92.286964\trec:   65.830956\tkl:   26.456007\n",
      "====> Epoch: 414 Average train loss: 92.1261\n",
      "====> Validation set loss: 95.1315\n",
      "====> Validation set kl: 25.8648\n",
      "Epoch: 415 [  100/50000 ( 0%)]  \tLoss:   92.161720\trec:   66.071930\tkl:   26.089790\n",
      "Epoch: 415 [10100/50000 (20%)]  \tLoss:   89.430290\trec:   63.926594\tkl:   25.503691\n",
      "Epoch: 415 [20100/50000 (40%)]  \tLoss:   92.701279\trec:   67.086716\tkl:   25.614563\n",
      "Epoch: 415 [30100/50000 (60%)]  \tLoss:   88.856476\trec:   63.725918\tkl:   25.130554\n",
      "Epoch: 415 [40100/50000 (80%)]  \tLoss:   96.047798\trec:   69.496750\tkl:   26.551052\n",
      "====> Epoch: 415 Average train loss: 92.0858\n",
      "====> Validation set loss: 94.9863\n",
      "====> Validation set kl: 25.9230\n",
      "Epoch: 416 [  100/50000 ( 0%)]  \tLoss:   99.658829\trec:   72.378746\tkl:   27.280083\n",
      "Epoch: 416 [10100/50000 (20%)]  \tLoss:   92.484001\trec:   67.410667\tkl:   25.073332\n",
      "Epoch: 416 [20100/50000 (40%)]  \tLoss:   90.376266\trec:   64.295647\tkl:   26.080622\n",
      "Epoch: 416 [30100/50000 (60%)]  \tLoss:   91.613396\trec:   65.694420\tkl:   25.918976\n",
      "Epoch: 416 [40100/50000 (80%)]  \tLoss:   92.640854\trec:   66.582047\tkl:   26.058811\n",
      "====> Epoch: 416 Average train loss: 92.1036\n",
      "====> Validation set loss: 95.0853\n",
      "====> Validation set kl: 25.8278\n",
      "Epoch: 417 [  100/50000 ( 0%)]  \tLoss:   93.389214\trec:   67.075348\tkl:   26.313873\n",
      "Epoch: 417 [10100/50000 (20%)]  \tLoss:   89.032280\trec:   63.964314\tkl:   25.067966\n",
      "Epoch: 417 [20100/50000 (40%)]  \tLoss:   94.259941\trec:   68.232765\tkl:   26.027182\n",
      "Epoch: 417 [30100/50000 (60%)]  \tLoss:   92.612778\trec:   67.207542\tkl:   25.405241\n",
      "Epoch: 417 [40100/50000 (80%)]  \tLoss:   88.538933\trec:   63.150902\tkl:   25.388031\n",
      "====> Epoch: 417 Average train loss: 92.0781\n",
      "====> Validation set loss: 94.9225\n",
      "====> Validation set kl: 25.9604\n",
      "Epoch: 418 [  100/50000 ( 0%)]  \tLoss:   88.654091\trec:   64.163246\tkl:   24.490841\n",
      "Epoch: 418 [10100/50000 (20%)]  \tLoss:   93.352333\trec:   68.056557\tkl:   25.295776\n",
      "Epoch: 418 [20100/50000 (40%)]  \tLoss:   91.326065\trec:   65.417046\tkl:   25.909019\n",
      "Epoch: 418 [30100/50000 (60%)]  \tLoss:   91.088806\trec:   65.446770\tkl:   25.642040\n",
      "Epoch: 418 [40100/50000 (80%)]  \tLoss:   89.558823\trec:   64.199394\tkl:   25.359428\n",
      "====> Epoch: 418 Average train loss: 92.0935\n",
      "====> Validation set loss: 95.0195\n",
      "====> Validation set kl: 26.0036\n",
      "Epoch: 419 [  100/50000 ( 0%)]  \tLoss:   94.581718\trec:   67.449928\tkl:   27.131783\n",
      "Epoch: 419 [10100/50000 (20%)]  \tLoss:   95.413864\trec:   69.225098\tkl:   26.188759\n",
      "Epoch: 419 [20100/50000 (40%)]  \tLoss:   95.288872\trec:   69.535591\tkl:   25.753283\n",
      "Epoch: 419 [30100/50000 (60%)]  \tLoss:   89.028862\trec:   63.205410\tkl:   25.823460\n",
      "Epoch: 419 [40100/50000 (80%)]  \tLoss:   92.471001\trec:   67.418221\tkl:   25.052780\n",
      "====> Epoch: 419 Average train loss: 92.0716\n",
      "====> Validation set loss: 95.0194\n",
      "====> Validation set kl: 25.9038\n",
      "Epoch: 420 [  100/50000 ( 0%)]  \tLoss:   90.897705\trec:   65.404404\tkl:   25.493299\n",
      "Epoch: 420 [10100/50000 (20%)]  \tLoss:   92.498268\trec:   66.535934\tkl:   25.962332\n",
      "Epoch: 420 [20100/50000 (40%)]  \tLoss:   89.665077\trec:   63.610790\tkl:   26.054287\n",
      "Epoch: 420 [30100/50000 (60%)]  \tLoss:   91.054543\trec:   65.270096\tkl:   25.784443\n",
      "Epoch: 420 [40100/50000 (80%)]  \tLoss:   91.150497\trec:   65.162926\tkl:   25.987572\n",
      "====> Epoch: 420 Average train loss: 92.0940\n",
      "====> Validation set loss: 95.1018\n",
      "====> Validation set kl: 25.7796\n",
      "Epoch: 421 [  100/50000 ( 0%)]  \tLoss:   94.927948\trec:   68.146408\tkl:   26.781546\n",
      "Epoch: 421 [10100/50000 (20%)]  \tLoss:   88.869743\trec:   63.687195\tkl:   25.182552\n",
      "Epoch: 421 [20100/50000 (40%)]  \tLoss:   89.510078\trec:   63.979862\tkl:   25.530212\n",
      "Epoch: 421 [30100/50000 (60%)]  \tLoss:   89.595848\trec:   63.803894\tkl:   25.791954\n",
      "Epoch: 421 [40100/50000 (80%)]  \tLoss:   89.333786\trec:   63.396629\tkl:   25.937162\n",
      "====> Epoch: 421 Average train loss: 92.0711\n",
      "====> Validation set loss: 95.1311\n",
      "====> Validation set kl: 25.9852\n",
      "Epoch: 422 [  100/50000 ( 0%)]  \tLoss:   89.154198\trec:   63.417133\tkl:   25.737070\n",
      "Epoch: 422 [10100/50000 (20%)]  \tLoss:   89.931015\trec:   64.591850\tkl:   25.339165\n",
      "Epoch: 422 [20100/50000 (40%)]  \tLoss:   91.827614\trec:   66.632912\tkl:   25.194706\n",
      "Epoch: 422 [30100/50000 (60%)]  \tLoss:   92.587372\trec:   66.270592\tkl:   26.316780\n",
      "Epoch: 422 [40100/50000 (80%)]  \tLoss:   92.817123\trec:   66.267052\tkl:   26.550076\n",
      "====> Epoch: 422 Average train loss: 92.0825\n",
      "====> Validation set loss: 95.0545\n",
      "====> Validation set kl: 26.0485\n",
      "Epoch: 423 [  100/50000 ( 0%)]  \tLoss:   95.722633\trec:   68.894615\tkl:   26.828020\n",
      "Epoch: 423 [10100/50000 (20%)]  \tLoss:   93.990417\trec:   67.882248\tkl:   26.108173\n",
      "Epoch: 423 [20100/50000 (40%)]  \tLoss:   93.888100\trec:   68.042458\tkl:   25.845644\n",
      "Epoch: 423 [30100/50000 (60%)]  \tLoss:   91.652809\trec:   65.585884\tkl:   26.066933\n",
      "Epoch: 423 [40100/50000 (80%)]  \tLoss:   94.938843\trec:   68.404701\tkl:   26.534147\n",
      "====> Epoch: 423 Average train loss: 92.0550\n",
      "====> Validation set loss: 95.0708\n",
      "====> Validation set kl: 25.9453\n",
      "Epoch: 424 [  100/50000 ( 0%)]  \tLoss:   92.930695\trec:   66.838722\tkl:   26.091967\n",
      "Epoch: 424 [10100/50000 (20%)]  \tLoss:   87.946251\trec:   62.245209\tkl:   25.701036\n",
      "Epoch: 424 [20100/50000 (40%)]  \tLoss:   94.728600\trec:   68.740402\tkl:   25.988195\n",
      "Epoch: 424 [30100/50000 (60%)]  \tLoss:   91.318970\trec:   64.624176\tkl:   26.694792\n",
      "Epoch: 424 [40100/50000 (80%)]  \tLoss:   88.630096\trec:   63.595188\tkl:   25.034912\n",
      "====> Epoch: 424 Average train loss: 92.0653\n",
      "====> Validation set loss: 95.0741\n",
      "====> Validation set kl: 25.9577\n",
      "Epoch: 425 [  100/50000 ( 0%)]  \tLoss:   95.363960\trec:   68.838127\tkl:   26.525831\n",
      "Epoch: 425 [10100/50000 (20%)]  \tLoss:   90.547096\trec:   64.681198\tkl:   25.865896\n",
      "Epoch: 425 [20100/50000 (40%)]  \tLoss:   90.534721\trec:   64.824631\tkl:   25.710096\n",
      "Epoch: 425 [30100/50000 (60%)]  \tLoss:   89.952065\trec:   64.887985\tkl:   25.064091\n",
      "Epoch: 425 [40100/50000 (80%)]  \tLoss:   95.937332\trec:   69.560318\tkl:   26.377014\n",
      "====> Epoch: 425 Average train loss: 92.0629\n",
      "====> Validation set loss: 95.1318\n",
      "====> Validation set kl: 25.8792\n",
      "Epoch: 426 [  100/50000 ( 0%)]  \tLoss:   89.455757\trec:   64.030891\tkl:   25.424873\n",
      "Epoch: 426 [10100/50000 (20%)]  \tLoss:   92.181953\trec:   65.988663\tkl:   26.193295\n",
      "Epoch: 426 [20100/50000 (40%)]  \tLoss:   92.999657\trec:   67.110207\tkl:   25.889450\n",
      "Epoch: 426 [30100/50000 (60%)]  \tLoss:   90.834633\trec:   64.995758\tkl:   25.838875\n",
      "Epoch: 426 [40100/50000 (80%)]  \tLoss:   91.773140\trec:   66.477402\tkl:   25.295740\n",
      "====> Epoch: 426 Average train loss: 92.0376\n",
      "====> Validation set loss: 94.9405\n",
      "====> Validation set kl: 25.9145\n",
      "Epoch: 427 [  100/50000 ( 0%)]  \tLoss:   91.590698\trec:   65.547173\tkl:   26.043530\n",
      "Epoch: 427 [10100/50000 (20%)]  \tLoss:   95.453369\trec:   68.328667\tkl:   27.124697\n",
      "Epoch: 427 [20100/50000 (40%)]  \tLoss:   93.398415\trec:   66.347359\tkl:   27.051060\n",
      "Epoch: 427 [30100/50000 (60%)]  \tLoss:   96.173592\trec:   69.198296\tkl:   26.975294\n",
      "Epoch: 427 [40100/50000 (80%)]  \tLoss:   91.537712\trec:   65.240593\tkl:   26.297121\n",
      "====> Epoch: 427 Average train loss: 92.0346\n",
      "====> Validation set loss: 95.0175\n",
      "====> Validation set kl: 25.9173\n",
      "Epoch: 428 [  100/50000 ( 0%)]  \tLoss:   92.149796\trec:   65.952515\tkl:   26.197273\n",
      "Epoch: 428 [10100/50000 (20%)]  \tLoss:   88.906822\trec:   62.446880\tkl:   26.459944\n",
      "Epoch: 428 [20100/50000 (40%)]  \tLoss:   89.997627\trec:   64.242966\tkl:   25.754665\n",
      "Epoch: 428 [30100/50000 (60%)]  \tLoss:   90.712143\trec:   65.564697\tkl:   25.147451\n",
      "Epoch: 428 [40100/50000 (80%)]  \tLoss:   90.214531\trec:   64.400017\tkl:   25.814514\n",
      "====> Epoch: 428 Average train loss: 92.0158\n",
      "====> Validation set loss: 95.0118\n",
      "====> Validation set kl: 25.8674\n",
      "Epoch: 429 [  100/50000 ( 0%)]  \tLoss:   93.930893\trec:   68.373146\tkl:   25.557749\n",
      "Epoch: 429 [10100/50000 (20%)]  \tLoss:   90.759644\trec:   65.138565\tkl:   25.621088\n",
      "Epoch: 429 [20100/50000 (40%)]  \tLoss:   92.419121\trec:   66.784225\tkl:   25.634892\n",
      "Epoch: 429 [30100/50000 (60%)]  \tLoss:   90.478394\trec:   65.051743\tkl:   25.426653\n",
      "Epoch: 429 [40100/50000 (80%)]  \tLoss:   91.121178\trec:   65.679314\tkl:   25.441862\n",
      "====> Epoch: 429 Average train loss: 92.0384\n",
      "====> Validation set loss: 94.9905\n",
      "====> Validation set kl: 25.9183\n",
      "Epoch: 430 [  100/50000 ( 0%)]  \tLoss:   96.087593\trec:   69.377892\tkl:   26.709703\n",
      "Epoch: 430 [10100/50000 (20%)]  \tLoss:   93.038574\trec:   67.785278\tkl:   25.253290\n",
      "Epoch: 430 [20100/50000 (40%)]  \tLoss:   89.836395\trec:   64.144463\tkl:   25.691931\n",
      "Epoch: 430 [30100/50000 (60%)]  \tLoss:   93.000839\trec:   66.404732\tkl:   26.596107\n",
      "Epoch: 430 [40100/50000 (80%)]  \tLoss:   87.867851\trec:   62.751446\tkl:   25.116407\n",
      "====> Epoch: 430 Average train loss: 92.0201\n",
      "====> Validation set loss: 94.9491\n",
      "====> Validation set kl: 25.8961\n",
      "Epoch: 431 [  100/50000 ( 0%)]  \tLoss:   89.840782\trec:   64.672783\tkl:   25.167997\n",
      "Epoch: 431 [10100/50000 (20%)]  \tLoss:   93.442184\trec:   67.172745\tkl:   26.269444\n",
      "Epoch: 431 [20100/50000 (40%)]  \tLoss:   93.140137\trec:   67.168922\tkl:   25.971205\n",
      "Epoch: 431 [30100/50000 (60%)]  \tLoss:   93.681801\trec:   67.178543\tkl:   26.503263\n",
      "Epoch: 431 [40100/50000 (80%)]  \tLoss:   89.915543\trec:   63.910614\tkl:   26.004934\n",
      "====> Epoch: 431 Average train loss: 92.0165\n",
      "====> Validation set loss: 95.0529\n",
      "====> Validation set kl: 25.8019\n",
      "Epoch: 432 [  100/50000 ( 0%)]  \tLoss:   95.823906\trec:   69.559044\tkl:   26.264866\n",
      "Epoch: 432 [10100/50000 (20%)]  \tLoss:   95.479614\trec:   68.485153\tkl:   26.994465\n",
      "Epoch: 432 [20100/50000 (40%)]  \tLoss:   89.643219\trec:   64.261337\tkl:   25.381889\n",
      "Epoch: 432 [30100/50000 (60%)]  \tLoss:   90.955048\trec:   65.115562\tkl:   25.839485\n",
      "Epoch: 432 [40100/50000 (80%)]  \tLoss:   91.473320\trec:   66.068687\tkl:   25.404633\n",
      "====> Epoch: 432 Average train loss: 92.0208\n",
      "====> Validation set loss: 94.9955\n",
      "====> Validation set kl: 25.8527\n",
      "Epoch: 433 [  100/50000 ( 0%)]  \tLoss:   92.448418\trec:   65.959137\tkl:   26.489279\n",
      "Epoch: 433 [10100/50000 (20%)]  \tLoss:   91.901558\trec:   66.175362\tkl:   25.726200\n",
      "Epoch: 433 [20100/50000 (40%)]  \tLoss:   92.043358\trec:   66.257507\tkl:   25.785851\n",
      "Epoch: 433 [30100/50000 (60%)]  \tLoss:   90.757362\trec:   65.093002\tkl:   25.664360\n",
      "Epoch: 433 [40100/50000 (80%)]  \tLoss:   90.909615\trec:   65.398972\tkl:   25.510645\n",
      "====> Epoch: 433 Average train loss: 92.0018\n",
      "====> Validation set loss: 95.0610\n",
      "====> Validation set kl: 26.0348\n",
      "Epoch: 434 [  100/50000 ( 0%)]  \tLoss:   88.748856\trec:   63.414169\tkl:   25.334686\n",
      "Epoch: 434 [10100/50000 (20%)]  \tLoss:   88.848824\trec:   63.238857\tkl:   25.609968\n",
      "Epoch: 434 [20100/50000 (40%)]  \tLoss:   91.357224\trec:   65.098145\tkl:   26.259077\n",
      "Epoch: 434 [30100/50000 (60%)]  \tLoss:   97.855080\trec:   71.298424\tkl:   26.556652\n",
      "Epoch: 434 [40100/50000 (80%)]  \tLoss:   90.158455\trec:   64.215698\tkl:   25.942759\n",
      "====> Epoch: 434 Average train loss: 92.0086\n",
      "====> Validation set loss: 94.9991\n",
      "====> Validation set kl: 25.9965\n",
      "Epoch: 435 [  100/50000 ( 0%)]  \tLoss:   89.631989\trec:   64.392632\tkl:   25.239353\n",
      "Epoch: 435 [10100/50000 (20%)]  \tLoss:   89.358376\trec:   63.745571\tkl:   25.612804\n",
      "Epoch: 435 [20100/50000 (40%)]  \tLoss:   89.768150\trec:   64.513123\tkl:   25.255032\n",
      "Epoch: 435 [30100/50000 (60%)]  \tLoss:   93.455292\trec:   66.857300\tkl:   26.597984\n",
      "Epoch: 435 [40100/50000 (80%)]  \tLoss:   90.136620\trec:   65.251732\tkl:   24.884888\n",
      "====> Epoch: 435 Average train loss: 91.9832\n",
      "====> Validation set loss: 94.9667\n",
      "====> Validation set kl: 25.9126\n",
      "Epoch: 436 [  100/50000 ( 0%)]  \tLoss:   91.824585\trec:   66.179008\tkl:   25.645578\n",
      "Epoch: 436 [10100/50000 (20%)]  \tLoss:   94.773247\trec:   68.530876\tkl:   26.242373\n",
      "Epoch: 436 [20100/50000 (40%)]  \tLoss:   97.956909\trec:   70.944160\tkl:   27.012753\n",
      "Epoch: 436 [30100/50000 (60%)]  \tLoss:   94.271385\trec:   68.170654\tkl:   26.100721\n",
      "Epoch: 436 [40100/50000 (80%)]  \tLoss:   94.379951\trec:   68.610184\tkl:   25.769766\n",
      "====> Epoch: 436 Average train loss: 91.9844\n",
      "====> Validation set loss: 94.9994\n",
      "====> Validation set kl: 26.0334\n",
      "Epoch: 437 [  100/50000 ( 0%)]  \tLoss:   92.562477\trec:   66.259239\tkl:   26.303236\n",
      "Epoch: 437 [10100/50000 (20%)]  \tLoss:   92.524200\trec:   66.469147\tkl:   26.055048\n",
      "Epoch: 437 [20100/50000 (40%)]  \tLoss:   95.354843\trec:   69.479942\tkl:   25.874901\n",
      "Epoch: 437 [30100/50000 (60%)]  \tLoss:   91.878471\trec:   66.057098\tkl:   25.821381\n",
      "Epoch: 437 [40100/50000 (80%)]  \tLoss:   92.232162\trec:   66.365875\tkl:   25.866299\n",
      "====> Epoch: 437 Average train loss: 91.9757\n",
      "====> Validation set loss: 94.9851\n",
      "====> Validation set kl: 25.9113\n",
      "Epoch: 438 [  100/50000 ( 0%)]  \tLoss:   88.287476\trec:   63.760838\tkl:   24.526646\n",
      "Epoch: 438 [10100/50000 (20%)]  \tLoss:   91.663612\trec:   65.576393\tkl:   26.087219\n",
      "Epoch: 438 [20100/50000 (40%)]  \tLoss:   92.327797\trec:   65.403961\tkl:   26.923840\n",
      "Epoch: 438 [30100/50000 (60%)]  \tLoss:   91.374969\trec:   65.889542\tkl:   25.485424\n",
      "Epoch: 438 [40100/50000 (80%)]  \tLoss:   91.734726\trec:   65.809517\tkl:   25.925207\n",
      "====> Epoch: 438 Average train loss: 91.9932\n",
      "====> Validation set loss: 94.9400\n",
      "====> Validation set kl: 25.8547\n",
      "Epoch: 439 [  100/50000 ( 0%)]  \tLoss:   88.511070\trec:   63.421616\tkl:   25.089462\n",
      "Epoch: 439 [10100/50000 (20%)]  \tLoss:   91.641510\trec:   65.806511\tkl:   25.835001\n",
      "Epoch: 439 [20100/50000 (40%)]  \tLoss:   89.588081\trec:   63.799324\tkl:   25.788759\n",
      "Epoch: 439 [30100/50000 (60%)]  \tLoss:   93.848541\trec:   67.284477\tkl:   26.564060\n",
      "Epoch: 439 [40100/50000 (80%)]  \tLoss:   89.233871\trec:   63.238407\tkl:   25.995466\n",
      "====> Epoch: 439 Average train loss: 91.9650\n",
      "====> Validation set loss: 94.9794\n",
      "====> Validation set kl: 26.0449\n",
      "Epoch: 440 [  100/50000 ( 0%)]  \tLoss:   92.756363\trec:   66.350090\tkl:   26.406271\n",
      "Epoch: 440 [10100/50000 (20%)]  \tLoss:   95.550102\trec:   69.323494\tkl:   26.226614\n",
      "Epoch: 440 [20100/50000 (40%)]  \tLoss:   91.291534\trec:   65.660995\tkl:   25.630539\n",
      "Epoch: 440 [30100/50000 (60%)]  \tLoss:   90.476746\trec:   64.528633\tkl:   25.948112\n",
      "Epoch: 440 [40100/50000 (80%)]  \tLoss:   91.707848\trec:   66.963776\tkl:   24.744070\n",
      "====> Epoch: 440 Average train loss: 91.9580\n",
      "====> Validation set loss: 95.0322\n",
      "====> Validation set kl: 25.9499\n",
      "Epoch: 441 [  100/50000 ( 0%)]  \tLoss:   92.443100\trec:   65.552849\tkl:   26.890251\n",
      "Epoch: 441 [10100/50000 (20%)]  \tLoss:   91.181030\trec:   65.480553\tkl:   25.700480\n",
      "Epoch: 441 [20100/50000 (40%)]  \tLoss:   94.182671\trec:   68.156441\tkl:   26.026224\n",
      "Epoch: 441 [30100/50000 (60%)]  \tLoss:   92.966522\trec:   67.054771\tkl:   25.911751\n",
      "Epoch: 441 [40100/50000 (80%)]  \tLoss:   92.302437\trec:   65.883331\tkl:   26.419115\n",
      "====> Epoch: 441 Average train loss: 91.9661\n",
      "====> Validation set loss: 94.8908\n",
      "====> Validation set kl: 25.9499\n",
      "Epoch: 442 [  100/50000 ( 0%)]  \tLoss:   88.043037\trec:   62.599804\tkl:   25.443232\n",
      "Epoch: 442 [10100/50000 (20%)]  \tLoss:   89.279190\trec:   63.575718\tkl:   25.703468\n",
      "Epoch: 442 [20100/50000 (40%)]  \tLoss:   90.896980\trec:   65.236847\tkl:   25.660131\n",
      "Epoch: 442 [30100/50000 (60%)]  \tLoss:   94.622078\trec:   68.009064\tkl:   26.613016\n",
      "Epoch: 442 [40100/50000 (80%)]  \tLoss:   89.812202\trec:   64.056824\tkl:   25.755381\n",
      "====> Epoch: 442 Average train loss: 91.9377\n",
      "====> Validation set loss: 95.0258\n",
      "====> Validation set kl: 25.9922\n",
      "Epoch: 443 [  100/50000 ( 0%)]  \tLoss:   89.121208\trec:   63.693119\tkl:   25.428085\n",
      "Epoch: 443 [10100/50000 (20%)]  \tLoss:   92.687088\trec:   66.905396\tkl:   25.781685\n",
      "Epoch: 443 [20100/50000 (40%)]  \tLoss:   95.522186\trec:   69.138199\tkl:   26.383984\n",
      "Epoch: 443 [30100/50000 (60%)]  \tLoss:   95.227943\trec:   68.584167\tkl:   26.643776\n",
      "Epoch: 443 [40100/50000 (80%)]  \tLoss:   92.776001\trec:   66.849060\tkl:   25.926945\n",
      "====> Epoch: 443 Average train loss: 91.9185\n",
      "====> Validation set loss: 94.9584\n",
      "====> Validation set kl: 25.7526\n",
      "Epoch: 444 [  100/50000 ( 0%)]  \tLoss:   93.724556\trec:   68.088036\tkl:   25.636520\n",
      "Epoch: 444 [10100/50000 (20%)]  \tLoss:   98.310623\trec:   71.850311\tkl:   26.460314\n",
      "Epoch: 444 [20100/50000 (40%)]  \tLoss:   90.950165\trec:   64.901505\tkl:   26.048664\n",
      "Epoch: 444 [30100/50000 (60%)]  \tLoss:   95.197868\trec:   67.871002\tkl:   27.326874\n",
      "Epoch: 444 [40100/50000 (80%)]  \tLoss:   90.584450\trec:   64.930641\tkl:   25.653809\n",
      "====> Epoch: 444 Average train loss: 91.9421\n",
      "====> Validation set loss: 94.9617\n",
      "====> Validation set kl: 25.9264\n",
      "Epoch: 445 [  100/50000 ( 0%)]  \tLoss:   88.562408\trec:   63.229969\tkl:   25.332438\n",
      "Epoch: 445 [10100/50000 (20%)]  \tLoss:   89.976707\trec:   64.340050\tkl:   25.636658\n",
      "Epoch: 445 [20100/50000 (40%)]  \tLoss:   92.389137\trec:   66.456833\tkl:   25.932308\n",
      "Epoch: 445 [30100/50000 (60%)]  \tLoss:   92.495468\trec:   67.020279\tkl:   25.475189\n",
      "Epoch: 445 [40100/50000 (80%)]  \tLoss:   92.389687\trec:   66.906303\tkl:   25.483383\n",
      "====> Epoch: 445 Average train loss: 91.9327\n",
      "====> Validation set loss: 94.8902\n",
      "====> Validation set kl: 25.9227\n",
      "Epoch: 446 [  100/50000 ( 0%)]  \tLoss:   92.305603\trec:   65.921539\tkl:   26.384069\n",
      "Epoch: 446 [10100/50000 (20%)]  \tLoss:   87.394470\trec:   62.334217\tkl:   25.060259\n",
      "Epoch: 446 [20100/50000 (40%)]  \tLoss:   89.883240\trec:   64.400253\tkl:   25.482983\n",
      "Epoch: 446 [30100/50000 (60%)]  \tLoss:   90.977089\trec:   65.388512\tkl:   25.588575\n",
      "Epoch: 446 [40100/50000 (80%)]  \tLoss:   92.269882\trec:   66.946709\tkl:   25.323168\n",
      "====> Epoch: 446 Average train loss: 91.9264\n",
      "====> Validation set loss: 94.9612\n",
      "====> Validation set kl: 25.9137\n",
      "Epoch: 447 [  100/50000 ( 0%)]  \tLoss:   91.459000\trec:   65.520027\tkl:   25.938976\n",
      "Epoch: 447 [10100/50000 (20%)]  \tLoss:   92.324089\trec:   66.176361\tkl:   26.147732\n",
      "Epoch: 447 [20100/50000 (40%)]  \tLoss:   98.024315\trec:   70.977646\tkl:   27.046669\n",
      "Epoch: 447 [30100/50000 (60%)]  \tLoss:   88.981972\trec:   63.038982\tkl:   25.942987\n",
      "Epoch: 447 [40100/50000 (80%)]  \tLoss:   88.691856\trec:   64.076004\tkl:   24.615845\n",
      "====> Epoch: 447 Average train loss: 91.9136\n",
      "====> Validation set loss: 94.9674\n",
      "====> Validation set kl: 26.0079\n",
      "Epoch: 448 [  100/50000 ( 0%)]  \tLoss:   94.713593\trec:   67.657295\tkl:   27.056301\n",
      "Epoch: 448 [10100/50000 (20%)]  \tLoss:   90.305435\trec:   64.276199\tkl:   26.029240\n",
      "Epoch: 448 [20100/50000 (40%)]  \tLoss:   94.050850\trec:   67.118675\tkl:   26.932169\n",
      "Epoch: 448 [30100/50000 (60%)]  \tLoss:   95.848160\trec:   69.340431\tkl:   26.507729\n",
      "Epoch: 448 [40100/50000 (80%)]  \tLoss:   89.264824\trec:   63.660473\tkl:   25.604353\n",
      "====> Epoch: 448 Average train loss: 91.9368\n",
      "====> Validation set loss: 95.0344\n",
      "====> Validation set kl: 26.0623\n",
      "Epoch: 449 [  100/50000 ( 0%)]  \tLoss:   93.028046\trec:   67.166153\tkl:   25.861898\n",
      "Epoch: 449 [10100/50000 (20%)]  \tLoss:   93.052460\trec:   67.630310\tkl:   25.422148\n",
      "Epoch: 449 [20100/50000 (40%)]  \tLoss:   89.761726\trec:   65.029884\tkl:   24.731848\n",
      "Epoch: 449 [30100/50000 (60%)]  \tLoss:   91.124176\trec:   65.172966\tkl:   25.951210\n",
      "Epoch: 449 [40100/50000 (80%)]  \tLoss:   91.319023\trec:   64.960594\tkl:   26.358433\n",
      "====> Epoch: 449 Average train loss: 91.8866\n",
      "====> Validation set loss: 94.9673\n",
      "====> Validation set kl: 25.6594\n",
      "Epoch: 450 [  100/50000 ( 0%)]  \tLoss:   95.343651\trec:   68.770798\tkl:   26.572851\n",
      "Epoch: 450 [10100/50000 (20%)]  \tLoss:   89.987984\trec:   64.043945\tkl:   25.944048\n",
      "Epoch: 450 [20100/50000 (40%)]  \tLoss:   95.916328\trec:   69.402534\tkl:   26.513786\n",
      "Epoch: 450 [30100/50000 (60%)]  \tLoss:   94.473946\trec:   67.825310\tkl:   26.648642\n",
      "Epoch: 450 [40100/50000 (80%)]  \tLoss:   87.862717\trec:   63.037937\tkl:   24.824776\n",
      "====> Epoch: 450 Average train loss: 91.9112\n",
      "====> Validation set loss: 94.8779\n",
      "====> Validation set kl: 25.9004\n",
      "Epoch: 451 [  100/50000 ( 0%)]  \tLoss:   92.660332\trec:   66.652946\tkl:   26.007385\n",
      "Epoch: 451 [10100/50000 (20%)]  \tLoss:   87.469666\trec:   61.867832\tkl:   25.601837\n",
      "Epoch: 451 [20100/50000 (40%)]  \tLoss:   93.646225\trec:   68.094147\tkl:   25.552076\n",
      "Epoch: 451 [30100/50000 (60%)]  \tLoss:   91.792587\trec:   64.771202\tkl:   27.021389\n",
      "Epoch: 451 [40100/50000 (80%)]  \tLoss:   91.259033\trec:   65.015228\tkl:   26.243801\n",
      "====> Epoch: 451 Average train loss: 91.9114\n",
      "====> Validation set loss: 94.9722\n",
      "====> Validation set kl: 25.9472\n",
      "Epoch: 452 [  100/50000 ( 0%)]  \tLoss:   88.884560\trec:   63.763062\tkl:   25.121498\n",
      "Epoch: 452 [10100/50000 (20%)]  \tLoss:   93.237404\trec:   67.355492\tkl:   25.881908\n",
      "Epoch: 452 [20100/50000 (40%)]  \tLoss:   90.559799\trec:   64.758232\tkl:   25.801577\n",
      "Epoch: 452 [30100/50000 (60%)]  \tLoss:   89.178253\trec:   63.980644\tkl:   25.197609\n",
      "Epoch: 452 [40100/50000 (80%)]  \tLoss:   94.966629\trec:   69.176094\tkl:   25.790537\n",
      "====> Epoch: 452 Average train loss: 91.9181\n",
      "====> Validation set loss: 95.0575\n",
      "====> Validation set kl: 26.1944\n",
      "Epoch: 453 [  100/50000 ( 0%)]  \tLoss:   93.278160\trec:   67.331268\tkl:   25.946896\n",
      "Epoch: 453 [10100/50000 (20%)]  \tLoss:   92.644432\trec:   65.264626\tkl:   27.379805\n",
      "Epoch: 453 [20100/50000 (40%)]  \tLoss:   91.777344\trec:   66.343178\tkl:   25.434168\n",
      "Epoch: 453 [30100/50000 (60%)]  \tLoss:   90.516495\trec:   64.141701\tkl:   26.374796\n",
      "Epoch: 453 [40100/50000 (80%)]  \tLoss:   95.872391\trec:   69.727844\tkl:   26.144543\n",
      "====> Epoch: 453 Average train loss: 91.8910\n",
      "====> Validation set loss: 95.0347\n",
      "====> Validation set kl: 25.8180\n",
      "Epoch: 454 [  100/50000 ( 0%)]  \tLoss:   96.967911\trec:   70.800316\tkl:   26.167587\n",
      "Epoch: 454 [10100/50000 (20%)]  \tLoss:   93.787338\trec:   66.859467\tkl:   26.927870\n",
      "Epoch: 454 [20100/50000 (40%)]  \tLoss:   90.913696\trec:   64.683052\tkl:   26.230644\n",
      "Epoch: 454 [30100/50000 (60%)]  \tLoss:   92.202644\trec:   66.323853\tkl:   25.878796\n",
      "Epoch: 454 [40100/50000 (80%)]  \tLoss:   89.908455\trec:   63.772820\tkl:   26.135630\n",
      "====> Epoch: 454 Average train loss: 91.8993\n",
      "====> Validation set loss: 94.9858\n",
      "====> Validation set kl: 25.8262\n",
      "Epoch: 455 [  100/50000 ( 0%)]  \tLoss:   94.149086\trec:   68.279160\tkl:   25.869934\n",
      "Epoch: 455 [10100/50000 (20%)]  \tLoss:   93.877266\trec:   67.669937\tkl:   26.207329\n",
      "Epoch: 455 [20100/50000 (40%)]  \tLoss:   90.965591\trec:   64.770348\tkl:   26.195248\n",
      "Epoch: 455 [30100/50000 (60%)]  \tLoss:   91.972084\trec:   66.294342\tkl:   25.677753\n",
      "Epoch: 455 [40100/50000 (80%)]  \tLoss:   89.438866\trec:   63.833210\tkl:   25.605652\n",
      "====> Epoch: 455 Average train loss: 91.8881\n",
      "====> Validation set loss: 94.9388\n",
      "====> Validation set kl: 25.9767\n",
      "Epoch: 456 [  100/50000 ( 0%)]  \tLoss:   91.188667\trec:   64.868164\tkl:   26.320513\n",
      "Epoch: 456 [10100/50000 (20%)]  \tLoss:   89.655533\trec:   63.994228\tkl:   25.661306\n",
      "Epoch: 456 [20100/50000 (40%)]  \tLoss:   89.062920\trec:   63.093662\tkl:   25.969255\n",
      "Epoch: 456 [30100/50000 (60%)]  \tLoss:   91.995323\trec:   65.940590\tkl:   26.054731\n",
      "Epoch: 456 [40100/50000 (80%)]  \tLoss:   90.261505\trec:   64.939262\tkl:   25.322241\n",
      "====> Epoch: 456 Average train loss: 91.8583\n",
      "====> Validation set loss: 94.9121\n",
      "====> Validation set kl: 25.8768\n",
      "Epoch: 457 [  100/50000 ( 0%)]  \tLoss:   91.846657\trec:   65.145744\tkl:   26.700918\n",
      "Epoch: 457 [10100/50000 (20%)]  \tLoss:   93.914780\trec:   68.112915\tkl:   25.801870\n",
      "Epoch: 457 [20100/50000 (40%)]  \tLoss:   92.608681\trec:   66.920464\tkl:   25.688210\n",
      "Epoch: 457 [30100/50000 (60%)]  \tLoss:   91.457413\trec:   65.440788\tkl:   26.016619\n",
      "Epoch: 457 [40100/50000 (80%)]  \tLoss:   94.168266\trec:   67.250931\tkl:   26.917337\n",
      "====> Epoch: 457 Average train loss: 91.8544\n",
      "====> Validation set loss: 94.8901\n",
      "====> Validation set kl: 25.9696\n",
      "Epoch: 458 [  100/50000 ( 0%)]  \tLoss:   95.373077\trec:   69.177460\tkl:   26.195612\n",
      "Epoch: 458 [10100/50000 (20%)]  \tLoss:   90.218849\trec:   64.543076\tkl:   25.675776\n",
      "Epoch: 458 [20100/50000 (40%)]  \tLoss:   95.711754\trec:   69.412987\tkl:   26.298775\n",
      "Epoch: 458 [30100/50000 (60%)]  \tLoss:   95.762810\trec:   69.041779\tkl:   26.721031\n",
      "Epoch: 458 [40100/50000 (80%)]  \tLoss:   88.579216\trec:   63.002087\tkl:   25.577124\n",
      "====> Epoch: 458 Average train loss: 91.8460\n",
      "====> Validation set loss: 94.9113\n",
      "====> Validation set kl: 25.9535\n",
      "Epoch: 459 [  100/50000 ( 0%)]  \tLoss:   92.080429\trec:   66.444557\tkl:   25.635874\n",
      "Epoch: 459 [10100/50000 (20%)]  \tLoss:   94.520477\trec:   67.827507\tkl:   26.692966\n",
      "Epoch: 459 [20100/50000 (40%)]  \tLoss:   93.133064\trec:   66.861824\tkl:   26.271240\n",
      "Epoch: 459 [30100/50000 (60%)]  \tLoss:   95.554001\trec:   68.832291\tkl:   26.721710\n",
      "Epoch: 459 [40100/50000 (80%)]  \tLoss:   90.505211\trec:   64.346848\tkl:   26.158365\n",
      "====> Epoch: 459 Average train loss: 91.8688\n",
      "====> Validation set loss: 94.9627\n",
      "====> Validation set kl: 25.8861\n",
      "Epoch: 460 [  100/50000 ( 0%)]  \tLoss:   90.112282\trec:   64.313560\tkl:   25.798716\n",
      "Epoch: 460 [10100/50000 (20%)]  \tLoss:   90.677681\trec:   64.917213\tkl:   25.760466\n",
      "Epoch: 460 [20100/50000 (40%)]  \tLoss:   89.976227\trec:   64.885712\tkl:   25.090523\n",
      "Epoch: 460 [30100/50000 (60%)]  \tLoss:   94.980019\trec:   68.428162\tkl:   26.551859\n",
      "Epoch: 460 [40100/50000 (80%)]  \tLoss:   93.561714\trec:   66.361732\tkl:   27.199982\n",
      "====> Epoch: 460 Average train loss: 91.8571\n",
      "====> Validation set loss: 94.9209\n",
      "====> Validation set kl: 25.9331\n",
      "Epoch: 461 [  100/50000 ( 0%)]  \tLoss:   91.873863\trec:   66.041710\tkl:   25.832155\n",
      "Epoch: 461 [10100/50000 (20%)]  \tLoss:   92.451027\trec:   65.869080\tkl:   26.581940\n",
      "Epoch: 461 [20100/50000 (40%)]  \tLoss:   95.016495\trec:   68.559944\tkl:   26.456545\n",
      "Epoch: 461 [30100/50000 (60%)]  \tLoss:   91.289238\trec:   65.065125\tkl:   26.224112\n",
      "Epoch: 461 [40100/50000 (80%)]  \tLoss:   89.456642\trec:   64.420433\tkl:   25.036207\n",
      "====> Epoch: 461 Average train loss: 91.8427\n",
      "====> Validation set loss: 94.9727\n",
      "====> Validation set kl: 26.1387\n",
      "Epoch: 462 [  100/50000 ( 0%)]  \tLoss:   91.023415\trec:   64.341545\tkl:   26.681877\n",
      "Epoch: 462 [10100/50000 (20%)]  \tLoss:   89.681305\trec:   63.855183\tkl:   25.826124\n",
      "Epoch: 462 [20100/50000 (40%)]  \tLoss:   87.445747\trec:   61.928669\tkl:   25.517082\n",
      "Epoch: 462 [30100/50000 (60%)]  \tLoss:   91.108101\trec:   65.552887\tkl:   25.555220\n",
      "Epoch: 462 [40100/50000 (80%)]  \tLoss:   92.064240\trec:   65.992378\tkl:   26.071856\n",
      "====> Epoch: 462 Average train loss: 91.8169\n",
      "====> Validation set loss: 94.8998\n",
      "====> Validation set kl: 25.9114\n",
      "Epoch: 463 [  100/50000 ( 0%)]  \tLoss:   90.312965\trec:   64.869560\tkl:   25.443403\n",
      "Epoch: 463 [10100/50000 (20%)]  \tLoss:   94.234848\trec:   67.952782\tkl:   26.282072\n",
      "Epoch: 463 [20100/50000 (40%)]  \tLoss:   93.587189\trec:   67.345329\tkl:   26.241858\n",
      "Epoch: 463 [30100/50000 (60%)]  \tLoss:   93.693443\trec:   66.904022\tkl:   26.789423\n",
      "Epoch: 463 [40100/50000 (80%)]  \tLoss:   97.569923\trec:   70.661560\tkl:   26.908363\n",
      "====> Epoch: 463 Average train loss: 91.8551\n",
      "====> Validation set loss: 94.8354\n",
      "====> Validation set kl: 26.0792\n",
      "Epoch: 464 [  100/50000 ( 0%)]  \tLoss:   94.590126\trec:   67.214767\tkl:   27.375362\n",
      "Epoch: 464 [10100/50000 (20%)]  \tLoss:   92.827118\trec:   66.686501\tkl:   26.140615\n",
      "Epoch: 464 [20100/50000 (40%)]  \tLoss:   88.279121\trec:   63.396767\tkl:   24.882349\n",
      "Epoch: 464 [30100/50000 (60%)]  \tLoss:   92.221786\trec:   66.634659\tkl:   25.587132\n",
      "Epoch: 464 [40100/50000 (80%)]  \tLoss:   95.039314\trec:   69.019821\tkl:   26.019487\n",
      "====> Epoch: 464 Average train loss: 91.8384\n",
      "====> Validation set loss: 94.7971\n",
      "====> Validation set kl: 25.8681\n",
      "Epoch: 465 [  100/50000 ( 0%)]  \tLoss:   91.027145\trec:   65.954926\tkl:   25.072224\n",
      "Epoch: 465 [10100/50000 (20%)]  \tLoss:   90.978180\trec:   64.399239\tkl:   26.578945\n",
      "Epoch: 465 [20100/50000 (40%)]  \tLoss:   89.965782\trec:   64.604988\tkl:   25.360790\n",
      "Epoch: 465 [30100/50000 (60%)]  \tLoss:   95.240021\trec:   68.687080\tkl:   26.552938\n",
      "Epoch: 465 [40100/50000 (80%)]  \tLoss:   92.113632\trec:   66.169693\tkl:   25.943930\n",
      "====> Epoch: 465 Average train loss: 91.8250\n",
      "====> Validation set loss: 94.9150\n",
      "====> Validation set kl: 26.0431\n",
      "Epoch: 466 [  100/50000 ( 0%)]  \tLoss:   84.970932\trec:   60.011745\tkl:   24.959187\n",
      "Epoch: 466 [10100/50000 (20%)]  \tLoss:   89.801369\trec:   63.738583\tkl:   26.062782\n",
      "Epoch: 466 [20100/50000 (40%)]  \tLoss:   88.943092\trec:   64.305183\tkl:   24.637913\n",
      "Epoch: 466 [30100/50000 (60%)]  \tLoss:   93.187111\trec:   67.012459\tkl:   26.174644\n",
      "Epoch: 466 [40100/50000 (80%)]  \tLoss:   95.064240\trec:   68.634689\tkl:   26.429548\n",
      "====> Epoch: 466 Average train loss: 91.8015\n",
      "====> Validation set loss: 94.9234\n",
      "====> Validation set kl: 26.1045\n",
      "Epoch: 467 [  100/50000 ( 0%)]  \tLoss:   94.627350\trec:   67.358604\tkl:   27.268751\n",
      "Epoch: 467 [10100/50000 (20%)]  \tLoss:   90.763191\trec:   65.819000\tkl:   24.944189\n",
      "Epoch: 467 [20100/50000 (40%)]  \tLoss:   90.541649\trec:   64.516945\tkl:   26.024704\n",
      "Epoch: 467 [30100/50000 (60%)]  \tLoss:   93.572594\trec:   67.836319\tkl:   25.736269\n",
      "Epoch: 467 [40100/50000 (80%)]  \tLoss:   93.854668\trec:   67.135033\tkl:   26.719633\n",
      "====> Epoch: 467 Average train loss: 91.8237\n",
      "====> Validation set loss: 94.8925\n",
      "====> Validation set kl: 25.8353\n",
      "Epoch: 468 [  100/50000 ( 0%)]  \tLoss:   93.970146\trec:   68.220245\tkl:   25.749905\n",
      "Epoch: 468 [10100/50000 (20%)]  \tLoss:   91.147812\trec:   65.552872\tkl:   25.594940\n",
      "Epoch: 468 [20100/50000 (40%)]  \tLoss:   88.819550\trec:   63.393967\tkl:   25.425577\n",
      "Epoch: 468 [30100/50000 (60%)]  \tLoss:   91.564423\trec:   66.528717\tkl:   25.035706\n",
      "Epoch: 468 [40100/50000 (80%)]  \tLoss:   90.639053\trec:   64.955620\tkl:   25.683430\n",
      "====> Epoch: 468 Average train loss: 91.7952\n",
      "====> Validation set loss: 94.9115\n",
      "====> Validation set kl: 26.0442\n",
      "Epoch: 469 [  100/50000 ( 0%)]  \tLoss:   89.314941\trec:   63.354939\tkl:   25.960001\n",
      "Epoch: 469 [10100/50000 (20%)]  \tLoss:   87.795311\trec:   62.063778\tkl:   25.731535\n",
      "Epoch: 469 [20100/50000 (40%)]  \tLoss:   92.142250\trec:   65.701775\tkl:   26.440477\n",
      "Epoch: 469 [30100/50000 (60%)]  \tLoss:   91.141876\trec:   65.216255\tkl:   25.925610\n",
      "Epoch: 469 [40100/50000 (80%)]  \tLoss:   93.623047\trec:   67.211594\tkl:   26.411459\n",
      "====> Epoch: 469 Average train loss: 91.7851\n",
      "====> Validation set loss: 94.9031\n",
      "====> Validation set kl: 25.8327\n",
      "Epoch: 470 [  100/50000 ( 0%)]  \tLoss:   88.668472\trec:   64.449516\tkl:   24.218950\n",
      "Epoch: 470 [10100/50000 (20%)]  \tLoss:   96.276405\trec:   69.796944\tkl:   26.479467\n",
      "Epoch: 470 [20100/50000 (40%)]  \tLoss:   92.836472\trec:   66.695969\tkl:   26.140507\n",
      "Epoch: 470 [30100/50000 (60%)]  \tLoss:   95.241081\trec:   68.130791\tkl:   27.110291\n",
      "Epoch: 470 [40100/50000 (80%)]  \tLoss:   94.451706\trec:   68.032135\tkl:   26.419567\n",
      "====> Epoch: 470 Average train loss: 91.7811\n",
      "====> Validation set loss: 94.9622\n",
      "====> Validation set kl: 26.0104\n",
      "Epoch: 471 [  100/50000 ( 0%)]  \tLoss:   91.134109\trec:   65.486313\tkl:   25.647800\n",
      "Epoch: 471 [10100/50000 (20%)]  \tLoss:   93.661743\trec:   67.119133\tkl:   26.542614\n",
      "Epoch: 471 [20100/50000 (40%)]  \tLoss:   93.701317\trec:   67.560463\tkl:   26.140852\n",
      "Epoch: 471 [30100/50000 (60%)]  \tLoss:   89.702248\trec:   64.386559\tkl:   25.315687\n",
      "Epoch: 471 [40100/50000 (80%)]  \tLoss:   91.175819\trec:   65.637482\tkl:   25.538340\n",
      "====> Epoch: 471 Average train loss: 91.7906\n",
      "====> Validation set loss: 94.9181\n",
      "====> Validation set kl: 26.0131\n",
      "Epoch: 472 [  100/50000 ( 0%)]  \tLoss:   90.918816\trec:   65.211372\tkl:   25.707438\n",
      "Epoch: 472 [10100/50000 (20%)]  \tLoss:   92.976929\trec:   66.429314\tkl:   26.547619\n",
      "Epoch: 472 [20100/50000 (40%)]  \tLoss:   93.192245\trec:   67.349602\tkl:   25.842646\n",
      "Epoch: 472 [30100/50000 (60%)]  \tLoss:   91.505424\trec:   65.642479\tkl:   25.862946\n",
      "Epoch: 472 [40100/50000 (80%)]  \tLoss:   88.599503\trec:   62.734062\tkl:   25.865438\n",
      "====> Epoch: 472 Average train loss: 91.7762\n",
      "====> Validation set loss: 94.9060\n",
      "====> Validation set kl: 26.0257\n",
      "Epoch: 473 [  100/50000 ( 0%)]  \tLoss:   88.546577\trec:   63.241516\tkl:   25.305061\n",
      "Epoch: 473 [10100/50000 (20%)]  \tLoss:   92.719902\trec:   66.008575\tkl:   26.711327\n",
      "Epoch: 473 [20100/50000 (40%)]  \tLoss:   92.620903\trec:   67.771080\tkl:   24.849827\n",
      "Epoch: 473 [30100/50000 (60%)]  \tLoss:   91.373940\trec:   64.996239\tkl:   26.377699\n",
      "Epoch: 473 [40100/50000 (80%)]  \tLoss:   94.208862\trec:   68.176750\tkl:   26.032118\n",
      "====> Epoch: 473 Average train loss: 91.7737\n",
      "====> Validation set loss: 94.9978\n",
      "====> Validation set kl: 26.0508\n",
      "Epoch: 474 [  100/50000 ( 0%)]  \tLoss:   90.083214\trec:   64.101349\tkl:   25.981857\n",
      "Epoch: 474 [10100/50000 (20%)]  \tLoss:   92.152481\trec:   66.108147\tkl:   26.044334\n",
      "Epoch: 474 [20100/50000 (40%)]  \tLoss:   94.442223\trec:   67.039917\tkl:   27.402300\n",
      "Epoch: 474 [30100/50000 (60%)]  \tLoss:   92.988571\trec:   67.014069\tkl:   25.974501\n",
      "Epoch: 474 [40100/50000 (80%)]  \tLoss:   94.728897\trec:   68.723824\tkl:   26.005070\n",
      "====> Epoch: 474 Average train loss: 91.7762\n",
      "====> Validation set loss: 94.9784\n",
      "====> Validation set kl: 26.0058\n",
      "Epoch: 475 [  100/50000 ( 0%)]  \tLoss:   91.373756\trec:   64.876640\tkl:   26.497122\n",
      "Epoch: 475 [10100/50000 (20%)]  \tLoss:   94.705986\trec:   67.785820\tkl:   26.920166\n",
      "Epoch: 475 [20100/50000 (40%)]  \tLoss:   86.516441\trec:   61.594906\tkl:   24.921539\n",
      "Epoch: 475 [30100/50000 (60%)]  \tLoss:   93.161652\trec:   66.463966\tkl:   26.697678\n",
      "Epoch: 475 [40100/50000 (80%)]  \tLoss:   92.698601\trec:   67.024391\tkl:   25.674212\n",
      "====> Epoch: 475 Average train loss: 91.7704\n",
      "====> Validation set loss: 94.8451\n",
      "====> Validation set kl: 26.1085\n",
      "Epoch: 476 [  100/50000 ( 0%)]  \tLoss:   90.541313\trec:   64.606049\tkl:   25.935270\n",
      "Epoch: 476 [10100/50000 (20%)]  \tLoss:   90.681259\trec:   64.678688\tkl:   26.002565\n",
      "Epoch: 476 [20100/50000 (40%)]  \tLoss:   89.378822\trec:   64.264717\tkl:   25.114113\n",
      "Epoch: 476 [30100/50000 (60%)]  \tLoss:   92.225235\trec:   66.073502\tkl:   26.151724\n",
      "Epoch: 476 [40100/50000 (80%)]  \tLoss:   89.039062\trec:   64.025887\tkl:   25.013170\n",
      "====> Epoch: 476 Average train loss: 91.7372\n",
      "====> Validation set loss: 94.9427\n",
      "====> Validation set kl: 25.9912\n",
      "Epoch: 477 [  100/50000 ( 0%)]  \tLoss:   89.007500\trec:   63.447937\tkl:   25.559561\n",
      "Epoch: 477 [10100/50000 (20%)]  \tLoss:   92.931023\trec:   67.708977\tkl:   25.222050\n",
      "Epoch: 477 [20100/50000 (40%)]  \tLoss:   90.567673\trec:   64.152443\tkl:   26.415230\n",
      "Epoch: 477 [30100/50000 (60%)]  \tLoss:   93.658112\trec:   66.858658\tkl:   26.799452\n",
      "Epoch: 477 [40100/50000 (80%)]  \tLoss:   87.705582\trec:   62.052498\tkl:   25.653080\n",
      "====> Epoch: 477 Average train loss: 91.7527\n",
      "====> Validation set loss: 94.9665\n",
      "====> Validation set kl: 26.1382\n",
      "Epoch: 478 [  100/50000 ( 0%)]  \tLoss:   93.673622\trec:   67.463379\tkl:   26.210243\n",
      "Epoch: 478 [10100/50000 (20%)]  \tLoss:   93.346191\trec:   67.253693\tkl:   26.092499\n",
      "Epoch: 478 [20100/50000 (40%)]  \tLoss:   89.290215\trec:   62.792988\tkl:   26.497221\n",
      "Epoch: 478 [30100/50000 (60%)]  \tLoss:   95.221169\trec:   69.056656\tkl:   26.164511\n",
      "Epoch: 478 [40100/50000 (80%)]  \tLoss:   92.955841\trec:   66.900688\tkl:   26.055145\n",
      "====> Epoch: 478 Average train loss: 91.7563\n",
      "====> Validation set loss: 94.8244\n",
      "====> Validation set kl: 26.0170\n",
      "Epoch: 479 [  100/50000 ( 0%)]  \tLoss:   87.979980\trec:   62.971268\tkl:   25.008711\n",
      "Epoch: 479 [10100/50000 (20%)]  \tLoss:   91.639320\trec:   65.556236\tkl:   26.083088\n",
      "Epoch: 479 [20100/50000 (40%)]  \tLoss:   90.193024\trec:   64.571671\tkl:   25.621351\n",
      "Epoch: 479 [30100/50000 (60%)]  \tLoss:   94.189804\trec:   67.797783\tkl:   26.392023\n",
      "Epoch: 479 [40100/50000 (80%)]  \tLoss:   93.270020\trec:   67.368706\tkl:   25.901306\n",
      "====> Epoch: 479 Average train loss: 91.7623\n",
      "====> Validation set loss: 94.8322\n",
      "====> Validation set kl: 25.9474\n",
      "Epoch: 480 [  100/50000 ( 0%)]  \tLoss:   92.181725\trec:   66.270454\tkl:   25.911266\n",
      "Epoch: 480 [10100/50000 (20%)]  \tLoss:   92.954102\trec:   67.542213\tkl:   25.411890\n",
      "Epoch: 480 [20100/50000 (40%)]  \tLoss:   92.205704\trec:   66.259216\tkl:   25.946489\n",
      "Epoch: 480 [30100/50000 (60%)]  \tLoss:   91.234489\trec:   65.494797\tkl:   25.739693\n",
      "Epoch: 480 [40100/50000 (80%)]  \tLoss:   86.093208\trec:   60.882977\tkl:   25.210234\n",
      "====> Epoch: 480 Average train loss: 91.7331\n",
      "====> Validation set loss: 94.9022\n",
      "====> Validation set kl: 25.8857\n",
      "Epoch: 481 [  100/50000 ( 0%)]  \tLoss:   92.628082\trec:   65.990059\tkl:   26.638021\n",
      "Epoch: 481 [10100/50000 (20%)]  \tLoss:   93.267448\trec:   66.634781\tkl:   26.632664\n",
      "Epoch: 481 [20100/50000 (40%)]  \tLoss:   94.120712\trec:   68.329803\tkl:   25.790911\n",
      "Epoch: 481 [30100/50000 (60%)]  \tLoss:   93.735817\trec:   67.372375\tkl:   26.363443\n",
      "Epoch: 481 [40100/50000 (80%)]  \tLoss:   89.522705\trec:   64.325249\tkl:   25.197451\n",
      "====> Epoch: 481 Average train loss: 91.7311\n",
      "====> Validation set loss: 95.0078\n",
      "====> Validation set kl: 26.0280\n",
      "Epoch: 482 [  100/50000 ( 0%)]  \tLoss:   90.567047\trec:   63.759392\tkl:   26.807661\n",
      "Epoch: 482 [10100/50000 (20%)]  \tLoss:   90.231209\trec:   64.794090\tkl:   25.437119\n",
      "Epoch: 482 [20100/50000 (40%)]  \tLoss:   93.593140\trec:   67.541924\tkl:   26.051216\n",
      "Epoch: 482 [30100/50000 (60%)]  \tLoss:   90.131607\trec:   64.555855\tkl:   25.575752\n",
      "Epoch: 482 [40100/50000 (80%)]  \tLoss:   95.917938\trec:   69.640785\tkl:   26.277155\n",
      "====> Epoch: 482 Average train loss: 91.7489\n",
      "====> Validation set loss: 94.8478\n",
      "====> Validation set kl: 25.8822\n",
      "Epoch: 483 [  100/50000 ( 0%)]  \tLoss:   87.185448\trec:   61.588268\tkl:   25.597178\n",
      "Epoch: 483 [10100/50000 (20%)]  \tLoss:   94.147812\trec:   68.194389\tkl:   25.953423\n",
      "Epoch: 483 [20100/50000 (40%)]  \tLoss:   96.218658\trec:   69.381554\tkl:   26.837107\n",
      "Epoch: 483 [30100/50000 (60%)]  \tLoss:   94.585342\trec:   67.103897\tkl:   27.481441\n",
      "Epoch: 483 [40100/50000 (80%)]  \tLoss:   92.148613\trec:   66.914993\tkl:   25.233618\n",
      "====> Epoch: 483 Average train loss: 91.7250\n",
      "====> Validation set loss: 94.9135\n",
      "====> Validation set kl: 25.7975\n",
      "Epoch: 484 [  100/50000 ( 0%)]  \tLoss:   91.789780\trec:   66.235001\tkl:   25.554785\n",
      "Epoch: 484 [10100/50000 (20%)]  \tLoss:   95.806648\trec:   69.794205\tkl:   26.012438\n",
      "Epoch: 484 [20100/50000 (40%)]  \tLoss:   93.363922\trec:   67.290161\tkl:   26.073761\n",
      "Epoch: 484 [30100/50000 (60%)]  \tLoss:   92.158379\trec:   65.701340\tkl:   26.457041\n",
      "Epoch: 484 [40100/50000 (80%)]  \tLoss:   92.878227\trec:   67.024277\tkl:   25.853954\n",
      "====> Epoch: 484 Average train loss: 91.7224\n",
      "====> Validation set loss: 94.8123\n",
      "====> Validation set kl: 25.9391\n",
      "Epoch: 485 [  100/50000 ( 0%)]  \tLoss:   91.518341\trec:   65.145645\tkl:   26.372692\n",
      "Epoch: 485 [10100/50000 (20%)]  \tLoss:   91.883064\trec:   65.554688\tkl:   26.328379\n",
      "Epoch: 485 [20100/50000 (40%)]  \tLoss:   88.869019\trec:   64.016479\tkl:   24.852543\n",
      "Epoch: 485 [30100/50000 (60%)]  \tLoss:   91.750366\trec:   66.463638\tkl:   25.286734\n",
      "Epoch: 485 [40100/50000 (80%)]  \tLoss:   93.515976\trec:   67.606270\tkl:   25.909712\n",
      "====> Epoch: 485 Average train loss: 91.7080\n",
      "====> Validation set loss: 94.8157\n",
      "====> Validation set kl: 26.1191\n",
      "Epoch: 486 [  100/50000 ( 0%)]  \tLoss:   92.758484\trec:   65.579399\tkl:   27.179085\n",
      "Epoch: 486 [10100/50000 (20%)]  \tLoss:   92.124123\trec:   65.901169\tkl:   26.222952\n",
      "Epoch: 486 [20100/50000 (40%)]  \tLoss:   91.522049\trec:   65.688095\tkl:   25.833950\n",
      "Epoch: 486 [30100/50000 (60%)]  \tLoss:   92.890045\trec:   66.045906\tkl:   26.844139\n",
      "Epoch: 486 [40100/50000 (80%)]  \tLoss:   92.431961\trec:   66.642502\tkl:   25.789465\n",
      "====> Epoch: 486 Average train loss: 91.7132\n",
      "====> Validation set loss: 94.8252\n",
      "====> Validation set kl: 26.0610\n",
      "Epoch: 487 [  100/50000 ( 0%)]  \tLoss:   97.718117\trec:   70.655464\tkl:   27.062643\n",
      "Epoch: 487 [10100/50000 (20%)]  \tLoss:   91.855789\trec:   65.616890\tkl:   26.238899\n",
      "Epoch: 487 [20100/50000 (40%)]  \tLoss:   90.485718\trec:   65.057983\tkl:   25.427731\n",
      "Epoch: 487 [30100/50000 (60%)]  \tLoss:   92.194237\trec:   66.051628\tkl:   26.142612\n",
      "Epoch: 487 [40100/50000 (80%)]  \tLoss:   91.752861\trec:   65.098602\tkl:   26.654259\n",
      "====> Epoch: 487 Average train loss: 91.7363\n",
      "====> Validation set loss: 94.7907\n",
      "====> Validation set kl: 26.1406\n",
      "Epoch: 488 [  100/50000 ( 0%)]  \tLoss:   94.867218\trec:   67.820839\tkl:   27.046373\n",
      "Epoch: 488 [10100/50000 (20%)]  \tLoss:   88.456421\trec:   63.831257\tkl:   24.625162\n",
      "Epoch: 488 [20100/50000 (40%)]  \tLoss:   95.956528\trec:   68.637794\tkl:   27.318733\n",
      "Epoch: 488 [30100/50000 (60%)]  \tLoss:   86.855721\trec:   61.016640\tkl:   25.839081\n",
      "Epoch: 488 [40100/50000 (80%)]  \tLoss:   94.313126\trec:   67.957314\tkl:   26.355814\n",
      "====> Epoch: 488 Average train loss: 91.6918\n",
      "====> Validation set loss: 94.8765\n",
      "====> Validation set kl: 25.8875\n",
      "Epoch: 489 [  100/50000 ( 0%)]  \tLoss:   94.005493\trec:   67.773331\tkl:   26.232168\n",
      "Epoch: 489 [10100/50000 (20%)]  \tLoss:   90.822105\trec:   65.486809\tkl:   25.335295\n",
      "Epoch: 489 [20100/50000 (40%)]  \tLoss:   92.518532\trec:   66.307785\tkl:   26.210754\n",
      "Epoch: 489 [30100/50000 (60%)]  \tLoss:   91.037537\trec:   65.025368\tkl:   26.012167\n",
      "Epoch: 489 [40100/50000 (80%)]  \tLoss:   92.798653\trec:   66.715324\tkl:   26.083332\n",
      "====> Epoch: 489 Average train loss: 91.6910\n",
      "====> Validation set loss: 94.7431\n",
      "====> Validation set kl: 25.8557\n",
      "Epoch: 490 [  100/50000 ( 0%)]  \tLoss:   93.005196\trec:   66.938408\tkl:   26.066790\n",
      "Epoch: 490 [10100/50000 (20%)]  \tLoss:   93.573883\trec:   67.421089\tkl:   26.152798\n",
      "Epoch: 490 [20100/50000 (40%)]  \tLoss:   92.263535\trec:   64.975029\tkl:   27.288500\n",
      "Epoch: 490 [30100/50000 (60%)]  \tLoss:   92.946297\trec:   66.360611\tkl:   26.585686\n",
      "Epoch: 490 [40100/50000 (80%)]  \tLoss:   89.329155\trec:   64.148849\tkl:   25.180307\n",
      "====> Epoch: 490 Average train loss: 91.7139\n",
      "====> Validation set loss: 94.8269\n",
      "====> Validation set kl: 26.0895\n",
      "Epoch: 491 [  100/50000 ( 0%)]  \tLoss:   91.672768\trec:   65.088150\tkl:   26.584621\n",
      "Epoch: 491 [10100/50000 (20%)]  \tLoss:   91.790703\trec:   65.983879\tkl:   25.806831\n",
      "Epoch: 491 [20100/50000 (40%)]  \tLoss:   94.168945\trec:   68.663788\tkl:   25.505156\n",
      "Epoch: 491 [30100/50000 (60%)]  \tLoss:   90.150215\trec:   64.583527\tkl:   25.566679\n",
      "Epoch: 491 [40100/50000 (80%)]  \tLoss:   89.289291\trec:   63.297947\tkl:   25.991346\n",
      "====> Epoch: 491 Average train loss: 91.6966\n",
      "====> Validation set loss: 94.7777\n",
      "====> Validation set kl: 25.7764\n",
      "Epoch: 492 [  100/50000 ( 0%)]  \tLoss:   92.071014\trec:   65.843208\tkl:   26.227810\n",
      "Epoch: 492 [10100/50000 (20%)]  \tLoss:   86.421707\trec:   61.090164\tkl:   25.331545\n",
      "Epoch: 492 [20100/50000 (40%)]  \tLoss:   92.866318\trec:   66.951851\tkl:   25.914469\n",
      "Epoch: 492 [30100/50000 (60%)]  \tLoss:   94.489685\trec:   67.283272\tkl:   27.206417\n",
      "Epoch: 492 [40100/50000 (80%)]  \tLoss:   93.342926\trec:   67.746117\tkl:   25.596815\n",
      "====> Epoch: 492 Average train loss: 91.6961\n",
      "====> Validation set loss: 94.6518\n",
      "====> Validation set kl: 26.1796\n",
      "Epoch: 493 [  100/50000 ( 0%)]  \tLoss:   90.885475\trec:   63.871986\tkl:   27.013489\n",
      "Epoch: 493 [10100/50000 (20%)]  \tLoss:   91.188408\trec:   65.473465\tkl:   25.714939\n",
      "Epoch: 493 [20100/50000 (40%)]  \tLoss:   92.502586\trec:   66.457657\tkl:   26.044930\n",
      "Epoch: 493 [30100/50000 (60%)]  \tLoss:   89.122780\trec:   63.851894\tkl:   25.270885\n",
      "Epoch: 493 [40100/50000 (80%)]  \tLoss:   95.683754\trec:   69.003510\tkl:   26.680248\n",
      "====> Epoch: 493 Average train loss: 91.6351\n",
      "====> Validation set loss: 94.6810\n",
      "====> Validation set kl: 26.1510\n",
      "Epoch: 494 [  100/50000 ( 0%)]  \tLoss:   88.357422\trec:   62.024441\tkl:   26.332983\n",
      "Epoch: 494 [10100/50000 (20%)]  \tLoss:   91.507797\trec:   66.078629\tkl:   25.429169\n",
      "Epoch: 494 [20100/50000 (40%)]  \tLoss:   95.606689\trec:   69.220772\tkl:   26.385912\n",
      "Epoch: 494 [30100/50000 (60%)]  \tLoss:   89.398857\trec:   64.008209\tkl:   25.390652\n",
      "Epoch: 494 [40100/50000 (80%)]  \tLoss:   87.155014\trec:   62.263870\tkl:   24.891142\n",
      "====> Epoch: 494 Average train loss: 91.7011\n",
      "====> Validation set loss: 94.8311\n",
      "====> Validation set kl: 25.7594\n",
      "Epoch: 495 [  100/50000 ( 0%)]  \tLoss:   92.862236\trec:   66.695663\tkl:   26.166569\n",
      "Epoch: 495 [10100/50000 (20%)]  \tLoss:   90.865074\trec:   64.646591\tkl:   26.218491\n",
      "Epoch: 495 [20100/50000 (40%)]  \tLoss:   86.714684\trec:   61.862888\tkl:   24.851791\n",
      "Epoch: 495 [30100/50000 (60%)]  \tLoss:   91.326462\trec:   66.476929\tkl:   24.849533\n",
      "Epoch: 495 [40100/50000 (80%)]  \tLoss:   91.765701\trec:   66.052696\tkl:   25.713003\n",
      "====> Epoch: 495 Average train loss: 91.6568\n",
      "====> Validation set loss: 94.8109\n",
      "====> Validation set kl: 25.9495\n",
      "Epoch: 496 [  100/50000 ( 0%)]  \tLoss:   92.010963\trec:   65.555313\tkl:   26.455656\n",
      "Epoch: 496 [10100/50000 (20%)]  \tLoss:   93.725342\trec:   67.321983\tkl:   26.403357\n",
      "Epoch: 496 [20100/50000 (40%)]  \tLoss:   92.412994\trec:   66.534729\tkl:   25.878263\n",
      "Epoch: 496 [30100/50000 (60%)]  \tLoss:   90.434746\trec:   64.548790\tkl:   25.885962\n",
      "Epoch: 496 [40100/50000 (80%)]  \tLoss:   94.849747\trec:   67.756035\tkl:   27.093710\n",
      "====> Epoch: 496 Average train loss: 91.6481\n",
      "====> Validation set loss: 94.8896\n",
      "====> Validation set kl: 25.9518\n",
      "Epoch: 497 [  100/50000 ( 0%)]  \tLoss:   83.008713\trec:   59.401386\tkl:   23.607328\n",
      "Epoch: 497 [10100/50000 (20%)]  \tLoss:   91.252411\trec:   65.573853\tkl:   25.678556\n",
      "Epoch: 497 [20100/50000 (40%)]  \tLoss:   95.269386\trec:   68.165703\tkl:   27.103683\n",
      "Epoch: 497 [30100/50000 (60%)]  \tLoss:   92.785370\trec:   66.877739\tkl:   25.907629\n",
      "Epoch: 497 [40100/50000 (80%)]  \tLoss:   92.179184\trec:   65.746880\tkl:   26.432302\n",
      "====> Epoch: 497 Average train loss: 91.6533\n",
      "====> Validation set loss: 94.9404\n",
      "====> Validation set kl: 25.9919\n",
      "Epoch: 498 [  100/50000 ( 0%)]  \tLoss:   90.883057\trec:   64.582306\tkl:   26.300755\n",
      "Epoch: 498 [10100/50000 (20%)]  \tLoss:   92.570076\trec:   66.051964\tkl:   26.518120\n",
      "Epoch: 498 [20100/50000 (40%)]  \tLoss:   93.460114\trec:   67.136536\tkl:   26.323574\n",
      "Epoch: 498 [30100/50000 (60%)]  \tLoss:   93.575485\trec:   66.569679\tkl:   27.005816\n",
      "Epoch: 498 [40100/50000 (80%)]  \tLoss:   93.602455\trec:   67.264900\tkl:   26.337563\n",
      "====> Epoch: 498 Average train loss: 91.6568\n",
      "====> Validation set loss: 94.7804\n",
      "====> Validation set kl: 26.0347\n",
      "Epoch: 499 [  100/50000 ( 0%)]  \tLoss:   93.303604\trec:   67.370766\tkl:   25.932837\n",
      "Epoch: 499 [10100/50000 (20%)]  \tLoss:   92.291649\trec:   64.916473\tkl:   27.375177\n",
      "Epoch: 499 [20100/50000 (40%)]  \tLoss:   94.094841\trec:   68.575005\tkl:   25.519836\n",
      "Epoch: 499 [30100/50000 (60%)]  \tLoss:   86.518486\trec:   60.614502\tkl:   25.903984\n",
      "Epoch: 499 [40100/50000 (80%)]  \tLoss:   93.677147\trec:   65.965462\tkl:   27.711679\n",
      "====> Epoch: 499 Average train loss: 91.6445\n",
      "====> Validation set loss: 94.7748\n",
      "====> Validation set kl: 26.0104\n",
      "Epoch: 500 [  100/50000 ( 0%)]  \tLoss:   94.822075\trec:   68.307892\tkl:   26.514187\n",
      "Epoch: 500 [10100/50000 (20%)]  \tLoss:   90.048416\trec:   64.389793\tkl:   25.658617\n",
      "Epoch: 500 [20100/50000 (40%)]  \tLoss:   94.125328\trec:   67.317307\tkl:   26.808023\n",
      "Epoch: 500 [30100/50000 (60%)]  \tLoss:   90.275642\trec:   64.333748\tkl:   25.941896\n",
      "Epoch: 500 [40100/50000 (80%)]  \tLoss:   88.810211\trec:   63.981922\tkl:   24.828291\n",
      "====> Epoch: 500 Average train loss: 91.6548\n",
      "====> Validation set loss: 94.8036\n",
      "====> Validation set kl: 25.8811\n",
      "Epoch: 501 [  100/50000 ( 0%)]  \tLoss:   93.111679\trec:   67.077782\tkl:   26.033901\n",
      "Epoch: 501 [10100/50000 (20%)]  \tLoss:   90.362289\trec:   64.955437\tkl:   25.406853\n",
      "Epoch: 501 [20100/50000 (40%)]  \tLoss:   90.453903\trec:   63.884422\tkl:   26.569487\n",
      "Epoch: 501 [30100/50000 (60%)]  \tLoss:   92.643883\trec:   66.168839\tkl:   26.475039\n",
      "Epoch: 501 [40100/50000 (80%)]  \tLoss:   91.344742\trec:   66.252586\tkl:   25.092163\n",
      "====> Epoch: 501 Average train loss: 91.6536\n",
      "====> Validation set loss: 94.8579\n",
      "====> Validation set kl: 26.0114\n",
      "Epoch: 502 [  100/50000 ( 0%)]  \tLoss:   95.727798\trec:   68.630203\tkl:   27.097595\n",
      "Epoch: 502 [10100/50000 (20%)]  \tLoss:   91.876686\trec:   65.640404\tkl:   26.236280\n",
      "Epoch: 502 [20100/50000 (40%)]  \tLoss:   93.905807\trec:   67.619347\tkl:   26.286465\n",
      "Epoch: 502 [30100/50000 (60%)]  \tLoss:   87.752068\trec:   62.234200\tkl:   25.517870\n",
      "Epoch: 502 [40100/50000 (80%)]  \tLoss:   92.689728\trec:   66.319092\tkl:   26.370636\n",
      "====> Epoch: 502 Average train loss: 91.6577\n",
      "====> Validation set loss: 94.9097\n",
      "====> Validation set kl: 26.1349\n",
      "Epoch: 503 [  100/50000 ( 0%)]  \tLoss:   83.615173\trec:   58.814667\tkl:   24.800510\n",
      "Epoch: 503 [10100/50000 (20%)]  \tLoss:   91.473022\trec:   66.008644\tkl:   25.464384\n",
      "Epoch: 503 [20100/50000 (40%)]  \tLoss:   86.217323\trec:   61.031696\tkl:   25.185621\n",
      "Epoch: 503 [30100/50000 (60%)]  \tLoss:   90.900574\trec:   65.069420\tkl:   25.831150\n",
      "Epoch: 503 [40100/50000 (80%)]  \tLoss:   90.783775\trec:   65.586189\tkl:   25.197588\n",
      "====> Epoch: 503 Average train loss: 91.6373\n",
      "====> Validation set loss: 94.7765\n",
      "====> Validation set kl: 25.9072\n",
      "Epoch: 504 [  100/50000 ( 0%)]  \tLoss:   90.646072\trec:   64.794846\tkl:   25.851227\n",
      "Epoch: 504 [10100/50000 (20%)]  \tLoss:   89.649239\trec:   63.776237\tkl:   25.872997\n",
      "Epoch: 504 [20100/50000 (40%)]  \tLoss:   91.438904\trec:   66.125015\tkl:   25.313890\n",
      "Epoch: 504 [30100/50000 (60%)]  \tLoss:   92.627495\trec:   66.651764\tkl:   25.975731\n",
      "Epoch: 504 [40100/50000 (80%)]  \tLoss:   90.091385\trec:   63.975586\tkl:   26.115795\n",
      "====> Epoch: 504 Average train loss: 91.6175\n",
      "====> Validation set loss: 94.8175\n",
      "====> Validation set kl: 26.0605\n",
      "Epoch: 505 [  100/50000 ( 0%)]  \tLoss:   93.074860\trec:   67.110001\tkl:   25.964861\n",
      "Epoch: 505 [10100/50000 (20%)]  \tLoss:   96.953209\trec:   69.999077\tkl:   26.954132\n",
      "Epoch: 505 [20100/50000 (40%)]  \tLoss:   93.512985\trec:   66.990921\tkl:   26.522064\n",
      "Epoch: 505 [30100/50000 (60%)]  \tLoss:   91.043709\trec:   65.441101\tkl:   25.602602\n",
      "Epoch: 505 [40100/50000 (80%)]  \tLoss:   93.925331\trec:   68.870049\tkl:   25.055277\n",
      "====> Epoch: 505 Average train loss: 91.6083\n",
      "====> Validation set loss: 94.8722\n",
      "====> Validation set kl: 26.1455\n",
      "Epoch: 506 [  100/50000 ( 0%)]  \tLoss:   89.477036\trec:   62.964470\tkl:   26.512569\n",
      "Epoch: 506 [10100/50000 (20%)]  \tLoss:   94.641464\trec:   68.212517\tkl:   26.428940\n",
      "Epoch: 506 [20100/50000 (40%)]  \tLoss:   91.726288\trec:   64.790520\tkl:   26.935768\n",
      "Epoch: 506 [30100/50000 (60%)]  \tLoss:   90.857582\trec:   65.186806\tkl:   25.670784\n",
      "Epoch: 506 [40100/50000 (80%)]  \tLoss:   92.378708\trec:   66.432236\tkl:   25.946468\n",
      "====> Epoch: 506 Average train loss: 91.5932\n",
      "====> Validation set loss: 94.8358\n",
      "====> Validation set kl: 26.0472\n",
      "Epoch: 507 [  100/50000 ( 0%)]  \tLoss:   93.293243\trec:   66.559029\tkl:   26.734211\n",
      "Epoch: 507 [10100/50000 (20%)]  \tLoss:   90.163925\trec:   64.693253\tkl:   25.470671\n",
      "Epoch: 507 [20100/50000 (40%)]  \tLoss:   89.984428\trec:   63.905922\tkl:   26.078505\n",
      "Epoch: 507 [30100/50000 (60%)]  \tLoss:   93.084755\trec:   66.619232\tkl:   26.465519\n",
      "Epoch: 507 [40100/50000 (80%)]  \tLoss:   87.472580\trec:   62.111607\tkl:   25.360973\n",
      "====> Epoch: 507 Average train loss: 91.6031\n",
      "====> Validation set loss: 94.8077\n",
      "====> Validation set kl: 25.7699\n",
      "Epoch: 508 [  100/50000 ( 0%)]  \tLoss:   92.699120\trec:   67.187927\tkl:   25.511196\n",
      "Epoch: 508 [10100/50000 (20%)]  \tLoss:   92.009186\trec:   65.607544\tkl:   26.401642\n",
      "Epoch: 508 [20100/50000 (40%)]  \tLoss:   92.337730\trec:   66.563087\tkl:   25.774652\n",
      "Epoch: 508 [30100/50000 (60%)]  \tLoss:   92.085594\trec:   65.773209\tkl:   26.312382\n",
      "Epoch: 508 [40100/50000 (80%)]  \tLoss:   91.351929\trec:   66.203949\tkl:   25.147984\n",
      "====> Epoch: 508 Average train loss: 91.5874\n",
      "====> Validation set loss: 94.7401\n",
      "====> Validation set kl: 26.0441\n",
      "Epoch: 509 [  100/50000 ( 0%)]  \tLoss:   91.095757\trec:   65.148064\tkl:   25.947697\n",
      "Epoch: 509 [10100/50000 (20%)]  \tLoss:   89.930298\trec:   64.658691\tkl:   25.271614\n",
      "Epoch: 509 [20100/50000 (40%)]  \tLoss:   90.576851\trec:   65.208916\tkl:   25.367945\n",
      "Epoch: 509 [30100/50000 (60%)]  \tLoss:   94.809341\trec:   67.569107\tkl:   27.240242\n",
      "Epoch: 509 [40100/50000 (80%)]  \tLoss:   92.548317\trec:   66.755470\tkl:   25.792850\n",
      "====> Epoch: 509 Average train loss: 91.5916\n",
      "====> Validation set loss: 94.8088\n",
      "====> Validation set kl: 26.1049\n",
      "Epoch: 510 [  100/50000 ( 0%)]  \tLoss:   94.358032\trec:   68.358147\tkl:   25.999889\n",
      "Epoch: 510 [10100/50000 (20%)]  \tLoss:   91.300797\trec:   65.308640\tkl:   25.992163\n",
      "Epoch: 510 [20100/50000 (40%)]  \tLoss:   92.845154\trec:   66.333565\tkl:   26.511589\n",
      "Epoch: 510 [30100/50000 (60%)]  \tLoss:   93.730240\trec:   67.288330\tkl:   26.441914\n",
      "Epoch: 510 [40100/50000 (80%)]  \tLoss:   91.165215\trec:   65.030235\tkl:   26.134977\n",
      "====> Epoch: 510 Average train loss: 91.6060\n",
      "====> Validation set loss: 94.8280\n",
      "====> Validation set kl: 25.8920\n",
      "Epoch: 511 [  100/50000 ( 0%)]  \tLoss:   91.217155\trec:   65.932365\tkl:   25.284798\n",
      "Epoch: 511 [10100/50000 (20%)]  \tLoss:   91.765816\trec:   65.849686\tkl:   25.916132\n",
      "Epoch: 511 [20100/50000 (40%)]  \tLoss:   96.235268\trec:   69.432693\tkl:   26.802572\n",
      "Epoch: 511 [30100/50000 (60%)]  \tLoss:   91.065392\trec:   65.281723\tkl:   25.783665\n",
      "Epoch: 511 [40100/50000 (80%)]  \tLoss:   96.211853\trec:   69.320488\tkl:   26.891369\n",
      "====> Epoch: 511 Average train loss: 91.5636\n",
      "====> Validation set loss: 94.8001\n",
      "====> Validation set kl: 25.9332\n",
      "Epoch: 512 [  100/50000 ( 0%)]  \tLoss:   89.438217\trec:   62.679104\tkl:   26.759111\n",
      "Epoch: 512 [10100/50000 (20%)]  \tLoss:   86.709038\trec:   61.509922\tkl:   25.199118\n",
      "Epoch: 512 [20100/50000 (40%)]  \tLoss:   92.821190\trec:   67.242760\tkl:   25.578423\n",
      "Epoch: 512 [30100/50000 (60%)]  \tLoss:   91.537918\trec:   65.706932\tkl:   25.830986\n",
      "Epoch: 512 [40100/50000 (80%)]  \tLoss:   88.819298\trec:   63.675808\tkl:   25.143490\n",
      "====> Epoch: 512 Average train loss: 91.5734\n",
      "====> Validation set loss: 94.9529\n",
      "====> Validation set kl: 26.0432\n",
      "Epoch: 513 [  100/50000 ( 0%)]  \tLoss:   93.386452\trec:   66.924782\tkl:   26.461670\n",
      "Epoch: 513 [10100/50000 (20%)]  \tLoss:   92.641739\trec:   65.757584\tkl:   26.884155\n",
      "Epoch: 513 [20100/50000 (40%)]  \tLoss:   93.641014\trec:   67.206581\tkl:   26.434435\n",
      "Epoch: 513 [30100/50000 (60%)]  \tLoss:   94.765785\trec:   67.662148\tkl:   27.103640\n",
      "Epoch: 513 [40100/50000 (80%)]  \tLoss:   84.386108\trec:   59.845242\tkl:   24.540867\n",
      "====> Epoch: 513 Average train loss: 91.5822\n",
      "====> Validation set loss: 94.7622\n",
      "====> Validation set kl: 26.2464\n",
      "Epoch: 514 [  100/50000 ( 0%)]  \tLoss:   85.533394\trec:   60.698250\tkl:   24.835148\n",
      "Epoch: 514 [10100/50000 (20%)]  \tLoss:   91.749672\trec:   65.421974\tkl:   26.327702\n",
      "Epoch: 514 [20100/50000 (40%)]  \tLoss:   95.482399\trec:   68.538193\tkl:   26.944214\n",
      "Epoch: 514 [30100/50000 (60%)]  \tLoss:   93.152534\trec:   67.521156\tkl:   25.631386\n",
      "Epoch: 514 [40100/50000 (80%)]  \tLoss:   89.899933\trec:   64.384537\tkl:   25.515392\n",
      "====> Epoch: 514 Average train loss: 91.5422\n",
      "====> Validation set loss: 94.8004\n",
      "====> Validation set kl: 26.0717\n",
      "Epoch: 515 [  100/50000 ( 0%)]  \tLoss:   90.923874\trec:   64.774940\tkl:   26.148933\n",
      "Epoch: 515 [10100/50000 (20%)]  \tLoss:   95.057007\trec:   68.290665\tkl:   26.766342\n",
      "Epoch: 515 [20100/50000 (40%)]  \tLoss:   91.381142\trec:   65.241226\tkl:   26.139910\n",
      "Epoch: 515 [30100/50000 (60%)]  \tLoss:   94.114021\trec:   67.680260\tkl:   26.433756\n",
      "Epoch: 515 [40100/50000 (80%)]  \tLoss:   94.483681\trec:   68.513321\tkl:   25.970364\n",
      "====> Epoch: 515 Average train loss: 91.5687\n",
      "====> Validation set loss: 94.8090\n",
      "====> Validation set kl: 26.1819\n",
      "Epoch: 516 [  100/50000 ( 0%)]  \tLoss:   93.291641\trec:   66.796906\tkl:   26.494738\n",
      "Epoch: 516 [10100/50000 (20%)]  \tLoss:   92.574081\trec:   66.552361\tkl:   26.021715\n",
      "Epoch: 516 [20100/50000 (40%)]  \tLoss:   89.087166\trec:   63.635567\tkl:   25.451607\n",
      "Epoch: 516 [30100/50000 (60%)]  \tLoss:   94.284721\trec:   67.217110\tkl:   27.067617\n",
      "Epoch: 516 [40100/50000 (80%)]  \tLoss:   91.962318\trec:   65.694038\tkl:   26.268284\n",
      "====> Epoch: 516 Average train loss: 91.5720\n",
      "====> Validation set loss: 94.8023\n",
      "====> Validation set kl: 25.7541\n",
      "Epoch: 517 [  100/50000 ( 0%)]  \tLoss:   94.127861\trec:   67.107628\tkl:   27.020237\n",
      "Epoch: 517 [10100/50000 (20%)]  \tLoss:   88.398628\trec:   63.177761\tkl:   25.220865\n",
      "Epoch: 517 [20100/50000 (40%)]  \tLoss:   92.460045\trec:   66.087029\tkl:   26.373020\n",
      "Epoch: 517 [30100/50000 (60%)]  \tLoss:   89.327049\trec:   63.889935\tkl:   25.437113\n",
      "Epoch: 517 [40100/50000 (80%)]  \tLoss:   91.086441\trec:   65.149368\tkl:   25.937073\n",
      "====> Epoch: 517 Average train loss: 91.5530\n",
      "====> Validation set loss: 94.7852\n",
      "====> Validation set kl: 25.8857\n",
      "Epoch: 518 [  100/50000 ( 0%)]  \tLoss:   92.122940\trec:   66.794167\tkl:   25.328770\n",
      "Epoch: 518 [10100/50000 (20%)]  \tLoss:   92.209213\trec:   66.196938\tkl:   26.012285\n",
      "Epoch: 518 [20100/50000 (40%)]  \tLoss:   96.209969\trec:   69.457909\tkl:   26.752060\n",
      "Epoch: 518 [30100/50000 (60%)]  \tLoss:   92.431061\trec:   66.115967\tkl:   26.315098\n",
      "Epoch: 518 [40100/50000 (80%)]  \tLoss:   88.395622\trec:   62.130371\tkl:   26.265259\n",
      "====> Epoch: 518 Average train loss: 91.5448\n",
      "====> Validation set loss: 94.8780\n",
      "====> Validation set kl: 26.0850\n",
      "Epoch: 519 [  100/50000 ( 0%)]  \tLoss:   93.868103\trec:   66.777580\tkl:   27.090519\n",
      "Epoch: 519 [10100/50000 (20%)]  \tLoss:   91.309708\trec:   64.702240\tkl:   26.607460\n",
      "Epoch: 519 [20100/50000 (40%)]  \tLoss:   94.152840\trec:   67.645058\tkl:   26.507780\n",
      "Epoch: 519 [30100/50000 (60%)]  \tLoss:   94.266899\trec:   68.536652\tkl:   25.730255\n",
      "Epoch: 519 [40100/50000 (80%)]  \tLoss:   94.405151\trec:   67.809219\tkl:   26.595932\n",
      "====> Epoch: 519 Average train loss: 91.5627\n",
      "====> Validation set loss: 94.7671\n",
      "====> Validation set kl: 26.0408\n",
      "Epoch: 520 [  100/50000 ( 0%)]  \tLoss:   95.044815\trec:   67.662361\tkl:   27.382448\n",
      "Epoch: 520 [10100/50000 (20%)]  \tLoss:   90.146202\trec:   63.791756\tkl:   26.354445\n",
      "Epoch: 520 [20100/50000 (40%)]  \tLoss:   90.517830\trec:   64.453865\tkl:   26.063965\n",
      "Epoch: 520 [30100/50000 (60%)]  \tLoss:   95.900848\trec:   69.002655\tkl:   26.898193\n",
      "Epoch: 520 [40100/50000 (80%)]  \tLoss:   91.390846\trec:   64.985573\tkl:   26.405270\n",
      "====> Epoch: 520 Average train loss: 91.5267\n",
      "====> Validation set loss: 94.9514\n",
      "====> Validation set kl: 26.1483\n",
      "Epoch: 521 [  100/50000 ( 0%)]  \tLoss:   90.202431\trec:   64.357811\tkl:   25.844616\n",
      "Epoch: 521 [10100/50000 (20%)]  \tLoss:   92.654663\trec:   66.188164\tkl:   26.466499\n",
      "Epoch: 521 [20100/50000 (40%)]  \tLoss:   92.920547\trec:   67.194855\tkl:   25.725698\n",
      "Epoch: 521 [30100/50000 (60%)]  \tLoss:   90.972481\trec:   64.348183\tkl:   26.624298\n",
      "Epoch: 521 [40100/50000 (80%)]  \tLoss:   90.418648\trec:   64.758003\tkl:   25.660648\n",
      "====> Epoch: 521 Average train loss: 91.5266\n",
      "====> Validation set loss: 94.7355\n",
      "====> Validation set kl: 26.0667\n",
      "Epoch: 522 [  100/50000 ( 0%)]  \tLoss:   95.173241\trec:   68.110550\tkl:   27.062695\n",
      "Epoch: 522 [10100/50000 (20%)]  \tLoss:   90.425392\trec:   64.908279\tkl:   25.517115\n",
      "Epoch: 522 [20100/50000 (40%)]  \tLoss:   88.799591\trec:   62.611248\tkl:   26.188339\n",
      "Epoch: 522 [30100/50000 (60%)]  \tLoss:   93.167587\trec:   66.815865\tkl:   26.351721\n",
      "Epoch: 522 [40100/50000 (80%)]  \tLoss:   91.838455\trec:   66.386162\tkl:   25.452284\n",
      "====> Epoch: 522 Average train loss: 91.5214\n",
      "====> Validation set loss: 94.7216\n",
      "====> Validation set kl: 25.9512\n",
      "Epoch: 523 [  100/50000 ( 0%)]  \tLoss:   90.738525\trec:   64.900230\tkl:   25.838293\n",
      "Epoch: 523 [10100/50000 (20%)]  \tLoss:   92.171425\trec:   66.176559\tkl:   25.994860\n",
      "Epoch: 523 [20100/50000 (40%)]  \tLoss:   92.202827\trec:   66.782219\tkl:   25.420620\n",
      "Epoch: 523 [30100/50000 (60%)]  \tLoss:   96.796768\trec:   69.781548\tkl:   27.015211\n",
      "Epoch: 523 [40100/50000 (80%)]  \tLoss:   92.385880\trec:   66.234100\tkl:   26.151781\n",
      "====> Epoch: 523 Average train loss: 91.5492\n",
      "====> Validation set loss: 94.7351\n",
      "====> Validation set kl: 26.0277\n",
      "Epoch: 524 [  100/50000 ( 0%)]  \tLoss:   86.519821\trec:   61.417847\tkl:   25.101982\n",
      "Epoch: 524 [10100/50000 (20%)]  \tLoss:   88.390778\trec:   62.445751\tkl:   25.945030\n",
      "Epoch: 524 [20100/50000 (40%)]  \tLoss:   92.777870\trec:   66.686203\tkl:   26.091675\n",
      "Epoch: 524 [30100/50000 (60%)]  \tLoss:   89.956734\trec:   64.208405\tkl:   25.748327\n",
      "Epoch: 524 [40100/50000 (80%)]  \tLoss:   94.695778\trec:   67.730881\tkl:   26.964897\n",
      "====> Epoch: 524 Average train loss: 91.5421\n",
      "====> Validation set loss: 94.8508\n",
      "====> Validation set kl: 26.2506\n",
      "Epoch: 525 [  100/50000 ( 0%)]  \tLoss:   90.522751\trec:   64.905945\tkl:   25.616800\n",
      "Epoch: 525 [10100/50000 (20%)]  \tLoss:   91.470566\trec:   65.561134\tkl:   25.909428\n",
      "Epoch: 525 [20100/50000 (40%)]  \tLoss:   91.779694\trec:   65.680588\tkl:   26.099112\n",
      "Epoch: 525 [30100/50000 (60%)]  \tLoss:   88.907143\trec:   63.030743\tkl:   25.876400\n",
      "Epoch: 525 [40100/50000 (80%)]  \tLoss:   89.413277\trec:   64.214073\tkl:   25.199203\n",
      "====> Epoch: 525 Average train loss: 91.5318\n",
      "====> Validation set loss: 94.7836\n",
      "====> Validation set kl: 26.0289\n",
      "Epoch: 526 [  100/50000 ( 0%)]  \tLoss:   89.071686\trec:   63.975712\tkl:   25.095978\n",
      "Epoch: 526 [10100/50000 (20%)]  \tLoss:   87.196236\trec:   61.155319\tkl:   26.040915\n",
      "Epoch: 526 [20100/50000 (40%)]  \tLoss:   90.947968\trec:   64.547440\tkl:   26.400532\n",
      "Epoch: 526 [30100/50000 (60%)]  \tLoss:   93.332909\trec:   67.553223\tkl:   25.779686\n",
      "Epoch: 526 [40100/50000 (80%)]  \tLoss:   92.637306\trec:   66.884315\tkl:   25.752993\n",
      "====> Epoch: 526 Average train loss: 91.5293\n",
      "====> Validation set loss: 94.8606\n",
      "====> Validation set kl: 26.1297\n",
      "Epoch: 527 [  100/50000 ( 0%)]  \tLoss:   90.803894\trec:   64.420181\tkl:   26.383711\n",
      "Epoch: 527 [10100/50000 (20%)]  \tLoss:   92.590126\trec:   67.032394\tkl:   25.557732\n",
      "Epoch: 527 [20100/50000 (40%)]  \tLoss:   95.568253\trec:   69.301872\tkl:   26.266384\n",
      "Epoch: 527 [30100/50000 (60%)]  \tLoss:   92.319862\trec:   65.638542\tkl:   26.681322\n",
      "Epoch: 527 [40100/50000 (80%)]  \tLoss:   90.423393\trec:   65.086922\tkl:   25.336470\n",
      "====> Epoch: 527 Average train loss: 91.5090\n",
      "====> Validation set loss: 94.6944\n",
      "====> Validation set kl: 26.0137\n",
      "Epoch: 528 [  100/50000 ( 0%)]  \tLoss:   90.684975\trec:   65.114769\tkl:   25.570210\n",
      "Epoch: 528 [10100/50000 (20%)]  \tLoss:   87.626915\trec:   61.224609\tkl:   26.402300\n",
      "Epoch: 528 [20100/50000 (40%)]  \tLoss:   89.688393\trec:   65.189972\tkl:   24.498426\n",
      "Epoch: 528 [30100/50000 (60%)]  \tLoss:   88.699089\trec:   62.452888\tkl:   26.246199\n",
      "Epoch: 528 [40100/50000 (80%)]  \tLoss:   90.672592\trec:   64.665611\tkl:   26.006985\n",
      "====> Epoch: 528 Average train loss: 91.5170\n",
      "====> Validation set loss: 94.8414\n",
      "====> Validation set kl: 26.1880\n",
      "Epoch: 529 [  100/50000 ( 0%)]  \tLoss:   91.379082\trec:   64.524918\tkl:   26.854160\n",
      "Epoch: 529 [10100/50000 (20%)]  \tLoss:   92.524879\trec:   66.355125\tkl:   26.169758\n",
      "Epoch: 529 [20100/50000 (40%)]  \tLoss:   90.306030\trec:   64.075294\tkl:   26.230747\n",
      "Epoch: 529 [30100/50000 (60%)]  \tLoss:   88.294930\trec:   63.156372\tkl:   25.138557\n",
      "Epoch: 529 [40100/50000 (80%)]  \tLoss:   89.260544\trec:   63.279873\tkl:   25.980679\n",
      "====> Epoch: 529 Average train loss: 91.5207\n",
      "====> Validation set loss: 94.6688\n",
      "====> Validation set kl: 26.1212\n",
      "Epoch: 530 [  100/50000 ( 0%)]  \tLoss:   89.871063\trec:   63.774422\tkl:   26.096642\n",
      "Epoch: 530 [10100/50000 (20%)]  \tLoss:   93.158478\trec:   66.849327\tkl:   26.309139\n",
      "Epoch: 530 [20100/50000 (40%)]  \tLoss:   90.143166\trec:   64.896027\tkl:   25.247133\n",
      "Epoch: 530 [30100/50000 (60%)]  \tLoss:   92.926559\trec:   65.959457\tkl:   26.967098\n",
      "Epoch: 530 [40100/50000 (80%)]  \tLoss:   91.218803\trec:   65.377136\tkl:   25.841665\n",
      "====> Epoch: 530 Average train loss: 91.5122\n",
      "====> Validation set loss: 94.8548\n",
      "====> Validation set kl: 26.0007\n",
      "Epoch: 531 [  100/50000 ( 0%)]  \tLoss:   92.616173\trec:   66.179977\tkl:   26.436188\n",
      "Epoch: 531 [10100/50000 (20%)]  \tLoss:   92.387733\trec:   66.323029\tkl:   26.064709\n",
      "Epoch: 531 [20100/50000 (40%)]  \tLoss:   88.840263\trec:   63.274727\tkl:   25.565538\n",
      "Epoch: 531 [30100/50000 (60%)]  \tLoss:   90.466866\trec:   64.070152\tkl:   26.396706\n",
      "Epoch: 531 [40100/50000 (80%)]  \tLoss:   93.125893\trec:   67.249771\tkl:   25.876127\n",
      "====> Epoch: 531 Average train loss: 91.4687\n",
      "====> Validation set loss: 94.7306\n",
      "====> Validation set kl: 26.1427\n",
      "Epoch: 532 [  100/50000 ( 0%)]  \tLoss:   88.598091\trec:   62.544960\tkl:   26.053137\n",
      "Epoch: 532 [10100/50000 (20%)]  \tLoss:   91.931252\trec:   65.166695\tkl:   26.764553\n",
      "Epoch: 532 [20100/50000 (40%)]  \tLoss:   91.484200\trec:   65.011879\tkl:   26.472317\n",
      "Epoch: 532 [30100/50000 (60%)]  \tLoss:   92.408257\trec:   66.440155\tkl:   25.968100\n",
      "Epoch: 532 [40100/50000 (80%)]  \tLoss:   93.381554\trec:   67.080200\tkl:   26.301348\n",
      "====> Epoch: 532 Average train loss: 91.5005\n",
      "====> Validation set loss: 94.8004\n",
      "====> Validation set kl: 26.0754\n",
      "Epoch: 533 [  100/50000 ( 0%)]  \tLoss:   88.963554\trec:   63.314556\tkl:   25.648996\n",
      "Epoch: 533 [10100/50000 (20%)]  \tLoss:   89.289352\trec:   62.514961\tkl:   26.774393\n",
      "Epoch: 533 [20100/50000 (40%)]  \tLoss:   95.477692\trec:   68.944595\tkl:   26.533091\n",
      "Epoch: 533 [30100/50000 (60%)]  \tLoss:   91.518173\trec:   65.091797\tkl:   26.426378\n",
      "Epoch: 533 [40100/50000 (80%)]  \tLoss:   93.078934\trec:   66.364059\tkl:   26.714874\n",
      "====> Epoch: 533 Average train loss: 91.5039\n",
      "====> Validation set loss: 94.8252\n",
      "====> Validation set kl: 26.1043\n",
      "Epoch: 534 [  100/50000 ( 0%)]  \tLoss:   90.939667\trec:   65.236015\tkl:   25.703648\n",
      "Epoch: 534 [10100/50000 (20%)]  \tLoss:   92.386772\trec:   66.261940\tkl:   26.124836\n",
      "Epoch: 534 [20100/50000 (40%)]  \tLoss:   92.200798\trec:   65.730179\tkl:   26.470617\n",
      "Epoch: 534 [30100/50000 (60%)]  \tLoss:   86.706589\trec:   60.443989\tkl:   26.262604\n",
      "Epoch: 534 [40100/50000 (80%)]  \tLoss:   94.849037\trec:   68.748322\tkl:   26.100712\n",
      "====> Epoch: 534 Average train loss: 91.4892\n",
      "====> Validation set loss: 94.8280\n",
      "====> Validation set kl: 25.9243\n",
      "Epoch: 535 [  100/50000 ( 0%)]  \tLoss:   89.994530\trec:   64.410301\tkl:   25.584229\n",
      "Epoch: 535 [10100/50000 (20%)]  \tLoss:   94.564308\trec:   68.278854\tkl:   26.285448\n",
      "Epoch: 535 [20100/50000 (40%)]  \tLoss:   90.956856\trec:   64.923592\tkl:   26.033258\n",
      "Epoch: 535 [30100/50000 (60%)]  \tLoss:   92.935204\trec:   66.396942\tkl:   26.538263\n",
      "Epoch: 535 [40100/50000 (80%)]  \tLoss:   94.367393\trec:   68.853531\tkl:   25.513859\n",
      "====> Epoch: 535 Average train loss: 91.4621\n",
      "====> Validation set loss: 94.7334\n",
      "====> Validation set kl: 26.0691\n",
      "Epoch: 536 [  100/50000 ( 0%)]  \tLoss:   89.250114\trec:   63.389500\tkl:   25.860617\n",
      "Epoch: 536 [10100/50000 (20%)]  \tLoss:   91.016640\trec:   65.465500\tkl:   25.551138\n",
      "Epoch: 536 [20100/50000 (40%)]  \tLoss:   88.044449\trec:   62.287552\tkl:   25.756897\n",
      "Epoch: 536 [30100/50000 (60%)]  \tLoss:   91.578667\trec:   65.216949\tkl:   26.361715\n",
      "Epoch: 536 [40100/50000 (80%)]  \tLoss:   91.705307\trec:   65.018852\tkl:   26.686449\n",
      "====> Epoch: 536 Average train loss: 91.4759\n",
      "====> Validation set loss: 94.9043\n",
      "====> Validation set kl: 26.2019\n",
      "Epoch: 537 [  100/50000 ( 0%)]  \tLoss:   92.489372\trec:   65.852104\tkl:   26.637270\n",
      "Epoch: 537 [10100/50000 (20%)]  \tLoss:   86.343651\trec:   61.718719\tkl:   24.624929\n",
      "Epoch: 537 [20100/50000 (40%)]  \tLoss:   90.564323\trec:   64.749733\tkl:   25.814594\n",
      "Epoch: 537 [30100/50000 (60%)]  \tLoss:   91.001114\trec:   64.288651\tkl:   26.712458\n",
      "Epoch: 537 [40100/50000 (80%)]  \tLoss:   96.404839\trec:   69.420563\tkl:   26.984282\n",
      "====> Epoch: 537 Average train loss: 91.4662\n",
      "====> Validation set loss: 94.8503\n",
      "====> Validation set kl: 26.2596\n",
      "Epoch: 538 [  100/50000 ( 0%)]  \tLoss:   92.678848\trec:   65.963173\tkl:   26.715677\n",
      "Epoch: 538 [10100/50000 (20%)]  \tLoss:   88.857643\trec:   63.420918\tkl:   25.436726\n",
      "Epoch: 538 [20100/50000 (40%)]  \tLoss:   92.821320\trec:   67.024551\tkl:   25.796761\n",
      "Epoch: 538 [30100/50000 (60%)]  \tLoss:   92.963921\trec:   65.938484\tkl:   27.025442\n",
      "Epoch: 538 [40100/50000 (80%)]  \tLoss:   91.125854\trec:   64.930779\tkl:   26.195082\n",
      "====> Epoch: 538 Average train loss: 91.4384\n",
      "====> Validation set loss: 94.7635\n",
      "====> Validation set kl: 26.0658\n",
      "Epoch: 539 [  100/50000 ( 0%)]  \tLoss:   90.096489\trec:   64.536407\tkl:   25.560087\n",
      "Epoch: 539 [10100/50000 (20%)]  \tLoss:   92.530251\trec:   65.845665\tkl:   26.684589\n",
      "Epoch: 539 [20100/50000 (40%)]  \tLoss:   94.058487\trec:   67.316216\tkl:   26.742270\n",
      "Epoch: 539 [30100/50000 (60%)]  \tLoss:   95.049767\trec:   68.236626\tkl:   26.813137\n",
      "Epoch: 539 [40100/50000 (80%)]  \tLoss:   93.085213\trec:   67.038994\tkl:   26.046225\n",
      "====> Epoch: 539 Average train loss: 91.4530\n",
      "====> Validation set loss: 94.7781\n",
      "====> Validation set kl: 25.9883\n",
      "Epoch: 540 [  100/50000 ( 0%)]  \tLoss:   88.557648\trec:   62.432640\tkl:   26.125008\n",
      "Epoch: 540 [10100/50000 (20%)]  \tLoss:   87.856163\trec:   62.244980\tkl:   25.611183\n",
      "Epoch: 540 [20100/50000 (40%)]  \tLoss:   92.282066\trec:   66.251198\tkl:   26.030874\n",
      "Epoch: 540 [30100/50000 (60%)]  \tLoss:   93.545700\trec:   67.192802\tkl:   26.352890\n",
      "Epoch: 540 [40100/50000 (80%)]  \tLoss:   90.290039\trec:   64.455444\tkl:   25.834589\n",
      "====> Epoch: 540 Average train loss: 91.4541\n",
      "====> Validation set loss: 94.6774\n",
      "====> Validation set kl: 25.9670\n",
      "Epoch: 541 [  100/50000 ( 0%)]  \tLoss:   90.677948\trec:   64.152901\tkl:   26.525047\n",
      "Epoch: 541 [10100/50000 (20%)]  \tLoss:   91.441032\trec:   65.205261\tkl:   26.235765\n",
      "Epoch: 541 [20100/50000 (40%)]  \tLoss:   91.667809\trec:   65.555870\tkl:   26.111940\n",
      "Epoch: 541 [30100/50000 (60%)]  \tLoss:   93.806396\trec:   67.560471\tkl:   26.245930\n",
      "Epoch: 541 [40100/50000 (80%)]  \tLoss:   89.083954\trec:   62.572353\tkl:   26.511600\n",
      "====> Epoch: 541 Average train loss: 91.4359\n",
      "====> Validation set loss: 94.7087\n",
      "====> Validation set kl: 26.0488\n",
      "Epoch: 542 [  100/50000 ( 0%)]  \tLoss:   89.293152\trec:   63.457138\tkl:   25.836018\n",
      "Epoch: 542 [10100/50000 (20%)]  \tLoss:   89.167580\trec:   62.617870\tkl:   26.549702\n",
      "Epoch: 542 [20100/50000 (40%)]  \tLoss:   89.658318\trec:   63.560673\tkl:   26.097651\n",
      "Epoch: 542 [30100/50000 (60%)]  \tLoss:   95.075821\trec:   68.694336\tkl:   26.381483\n",
      "Epoch: 542 [40100/50000 (80%)]  \tLoss:   91.096886\trec:   64.974358\tkl:   26.122524\n",
      "====> Epoch: 542 Average train loss: 91.4276\n",
      "====> Validation set loss: 94.7569\n",
      "====> Validation set kl: 25.9428\n",
      "Epoch: 543 [  100/50000 ( 0%)]  \tLoss:   92.810310\trec:   67.752594\tkl:   25.057716\n",
      "Epoch: 543 [10100/50000 (20%)]  \tLoss:   91.090782\trec:   65.691696\tkl:   25.399086\n",
      "Epoch: 543 [20100/50000 (40%)]  \tLoss:   90.361870\trec:   64.866394\tkl:   25.495474\n",
      "Epoch: 543 [30100/50000 (60%)]  \tLoss:   94.533287\trec:   68.137459\tkl:   26.395826\n",
      "Epoch: 543 [40100/50000 (80%)]  \tLoss:   92.324547\trec:   65.440384\tkl:   26.884165\n",
      "====> Epoch: 543 Average train loss: 91.4260\n",
      "====> Validation set loss: 94.6307\n",
      "====> Validation set kl: 26.0290\n",
      "Epoch: 544 [  100/50000 ( 0%)]  \tLoss:   88.779343\trec:   63.866844\tkl:   24.912502\n",
      "Epoch: 544 [10100/50000 (20%)]  \tLoss:   87.986794\trec:   62.765331\tkl:   25.221466\n",
      "Epoch: 544 [20100/50000 (40%)]  \tLoss:   89.454628\trec:   63.583084\tkl:   25.871540\n",
      "Epoch: 544 [30100/50000 (60%)]  \tLoss:   95.866615\trec:   69.522720\tkl:   26.343901\n",
      "Epoch: 544 [40100/50000 (80%)]  \tLoss:   92.760239\trec:   66.596535\tkl:   26.163713\n",
      "====> Epoch: 544 Average train loss: 91.4407\n",
      "====> Validation set loss: 94.7316\n",
      "====> Validation set kl: 26.1656\n",
      "Epoch: 545 [  100/50000 ( 0%)]  \tLoss:   89.945625\trec:   63.305874\tkl:   26.639746\n",
      "Epoch: 545 [10100/50000 (20%)]  \tLoss:   92.334213\trec:   66.159721\tkl:   26.174490\n",
      "Epoch: 545 [20100/50000 (40%)]  \tLoss:   93.912300\trec:   66.919937\tkl:   26.992363\n",
      "Epoch: 545 [30100/50000 (60%)]  \tLoss:   94.470917\trec:   67.367432\tkl:   27.103485\n",
      "Epoch: 545 [40100/50000 (80%)]  \tLoss:   94.356735\trec:   67.225327\tkl:   27.131416\n",
      "====> Epoch: 545 Average train loss: 91.4199\n",
      "====> Validation set loss: 94.7372\n",
      "====> Validation set kl: 26.0418\n",
      "Epoch: 546 [  100/50000 ( 0%)]  \tLoss:   89.765312\trec:   64.262352\tkl:   25.502958\n",
      "Epoch: 546 [10100/50000 (20%)]  \tLoss:   90.923256\trec:   65.549187\tkl:   25.374069\n",
      "Epoch: 546 [20100/50000 (40%)]  \tLoss:   91.428581\trec:   65.030624\tkl:   26.397961\n",
      "Epoch: 546 [30100/50000 (60%)]  \tLoss:   86.604599\trec:   61.559639\tkl:   25.044960\n",
      "Epoch: 546 [40100/50000 (80%)]  \tLoss:   91.328789\trec:   65.080734\tkl:   26.248051\n",
      "====> Epoch: 546 Average train loss: 91.4210\n",
      "====> Validation set loss: 94.7290\n",
      "====> Validation set kl: 26.0866\n",
      "Epoch: 547 [  100/50000 ( 0%)]  \tLoss:   90.635063\trec:   64.943558\tkl:   25.691511\n",
      "Epoch: 547 [10100/50000 (20%)]  \tLoss:   88.938629\trec:   63.047264\tkl:   25.891365\n",
      "Epoch: 547 [20100/50000 (40%)]  \tLoss:   89.378632\trec:   63.874023\tkl:   25.504614\n",
      "Epoch: 547 [30100/50000 (60%)]  \tLoss:   91.682945\trec:   65.569702\tkl:   26.113243\n",
      "Epoch: 547 [40100/50000 (80%)]  \tLoss:   90.633400\trec:   64.891281\tkl:   25.742123\n",
      "====> Epoch: 547 Average train loss: 91.4251\n",
      "====> Validation set loss: 94.7357\n",
      "====> Validation set kl: 25.9852\n",
      "Epoch: 548 [  100/50000 ( 0%)]  \tLoss:   93.886581\trec:   67.586182\tkl:   26.300394\n",
      "Epoch: 548 [10100/50000 (20%)]  \tLoss:   92.175621\trec:   66.032997\tkl:   26.142626\n",
      "Epoch: 548 [20100/50000 (40%)]  \tLoss:   86.933815\trec:   61.632664\tkl:   25.301151\n",
      "Epoch: 548 [30100/50000 (60%)]  \tLoss:   91.152946\trec:   64.630417\tkl:   26.522530\n",
      "Epoch: 548 [40100/50000 (80%)]  \tLoss:   89.547989\trec:   64.412674\tkl:   25.135307\n",
      "====> Epoch: 548 Average train loss: 91.4040\n",
      "====> Validation set loss: 94.7736\n",
      "====> Validation set kl: 26.1406\n",
      "Epoch: 549 [  100/50000 ( 0%)]  \tLoss:   96.164803\trec:   69.608292\tkl:   26.556509\n",
      "Epoch: 549 [10100/50000 (20%)]  \tLoss:   90.429520\trec:   64.254372\tkl:   26.175146\n",
      "Epoch: 549 [20100/50000 (40%)]  \tLoss:   89.590836\trec:   62.948025\tkl:   26.642809\n",
      "Epoch: 549 [30100/50000 (60%)]  \tLoss:   91.435410\trec:   65.341980\tkl:   26.093424\n",
      "Epoch: 549 [40100/50000 (80%)]  \tLoss:   88.663605\trec:   62.633114\tkl:   26.030487\n",
      "====> Epoch: 549 Average train loss: 91.4314\n",
      "====> Validation set loss: 94.7769\n",
      "====> Validation set kl: 26.2090\n",
      "Epoch: 550 [  100/50000 ( 0%)]  \tLoss:   92.247498\trec:   65.470840\tkl:   26.776649\n",
      "Epoch: 550 [10100/50000 (20%)]  \tLoss:   90.529922\trec:   64.277847\tkl:   26.252068\n",
      "Epoch: 550 [20100/50000 (40%)]  \tLoss:   95.737221\trec:   68.879242\tkl:   26.857985\n",
      "Epoch: 550 [30100/50000 (60%)]  \tLoss:   90.565460\trec:   65.517288\tkl:   25.048168\n",
      "Epoch: 550 [40100/50000 (80%)]  \tLoss:   91.873573\trec:   65.524094\tkl:   26.349478\n",
      "====> Epoch: 550 Average train loss: 91.4258\n",
      "====> Validation set loss: 94.6903\n",
      "====> Validation set kl: 26.1522\n",
      "Epoch: 551 [  100/50000 ( 0%)]  \tLoss:   88.933769\trec:   63.283730\tkl:   25.650036\n",
      "Epoch: 551 [10100/50000 (20%)]  \tLoss:   89.647499\trec:   62.806526\tkl:   26.840977\n",
      "Epoch: 551 [20100/50000 (40%)]  \tLoss:   94.865601\trec:   68.667564\tkl:   26.198034\n",
      "Epoch: 551 [30100/50000 (60%)]  \tLoss:   89.524185\trec:   65.004715\tkl:   24.519474\n",
      "Epoch: 551 [40100/50000 (80%)]  \tLoss:   91.975693\trec:   65.475281\tkl:   26.500406\n",
      "====> Epoch: 551 Average train loss: 91.4133\n",
      "====> Validation set loss: 94.7523\n",
      "====> Validation set kl: 26.0231\n",
      "Epoch: 552 [  100/50000 ( 0%)]  \tLoss:   92.690430\trec:   66.871140\tkl:   25.819286\n",
      "Epoch: 552 [10100/50000 (20%)]  \tLoss:   95.288902\trec:   68.161201\tkl:   27.127699\n",
      "Epoch: 552 [20100/50000 (40%)]  \tLoss:   91.946815\trec:   65.896042\tkl:   26.050772\n",
      "Epoch: 552 [30100/50000 (60%)]  \tLoss:   96.158791\trec:   69.542549\tkl:   26.616238\n",
      "Epoch: 552 [40100/50000 (80%)]  \tLoss:   90.261147\trec:   63.862595\tkl:   26.398556\n",
      "====> Epoch: 552 Average train loss: 91.4101\n",
      "====> Validation set loss: 94.6902\n",
      "====> Validation set kl: 26.1858\n",
      "Epoch: 553 [  100/50000 ( 0%)]  \tLoss:   91.823975\trec:   65.816895\tkl:   26.007074\n",
      "Epoch: 553 [10100/50000 (20%)]  \tLoss:   96.342964\trec:   69.814430\tkl:   26.528538\n",
      "Epoch: 553 [20100/50000 (40%)]  \tLoss:   90.290642\trec:   63.757114\tkl:   26.533529\n",
      "Epoch: 553 [30100/50000 (60%)]  \tLoss:   88.667809\trec:   63.401794\tkl:   25.266018\n",
      "Epoch: 553 [40100/50000 (80%)]  \tLoss:   91.360672\trec:   65.111473\tkl:   26.249197\n",
      "====> Epoch: 553 Average train loss: 91.3897\n",
      "====> Validation set loss: 94.8206\n",
      "====> Validation set kl: 26.1859\n",
      "Epoch: 554 [  100/50000 ( 0%)]  \tLoss:   90.304794\trec:   64.561180\tkl:   25.743612\n",
      "Epoch: 554 [10100/50000 (20%)]  \tLoss:   89.405487\trec:   63.667412\tkl:   25.738075\n",
      "Epoch: 554 [20100/50000 (40%)]  \tLoss:   92.261345\trec:   66.040733\tkl:   26.220617\n",
      "Epoch: 554 [30100/50000 (60%)]  \tLoss:   88.767456\trec:   63.517559\tkl:   25.249897\n",
      "Epoch: 554 [40100/50000 (80%)]  \tLoss:   93.946335\trec:   67.381393\tkl:   26.564941\n",
      "====> Epoch: 554 Average train loss: 91.3713\n",
      "====> Validation set loss: 94.6529\n",
      "====> Validation set kl: 26.0896\n",
      "Epoch: 555 [  100/50000 ( 0%)]  \tLoss:   92.083839\trec:   66.255989\tkl:   25.827847\n",
      "Epoch: 555 [10100/50000 (20%)]  \tLoss:   91.084839\trec:   64.690338\tkl:   26.394501\n",
      "Epoch: 555 [20100/50000 (40%)]  \tLoss:   90.851662\trec:   64.424858\tkl:   26.426798\n",
      "Epoch: 555 [30100/50000 (60%)]  \tLoss:   91.862106\trec:   65.340080\tkl:   26.522036\n",
      "Epoch: 555 [40100/50000 (80%)]  \tLoss:   90.283844\trec:   64.029373\tkl:   26.254475\n",
      "====> Epoch: 555 Average train loss: 91.3923\n",
      "====> Validation set loss: 94.7684\n",
      "====> Validation set kl: 26.1287\n",
      "Epoch: 556 [  100/50000 ( 0%)]  \tLoss:   91.646294\trec:   65.100533\tkl:   26.545767\n",
      "Epoch: 556 [10100/50000 (20%)]  \tLoss:   87.932213\trec:   62.575554\tkl:   25.356659\n",
      "Epoch: 556 [20100/50000 (40%)]  \tLoss:   92.407013\trec:   66.430504\tkl:   25.976501\n",
      "Epoch: 556 [30100/50000 (60%)]  \tLoss:   95.418770\trec:   68.259979\tkl:   27.158792\n",
      "Epoch: 556 [40100/50000 (80%)]  \tLoss:   93.837387\trec:   66.960426\tkl:   26.876961\n",
      "====> Epoch: 556 Average train loss: 91.3712\n",
      "====> Validation set loss: 94.6540\n",
      "====> Validation set kl: 26.0338\n",
      "Epoch: 557 [  100/50000 ( 0%)]  \tLoss:   91.845505\trec:   66.371376\tkl:   25.474131\n",
      "Epoch: 557 [10100/50000 (20%)]  \tLoss:   89.541336\trec:   63.093708\tkl:   26.447624\n",
      "Epoch: 557 [20100/50000 (40%)]  \tLoss:   95.400505\trec:   68.483345\tkl:   26.917160\n",
      "Epoch: 557 [30100/50000 (60%)]  \tLoss:   88.138634\trec:   62.945076\tkl:   25.193550\n",
      "Epoch: 557 [40100/50000 (80%)]  \tLoss:   91.098114\trec:   66.322334\tkl:   24.775782\n",
      "====> Epoch: 557 Average train loss: 91.3813\n",
      "====> Validation set loss: 94.6590\n",
      "====> Validation set kl: 26.1063\n",
      "Epoch: 558 [  100/50000 ( 0%)]  \tLoss:   93.052330\trec:   66.795242\tkl:   26.257090\n",
      "Epoch: 558 [10100/50000 (20%)]  \tLoss:   90.158653\trec:   64.149094\tkl:   26.009552\n",
      "Epoch: 558 [20100/50000 (40%)]  \tLoss:   92.639648\trec:   66.596153\tkl:   26.043495\n",
      "Epoch: 558 [30100/50000 (60%)]  \tLoss:   92.783104\trec:   66.251732\tkl:   26.531366\n",
      "Epoch: 558 [40100/50000 (80%)]  \tLoss:   92.935135\trec:   66.346886\tkl:   26.588257\n",
      "====> Epoch: 558 Average train loss: 91.3816\n",
      "====> Validation set loss: 94.7239\n",
      "====> Validation set kl: 26.0283\n",
      "Epoch: 559 [  100/50000 ( 0%)]  \tLoss:   97.163918\trec:   70.023804\tkl:   27.140106\n",
      "Epoch: 559 [10100/50000 (20%)]  \tLoss:   90.462234\trec:   64.912415\tkl:   25.549818\n",
      "Epoch: 559 [20100/50000 (40%)]  \tLoss:   93.171768\trec:   66.321152\tkl:   26.850615\n",
      "Epoch: 559 [30100/50000 (60%)]  \tLoss:   90.064072\trec:   64.021095\tkl:   26.042980\n",
      "Epoch: 559 [40100/50000 (80%)]  \tLoss:   93.960220\trec:   67.460617\tkl:   26.499603\n",
      "====> Epoch: 559 Average train loss: 91.3704\n",
      "====> Validation set loss: 94.8013\n",
      "====> Validation set kl: 26.1769\n",
      "Epoch: 560 [  100/50000 ( 0%)]  \tLoss:   89.738983\trec:   64.043335\tkl:   25.695648\n",
      "Epoch: 560 [10100/50000 (20%)]  \tLoss:   88.413849\trec:   62.455715\tkl:   25.958126\n",
      "Epoch: 560 [20100/50000 (40%)]  \tLoss:   94.582397\trec:   68.521660\tkl:   26.060741\n",
      "Epoch: 560 [30100/50000 (60%)]  \tLoss:   92.733025\trec:   66.424423\tkl:   26.308601\n",
      "Epoch: 560 [40100/50000 (80%)]  \tLoss:   90.344208\trec:   63.822807\tkl:   26.521400\n",
      "====> Epoch: 560 Average train loss: 91.3470\n",
      "====> Validation set loss: 94.6675\n",
      "====> Validation set kl: 26.0723\n",
      "Epoch: 561 [  100/50000 ( 0%)]  \tLoss:   89.082664\trec:   63.641922\tkl:   25.440744\n",
      "Epoch: 561 [10100/50000 (20%)]  \tLoss:   91.824043\trec:   65.880722\tkl:   25.943321\n",
      "Epoch: 561 [20100/50000 (40%)]  \tLoss:   89.366150\trec:   63.950886\tkl:   25.415262\n",
      "Epoch: 561 [30100/50000 (60%)]  \tLoss:   93.295868\trec:   67.164246\tkl:   26.131620\n",
      "Epoch: 561 [40100/50000 (80%)]  \tLoss:   93.205460\trec:   66.778465\tkl:   26.426994\n",
      "====> Epoch: 561 Average train loss: 91.3671\n",
      "====> Validation set loss: 94.6869\n",
      "====> Validation set kl: 25.8981\n",
      "Epoch: 562 [  100/50000 ( 0%)]  \tLoss:   90.903481\trec:   64.793365\tkl:   26.110121\n",
      "Epoch: 562 [10100/50000 (20%)]  \tLoss:   87.625381\trec:   61.580418\tkl:   26.044958\n",
      "Epoch: 562 [20100/50000 (40%)]  \tLoss:   91.350502\trec:   65.840225\tkl:   25.510281\n",
      "Epoch: 562 [30100/50000 (60%)]  \tLoss:   89.254234\trec:   63.253819\tkl:   26.000414\n",
      "Epoch: 562 [40100/50000 (80%)]  \tLoss:   90.841713\trec:   64.568092\tkl:   26.273623\n",
      "====> Epoch: 562 Average train loss: 91.3468\n",
      "====> Validation set loss: 94.6239\n",
      "====> Validation set kl: 26.0810\n",
      "Epoch: 563 [  100/50000 ( 0%)]  \tLoss:   94.133698\trec:   67.976280\tkl:   26.157417\n",
      "Epoch: 563 [10100/50000 (20%)]  \tLoss:   93.665565\trec:   66.911720\tkl:   26.753845\n",
      "Epoch: 563 [20100/50000 (40%)]  \tLoss:   92.306030\trec:   65.252594\tkl:   27.053442\n",
      "Epoch: 563 [30100/50000 (60%)]  \tLoss:   88.752808\trec:   62.784946\tkl:   25.967871\n",
      "Epoch: 563 [40100/50000 (80%)]  \tLoss:   91.786797\trec:   65.443253\tkl:   26.343544\n",
      "====> Epoch: 563 Average train loss: 91.3586\n",
      "====> Validation set loss: 94.7162\n",
      "====> Validation set kl: 26.1396\n",
      "Epoch: 564 [  100/50000 ( 0%)]  \tLoss:   91.520340\trec:   65.680855\tkl:   25.839481\n",
      "Epoch: 564 [10100/50000 (20%)]  \tLoss:   93.839584\trec:   67.623619\tkl:   26.215963\n",
      "Epoch: 564 [20100/50000 (40%)]  \tLoss:   89.434891\trec:   62.719727\tkl:   26.715162\n",
      "Epoch: 564 [30100/50000 (60%)]  \tLoss:   92.091522\trec:   66.071045\tkl:   26.020483\n",
      "Epoch: 564 [40100/50000 (80%)]  \tLoss:   90.339966\trec:   64.257843\tkl:   26.082130\n",
      "====> Epoch: 564 Average train loss: 91.3427\n",
      "====> Validation set loss: 94.6171\n",
      "====> Validation set kl: 26.0521\n",
      "Epoch: 565 [  100/50000 ( 0%)]  \tLoss:   94.237595\trec:   67.430817\tkl:   26.806786\n",
      "Epoch: 565 [10100/50000 (20%)]  \tLoss:   94.777733\trec:   67.886711\tkl:   26.891022\n",
      "Epoch: 565 [20100/50000 (40%)]  \tLoss:   89.541229\trec:   63.420856\tkl:   26.120365\n",
      "Epoch: 565 [30100/50000 (60%)]  \tLoss:   93.065155\trec:   67.474892\tkl:   25.590265\n",
      "Epoch: 565 [40100/50000 (80%)]  \tLoss:   90.006241\trec:   62.644417\tkl:   27.361820\n",
      "====> Epoch: 565 Average train loss: 91.3480\n",
      "====> Validation set loss: 94.7013\n",
      "====> Validation set kl: 25.9278\n",
      "Epoch: 566 [  100/50000 ( 0%)]  \tLoss:   91.785973\trec:   64.990028\tkl:   26.795942\n",
      "Epoch: 566 [10100/50000 (20%)]  \tLoss:   94.484879\trec:   67.403923\tkl:   27.080952\n",
      "Epoch: 566 [20100/50000 (40%)]  \tLoss:   88.874306\trec:   63.326893\tkl:   25.547415\n",
      "Epoch: 566 [30100/50000 (60%)]  \tLoss:   90.813774\trec:   64.749138\tkl:   26.064638\n",
      "Epoch: 566 [40100/50000 (80%)]  \tLoss:   92.773270\trec:   67.424065\tkl:   25.349211\n",
      "====> Epoch: 566 Average train loss: 91.3341\n",
      "====> Validation set loss: 94.6719\n",
      "====> Validation set kl: 26.1009\n",
      "Epoch: 567 [  100/50000 ( 0%)]  \tLoss:   90.293221\trec:   64.001221\tkl:   26.292000\n",
      "Epoch: 567 [10100/50000 (20%)]  \tLoss:   92.755165\trec:   65.769966\tkl:   26.985191\n",
      "Epoch: 567 [20100/50000 (40%)]  \tLoss:   90.574097\trec:   64.939049\tkl:   25.635046\n",
      "Epoch: 567 [30100/50000 (60%)]  \tLoss:   92.625526\trec:   67.084381\tkl:   25.541140\n",
      "Epoch: 567 [40100/50000 (80%)]  \tLoss:   88.108200\trec:   62.430027\tkl:   25.678171\n",
      "====> Epoch: 567 Average train loss: 91.3481\n",
      "====> Validation set loss: 94.6881\n",
      "====> Validation set kl: 26.1293\n",
      "Epoch: 568 [  100/50000 ( 0%)]  \tLoss:   93.664360\trec:   66.590851\tkl:   27.073515\n",
      "Epoch: 568 [10100/50000 (20%)]  \tLoss:   86.383926\trec:   61.247070\tkl:   25.136850\n",
      "Epoch: 568 [20100/50000 (40%)]  \tLoss:   89.395630\trec:   63.744961\tkl:   25.650671\n",
      "Epoch: 568 [30100/50000 (60%)]  \tLoss:   91.341171\trec:   64.938927\tkl:   26.402239\n",
      "Epoch: 568 [40100/50000 (80%)]  \tLoss:   93.961678\trec:   67.829689\tkl:   26.131994\n",
      "====> Epoch: 568 Average train loss: 91.3450\n",
      "====> Validation set loss: 94.6524\n",
      "====> Validation set kl: 26.1653\n",
      "Epoch: 569 [  100/50000 ( 0%)]  \tLoss:   93.401367\trec:   66.535454\tkl:   26.865917\n",
      "Epoch: 569 [10100/50000 (20%)]  \tLoss:   95.462379\trec:   67.830986\tkl:   27.631395\n",
      "Epoch: 569 [20100/50000 (40%)]  \tLoss:   90.577339\trec:   65.131180\tkl:   25.446163\n",
      "Epoch: 569 [30100/50000 (60%)]  \tLoss:   94.117577\trec:   66.908699\tkl:   27.208879\n",
      "Epoch: 569 [40100/50000 (80%)]  \tLoss:   93.457016\trec:   67.635132\tkl:   25.821892\n",
      "====> Epoch: 569 Average train loss: 91.3254\n",
      "====> Validation set loss: 94.6963\n",
      "====> Validation set kl: 26.0841\n",
      "Epoch: 570 [  100/50000 ( 0%)]  \tLoss:   90.548462\trec:   64.103203\tkl:   26.445259\n",
      "Epoch: 570 [10100/50000 (20%)]  \tLoss:   89.868668\trec:   63.851151\tkl:   26.017513\n",
      "Epoch: 570 [20100/50000 (40%)]  \tLoss:   95.874855\trec:   69.262184\tkl:   26.612667\n",
      "Epoch: 570 [30100/50000 (60%)]  \tLoss:   91.703300\trec:   65.122177\tkl:   26.581123\n",
      "Epoch: 570 [40100/50000 (80%)]  \tLoss:   90.973228\trec:   65.187798\tkl:   25.785431\n",
      "====> Epoch: 570 Average train loss: 91.3148\n",
      "====> Validation set loss: 94.7158\n",
      "====> Validation set kl: 26.2089\n",
      "Epoch: 571 [  100/50000 ( 0%)]  \tLoss:   92.935173\trec:   66.503082\tkl:   26.432089\n",
      "Epoch: 571 [10100/50000 (20%)]  \tLoss:   90.717567\trec:   64.178810\tkl:   26.538759\n",
      "Epoch: 571 [20100/50000 (40%)]  \tLoss:   95.363083\trec:   68.657784\tkl:   26.705307\n",
      "Epoch: 571 [30100/50000 (60%)]  \tLoss:   90.223000\trec:   65.278366\tkl:   24.944626\n",
      "Epoch: 571 [40100/50000 (80%)]  \tLoss:   92.753593\trec:   66.085114\tkl:   26.668476\n",
      "====> Epoch: 571 Average train loss: 91.3372\n",
      "====> Validation set loss: 94.7287\n",
      "====> Validation set kl: 26.0636\n",
      "Epoch: 572 [  100/50000 ( 0%)]  \tLoss:   93.348183\trec:   67.072197\tkl:   26.275976\n",
      "Epoch: 572 [10100/50000 (20%)]  \tLoss:   91.300636\trec:   65.403809\tkl:   25.896824\n",
      "Epoch: 572 [20100/50000 (40%)]  \tLoss:   89.350662\trec:   63.678524\tkl:   25.672142\n",
      "Epoch: 572 [30100/50000 (60%)]  \tLoss:   93.303101\trec:   67.155273\tkl:   26.147829\n",
      "Epoch: 572 [40100/50000 (80%)]  \tLoss:   90.500076\trec:   64.494240\tkl:   26.005836\n",
      "====> Epoch: 572 Average train loss: 91.2961\n",
      "====> Validation set loss: 94.6471\n",
      "====> Validation set kl: 25.9769\n",
      "Epoch: 573 [  100/50000 ( 0%)]  \tLoss:   91.411629\trec:   64.751503\tkl:   26.660126\n",
      "Epoch: 573 [10100/50000 (20%)]  \tLoss:   90.918022\trec:   65.409119\tkl:   25.508904\n",
      "Epoch: 573 [20100/50000 (40%)]  \tLoss:   91.325134\trec:   66.092964\tkl:   25.232164\n",
      "Epoch: 573 [30100/50000 (60%)]  \tLoss:   95.902863\trec:   69.287872\tkl:   26.614986\n",
      "Epoch: 573 [40100/50000 (80%)]  \tLoss:   91.776962\trec:   65.770630\tkl:   26.006325\n",
      "====> Epoch: 573 Average train loss: 91.3151\n",
      "====> Validation set loss: 94.6838\n",
      "====> Validation set kl: 26.0112\n",
      "Epoch: 574 [  100/50000 ( 0%)]  \tLoss:   91.606285\trec:   65.133125\tkl:   26.473162\n",
      "Epoch: 574 [10100/50000 (20%)]  \tLoss:   90.037613\trec:   64.043884\tkl:   25.993732\n",
      "Epoch: 574 [20100/50000 (40%)]  \tLoss:   94.197037\trec:   67.712120\tkl:   26.484924\n",
      "Epoch: 574 [30100/50000 (60%)]  \tLoss:   93.048439\trec:   66.898766\tkl:   26.149677\n",
      "Epoch: 574 [40100/50000 (80%)]  \tLoss:   92.184372\trec:   66.291832\tkl:   25.892544\n",
      "====> Epoch: 574 Average train loss: 91.2761\n",
      "====> Validation set loss: 94.7412\n",
      "====> Validation set kl: 25.9521\n",
      "Epoch: 575 [  100/50000 ( 0%)]  \tLoss:   91.299789\trec:   66.485229\tkl:   24.814560\n",
      "Epoch: 575 [10100/50000 (20%)]  \tLoss:   91.716774\trec:   65.625847\tkl:   26.090921\n",
      "Epoch: 575 [20100/50000 (40%)]  \tLoss:   88.961609\trec:   63.585152\tkl:   25.376457\n",
      "Epoch: 575 [30100/50000 (60%)]  \tLoss:   93.948502\trec:   67.473854\tkl:   26.474649\n",
      "Epoch: 575 [40100/50000 (80%)]  \tLoss:   89.619888\trec:   63.968880\tkl:   25.651011\n",
      "====> Epoch: 575 Average train loss: 91.3139\n",
      "====> Validation set loss: 94.6546\n",
      "====> Validation set kl: 26.0071\n",
      "Epoch: 576 [  100/50000 ( 0%)]  \tLoss:   90.977089\trec:   64.462929\tkl:   26.514164\n",
      "Epoch: 576 [10100/50000 (20%)]  \tLoss:   89.288551\trec:   63.203064\tkl:   26.085487\n",
      "Epoch: 576 [20100/50000 (40%)]  \tLoss:   89.449013\trec:   64.271324\tkl:   25.177691\n",
      "Epoch: 576 [30100/50000 (60%)]  \tLoss:   95.430519\trec:   68.793701\tkl:   26.636816\n",
      "Epoch: 576 [40100/50000 (80%)]  \tLoss:   88.556854\trec:   62.816708\tkl:   25.740143\n",
      "====> Epoch: 576 Average train loss: 91.3103\n",
      "====> Validation set loss: 94.7437\n",
      "====> Validation set kl: 26.0194\n",
      "Epoch: 577 [  100/50000 ( 0%)]  \tLoss:   93.307793\trec:   67.229042\tkl:   26.078753\n",
      "Epoch: 577 [10100/50000 (20%)]  \tLoss:   92.683052\trec:   66.920341\tkl:   25.762707\n",
      "Epoch: 577 [20100/50000 (40%)]  \tLoss:   89.031143\trec:   63.707878\tkl:   25.323261\n",
      "Epoch: 577 [30100/50000 (60%)]  \tLoss:   95.220963\trec:   68.015076\tkl:   27.205885\n",
      "Epoch: 577 [40100/50000 (80%)]  \tLoss:   88.428696\trec:   61.538445\tkl:   26.890253\n",
      "====> Epoch: 577 Average train loss: 91.3019\n",
      "====> Validation set loss: 94.7151\n",
      "====> Validation set kl: 26.1153\n",
      "Epoch: 578 [  100/50000 ( 0%)]  \tLoss:   87.862579\trec:   62.858234\tkl:   25.004335\n",
      "Epoch: 578 [10100/50000 (20%)]  \tLoss:   84.837204\trec:   59.918842\tkl:   24.918369\n",
      "Epoch: 578 [20100/50000 (40%)]  \tLoss:   93.042458\trec:   67.237442\tkl:   25.805023\n",
      "Epoch: 578 [30100/50000 (60%)]  \tLoss:   90.282158\trec:   64.839653\tkl:   25.442501\n",
      "Epoch: 578 [40100/50000 (80%)]  \tLoss:   94.902885\trec:   68.317505\tkl:   26.585388\n",
      "====> Epoch: 578 Average train loss: 91.3039\n",
      "====> Validation set loss: 94.7096\n",
      "====> Validation set kl: 26.1833\n",
      "Epoch: 579 [  100/50000 ( 0%)]  \tLoss:   87.904793\trec:   62.073837\tkl:   25.830956\n",
      "Epoch: 579 [10100/50000 (20%)]  \tLoss:   93.257500\trec:   66.435295\tkl:   26.822201\n",
      "Epoch: 579 [20100/50000 (40%)]  \tLoss:   92.940300\trec:   66.790298\tkl:   26.150002\n",
      "Epoch: 579 [30100/50000 (60%)]  \tLoss:   92.571968\trec:   66.515625\tkl:   26.056345\n",
      "Epoch: 579 [40100/50000 (80%)]  \tLoss:   92.321579\trec:   66.839058\tkl:   25.482513\n",
      "====> Epoch: 579 Average train loss: 91.2794\n",
      "====> Validation set loss: 94.6351\n",
      "====> Validation set kl: 26.0260\n",
      "Epoch: 580 [  100/50000 ( 0%)]  \tLoss:   91.094337\trec:   65.470695\tkl:   25.623632\n",
      "Epoch: 580 [10100/50000 (20%)]  \tLoss:   89.418053\trec:   63.534405\tkl:   25.883652\n",
      "Epoch: 580 [20100/50000 (40%)]  \tLoss:   93.406113\trec:   67.653740\tkl:   25.752371\n",
      "Epoch: 580 [30100/50000 (60%)]  \tLoss:   87.868027\trec:   62.416035\tkl:   25.451994\n",
      "Epoch: 580 [40100/50000 (80%)]  \tLoss:   91.661385\trec:   65.948761\tkl:   25.712629\n",
      "====> Epoch: 580 Average train loss: 91.2953\n",
      "====> Validation set loss: 94.6770\n",
      "====> Validation set kl: 26.2864\n",
      "Epoch: 581 [  100/50000 ( 0%)]  \tLoss:   91.457146\trec:   64.835152\tkl:   26.622002\n",
      "Epoch: 581 [10100/50000 (20%)]  \tLoss:   91.198830\trec:   64.147949\tkl:   27.050873\n",
      "Epoch: 581 [20100/50000 (40%)]  \tLoss:   90.007828\trec:   63.955887\tkl:   26.051943\n",
      "Epoch: 581 [30100/50000 (60%)]  \tLoss:   89.437126\trec:   62.654919\tkl:   26.782202\n",
      "Epoch: 581 [40100/50000 (80%)]  \tLoss:   90.558136\trec:   64.587944\tkl:   25.970184\n",
      "====> Epoch: 581 Average train loss: 91.2623\n",
      "====> Validation set loss: 94.6029\n",
      "====> Validation set kl: 26.1756\n",
      "Epoch: 582 [  100/50000 ( 0%)]  \tLoss:   91.739449\trec:   66.964676\tkl:   24.774775\n",
      "Epoch: 582 [10100/50000 (20%)]  \tLoss:   92.081207\trec:   65.515388\tkl:   26.565817\n",
      "Epoch: 582 [20100/50000 (40%)]  \tLoss:   91.372704\trec:   64.404144\tkl:   26.968561\n",
      "Epoch: 582 [30100/50000 (60%)]  \tLoss:   84.432312\trec:   59.543007\tkl:   24.889309\n",
      "Epoch: 582 [40100/50000 (80%)]  \tLoss:   90.432442\trec:   64.069214\tkl:   26.363228\n",
      "====> Epoch: 582 Average train loss: 91.2550\n",
      "====> Validation set loss: 94.7594\n",
      "====> Validation set kl: 26.0061\n",
      "Epoch: 583 [  100/50000 ( 0%)]  \tLoss:   91.343315\trec:   65.550438\tkl:   25.792885\n",
      "Epoch: 583 [10100/50000 (20%)]  \tLoss:   91.087128\trec:   65.084518\tkl:   26.002611\n",
      "Epoch: 583 [20100/50000 (40%)]  \tLoss:   92.392067\trec:   65.337234\tkl:   27.054832\n",
      "Epoch: 583 [30100/50000 (60%)]  \tLoss:   90.839569\trec:   64.014297\tkl:   26.825266\n",
      "Epoch: 583 [40100/50000 (80%)]  \tLoss:   92.642052\trec:   66.247498\tkl:   26.394552\n",
      "====> Epoch: 583 Average train loss: 91.2875\n",
      "====> Validation set loss: 94.7723\n",
      "====> Validation set kl: 26.0747\n",
      "Epoch: 584 [  100/50000 ( 0%)]  \tLoss:   88.248375\trec:   62.802315\tkl:   25.446064\n",
      "Epoch: 584 [10100/50000 (20%)]  \tLoss:   91.946548\trec:   65.738121\tkl:   26.208426\n",
      "Epoch: 584 [20100/50000 (40%)]  \tLoss:   91.214371\trec:   64.984322\tkl:   26.230043\n",
      "Epoch: 584 [30100/50000 (60%)]  \tLoss:   92.564400\trec:   66.123817\tkl:   26.440586\n",
      "Epoch: 584 [40100/50000 (80%)]  \tLoss:   87.554977\trec:   62.050068\tkl:   25.504917\n",
      "====> Epoch: 584 Average train loss: 91.2549\n",
      "====> Validation set loss: 94.6287\n",
      "====> Validation set kl: 26.1880\n",
      "Epoch: 585 [  100/50000 ( 0%)]  \tLoss:   92.543083\trec:   66.789513\tkl:   25.753578\n",
      "Epoch: 585 [10100/50000 (20%)]  \tLoss:   88.339714\trec:   61.963261\tkl:   26.376457\n",
      "Epoch: 585 [20100/50000 (40%)]  \tLoss:   90.765114\trec:   64.837898\tkl:   25.927221\n",
      "Epoch: 585 [30100/50000 (60%)]  \tLoss:   90.375740\trec:   64.189629\tkl:   26.186108\n",
      "Epoch: 585 [40100/50000 (80%)]  \tLoss:   91.835327\trec:   65.394646\tkl:   26.440681\n",
      "====> Epoch: 585 Average train loss: 91.2744\n",
      "====> Validation set loss: 94.7865\n",
      "====> Validation set kl: 26.1224\n",
      "Epoch: 586 [  100/50000 ( 0%)]  \tLoss:   87.347595\trec:   61.615704\tkl:   25.731890\n",
      "Epoch: 586 [10100/50000 (20%)]  \tLoss:   88.962784\trec:   63.586208\tkl:   25.376572\n",
      "Epoch: 586 [20100/50000 (40%)]  \tLoss:   91.262184\trec:   65.957603\tkl:   25.304585\n",
      "Epoch: 586 [30100/50000 (60%)]  \tLoss:   87.531540\trec:   62.207176\tkl:   25.324362\n",
      "Epoch: 586 [40100/50000 (80%)]  \tLoss:   90.713539\trec:   64.679268\tkl:   26.034277\n",
      "====> Epoch: 586 Average train loss: 91.2691\n",
      "====> Validation set loss: 94.7585\n",
      "====> Validation set kl: 26.0867\n",
      "Epoch: 587 [  100/50000 ( 0%)]  \tLoss:   87.438644\trec:   61.957035\tkl:   25.481607\n",
      "Epoch: 587 [10100/50000 (20%)]  \tLoss:   88.222870\trec:   62.120476\tkl:   26.102394\n",
      "Epoch: 587 [20100/50000 (40%)]  \tLoss:   90.747353\trec:   64.115631\tkl:   26.631718\n",
      "Epoch: 587 [30100/50000 (60%)]  \tLoss:   92.144722\trec:   65.988503\tkl:   26.156216\n",
      "Epoch: 587 [40100/50000 (80%)]  \tLoss:   93.179787\trec:   66.266129\tkl:   26.913652\n",
      "====> Epoch: 587 Average train loss: 91.2518\n",
      "====> Validation set loss: 94.6962\n",
      "====> Validation set kl: 26.1296\n",
      "Epoch: 588 [  100/50000 ( 0%)]  \tLoss:   88.510475\trec:   62.924530\tkl:   25.585949\n",
      "Epoch: 588 [10100/50000 (20%)]  \tLoss:   89.783028\trec:   63.427963\tkl:   26.355068\n",
      "Epoch: 588 [20100/50000 (40%)]  \tLoss:   92.037384\trec:   65.283348\tkl:   26.754030\n",
      "Epoch: 588 [30100/50000 (60%)]  \tLoss:   92.024216\trec:   65.540779\tkl:   26.483433\n",
      "Epoch: 588 [40100/50000 (80%)]  \tLoss:   94.176132\trec:   67.649918\tkl:   26.526215\n",
      "====> Epoch: 588 Average train loss: 91.2410\n",
      "====> Validation set loss: 94.7300\n",
      "====> Validation set kl: 26.1551\n",
      "Epoch: 589 [  100/50000 ( 0%)]  \tLoss:   90.210594\trec:   63.965267\tkl:   26.245327\n",
      "Epoch: 589 [10100/50000 (20%)]  \tLoss:   92.117050\trec:   65.723114\tkl:   26.393930\n",
      "Epoch: 589 [20100/50000 (40%)]  \tLoss:   88.768074\trec:   63.168816\tkl:   25.599257\n",
      "Epoch: 589 [30100/50000 (60%)]  \tLoss:   92.427246\trec:   66.273048\tkl:   26.154198\n",
      "Epoch: 589 [40100/50000 (80%)]  \tLoss:   90.702400\trec:   64.134605\tkl:   26.567802\n",
      "====> Epoch: 589 Average train loss: 91.2338\n",
      "====> Validation set loss: 94.6648\n",
      "====> Validation set kl: 26.3346\n",
      "Epoch: 590 [  100/50000 ( 0%)]  \tLoss:   90.258461\trec:   63.958252\tkl:   26.300211\n",
      "Epoch: 590 [10100/50000 (20%)]  \tLoss:   92.531883\trec:   66.862457\tkl:   25.669424\n",
      "Epoch: 590 [20100/50000 (40%)]  \tLoss:   90.365112\trec:   65.426552\tkl:   24.938566\n",
      "Epoch: 590 [30100/50000 (60%)]  \tLoss:   92.097458\trec:   66.028656\tkl:   26.068800\n",
      "Epoch: 590 [40100/50000 (80%)]  \tLoss:   90.593178\trec:   65.083084\tkl:   25.510101\n",
      "====> Epoch: 590 Average train loss: 91.2393\n",
      "====> Validation set loss: 94.6171\n",
      "====> Validation set kl: 26.2004\n",
      "Epoch: 591 [  100/50000 ( 0%)]  \tLoss:   89.438660\trec:   63.933216\tkl:   25.505442\n",
      "Epoch: 591 [10100/50000 (20%)]  \tLoss:   91.957664\trec:   65.414841\tkl:   26.542820\n",
      "Epoch: 591 [20100/50000 (40%)]  \tLoss:   89.108498\trec:   62.865623\tkl:   26.242868\n",
      "Epoch: 591 [30100/50000 (60%)]  \tLoss:   89.769112\trec:   63.807381\tkl:   25.961731\n",
      "Epoch: 591 [40100/50000 (80%)]  \tLoss:   93.691582\trec:   66.860603\tkl:   26.830971\n",
      "====> Epoch: 591 Average train loss: 91.2299\n",
      "====> Validation set loss: 94.6208\n",
      "====> Validation set kl: 26.0508\n",
      "Epoch: 592 [  100/50000 ( 0%)]  \tLoss:   92.151573\trec:   65.598785\tkl:   26.552786\n",
      "Epoch: 592 [10100/50000 (20%)]  \tLoss:   90.766060\trec:   63.910145\tkl:   26.855919\n",
      "Epoch: 592 [20100/50000 (40%)]  \tLoss:   91.215385\trec:   65.470581\tkl:   25.744804\n",
      "Epoch: 592 [30100/50000 (60%)]  \tLoss:   88.792389\trec:   63.520008\tkl:   25.272385\n",
      "Epoch: 592 [40100/50000 (80%)]  \tLoss:   90.996407\trec:   65.902008\tkl:   25.094391\n",
      "====> Epoch: 592 Average train loss: 91.2384\n",
      "====> Validation set loss: 94.7300\n",
      "====> Validation set kl: 26.0661\n",
      "Epoch: 593 [  100/50000 ( 0%)]  \tLoss:   93.895889\trec:   66.904465\tkl:   26.991425\n",
      "Epoch: 593 [10100/50000 (20%)]  \tLoss:   93.480782\trec:   66.578857\tkl:   26.901924\n",
      "Epoch: 593 [20100/50000 (40%)]  \tLoss:   91.488708\trec:   65.652443\tkl:   25.836273\n",
      "Epoch: 593 [30100/50000 (60%)]  \tLoss:   91.267654\trec:   65.416672\tkl:   25.850983\n",
      "Epoch: 593 [40100/50000 (80%)]  \tLoss:   87.172592\trec:   61.368896\tkl:   25.803698\n",
      "====> Epoch: 593 Average train loss: 91.2282\n",
      "====> Validation set loss: 94.7479\n",
      "====> Validation set kl: 26.0551\n",
      "Epoch: 594 [  100/50000 ( 0%)]  \tLoss:   95.254608\trec:   67.994446\tkl:   27.260166\n",
      "Epoch: 594 [10100/50000 (20%)]  \tLoss:   90.594582\trec:   64.987747\tkl:   25.606829\n",
      "Epoch: 594 [20100/50000 (40%)]  \tLoss:   90.331459\trec:   65.033081\tkl:   25.298388\n",
      "Epoch: 594 [30100/50000 (60%)]  \tLoss:   93.959999\trec:   67.517830\tkl:   26.442162\n",
      "Epoch: 594 [40100/50000 (80%)]  \tLoss:   92.449890\trec:   66.265831\tkl:   26.184059\n",
      "====> Epoch: 594 Average train loss: 91.2387\n",
      "====> Validation set loss: 94.7201\n",
      "====> Validation set kl: 26.0807\n",
      "Epoch: 595 [  100/50000 ( 0%)]  \tLoss:   95.070953\trec:   68.564903\tkl:   26.506054\n",
      "Epoch: 595 [10100/50000 (20%)]  \tLoss:   87.766266\trec:   62.182362\tkl:   25.583902\n",
      "Epoch: 595 [20100/50000 (40%)]  \tLoss:   89.116432\trec:   63.348946\tkl:   25.767487\n",
      "Epoch: 595 [30100/50000 (60%)]  \tLoss:   88.958809\trec:   62.927250\tkl:   26.031555\n",
      "Epoch: 595 [40100/50000 (80%)]  \tLoss:   96.172615\trec:   69.282349\tkl:   26.890259\n",
      "====> Epoch: 595 Average train loss: 91.2060\n",
      "====> Validation set loss: 94.7365\n",
      "====> Validation set kl: 26.2160\n",
      "Epoch: 596 [  100/50000 ( 0%)]  \tLoss:   93.685059\trec:   66.461876\tkl:   27.223183\n",
      "Epoch: 596 [10100/50000 (20%)]  \tLoss:   88.817101\trec:   63.174198\tkl:   25.642902\n",
      "Epoch: 596 [20100/50000 (40%)]  \tLoss:   93.899139\trec:   67.230324\tkl:   26.668819\n",
      "Epoch: 596 [30100/50000 (60%)]  \tLoss:   90.714684\trec:   64.749580\tkl:   25.965101\n",
      "Epoch: 596 [40100/50000 (80%)]  \tLoss:   90.712067\trec:   64.445236\tkl:   26.266836\n",
      "====> Epoch: 596 Average train loss: 91.2331\n",
      "====> Validation set loss: 94.7805\n",
      "====> Validation set kl: 26.1212\n",
      "Epoch: 597 [  100/50000 ( 0%)]  \tLoss:   89.796196\trec:   64.235298\tkl:   25.560905\n",
      "Epoch: 597 [10100/50000 (20%)]  \tLoss:   90.859489\trec:   64.640747\tkl:   26.218740\n",
      "Epoch: 597 [20100/50000 (40%)]  \tLoss:   91.901970\trec:   65.425087\tkl:   26.476885\n",
      "Epoch: 597 [30100/50000 (60%)]  \tLoss:   86.034592\trec:   61.829693\tkl:   24.204895\n",
      "Epoch: 597 [40100/50000 (80%)]  \tLoss:   90.147789\trec:   64.570740\tkl:   25.577051\n",
      "====> Epoch: 597 Average train loss: 91.2006\n",
      "====> Validation set loss: 94.6433\n",
      "====> Validation set kl: 26.0526\n",
      "Epoch: 598 [  100/50000 ( 0%)]  \tLoss:   91.095497\trec:   64.895973\tkl:   26.199524\n",
      "Epoch: 598 [10100/50000 (20%)]  \tLoss:   91.769531\trec:   65.686035\tkl:   26.083488\n",
      "Epoch: 598 [20100/50000 (40%)]  \tLoss:   89.449638\trec:   63.076618\tkl:   26.373014\n",
      "Epoch: 598 [30100/50000 (60%)]  \tLoss:   89.097176\trec:   63.574177\tkl:   25.522995\n",
      "Epoch: 598 [40100/50000 (80%)]  \tLoss:   85.223114\trec:   59.431976\tkl:   25.791138\n",
      "====> Epoch: 598 Average train loss: 91.2211\n",
      "====> Validation set loss: 94.6240\n",
      "====> Validation set kl: 26.3800\n",
      "Epoch: 599 [  100/50000 ( 0%)]  \tLoss:   90.577896\trec:   64.749313\tkl:   25.828581\n",
      "Epoch: 599 [10100/50000 (20%)]  \tLoss:   89.989052\trec:   63.844624\tkl:   26.144428\n",
      "Epoch: 599 [20100/50000 (40%)]  \tLoss:   86.026642\trec:   60.530243\tkl:   25.496391\n",
      "Epoch: 599 [30100/50000 (60%)]  \tLoss:   93.074936\trec:   66.776283\tkl:   26.298658\n",
      "Epoch: 599 [40100/50000 (80%)]  \tLoss:   89.008575\trec:   62.990349\tkl:   26.018219\n",
      "====> Epoch: 599 Average train loss: 91.1967\n",
      "====> Validation set loss: 94.6908\n",
      "====> Validation set kl: 26.0572\n",
      "Epoch: 600 [  100/50000 ( 0%)]  \tLoss:   93.354744\trec:   66.325821\tkl:   27.028923\n",
      "Epoch: 600 [10100/50000 (20%)]  \tLoss:   92.610855\trec:   66.587555\tkl:   26.023298\n",
      "Epoch: 600 [20100/50000 (40%)]  \tLoss:   89.070045\trec:   63.309593\tkl:   25.760454\n",
      "Epoch: 600 [30100/50000 (60%)]  \tLoss:   92.086540\trec:   65.865242\tkl:   26.221304\n",
      "Epoch: 600 [40100/50000 (80%)]  \tLoss:   89.814156\trec:   63.308483\tkl:   26.505678\n",
      "====> Epoch: 600 Average train loss: 91.2174\n",
      "====> Validation set loss: 94.6679\n",
      "====> Validation set kl: 25.9204\n",
      "Epoch: 601 [  100/50000 ( 0%)]  \tLoss:   86.768669\trec:   60.174213\tkl:   26.594458\n",
      "Epoch: 601 [10100/50000 (20%)]  \tLoss:   91.914139\trec:   66.190681\tkl:   25.723459\n",
      "Epoch: 601 [20100/50000 (40%)]  \tLoss:   88.578941\trec:   62.818787\tkl:   25.760151\n",
      "Epoch: 601 [30100/50000 (60%)]  \tLoss:   89.383492\trec:   63.489502\tkl:   25.893995\n",
      "Epoch: 601 [40100/50000 (80%)]  \tLoss:   90.563446\trec:   64.910835\tkl:   25.652605\n",
      "====> Epoch: 601 Average train loss: 91.1996\n",
      "====> Validation set loss: 94.6093\n",
      "====> Validation set kl: 25.9767\n",
      "Epoch: 602 [  100/50000 ( 0%)]  \tLoss:   94.757431\trec:   68.464890\tkl:   26.292542\n",
      "Epoch: 602 [10100/50000 (20%)]  \tLoss:   89.681129\trec:   63.671978\tkl:   26.009157\n",
      "Epoch: 602 [20100/50000 (40%)]  \tLoss:   90.599464\trec:   64.893707\tkl:   25.705751\n",
      "Epoch: 602 [30100/50000 (60%)]  \tLoss:   92.089813\trec:   66.132492\tkl:   25.957321\n",
      "Epoch: 602 [40100/50000 (80%)]  \tLoss:   91.227638\trec:   65.443153\tkl:   25.784481\n",
      "====> Epoch: 602 Average train loss: 91.1960\n",
      "====> Validation set loss: 94.6530\n",
      "====> Validation set kl: 26.2688\n",
      "Epoch: 603 [  100/50000 ( 0%)]  \tLoss:   88.108849\trec:   62.209274\tkl:   25.899572\n",
      "Epoch: 603 [10100/50000 (20%)]  \tLoss:   90.785072\trec:   64.800026\tkl:   25.985046\n",
      "Epoch: 603 [20100/50000 (40%)]  \tLoss:   88.951088\trec:   63.074329\tkl:   25.876757\n",
      "Epoch: 603 [30100/50000 (60%)]  \tLoss:   94.420624\trec:   68.696030\tkl:   25.724600\n",
      "Epoch: 603 [40100/50000 (80%)]  \tLoss:   91.479179\trec:   65.137398\tkl:   26.341776\n",
      "====> Epoch: 603 Average train loss: 91.1975\n",
      "====> Validation set loss: 94.7437\n",
      "====> Validation set kl: 26.1192\n",
      "Epoch: 604 [  100/50000 ( 0%)]  \tLoss:   90.196251\trec:   64.759087\tkl:   25.437162\n",
      "Epoch: 604 [10100/50000 (20%)]  \tLoss:   88.546600\trec:   62.404636\tkl:   26.141964\n",
      "Epoch: 604 [20100/50000 (40%)]  \tLoss:   93.246941\trec:   67.292702\tkl:   25.954243\n",
      "Epoch: 604 [30100/50000 (60%)]  \tLoss:   90.372475\trec:   64.635254\tkl:   25.737221\n",
      "Epoch: 604 [40100/50000 (80%)]  \tLoss:   89.407417\trec:   63.123592\tkl:   26.283833\n",
      "====> Epoch: 604 Average train loss: 91.1923\n",
      "====> Validation set loss: 94.7005\n",
      "====> Validation set kl: 26.0614\n",
      "Epoch: 605 [  100/50000 ( 0%)]  \tLoss:   96.486206\trec:   69.481880\tkl:   27.004330\n",
      "Epoch: 605 [10100/50000 (20%)]  \tLoss:   90.689453\trec:   64.379517\tkl:   26.309937\n",
      "Epoch: 605 [20100/50000 (40%)]  \tLoss:   92.638359\trec:   66.290276\tkl:   26.348078\n",
      "Epoch: 605 [30100/50000 (60%)]  \tLoss:   89.105659\trec:   63.163017\tkl:   25.942644\n",
      "Epoch: 605 [40100/50000 (80%)]  \tLoss:   88.340508\trec:   62.489071\tkl:   25.851437\n",
      "====> Epoch: 605 Average train loss: 91.1794\n",
      "====> Validation set loss: 94.7097\n",
      "====> Validation set kl: 26.2580\n",
      "Epoch: 606 [  100/50000 ( 0%)]  \tLoss:   90.201836\trec:   64.220215\tkl:   25.981625\n",
      "Epoch: 606 [10100/50000 (20%)]  \tLoss:   89.435410\trec:   63.518322\tkl:   25.917080\n",
      "Epoch: 606 [20100/50000 (40%)]  \tLoss:   90.208511\trec:   63.572529\tkl:   26.635988\n",
      "Epoch: 606 [30100/50000 (60%)]  \tLoss:   92.994492\trec:   66.588966\tkl:   26.405531\n",
      "Epoch: 606 [40100/50000 (80%)]  \tLoss:   90.714882\trec:   64.287872\tkl:   26.427015\n",
      "====> Epoch: 606 Average train loss: 91.1863\n",
      "====> Validation set loss: 94.5642\n",
      "====> Validation set kl: 26.1461\n",
      "Epoch: 607 [  100/50000 ( 0%)]  \tLoss:   88.651695\trec:   63.093452\tkl:   25.558241\n",
      "Epoch: 607 [10100/50000 (20%)]  \tLoss:   89.910477\trec:   63.815742\tkl:   26.094738\n",
      "Epoch: 607 [20100/50000 (40%)]  \tLoss:   92.779297\trec:   67.022369\tkl:   25.756927\n",
      "Epoch: 607 [30100/50000 (60%)]  \tLoss:   87.416069\trec:   61.671638\tkl:   25.744429\n",
      "Epoch: 607 [40100/50000 (80%)]  \tLoss:   86.505493\trec:   61.102764\tkl:   25.402737\n",
      "====> Epoch: 607 Average train loss: 91.1717\n",
      "====> Validation set loss: 94.6457\n",
      "====> Validation set kl: 26.3031\n",
      "Epoch: 608 [  100/50000 ( 0%)]  \tLoss:   90.202965\trec:   64.364655\tkl:   25.838305\n",
      "Epoch: 608 [10100/50000 (20%)]  \tLoss:   91.114609\trec:   65.749168\tkl:   25.365437\n",
      "Epoch: 608 [20100/50000 (40%)]  \tLoss:   92.768654\trec:   65.923172\tkl:   26.845476\n",
      "Epoch: 608 [30100/50000 (60%)]  \tLoss:   90.681519\trec:   64.280899\tkl:   26.400618\n",
      "Epoch: 608 [40100/50000 (80%)]  \tLoss:   95.772339\trec:   68.584122\tkl:   27.188211\n",
      "====> Epoch: 608 Average train loss: 91.1848\n",
      "====> Validation set loss: 94.5722\n",
      "====> Validation set kl: 26.0754\n",
      "Epoch: 609 [  100/50000 ( 0%)]  \tLoss:   89.505211\trec:   64.583473\tkl:   24.921740\n",
      "Epoch: 609 [10100/50000 (20%)]  \tLoss:   91.120987\trec:   65.039581\tkl:   26.081398\n",
      "Epoch: 609 [20100/50000 (40%)]  \tLoss:   94.389015\trec:   67.097588\tkl:   27.291428\n",
      "Epoch: 609 [30100/50000 (60%)]  \tLoss:   90.341537\trec:   64.069717\tkl:   26.271820\n",
      "Epoch: 609 [40100/50000 (80%)]  \tLoss:   89.158150\trec:   63.595741\tkl:   25.562412\n",
      "====> Epoch: 609 Average train loss: 91.1693\n",
      "====> Validation set loss: 94.6573\n",
      "====> Validation set kl: 26.2907\n",
      "Epoch: 610 [  100/50000 ( 0%)]  \tLoss:   89.381248\trec:   63.317719\tkl:   26.063528\n",
      "Epoch: 610 [10100/50000 (20%)]  \tLoss:   90.816383\trec:   64.198944\tkl:   26.617441\n",
      "Epoch: 610 [20100/50000 (40%)]  \tLoss:   92.369957\trec:   66.079758\tkl:   26.290201\n",
      "Epoch: 610 [30100/50000 (60%)]  \tLoss:   90.769516\trec:   64.390984\tkl:   26.378536\n",
      "Epoch: 610 [40100/50000 (80%)]  \tLoss:   91.519470\trec:   65.163620\tkl:   26.355848\n",
      "====> Epoch: 610 Average train loss: 91.1573\n",
      "====> Validation set loss: 94.7381\n",
      "====> Validation set kl: 26.1136\n",
      "Epoch: 611 [  100/50000 ( 0%)]  \tLoss:   91.709190\trec:   65.605301\tkl:   26.103889\n",
      "Epoch: 611 [10100/50000 (20%)]  \tLoss:   94.495644\trec:   67.727295\tkl:   26.768349\n",
      "Epoch: 611 [20100/50000 (40%)]  \tLoss:   90.531303\trec:   64.392387\tkl:   26.138918\n",
      "Epoch: 611 [30100/50000 (60%)]  \tLoss:   89.816566\trec:   64.115265\tkl:   25.701305\n",
      "Epoch: 611 [40100/50000 (80%)]  \tLoss:   87.282990\trec:   62.539761\tkl:   24.743221\n",
      "====> Epoch: 611 Average train loss: 91.1796\n",
      "====> Validation set loss: 94.6562\n",
      "====> Validation set kl: 26.0708\n",
      "Epoch: 612 [  100/50000 ( 0%)]  \tLoss:   92.992706\trec:   66.590553\tkl:   26.402145\n",
      "Epoch: 612 [10100/50000 (20%)]  \tLoss:   92.886063\trec:   67.328102\tkl:   25.557959\n",
      "Epoch: 612 [20100/50000 (40%)]  \tLoss:   91.942017\trec:   65.809937\tkl:   26.132082\n",
      "Epoch: 612 [30100/50000 (60%)]  \tLoss:   87.122673\trec:   61.855888\tkl:   25.266781\n",
      "Epoch: 612 [40100/50000 (80%)]  \tLoss:   93.504898\trec:   66.706383\tkl:   26.798513\n",
      "====> Epoch: 612 Average train loss: 91.1629\n",
      "====> Validation set loss: 94.6264\n",
      "====> Validation set kl: 26.1325\n",
      "Epoch: 613 [  100/50000 ( 0%)]  \tLoss:   92.501938\trec:   65.888557\tkl:   26.613380\n",
      "Epoch: 613 [10100/50000 (20%)]  \tLoss:   92.574760\trec:   66.244598\tkl:   26.330160\n",
      "Epoch: 613 [20100/50000 (40%)]  \tLoss:   89.121384\trec:   63.186550\tkl:   25.934837\n",
      "Epoch: 613 [30100/50000 (60%)]  \tLoss:   93.504356\trec:   66.263016\tkl:   27.241335\n",
      "Epoch: 613 [40100/50000 (80%)]  \tLoss:   92.787994\trec:   66.179092\tkl:   26.608904\n",
      "====> Epoch: 613 Average train loss: 91.1403\n",
      "====> Validation set loss: 94.7194\n",
      "====> Validation set kl: 26.0859\n",
      "Epoch: 614 [  100/50000 ( 0%)]  \tLoss:   93.471436\trec:   67.337273\tkl:   26.134165\n",
      "Epoch: 614 [10100/50000 (20%)]  \tLoss:   89.936813\trec:   63.883545\tkl:   26.053267\n",
      "Epoch: 614 [20100/50000 (40%)]  \tLoss:   91.190041\trec:   64.415855\tkl:   26.774178\n",
      "Epoch: 614 [30100/50000 (60%)]  \tLoss:   92.186874\trec:   66.165230\tkl:   26.021645\n",
      "Epoch: 614 [40100/50000 (80%)]  \tLoss:   92.462341\trec:   65.788994\tkl:   26.673347\n",
      "====> Epoch: 614 Average train loss: 91.1470\n",
      "====> Validation set loss: 94.6516\n",
      "====> Validation set kl: 26.1998\n",
      "Epoch: 615 [  100/50000 ( 0%)]  \tLoss:   93.298767\trec:   65.955925\tkl:   27.342848\n",
      "Epoch: 615 [10100/50000 (20%)]  \tLoss:   94.359962\trec:   67.752129\tkl:   26.607826\n",
      "Epoch: 615 [20100/50000 (40%)]  \tLoss:   92.516533\trec:   66.176514\tkl:   26.340017\n",
      "Epoch: 615 [30100/50000 (60%)]  \tLoss:   88.773727\trec:   63.081848\tkl:   25.691874\n",
      "Epoch: 615 [40100/50000 (80%)]  \tLoss:   93.961967\trec:   67.480812\tkl:   26.481159\n",
      "====> Epoch: 615 Average train loss: 91.1311\n",
      "====> Validation set loss: 94.6751\n",
      "====> Validation set kl: 26.0681\n",
      "Epoch: 616 [  100/50000 ( 0%)]  \tLoss:   96.305008\trec:   69.008598\tkl:   27.296404\n",
      "Epoch: 616 [10100/50000 (20%)]  \tLoss:   87.654587\trec:   62.117771\tkl:   25.536818\n",
      "Epoch: 616 [20100/50000 (40%)]  \tLoss:   93.245255\trec:   66.302208\tkl:   26.943041\n",
      "Epoch: 616 [30100/50000 (60%)]  \tLoss:   93.340469\trec:   66.693192\tkl:   26.647274\n",
      "Epoch: 616 [40100/50000 (80%)]  \tLoss:   93.067101\trec:   66.107231\tkl:   26.959866\n",
      "====> Epoch: 616 Average train loss: 91.1286\n",
      "====> Validation set loss: 94.5727\n",
      "====> Validation set kl: 26.0561\n",
      "Epoch: 617 [  100/50000 ( 0%)]  \tLoss:   88.809746\trec:   62.783718\tkl:   26.026028\n",
      "Epoch: 617 [10100/50000 (20%)]  \tLoss:   91.650681\trec:   65.405029\tkl:   26.245649\n",
      "Epoch: 617 [20100/50000 (40%)]  \tLoss:   91.916435\trec:   65.834938\tkl:   26.081495\n",
      "Epoch: 617 [30100/50000 (60%)]  \tLoss:   95.014473\trec:   68.042358\tkl:   26.972107\n",
      "Epoch: 617 [40100/50000 (80%)]  \tLoss:   89.027557\trec:   63.718075\tkl:   25.309479\n",
      "====> Epoch: 617 Average train loss: 91.1301\n",
      "====> Validation set loss: 94.6567\n",
      "====> Validation set kl: 25.9579\n",
      "Epoch: 618 [  100/50000 ( 0%)]  \tLoss:   90.839989\trec:   65.304817\tkl:   25.535170\n",
      "Epoch: 618 [10100/50000 (20%)]  \tLoss:   88.793777\trec:   62.301205\tkl:   26.492571\n",
      "Epoch: 618 [20100/50000 (40%)]  \tLoss:   86.200699\trec:   61.835808\tkl:   24.364889\n",
      "Epoch: 618 [30100/50000 (60%)]  \tLoss:   91.006920\trec:   64.449562\tkl:   26.557364\n",
      "Epoch: 618 [40100/50000 (80%)]  \tLoss:   94.203865\trec:   67.539474\tkl:   26.664398\n",
      "====> Epoch: 618 Average train loss: 91.1354\n",
      "====> Validation set loss: 94.6754\n",
      "====> Validation set kl: 26.3478\n",
      "Epoch: 619 [  100/50000 ( 0%)]  \tLoss:   92.327553\trec:   65.738396\tkl:   26.589167\n",
      "Epoch: 619 [10100/50000 (20%)]  \tLoss:   89.078064\trec:   63.732216\tkl:   25.345844\n",
      "Epoch: 619 [20100/50000 (40%)]  \tLoss:   90.126755\trec:   64.498650\tkl:   25.628109\n",
      "Epoch: 619 [30100/50000 (60%)]  \tLoss:   92.112450\trec:   66.232048\tkl:   25.880407\n",
      "Epoch: 619 [40100/50000 (80%)]  \tLoss:   93.259399\trec:   66.305405\tkl:   26.953997\n",
      "====> Epoch: 619 Average train loss: 91.1241\n",
      "====> Validation set loss: 94.7046\n",
      "====> Validation set kl: 26.1731\n",
      "Epoch: 620 [  100/50000 ( 0%)]  \tLoss:   90.267632\trec:   63.986763\tkl:   26.280869\n",
      "Epoch: 620 [10100/50000 (20%)]  \tLoss:   92.614517\trec:   65.936775\tkl:   26.677746\n",
      "Epoch: 620 [20100/50000 (40%)]  \tLoss:   91.176918\trec:   64.951538\tkl:   26.225382\n",
      "Epoch: 620 [30100/50000 (60%)]  \tLoss:   86.688026\trec:   61.843594\tkl:   24.844435\n",
      "Epoch: 620 [40100/50000 (80%)]  \tLoss:   90.775322\trec:   64.622673\tkl:   26.152643\n",
      "====> Epoch: 620 Average train loss: 91.1415\n",
      "====> Validation set loss: 94.6534\n",
      "====> Validation set kl: 26.2195\n",
      "Epoch: 621 [  100/50000 ( 0%)]  \tLoss:   86.868561\trec:   62.417038\tkl:   24.451523\n",
      "Epoch: 621 [10100/50000 (20%)]  \tLoss:   91.515503\trec:   65.357994\tkl:   26.157518\n",
      "Epoch: 621 [20100/50000 (40%)]  \tLoss:   93.298744\trec:   66.666626\tkl:   26.632120\n",
      "Epoch: 621 [30100/50000 (60%)]  \tLoss:   89.545326\trec:   63.310757\tkl:   26.234575\n",
      "Epoch: 621 [40100/50000 (80%)]  \tLoss:   87.174606\trec:   61.816936\tkl:   25.357670\n",
      "====> Epoch: 621 Average train loss: 91.1283\n",
      "====> Validation set loss: 94.7275\n",
      "====> Validation set kl: 26.0609\n",
      "Epoch: 622 [  100/50000 ( 0%)]  \tLoss:   91.930817\trec:   65.942375\tkl:   25.988447\n",
      "Epoch: 622 [10100/50000 (20%)]  \tLoss:   90.281647\trec:   64.007095\tkl:   26.274549\n",
      "Epoch: 622 [20100/50000 (40%)]  \tLoss:   89.244835\trec:   63.640259\tkl:   25.604574\n",
      "Epoch: 622 [30100/50000 (60%)]  \tLoss:   90.035019\trec:   64.336746\tkl:   25.698277\n",
      "Epoch: 622 [40100/50000 (80%)]  \tLoss:   90.049965\trec:   63.420536\tkl:   26.629435\n",
      "====> Epoch: 622 Average train loss: 91.1204\n",
      "====> Validation set loss: 94.5897\n",
      "====> Validation set kl: 26.0474\n",
      "Epoch: 623 [  100/50000 ( 0%)]  \tLoss:   95.792809\trec:   68.828705\tkl:   26.964111\n",
      "Epoch: 623 [10100/50000 (20%)]  \tLoss:   90.155075\trec:   63.661510\tkl:   26.493568\n",
      "Epoch: 623 [20100/50000 (40%)]  \tLoss:   90.077301\trec:   63.630440\tkl:   26.446867\n",
      "Epoch: 623 [30100/50000 (60%)]  \tLoss:   89.259140\trec:   63.578934\tkl:   25.680210\n",
      "Epoch: 623 [40100/50000 (80%)]  \tLoss:   89.354561\trec:   63.454227\tkl:   25.900330\n",
      "====> Epoch: 623 Average train loss: 91.1095\n",
      "====> Validation set loss: 94.7344\n",
      "====> Validation set kl: 26.0481\n",
      "Epoch: 624 [  100/50000 ( 0%)]  \tLoss:   91.489403\trec:   65.166473\tkl:   26.322929\n",
      "Epoch: 624 [10100/50000 (20%)]  \tLoss:   91.721016\trec:   65.560181\tkl:   26.160835\n",
      "Epoch: 624 [20100/50000 (40%)]  \tLoss:   91.102684\trec:   65.111084\tkl:   25.991596\n",
      "Epoch: 624 [30100/50000 (60%)]  \tLoss:   90.818932\trec:   65.753365\tkl:   25.065571\n",
      "Epoch: 624 [40100/50000 (80%)]  \tLoss:   94.229088\trec:   67.070984\tkl:   27.158108\n",
      "====> Epoch: 624 Average train loss: 91.1152\n",
      "====> Validation set loss: 94.6507\n",
      "====> Validation set kl: 26.2158\n",
      "Epoch: 625 [  100/50000 ( 0%)]  \tLoss:   88.829391\trec:   62.960430\tkl:   25.868961\n",
      "Epoch: 625 [10100/50000 (20%)]  \tLoss:   90.607437\trec:   64.354614\tkl:   26.252827\n",
      "Epoch: 625 [20100/50000 (40%)]  \tLoss:   91.644630\trec:   65.170952\tkl:   26.473673\n",
      "Epoch: 625 [30100/50000 (60%)]  \tLoss:   88.779121\trec:   63.426746\tkl:   25.352377\n",
      "Epoch: 625 [40100/50000 (80%)]  \tLoss:   91.834297\trec:   65.473320\tkl:   26.360981\n",
      "====> Epoch: 625 Average train loss: 91.1170\n",
      "====> Validation set loss: 94.5530\n",
      "====> Validation set kl: 26.1111\n",
      "Epoch: 626 [  100/50000 ( 0%)]  \tLoss:   90.658913\trec:   64.894135\tkl:   25.764782\n",
      "Epoch: 626 [10100/50000 (20%)]  \tLoss:   91.283218\trec:   65.238136\tkl:   26.045084\n",
      "Epoch: 626 [20100/50000 (40%)]  \tLoss:   89.888855\trec:   63.562920\tkl:   26.325939\n",
      "Epoch: 626 [30100/50000 (60%)]  \tLoss:   91.595200\trec:   65.359314\tkl:   26.235886\n",
      "Epoch: 626 [40100/50000 (80%)]  \tLoss:   90.218376\trec:   64.468819\tkl:   25.749565\n",
      "====> Epoch: 626 Average train loss: 91.1033\n",
      "====> Validation set loss: 94.6580\n",
      "====> Validation set kl: 26.1581\n",
      "Epoch: 627 [  100/50000 ( 0%)]  \tLoss:   93.143829\trec:   66.684113\tkl:   26.459711\n",
      "Epoch: 627 [10100/50000 (20%)]  \tLoss:   94.344917\trec:   67.851562\tkl:   26.493357\n",
      "Epoch: 627 [20100/50000 (40%)]  \tLoss:   94.048706\trec:   66.974014\tkl:   27.074694\n",
      "Epoch: 627 [30100/50000 (60%)]  \tLoss:   95.535759\trec:   68.745255\tkl:   26.790497\n",
      "Epoch: 627 [40100/50000 (80%)]  \tLoss:   93.459213\trec:   67.527908\tkl:   25.931309\n",
      "====> Epoch: 627 Average train loss: 91.1113\n",
      "====> Validation set loss: 94.7294\n",
      "====> Validation set kl: 26.3230\n",
      "Epoch: 628 [  100/50000 ( 0%)]  \tLoss:   91.550308\trec:   65.011627\tkl:   26.538675\n",
      "Epoch: 628 [10100/50000 (20%)]  \tLoss:   92.358688\trec:   65.300056\tkl:   27.058630\n",
      "Epoch: 628 [20100/50000 (40%)]  \tLoss:   90.788406\trec:   65.055397\tkl:   25.733006\n",
      "Epoch: 628 [30100/50000 (60%)]  \tLoss:   91.469986\trec:   65.420898\tkl:   26.049089\n",
      "Epoch: 628 [40100/50000 (80%)]  \tLoss:   88.972809\trec:   63.006824\tkl:   25.965990\n",
      "====> Epoch: 628 Average train loss: 91.0939\n",
      "====> Validation set loss: 94.6745\n",
      "====> Validation set kl: 26.0588\n",
      "Epoch: 629 [  100/50000 ( 0%)]  \tLoss:   93.091904\trec:   67.008743\tkl:   26.083158\n",
      "Epoch: 629 [10100/50000 (20%)]  \tLoss:   92.684723\trec:   66.018913\tkl:   26.665810\n",
      "Epoch: 629 [20100/50000 (40%)]  \tLoss:   93.478493\trec:   67.060806\tkl:   26.417685\n",
      "Epoch: 629 [30100/50000 (60%)]  \tLoss:   84.415558\trec:   59.552551\tkl:   24.863003\n",
      "Epoch: 629 [40100/50000 (80%)]  \tLoss:   93.974129\trec:   67.018150\tkl:   26.955973\n",
      "====> Epoch: 629 Average train loss: 91.0781\n",
      "====> Validation set loss: 94.6041\n",
      "====> Validation set kl: 26.0208\n",
      "Epoch: 630 [  100/50000 ( 0%)]  \tLoss:   89.856987\trec:   63.600456\tkl:   26.256535\n",
      "Epoch: 630 [10100/50000 (20%)]  \tLoss:   93.082169\trec:   66.407936\tkl:   26.674231\n",
      "Epoch: 630 [20100/50000 (40%)]  \tLoss:   87.752510\trec:   62.482868\tkl:   25.269638\n",
      "Epoch: 630 [30100/50000 (60%)]  \tLoss:   90.864571\trec:   64.902145\tkl:   25.962423\n",
      "Epoch: 630 [40100/50000 (80%)]  \tLoss:   92.197052\trec:   65.218727\tkl:   26.978325\n",
      "====> Epoch: 630 Average train loss: 91.0886\n",
      "====> Validation set loss: 94.6958\n",
      "====> Validation set kl: 26.0442\n",
      "Epoch: 631 [  100/50000 ( 0%)]  \tLoss:   89.216988\trec:   63.780830\tkl:   25.436161\n",
      "Epoch: 631 [10100/50000 (20%)]  \tLoss:   92.496490\trec:   66.421745\tkl:   26.074751\n",
      "Epoch: 631 [20100/50000 (40%)]  \tLoss:   98.169174\trec:   70.783859\tkl:   27.385319\n",
      "Epoch: 631 [30100/50000 (60%)]  \tLoss:   96.053612\trec:   69.124420\tkl:   26.929192\n",
      "Epoch: 631 [40100/50000 (80%)]  \tLoss:   90.192841\trec:   64.557617\tkl:   25.635223\n",
      "====> Epoch: 631 Average train loss: 91.0896\n",
      "====> Validation set loss: 94.6017\n",
      "====> Validation set kl: 26.0832\n",
      "Epoch: 632 [  100/50000 ( 0%)]  \tLoss:   90.806274\trec:   64.825966\tkl:   25.980309\n",
      "Epoch: 632 [10100/50000 (20%)]  \tLoss:   89.448349\trec:   63.816483\tkl:   25.631865\n",
      "Epoch: 632 [20100/50000 (40%)]  \tLoss:   90.870918\trec:   64.476372\tkl:   26.394541\n",
      "Epoch: 632 [30100/50000 (60%)]  \tLoss:   90.861168\trec:   64.661476\tkl:   26.199699\n",
      "Epoch: 632 [40100/50000 (80%)]  \tLoss:   87.175056\trec:   61.986145\tkl:   25.188913\n",
      "====> Epoch: 632 Average train loss: 91.0926\n",
      "====> Validation set loss: 94.6802\n",
      "====> Validation set kl: 26.1915\n",
      "Epoch: 633 [  100/50000 ( 0%)]  \tLoss:   90.166893\trec:   64.307549\tkl:   25.859343\n",
      "Epoch: 633 [10100/50000 (20%)]  \tLoss:   90.997299\trec:   64.530060\tkl:   26.467241\n",
      "Epoch: 633 [20100/50000 (40%)]  \tLoss:   87.999947\trec:   61.341599\tkl:   26.658352\n",
      "Epoch: 633 [30100/50000 (60%)]  \tLoss:   93.177452\trec:   67.007935\tkl:   26.169514\n",
      "Epoch: 633 [40100/50000 (80%)]  \tLoss:   89.295967\trec:   63.189190\tkl:   26.106779\n",
      "====> Epoch: 633 Average train loss: 91.0700\n",
      "====> Validation set loss: 94.6173\n",
      "====> Validation set kl: 26.2729\n",
      "Epoch: 634 [  100/50000 ( 0%)]  \tLoss:   86.667580\trec:   61.090790\tkl:   25.576782\n",
      "Epoch: 634 [10100/50000 (20%)]  \tLoss:   92.196289\trec:   65.494629\tkl:   26.701662\n",
      "Epoch: 634 [20100/50000 (40%)]  \tLoss:   89.594391\trec:   64.028580\tkl:   25.565817\n",
      "Epoch: 634 [30100/50000 (60%)]  \tLoss:   93.009834\trec:   66.293625\tkl:   26.716209\n",
      "Epoch: 634 [40100/50000 (80%)]  \tLoss:   93.907555\trec:   67.381599\tkl:   26.525961\n",
      "====> Epoch: 634 Average train loss: 91.0722\n",
      "====> Validation set loss: 94.6509\n",
      "====> Validation set kl: 26.4057\n",
      "Epoch: 635 [  100/50000 ( 0%)]  \tLoss:   94.365593\trec:   67.587418\tkl:   26.778175\n",
      "Epoch: 635 [10100/50000 (20%)]  \tLoss:   92.223595\trec:   65.644997\tkl:   26.578600\n",
      "Epoch: 635 [20100/50000 (40%)]  \tLoss:   91.248749\trec:   66.378952\tkl:   24.869802\n",
      "Epoch: 635 [30100/50000 (60%)]  \tLoss:   90.518654\trec:   65.425194\tkl:   25.093454\n",
      "Epoch: 635 [40100/50000 (80%)]  \tLoss:   92.763649\trec:   66.957802\tkl:   25.805843\n",
      "====> Epoch: 635 Average train loss: 91.0616\n",
      "====> Validation set loss: 94.6343\n",
      "====> Validation set kl: 26.0372\n",
      "Epoch: 636 [  100/50000 ( 0%)]  \tLoss:   90.823914\trec:   64.804657\tkl:   26.019255\n",
      "Epoch: 636 [10100/50000 (20%)]  \tLoss:   90.862068\trec:   64.835793\tkl:   26.026274\n",
      "Epoch: 636 [20100/50000 (40%)]  \tLoss:   89.180252\trec:   63.087158\tkl:   26.093090\n",
      "Epoch: 636 [30100/50000 (60%)]  \tLoss:   89.555351\trec:   63.706707\tkl:   25.848646\n",
      "Epoch: 636 [40100/50000 (80%)]  \tLoss:   92.692886\trec:   66.713737\tkl:   25.979153\n",
      "====> Epoch: 636 Average train loss: 91.0637\n",
      "====> Validation set loss: 94.6347\n",
      "====> Validation set kl: 26.3070\n",
      "Epoch: 637 [  100/50000 ( 0%)]  \tLoss:   92.111160\trec:   65.947182\tkl:   26.163980\n",
      "Epoch: 637 [10100/50000 (20%)]  \tLoss:   91.167274\trec:   64.446198\tkl:   26.721081\n",
      "Epoch: 637 [20100/50000 (40%)]  \tLoss:   88.936821\trec:   62.535564\tkl:   26.401258\n",
      "Epoch: 637 [30100/50000 (60%)]  \tLoss:   88.819389\trec:   63.222298\tkl:   25.597099\n",
      "Epoch: 637 [40100/50000 (80%)]  \tLoss:   89.675797\trec:   63.741474\tkl:   25.934328\n",
      "====> Epoch: 637 Average train loss: 91.0699\n",
      "====> Validation set loss: 94.5671\n",
      "====> Validation set kl: 25.9877\n",
      "Epoch: 638 [  100/50000 ( 0%)]  \tLoss:   91.043114\trec:   65.201424\tkl:   25.841692\n",
      "Epoch: 638 [10100/50000 (20%)]  \tLoss:   87.222351\trec:   62.479382\tkl:   24.742971\n",
      "Epoch: 638 [20100/50000 (40%)]  \tLoss:   90.653236\trec:   64.062866\tkl:   26.590366\n",
      "Epoch: 638 [30100/50000 (60%)]  \tLoss:   92.825493\trec:   65.676704\tkl:   27.148788\n",
      "Epoch: 638 [40100/50000 (80%)]  \tLoss:   94.276970\trec:   67.418640\tkl:   26.858324\n",
      "====> Epoch: 638 Average train loss: 91.0574\n",
      "====> Validation set loss: 94.6592\n",
      "====> Validation set kl: 26.1436\n",
      "Epoch: 639 [  100/50000 ( 0%)]  \tLoss:   94.560417\trec:   67.534233\tkl:   27.026180\n",
      "Epoch: 639 [10100/50000 (20%)]  \tLoss:   90.564262\trec:   64.830765\tkl:   25.733500\n",
      "Epoch: 639 [20100/50000 (40%)]  \tLoss:   91.982475\trec:   64.583984\tkl:   27.398500\n",
      "Epoch: 639 [30100/50000 (60%)]  \tLoss:   94.017105\trec:   67.672745\tkl:   26.344355\n",
      "Epoch: 639 [40100/50000 (80%)]  \tLoss:   96.859619\trec:   68.638237\tkl:   28.221376\n",
      "====> Epoch: 639 Average train loss: 91.0538\n",
      "====> Validation set loss: 94.4949\n",
      "====> Validation set kl: 26.1711\n",
      "Epoch: 640 [  100/50000 ( 0%)]  \tLoss:   89.817123\trec:   63.683319\tkl:   26.133806\n",
      "Epoch: 640 [10100/50000 (20%)]  \tLoss:   91.512306\trec:   66.070610\tkl:   25.441690\n",
      "Epoch: 640 [20100/50000 (40%)]  \tLoss:   92.386703\trec:   64.953331\tkl:   27.433378\n",
      "Epoch: 640 [30100/50000 (60%)]  \tLoss:   92.298035\trec:   64.543304\tkl:   27.754730\n",
      "Epoch: 640 [40100/50000 (80%)]  \tLoss:   94.280914\trec:   68.163330\tkl:   26.117590\n",
      "====> Epoch: 640 Average train loss: 91.0405\n",
      "====> Validation set loss: 94.6400\n",
      "====> Validation set kl: 26.0389\n",
      "Epoch: 641 [  100/50000 ( 0%)]  \tLoss:   91.992462\trec:   66.012306\tkl:   25.980160\n",
      "Epoch: 641 [10100/50000 (20%)]  \tLoss:   93.768692\trec:   66.755127\tkl:   27.013563\n",
      "Epoch: 641 [20100/50000 (40%)]  \tLoss:   94.931168\trec:   68.063660\tkl:   26.867514\n",
      "Epoch: 641 [30100/50000 (60%)]  \tLoss:   92.789955\trec:   66.523438\tkl:   26.266520\n",
      "Epoch: 641 [40100/50000 (80%)]  \tLoss:   92.935448\trec:   65.809898\tkl:   27.125551\n",
      "====> Epoch: 641 Average train loss: 91.0668\n",
      "====> Validation set loss: 94.6412\n",
      "====> Validation set kl: 26.1245\n",
      "Epoch: 642 [  100/50000 ( 0%)]  \tLoss:   88.411423\trec:   62.771355\tkl:   25.640066\n",
      "Epoch: 642 [10100/50000 (20%)]  \tLoss:   90.434547\trec:   64.405907\tkl:   26.028643\n",
      "Epoch: 642 [20100/50000 (40%)]  \tLoss:   92.106323\trec:   65.300919\tkl:   26.805407\n",
      "Epoch: 642 [30100/50000 (60%)]  \tLoss:   92.877441\trec:   66.358856\tkl:   26.518578\n",
      "Epoch: 642 [40100/50000 (80%)]  \tLoss:   90.723885\trec:   64.419395\tkl:   26.304491\n",
      "====> Epoch: 642 Average train loss: 91.0402\n",
      "====> Validation set loss: 94.5684\n",
      "====> Validation set kl: 26.0681\n",
      "Epoch: 643 [  100/50000 ( 0%)]  \tLoss:   86.704865\trec:   61.632149\tkl:   25.072714\n",
      "Epoch: 643 [10100/50000 (20%)]  \tLoss:   89.641716\trec:   63.856430\tkl:   25.785288\n",
      "Epoch: 643 [20100/50000 (40%)]  \tLoss:   93.687820\trec:   66.697090\tkl:   26.990730\n",
      "Epoch: 643 [30100/50000 (60%)]  \tLoss:   86.736542\trec:   60.878544\tkl:   25.858000\n",
      "Epoch: 643 [40100/50000 (80%)]  \tLoss:   87.511818\trec:   61.870693\tkl:   25.641127\n",
      "====> Epoch: 643 Average train loss: 91.0573\n",
      "====> Validation set loss: 94.5890\n",
      "====> Validation set kl: 26.3176\n",
      "Epoch: 644 [  100/50000 ( 0%)]  \tLoss:   92.002090\trec:   65.831535\tkl:   26.170553\n",
      "Epoch: 644 [10100/50000 (20%)]  \tLoss:   92.873848\trec:   67.015060\tkl:   25.858786\n",
      "Epoch: 644 [20100/50000 (40%)]  \tLoss:   87.826950\trec:   62.380669\tkl:   25.446283\n",
      "Epoch: 644 [30100/50000 (60%)]  \tLoss:   91.775429\trec:   64.948463\tkl:   26.826965\n",
      "Epoch: 644 [40100/50000 (80%)]  \tLoss:   90.462906\trec:   65.547157\tkl:   24.915751\n",
      "====> Epoch: 644 Average train loss: 91.0574\n",
      "====> Validation set loss: 94.5181\n",
      "====> Validation set kl: 26.1650\n",
      "Epoch: 645 [  100/50000 ( 0%)]  \tLoss:   90.097031\trec:   63.889812\tkl:   26.207222\n",
      "Epoch: 645 [10100/50000 (20%)]  \tLoss:   91.191910\trec:   65.443390\tkl:   25.748526\n",
      "Epoch: 645 [20100/50000 (40%)]  \tLoss:   94.814392\trec:   68.401001\tkl:   26.413391\n",
      "Epoch: 645 [30100/50000 (60%)]  \tLoss:   90.846375\trec:   64.968384\tkl:   25.877987\n",
      "Epoch: 645 [40100/50000 (80%)]  \tLoss:   93.097595\trec:   67.242096\tkl:   25.855503\n",
      "====> Epoch: 645 Average train loss: 91.0584\n",
      "====> Validation set loss: 94.6356\n",
      "====> Validation set kl: 26.1284\n",
      "Epoch: 646 [  100/50000 ( 0%)]  \tLoss:   88.902031\trec:   62.991325\tkl:   25.910698\n",
      "Epoch: 646 [10100/50000 (20%)]  \tLoss:   89.712944\trec:   63.632881\tkl:   26.080070\n",
      "Epoch: 646 [20100/50000 (40%)]  \tLoss:   89.655563\trec:   64.253540\tkl:   25.402016\n",
      "Epoch: 646 [30100/50000 (60%)]  \tLoss:   92.416206\trec:   65.952446\tkl:   26.463762\n",
      "Epoch: 646 [40100/50000 (80%)]  \tLoss:   92.265625\trec:   66.537231\tkl:   25.728394\n",
      "====> Epoch: 646 Average train loss: 91.0313\n",
      "====> Validation set loss: 94.5348\n",
      "====> Validation set kl: 26.0195\n",
      "Epoch: 647 [  100/50000 ( 0%)]  \tLoss:   90.582680\trec:   64.323692\tkl:   26.258986\n",
      "Epoch: 647 [10100/50000 (20%)]  \tLoss:   97.648163\trec:   69.875885\tkl:   27.772278\n",
      "Epoch: 647 [20100/50000 (40%)]  \tLoss:   93.345284\trec:   67.133354\tkl:   26.211924\n",
      "Epoch: 647 [30100/50000 (60%)]  \tLoss:   86.529747\trec:   61.443352\tkl:   25.086390\n",
      "Epoch: 647 [40100/50000 (80%)]  \tLoss:   92.670509\trec:   65.138252\tkl:   27.532261\n",
      "====> Epoch: 647 Average train loss: 91.0502\n",
      "====> Validation set loss: 94.5402\n",
      "====> Validation set kl: 26.3356\n",
      "Epoch: 648 [  100/50000 ( 0%)]  \tLoss:   89.259995\trec:   64.114655\tkl:   25.145346\n",
      "Epoch: 648 [10100/50000 (20%)]  \tLoss:   93.246407\trec:   67.339371\tkl:   25.907036\n",
      "Epoch: 648 [20100/50000 (40%)]  \tLoss:   92.629059\trec:   66.353638\tkl:   26.275419\n",
      "Epoch: 648 [30100/50000 (60%)]  \tLoss:   94.766235\trec:   67.741463\tkl:   27.024773\n",
      "Epoch: 648 [40100/50000 (80%)]  \tLoss:   90.060898\trec:   63.762035\tkl:   26.298866\n",
      "====> Epoch: 648 Average train loss: 91.0174\n",
      "====> Validation set loss: 94.6041\n",
      "====> Validation set kl: 26.2340\n",
      "Epoch: 649 [  100/50000 ( 0%)]  \tLoss:   88.743279\trec:   62.586395\tkl:   26.156883\n",
      "Epoch: 649 [10100/50000 (20%)]  \tLoss:   89.034721\trec:   62.946659\tkl:   26.088068\n",
      "Epoch: 649 [20100/50000 (40%)]  \tLoss:   93.664429\trec:   67.654648\tkl:   26.009783\n",
      "Epoch: 649 [30100/50000 (60%)]  \tLoss:   92.784767\trec:   66.469063\tkl:   26.315708\n",
      "Epoch: 649 [40100/50000 (80%)]  \tLoss:   88.053810\trec:   62.168728\tkl:   25.885075\n",
      "====> Epoch: 649 Average train loss: 91.0298\n",
      "====> Validation set loss: 94.5397\n",
      "====> Validation set kl: 26.0198\n",
      "Epoch: 650 [  100/50000 ( 0%)]  \tLoss:   90.877472\trec:   65.811279\tkl:   25.066193\n",
      "Epoch: 650 [10100/50000 (20%)]  \tLoss:   92.569801\trec:   67.009972\tkl:   25.559835\n",
      "Epoch: 650 [20100/50000 (40%)]  \tLoss:   92.749550\trec:   65.615021\tkl:   27.134529\n",
      "Epoch: 650 [30100/50000 (60%)]  \tLoss:   93.260056\trec:   66.697769\tkl:   26.562286\n",
      "Epoch: 650 [40100/50000 (80%)]  \tLoss:   88.866440\trec:   63.333847\tkl:   25.532595\n",
      "====> Epoch: 650 Average train loss: 91.0761\n",
      "====> Validation set loss: 94.6177\n",
      "====> Validation set kl: 26.1442\n",
      "Epoch: 651 [  100/50000 ( 0%)]  \tLoss:   91.361015\trec:   65.560242\tkl:   25.800776\n",
      "Epoch: 651 [10100/50000 (20%)]  \tLoss:   90.064903\trec:   64.284882\tkl:   25.780016\n",
      "Epoch: 651 [20100/50000 (40%)]  \tLoss:   90.663193\trec:   64.846870\tkl:   25.816315\n",
      "Epoch: 651 [30100/50000 (60%)]  \tLoss:   86.314575\trec:   60.329948\tkl:   25.984631\n",
      "Epoch: 651 [40100/50000 (80%)]  \tLoss:   91.422325\trec:   65.136375\tkl:   26.285944\n",
      "====> Epoch: 651 Average train loss: 91.0323\n",
      "====> Validation set loss: 94.6529\n",
      "====> Validation set kl: 26.0013\n",
      "Epoch: 652 [  100/50000 ( 0%)]  \tLoss:   91.639351\trec:   65.305954\tkl:   26.333397\n",
      "Epoch: 652 [10100/50000 (20%)]  \tLoss:   91.838921\trec:   65.767029\tkl:   26.071894\n",
      "Epoch: 652 [20100/50000 (40%)]  \tLoss:   90.518181\trec:   64.584869\tkl:   25.933311\n",
      "Epoch: 652 [30100/50000 (60%)]  \tLoss:   89.468483\trec:   63.370506\tkl:   26.097979\n",
      "Epoch: 652 [40100/50000 (80%)]  \tLoss:   89.322853\trec:   63.876328\tkl:   25.446527\n",
      "====> Epoch: 652 Average train loss: 91.0111\n",
      "====> Validation set loss: 94.5740\n",
      "====> Validation set kl: 26.1264\n",
      "Epoch: 653 [  100/50000 ( 0%)]  \tLoss:   89.405548\trec:   63.576385\tkl:   25.829155\n",
      "Epoch: 653 [10100/50000 (20%)]  \tLoss:   89.835205\trec:   64.178535\tkl:   25.656662\n",
      "Epoch: 653 [20100/50000 (40%)]  \tLoss:   90.964752\trec:   65.287613\tkl:   25.677143\n",
      "Epoch: 653 [30100/50000 (60%)]  \tLoss:   89.570694\trec:   63.595501\tkl:   25.975193\n",
      "Epoch: 653 [40100/50000 (80%)]  \tLoss:   88.523689\trec:   62.218296\tkl:   26.305397\n",
      "====> Epoch: 653 Average train loss: 91.0258\n",
      "====> Validation set loss: 94.5331\n",
      "====> Validation set kl: 26.1805\n",
      "Epoch: 654 [  100/50000 ( 0%)]  \tLoss:   87.067322\trec:   61.716110\tkl:   25.351208\n",
      "Epoch: 654 [10100/50000 (20%)]  \tLoss:   90.206841\trec:   63.825092\tkl:   26.381752\n",
      "Epoch: 654 [20100/50000 (40%)]  \tLoss:   89.568535\trec:   63.594837\tkl:   25.973694\n",
      "Epoch: 654 [30100/50000 (60%)]  \tLoss:   90.941551\trec:   63.841255\tkl:   27.100298\n",
      "Epoch: 654 [40100/50000 (80%)]  \tLoss:   88.501404\trec:   62.853867\tkl:   25.647533\n",
      "====> Epoch: 654 Average train loss: 91.0031\n",
      "====> Validation set loss: 94.6455\n",
      "====> Validation set kl: 26.1970\n",
      "Epoch: 655 [  100/50000 ( 0%)]  \tLoss:   91.236481\trec:   64.864479\tkl:   26.372005\n",
      "Epoch: 655 [10100/50000 (20%)]  \tLoss:   93.127144\trec:   66.884697\tkl:   26.242445\n",
      "Epoch: 655 [20100/50000 (40%)]  \tLoss:   91.488731\trec:   65.264580\tkl:   26.224152\n",
      "Epoch: 655 [30100/50000 (60%)]  \tLoss:   90.276802\trec:   63.529217\tkl:   26.747585\n",
      "Epoch: 655 [40100/50000 (80%)]  \tLoss:   89.172607\trec:   63.764030\tkl:   25.408573\n",
      "====> Epoch: 655 Average train loss: 91.0208\n",
      "====> Validation set loss: 94.5175\n",
      "====> Validation set kl: 26.1792\n",
      "Epoch: 656 [  100/50000 ( 0%)]  \tLoss:   88.807373\trec:   63.846447\tkl:   24.960922\n",
      "Epoch: 656 [10100/50000 (20%)]  \tLoss:   89.900711\trec:   63.636597\tkl:   26.264112\n",
      "Epoch: 656 [20100/50000 (40%)]  \tLoss:   93.757202\trec:   67.264771\tkl:   26.492432\n",
      "Epoch: 656 [30100/50000 (60%)]  \tLoss:   94.218513\trec:   67.626892\tkl:   26.591623\n",
      "Epoch: 656 [40100/50000 (80%)]  \tLoss:   87.511658\trec:   62.318504\tkl:   25.193151\n",
      "====> Epoch: 656 Average train loss: 91.0189\n",
      "====> Validation set loss: 94.6292\n",
      "====> Validation set kl: 26.2266\n",
      "Epoch: 657 [  100/50000 ( 0%)]  \tLoss:   93.416794\trec:   67.164970\tkl:   26.251823\n",
      "Epoch: 657 [10100/50000 (20%)]  \tLoss:   90.471619\trec:   63.562458\tkl:   26.909163\n",
      "Epoch: 657 [20100/50000 (40%)]  \tLoss:   90.694763\trec:   64.262337\tkl:   26.432421\n",
      "Epoch: 657 [30100/50000 (60%)]  \tLoss:   91.064552\trec:   66.159172\tkl:   24.905375\n",
      "Epoch: 657 [40100/50000 (80%)]  \tLoss:   88.911812\trec:   63.017742\tkl:   25.894072\n",
      "====> Epoch: 657 Average train loss: 90.9918\n",
      "====> Validation set loss: 94.5630\n",
      "====> Validation set kl: 26.2177\n",
      "Epoch: 658 [  100/50000 ( 0%)]  \tLoss:   91.610497\trec:   64.661133\tkl:   26.949362\n",
      "Epoch: 658 [10100/50000 (20%)]  \tLoss:   89.810829\trec:   63.099934\tkl:   26.710890\n",
      "Epoch: 658 [20100/50000 (40%)]  \tLoss:   89.363266\trec:   64.600548\tkl:   24.762726\n",
      "Epoch: 658 [30100/50000 (60%)]  \tLoss:   93.431816\trec:   67.412216\tkl:   26.019604\n",
      "Epoch: 658 [40100/50000 (80%)]  \tLoss:   93.268410\trec:   66.038132\tkl:   27.230272\n",
      "====> Epoch: 658 Average train loss: 91.0037\n",
      "====> Validation set loss: 94.5129\n",
      "====> Validation set kl: 26.1159\n",
      "Epoch: 659 [  100/50000 ( 0%)]  \tLoss:   95.171318\trec:   68.268967\tkl:   26.902351\n",
      "Epoch: 659 [10100/50000 (20%)]  \tLoss:   88.368706\trec:   62.838886\tkl:   25.529819\n",
      "Epoch: 659 [20100/50000 (40%)]  \tLoss:   85.264793\trec:   60.021172\tkl:   25.243622\n",
      "Epoch: 659 [30100/50000 (60%)]  \tLoss:   91.286674\trec:   65.087379\tkl:   26.199297\n",
      "Epoch: 659 [40100/50000 (80%)]  \tLoss:   91.152672\trec:   64.514214\tkl:   26.638462\n",
      "====> Epoch: 659 Average train loss: 91.0020\n",
      "====> Validation set loss: 94.5406\n",
      "====> Validation set kl: 26.1767\n",
      "Epoch: 660 [  100/50000 ( 0%)]  \tLoss:   92.890236\trec:   67.210678\tkl:   25.679560\n",
      "Epoch: 660 [10100/50000 (20%)]  \tLoss:   89.729645\trec:   64.259308\tkl:   25.470341\n",
      "Epoch: 660 [20100/50000 (40%)]  \tLoss:   88.678444\trec:   63.691288\tkl:   24.987158\n",
      "Epoch: 660 [30100/50000 (60%)]  \tLoss:   90.426941\trec:   64.656914\tkl:   25.770021\n",
      "Epoch: 660 [40100/50000 (80%)]  \tLoss:   91.412392\trec:   65.905563\tkl:   25.506830\n",
      "====> Epoch: 660 Average train loss: 91.0039\n",
      "====> Validation set loss: 94.6133\n",
      "====> Validation set kl: 26.1140\n",
      "Epoch: 661 [  100/50000 ( 0%)]  \tLoss:   89.139038\trec:   62.924091\tkl:   26.214952\n",
      "Epoch: 661 [10100/50000 (20%)]  \tLoss:   88.818466\trec:   63.360043\tkl:   25.458426\n",
      "Epoch: 661 [20100/50000 (40%)]  \tLoss:   91.294350\trec:   65.268768\tkl:   26.025587\n",
      "Epoch: 661 [30100/50000 (60%)]  \tLoss:   88.688492\trec:   62.931786\tkl:   25.756714\n",
      "Epoch: 661 [40100/50000 (80%)]  \tLoss:   89.717110\trec:   64.593971\tkl:   25.123129\n",
      "====> Epoch: 661 Average train loss: 91.0076\n",
      "====> Validation set loss: 94.6212\n",
      "====> Validation set kl: 26.0525\n",
      "Epoch: 662 [  100/50000 ( 0%)]  \tLoss:   89.074478\trec:   63.792629\tkl:   25.281853\n",
      "Epoch: 662 [10100/50000 (20%)]  \tLoss:   86.607590\trec:   61.051430\tkl:   25.556156\n",
      "Epoch: 662 [20100/50000 (40%)]  \tLoss:   91.997772\trec:   65.469681\tkl:   26.528088\n",
      "Epoch: 662 [30100/50000 (60%)]  \tLoss:   89.539742\trec:   63.050613\tkl:   26.489132\n",
      "Epoch: 662 [40100/50000 (80%)]  \tLoss:   91.854744\trec:   65.222672\tkl:   26.632072\n",
      "====> Epoch: 662 Average train loss: 90.9706\n",
      "====> Validation set loss: 94.5052\n",
      "====> Validation set kl: 26.2792\n",
      "Epoch: 663 [  100/50000 ( 0%)]  \tLoss:   91.380913\trec:   64.420555\tkl:   26.960361\n",
      "Epoch: 663 [10100/50000 (20%)]  \tLoss:   90.071686\trec:   63.771248\tkl:   26.300436\n",
      "Epoch: 663 [20100/50000 (40%)]  \tLoss:   85.990685\trec:   59.881958\tkl:   26.108721\n",
      "Epoch: 663 [30100/50000 (60%)]  \tLoss:   93.723793\trec:   66.964577\tkl:   26.759220\n",
      "Epoch: 663 [40100/50000 (80%)]  \tLoss:   92.418663\trec:   65.853127\tkl:   26.565535\n",
      "====> Epoch: 663 Average train loss: 90.9567\n",
      "====> Validation set loss: 94.6462\n",
      "====> Validation set kl: 25.9150\n",
      "Epoch: 664 [  100/50000 ( 0%)]  \tLoss:   92.872169\trec:   66.593071\tkl:   26.279091\n",
      "Epoch: 664 [10100/50000 (20%)]  \tLoss:   90.535431\trec:   64.393646\tkl:   26.141785\n",
      "Epoch: 664 [20100/50000 (40%)]  \tLoss:   89.948883\trec:   63.970131\tkl:   25.978754\n",
      "Epoch: 664 [30100/50000 (60%)]  \tLoss:   89.125717\trec:   62.374958\tkl:   26.750763\n",
      "Epoch: 664 [40100/50000 (80%)]  \tLoss:   91.328644\trec:   64.557610\tkl:   26.771036\n",
      "====> Epoch: 664 Average train loss: 90.9834\n",
      "====> Validation set loss: 94.5243\n",
      "====> Validation set kl: 26.2134\n",
      "Epoch: 665 [  100/50000 ( 0%)]  \tLoss:   90.429794\trec:   63.835991\tkl:   26.593805\n",
      "Epoch: 665 [10100/50000 (20%)]  \tLoss:   90.748085\trec:   64.841080\tkl:   25.906996\n",
      "Epoch: 665 [20100/50000 (40%)]  \tLoss:   91.531151\trec:   64.931564\tkl:   26.599585\n",
      "Epoch: 665 [30100/50000 (60%)]  \tLoss:   91.719917\trec:   65.450134\tkl:   26.269783\n",
      "Epoch: 665 [40100/50000 (80%)]  \tLoss:   94.853561\trec:   68.772301\tkl:   26.081259\n",
      "====> Epoch: 665 Average train loss: 90.9849\n",
      "====> Validation set loss: 94.5364\n",
      "====> Validation set kl: 26.3127\n",
      "Epoch: 666 [  100/50000 ( 0%)]  \tLoss:   87.710518\trec:   62.661560\tkl:   25.048954\n",
      "Epoch: 666 [10100/50000 (20%)]  \tLoss:   91.200600\trec:   64.712822\tkl:   26.487783\n",
      "Epoch: 666 [20100/50000 (40%)]  \tLoss:   93.771881\trec:   67.034088\tkl:   26.737789\n",
      "Epoch: 666 [30100/50000 (60%)]  \tLoss:   97.026505\trec:   69.665161\tkl:   27.361340\n",
      "Epoch: 666 [40100/50000 (80%)]  \tLoss:   89.887314\trec:   64.505142\tkl:   25.382166\n",
      "====> Epoch: 666 Average train loss: 90.9771\n",
      "====> Validation set loss: 94.5662\n",
      "====> Validation set kl: 26.1865\n",
      "Epoch: 667 [  100/50000 ( 0%)]  \tLoss:   92.363770\trec:   65.645164\tkl:   26.718603\n",
      "Epoch: 667 [10100/50000 (20%)]  \tLoss:   89.305122\trec:   63.585823\tkl:   25.719301\n",
      "Epoch: 667 [20100/50000 (40%)]  \tLoss:   90.670967\trec:   64.958916\tkl:   25.712046\n",
      "Epoch: 667 [30100/50000 (60%)]  \tLoss:   89.748009\trec:   64.089699\tkl:   25.658304\n",
      "Epoch: 667 [40100/50000 (80%)]  \tLoss:   92.379364\trec:   66.584801\tkl:   25.794563\n",
      "====> Epoch: 667 Average train loss: 90.9595\n",
      "====> Validation set loss: 94.5593\n",
      "====> Validation set kl: 26.2382\n",
      "Epoch: 668 [  100/50000 ( 0%)]  \tLoss:   91.763168\trec:   65.647575\tkl:   26.115593\n",
      "Epoch: 668 [10100/50000 (20%)]  \tLoss:   89.953575\trec:   63.817772\tkl:   26.135799\n",
      "Epoch: 668 [20100/50000 (40%)]  \tLoss:   88.222984\trec:   63.090027\tkl:   25.132963\n",
      "Epoch: 668 [30100/50000 (60%)]  \tLoss:   91.729874\trec:   64.653847\tkl:   27.076023\n",
      "Epoch: 668 [40100/50000 (80%)]  \tLoss:   88.458824\trec:   62.614246\tkl:   25.844574\n",
      "====> Epoch: 668 Average train loss: 90.9520\n",
      "====> Validation set loss: 94.5869\n",
      "====> Validation set kl: 26.2496\n",
      "Epoch: 669 [  100/50000 ( 0%)]  \tLoss:   88.073494\trec:   62.004501\tkl:   26.068996\n",
      "Epoch: 669 [10100/50000 (20%)]  \tLoss:   90.302208\trec:   64.592964\tkl:   25.709240\n",
      "Epoch: 669 [20100/50000 (40%)]  \tLoss:   92.129013\trec:   65.267578\tkl:   26.861437\n",
      "Epoch: 669 [30100/50000 (60%)]  \tLoss:   87.000206\trec:   61.203056\tkl:   25.797146\n",
      "Epoch: 669 [40100/50000 (80%)]  \tLoss:   92.672050\trec:   66.398186\tkl:   26.273867\n",
      "====> Epoch: 669 Average train loss: 90.9659\n",
      "====> Validation set loss: 94.6404\n",
      "====> Validation set kl: 26.2656\n",
      "Epoch: 670 [  100/50000 ( 0%)]  \tLoss:   90.786209\trec:   65.563507\tkl:   25.222702\n",
      "Epoch: 670 [10100/50000 (20%)]  \tLoss:   92.099724\trec:   65.421913\tkl:   26.677809\n",
      "Epoch: 670 [20100/50000 (40%)]  \tLoss:   89.606010\trec:   63.411755\tkl:   26.194254\n",
      "Epoch: 670 [30100/50000 (60%)]  \tLoss:   90.170845\trec:   64.447304\tkl:   25.723545\n",
      "Epoch: 670 [40100/50000 (80%)]  \tLoss:   91.213417\trec:   64.779724\tkl:   26.433695\n",
      "====> Epoch: 670 Average train loss: 90.9778\n",
      "====> Validation set loss: 94.5709\n",
      "====> Validation set kl: 26.0266\n",
      "Epoch: 671 [  100/50000 ( 0%)]  \tLoss:   89.118195\trec:   63.815121\tkl:   25.303074\n",
      "Epoch: 671 [10100/50000 (20%)]  \tLoss:   93.059311\trec:   66.742218\tkl:   26.317104\n",
      "Epoch: 671 [20100/50000 (40%)]  \tLoss:   88.614433\trec:   62.571293\tkl:   26.043144\n",
      "Epoch: 671 [30100/50000 (60%)]  \tLoss:   92.449371\trec:   66.042145\tkl:   26.407227\n",
      "Epoch: 671 [40100/50000 (80%)]  \tLoss:   87.371895\trec:   62.205547\tkl:   25.166348\n",
      "====> Epoch: 671 Average train loss: 90.9599\n",
      "====> Validation set loss: 94.5868\n",
      "====> Validation set kl: 26.1807\n",
      "Epoch: 672 [  100/50000 ( 0%)]  \tLoss:   89.230888\trec:   63.559452\tkl:   25.671438\n",
      "Epoch: 672 [10100/50000 (20%)]  \tLoss:   87.063095\trec:   61.345531\tkl:   25.717560\n",
      "Epoch: 672 [20100/50000 (40%)]  \tLoss:   91.990410\trec:   65.172775\tkl:   26.817627\n",
      "Epoch: 672 [30100/50000 (60%)]  \tLoss:   90.945274\trec:   64.763565\tkl:   26.181709\n",
      "Epoch: 672 [40100/50000 (80%)]  \tLoss:   91.565887\trec:   65.311600\tkl:   26.254286\n",
      "====> Epoch: 672 Average train loss: 90.9651\n",
      "====> Validation set loss: 94.6010\n",
      "====> Validation set kl: 26.2546\n",
      "Epoch: 673 [  100/50000 ( 0%)]  \tLoss:   89.884903\trec:   63.963490\tkl:   25.921406\n",
      "Epoch: 673 [10100/50000 (20%)]  \tLoss:   88.266602\trec:   61.217621\tkl:   27.048977\n",
      "Epoch: 673 [20100/50000 (40%)]  \tLoss:   92.522079\trec:   66.262177\tkl:   26.259905\n",
      "Epoch: 673 [30100/50000 (60%)]  \tLoss:   90.386581\trec:   63.990124\tkl:   26.396450\n",
      "Epoch: 673 [40100/50000 (80%)]  \tLoss:   90.703857\trec:   64.890808\tkl:   25.813049\n",
      "====> Epoch: 673 Average train loss: 90.9369\n",
      "====> Validation set loss: 94.6433\n",
      "====> Validation set kl: 26.3874\n",
      "Epoch: 674 [  100/50000 ( 0%)]  \tLoss:   89.967911\trec:   63.134384\tkl:   26.833527\n",
      "Epoch: 674 [10100/50000 (20%)]  \tLoss:   91.652130\trec:   64.768356\tkl:   26.883774\n",
      "Epoch: 674 [20100/50000 (40%)]  \tLoss:   91.995117\trec:   65.445427\tkl:   26.549690\n",
      "Epoch: 674 [30100/50000 (60%)]  \tLoss:   91.768707\trec:   64.617935\tkl:   27.150774\n",
      "Epoch: 674 [40100/50000 (80%)]  \tLoss:   98.902168\trec:   71.271255\tkl:   27.630905\n",
      "====> Epoch: 674 Average train loss: 90.9285\n",
      "====> Validation set loss: 94.6116\n",
      "====> Validation set kl: 26.1785\n",
      "Epoch: 675 [  100/50000 ( 0%)]  \tLoss:   89.620232\trec:   63.375145\tkl:   26.245090\n",
      "Epoch: 675 [10100/50000 (20%)]  \tLoss:   88.943222\trec:   63.016239\tkl:   25.926981\n",
      "Epoch: 675 [20100/50000 (40%)]  \tLoss:   93.815590\trec:   67.666008\tkl:   26.149591\n",
      "Epoch: 675 [30100/50000 (60%)]  \tLoss:   91.748268\trec:   65.183716\tkl:   26.564548\n",
      "Epoch: 675 [40100/50000 (80%)]  \tLoss:   89.611031\trec:   64.154480\tkl:   25.456556\n",
      "====> Epoch: 675 Average train loss: 90.9566\n",
      "====> Validation set loss: 94.6136\n",
      "====> Validation set kl: 26.1571\n",
      "Epoch: 676 [  100/50000 ( 0%)]  \tLoss:   91.852966\trec:   65.341553\tkl:   26.511414\n",
      "Epoch: 676 [10100/50000 (20%)]  \tLoss:   92.665627\trec:   65.802643\tkl:   26.862984\n",
      "Epoch: 676 [20100/50000 (40%)]  \tLoss:   89.634377\trec:   63.709141\tkl:   25.925236\n",
      "Epoch: 676 [30100/50000 (60%)]  \tLoss:   89.166847\trec:   63.196037\tkl:   25.970808\n",
      "Epoch: 676 [40100/50000 (80%)]  \tLoss:   95.435410\trec:   68.935349\tkl:   26.500057\n",
      "====> Epoch: 676 Average train loss: 90.9355\n",
      "====> Validation set loss: 94.6787\n",
      "====> Validation set kl: 26.1391\n",
      "Epoch: 677 [  100/50000 ( 0%)]  \tLoss:   94.834892\trec:   68.773712\tkl:   26.061176\n",
      "Epoch: 677 [10100/50000 (20%)]  \tLoss:   85.504585\trec:   59.654522\tkl:   25.850073\n",
      "Epoch: 677 [20100/50000 (40%)]  \tLoss:   87.800369\trec:   62.113739\tkl:   25.686625\n",
      "Epoch: 677 [30100/50000 (60%)]  \tLoss:   91.764938\trec:   65.559334\tkl:   26.205603\n",
      "Epoch: 677 [40100/50000 (80%)]  \tLoss:   91.063202\trec:   65.417778\tkl:   25.645422\n",
      "====> Epoch: 677 Average train loss: 90.9138\n",
      "====> Validation set loss: 94.5613\n",
      "====> Validation set kl: 26.0110\n",
      "Epoch: 678 [  100/50000 ( 0%)]  \tLoss:   94.372917\trec:   67.273010\tkl:   27.099909\n",
      "Epoch: 678 [10100/50000 (20%)]  \tLoss:   90.068649\trec:   63.217690\tkl:   26.850962\n",
      "Epoch: 678 [20100/50000 (40%)]  \tLoss:   87.625916\trec:   63.358879\tkl:   24.267038\n",
      "Epoch: 678 [30100/50000 (60%)]  \tLoss:   87.820847\trec:   62.747425\tkl:   25.073425\n",
      "Epoch: 678 [40100/50000 (80%)]  \tLoss:   88.389526\trec:   63.191887\tkl:   25.197641\n",
      "====> Epoch: 678 Average train loss: 90.9249\n",
      "====> Validation set loss: 94.6113\n",
      "====> Validation set kl: 26.2079\n",
      "Epoch: 679 [  100/50000 ( 0%)]  \tLoss:   90.034187\trec:   63.447659\tkl:   26.586531\n",
      "Epoch: 679 [10100/50000 (20%)]  \tLoss:   86.082748\trec:   59.856762\tkl:   26.225990\n",
      "Epoch: 679 [20100/50000 (40%)]  \tLoss:   87.572975\trec:   61.477406\tkl:   26.095568\n",
      "Epoch: 679 [30100/50000 (60%)]  \tLoss:   89.641609\trec:   63.267437\tkl:   26.374172\n",
      "Epoch: 679 [40100/50000 (80%)]  \tLoss:   94.527901\trec:   67.163933\tkl:   27.363972\n",
      "====> Epoch: 679 Average train loss: 90.8975\n",
      "====> Validation set loss: 94.5889\n",
      "====> Validation set kl: 26.1838\n",
      "Epoch: 680 [  100/50000 ( 0%)]  \tLoss:   87.442848\trec:   61.986965\tkl:   25.455881\n",
      "Epoch: 680 [10100/50000 (20%)]  \tLoss:   90.631508\trec:   64.307579\tkl:   26.323933\n",
      "Epoch: 680 [20100/50000 (40%)]  \tLoss:   89.719658\trec:   63.554024\tkl:   26.165634\n",
      "Epoch: 680 [30100/50000 (60%)]  \tLoss:   91.906441\trec:   64.669189\tkl:   27.237257\n",
      "Epoch: 680 [40100/50000 (80%)]  \tLoss:   94.785477\trec:   68.236984\tkl:   26.548496\n",
      "====> Epoch: 680 Average train loss: 90.9404\n",
      "====> Validation set loss: 94.6920\n",
      "====> Validation set kl: 26.3149\n",
      "Epoch: 681 [  100/50000 ( 0%)]  \tLoss:   92.084732\trec:   65.707916\tkl:   26.376818\n",
      "Epoch: 681 [10100/50000 (20%)]  \tLoss:   92.801353\trec:   65.978516\tkl:   26.822834\n",
      "Epoch: 681 [20100/50000 (40%)]  \tLoss:   87.235245\trec:   61.836765\tkl:   25.398478\n",
      "Epoch: 681 [30100/50000 (60%)]  \tLoss:   93.871895\trec:   67.083099\tkl:   26.788797\n",
      "Epoch: 681 [40100/50000 (80%)]  \tLoss:   89.818619\trec:   64.484894\tkl:   25.333723\n",
      "====> Epoch: 681 Average train loss: 90.9249\n",
      "====> Validation set loss: 94.5663\n",
      "====> Validation set kl: 25.9819\n",
      "Epoch: 682 [  100/50000 ( 0%)]  \tLoss:   90.417152\trec:   63.770622\tkl:   26.646532\n",
      "Epoch: 682 [10100/50000 (20%)]  \tLoss:   89.936050\trec:   63.956825\tkl:   25.979223\n",
      "Epoch: 682 [20100/50000 (40%)]  \tLoss:   87.516037\trec:   61.589100\tkl:   25.926935\n",
      "Epoch: 682 [30100/50000 (60%)]  \tLoss:   91.897011\trec:   65.741646\tkl:   26.155371\n",
      "Epoch: 682 [40100/50000 (80%)]  \tLoss:   93.964111\trec:   66.193298\tkl:   27.770807\n",
      "====> Epoch: 682 Average train loss: 90.9008\n",
      "====> Validation set loss: 94.5550\n",
      "====> Validation set kl: 26.3854\n",
      "Epoch: 683 [  100/50000 ( 0%)]  \tLoss:   92.747849\trec:   66.479469\tkl:   26.268379\n",
      "Epoch: 683 [10100/50000 (20%)]  \tLoss:   88.810425\trec:   62.879040\tkl:   25.931383\n",
      "Epoch: 683 [20100/50000 (40%)]  \tLoss:   90.866287\trec:   65.076096\tkl:   25.790188\n",
      "Epoch: 683 [30100/50000 (60%)]  \tLoss:   87.648865\trec:   61.870644\tkl:   25.778227\n",
      "Epoch: 683 [40100/50000 (80%)]  \tLoss:   91.519760\trec:   65.355064\tkl:   26.164701\n",
      "====> Epoch: 683 Average train loss: 90.9253\n",
      "====> Validation set loss: 94.6031\n",
      "====> Validation set kl: 26.1292\n",
      "Epoch: 684 [  100/50000 ( 0%)]  \tLoss:   90.795700\trec:   64.465538\tkl:   26.330168\n",
      "Epoch: 684 [10100/50000 (20%)]  \tLoss:   90.869751\trec:   64.770546\tkl:   26.099209\n",
      "Epoch: 684 [20100/50000 (40%)]  \tLoss:   88.250809\trec:   62.948723\tkl:   25.302084\n",
      "Epoch: 684 [30100/50000 (60%)]  \tLoss:   90.142052\trec:   63.926228\tkl:   26.215815\n",
      "Epoch: 684 [40100/50000 (80%)]  \tLoss:   95.969429\trec:   68.800896\tkl:   27.168530\n",
      "====> Epoch: 684 Average train loss: 90.9073\n",
      "====> Validation set loss: 94.5444\n",
      "====> Validation set kl: 26.1321\n",
      "Epoch: 685 [  100/50000 ( 0%)]  \tLoss:   92.557457\trec:   65.747597\tkl:   26.809866\n",
      "Epoch: 685 [10100/50000 (20%)]  \tLoss:   93.336716\trec:   66.487236\tkl:   26.849478\n",
      "Epoch: 685 [20100/50000 (40%)]  \tLoss:   93.781616\trec:   67.112839\tkl:   26.668776\n",
      "Epoch: 685 [30100/50000 (60%)]  \tLoss:   92.479591\trec:   65.840340\tkl:   26.639250\n",
      "Epoch: 685 [40100/50000 (80%)]  \tLoss:   88.767906\trec:   62.280537\tkl:   26.487373\n",
      "====> Epoch: 685 Average train loss: 90.9280\n",
      "====> Validation set loss: 94.6142\n",
      "====> Validation set kl: 26.1601\n",
      "Epoch: 686 [  100/50000 ( 0%)]  \tLoss:   90.158096\trec:   62.790184\tkl:   27.367912\n",
      "Epoch: 686 [10100/50000 (20%)]  \tLoss:   87.971489\trec:   62.204002\tkl:   25.767492\n",
      "Epoch: 686 [20100/50000 (40%)]  \tLoss:   88.836777\trec:   62.763142\tkl:   26.073635\n",
      "Epoch: 686 [30100/50000 (60%)]  \tLoss:   89.810974\trec:   64.131958\tkl:   25.679008\n",
      "Epoch: 686 [40100/50000 (80%)]  \tLoss:   89.246193\trec:   62.814976\tkl:   26.431213\n",
      "====> Epoch: 686 Average train loss: 90.8931\n",
      "====> Validation set loss: 94.4867\n",
      "====> Validation set kl: 26.4061\n",
      "Epoch: 687 [  100/50000 ( 0%)]  \tLoss:   88.265854\trec:   62.239296\tkl:   26.026562\n",
      "Epoch: 687 [10100/50000 (20%)]  \tLoss:   92.750557\trec:   65.890434\tkl:   26.860115\n",
      "Epoch: 687 [20100/50000 (40%)]  \tLoss:   93.038895\trec:   66.227554\tkl:   26.811340\n",
      "Epoch: 687 [30100/50000 (60%)]  \tLoss:   94.046837\trec:   67.480568\tkl:   26.566263\n",
      "Epoch: 687 [40100/50000 (80%)]  \tLoss:   92.345787\trec:   65.311691\tkl:   27.034103\n",
      "====> Epoch: 687 Average train loss: 90.8852\n",
      "====> Validation set loss: 94.5215\n",
      "====> Validation set kl: 26.1472\n",
      "Epoch: 688 [  100/50000 ( 0%)]  \tLoss:   87.310303\trec:   61.506905\tkl:   25.803400\n",
      "Epoch: 688 [10100/50000 (20%)]  \tLoss:   87.614510\trec:   61.890816\tkl:   25.723696\n",
      "Epoch: 688 [20100/50000 (40%)]  \tLoss:   91.543610\trec:   65.395256\tkl:   26.148363\n",
      "Epoch: 688 [30100/50000 (60%)]  \tLoss:   92.043182\trec:   65.677986\tkl:   26.365192\n",
      "Epoch: 688 [40100/50000 (80%)]  \tLoss:   88.466736\trec:   62.494324\tkl:   25.972414\n",
      "====> Epoch: 688 Average train loss: 90.9003\n",
      "====> Validation set loss: 94.4976\n",
      "====> Validation set kl: 26.2711\n",
      "Epoch: 689 [  100/50000 ( 0%)]  \tLoss:   91.976219\trec:   64.866257\tkl:   27.109964\n",
      "Epoch: 689 [10100/50000 (20%)]  \tLoss:   91.328972\trec:   65.321312\tkl:   26.007660\n",
      "Epoch: 689 [20100/50000 (40%)]  \tLoss:   88.766914\trec:   63.157459\tkl:   25.609453\n",
      "Epoch: 689 [30100/50000 (60%)]  \tLoss:   91.236732\trec:   64.495560\tkl:   26.741171\n",
      "Epoch: 689 [40100/50000 (80%)]  \tLoss:   89.605476\trec:   64.164116\tkl:   25.441364\n",
      "====> Epoch: 689 Average train loss: 90.8763\n",
      "====> Validation set loss: 94.5472\n",
      "====> Validation set kl: 26.3072\n",
      "Epoch: 690 [  100/50000 ( 0%)]  \tLoss:   93.677299\trec:   66.286568\tkl:   27.390738\n",
      "Epoch: 690 [10100/50000 (20%)]  \tLoss:   91.966270\trec:   65.740044\tkl:   26.226223\n",
      "Epoch: 690 [20100/50000 (40%)]  \tLoss:   90.357117\trec:   64.315361\tkl:   26.041756\n",
      "Epoch: 690 [30100/50000 (60%)]  \tLoss:   91.148788\trec:   64.826485\tkl:   26.322304\n",
      "Epoch: 690 [40100/50000 (80%)]  \tLoss:   92.059792\trec:   65.438210\tkl:   26.621584\n",
      "====> Epoch: 690 Average train loss: 90.8925\n",
      "====> Validation set loss: 94.5687\n",
      "====> Validation set kl: 26.2342\n",
      "Epoch: 691 [  100/50000 ( 0%)]  \tLoss:   91.768456\trec:   65.462578\tkl:   26.305876\n",
      "Epoch: 691 [10100/50000 (20%)]  \tLoss:   88.841492\trec:   63.658890\tkl:   25.182600\n",
      "Epoch: 691 [20100/50000 (40%)]  \tLoss:   89.050545\trec:   63.370380\tkl:   25.680166\n",
      "Epoch: 691 [30100/50000 (60%)]  \tLoss:   90.013672\trec:   64.312584\tkl:   25.701084\n",
      "Epoch: 691 [40100/50000 (80%)]  \tLoss:   91.885643\trec:   65.576889\tkl:   26.308752\n",
      "====> Epoch: 691 Average train loss: 90.8866\n",
      "====> Validation set loss: 94.5509\n",
      "====> Validation set kl: 26.1435\n",
      "Epoch: 692 [  100/50000 ( 0%)]  \tLoss:   93.271339\trec:   66.397545\tkl:   26.873793\n",
      "Epoch: 692 [10100/50000 (20%)]  \tLoss:   85.777863\trec:   60.140823\tkl:   25.637037\n",
      "Epoch: 692 [20100/50000 (40%)]  \tLoss:   88.895561\trec:   62.670147\tkl:   26.225420\n",
      "Epoch: 692 [30100/50000 (60%)]  \tLoss:   93.811249\trec:   67.007828\tkl:   26.803415\n",
      "Epoch: 692 [40100/50000 (80%)]  \tLoss:   94.102295\trec:   67.239708\tkl:   26.862585\n",
      "====> Epoch: 692 Average train loss: 90.9077\n",
      "====> Validation set loss: 94.4944\n",
      "====> Validation set kl: 26.4519\n",
      "Epoch: 693 [  100/50000 ( 0%)]  \tLoss:   94.663300\trec:   66.427223\tkl:   28.236078\n",
      "Epoch: 693 [10100/50000 (20%)]  \tLoss:   90.976822\trec:   64.538300\tkl:   26.438528\n",
      "Epoch: 693 [20100/50000 (40%)]  \tLoss:   92.781746\trec:   65.940285\tkl:   26.841461\n",
      "Epoch: 693 [30100/50000 (60%)]  \tLoss:   91.891716\trec:   65.994698\tkl:   25.897017\n",
      "Epoch: 693 [40100/50000 (80%)]  \tLoss:   91.682701\trec:   65.448814\tkl:   26.233883\n",
      "====> Epoch: 693 Average train loss: 90.8852\n",
      "====> Validation set loss: 94.6496\n",
      "====> Validation set kl: 26.4273\n",
      "Epoch: 694 [  100/50000 ( 0%)]  \tLoss:   89.953087\trec:   63.716206\tkl:   26.236877\n",
      "Epoch: 694 [10100/50000 (20%)]  \tLoss:   92.104958\trec:   66.455925\tkl:   25.649033\n",
      "Epoch: 694 [20100/50000 (40%)]  \tLoss:   89.643990\trec:   64.150818\tkl:   25.493172\n",
      "Epoch: 694 [30100/50000 (60%)]  \tLoss:   92.884041\trec:   66.053101\tkl:   26.830942\n",
      "Epoch: 694 [40100/50000 (80%)]  \tLoss:   92.707535\trec:   65.909859\tkl:   26.797686\n",
      "====> Epoch: 694 Average train loss: 90.8731\n",
      "====> Validation set loss: 94.5097\n",
      "====> Validation set kl: 26.3052\n",
      "Epoch: 695 [  100/50000 ( 0%)]  \tLoss:   84.748726\trec:   59.402500\tkl:   25.346230\n",
      "Epoch: 695 [10100/50000 (20%)]  \tLoss:   91.610435\trec:   64.851662\tkl:   26.758781\n",
      "Epoch: 695 [20100/50000 (40%)]  \tLoss:   88.869858\trec:   62.586399\tkl:   26.283459\n",
      "Epoch: 695 [30100/50000 (60%)]  \tLoss:   92.513298\trec:   65.748131\tkl:   26.765165\n",
      "Epoch: 695 [40100/50000 (80%)]  \tLoss:   93.010231\trec:   65.730568\tkl:   27.279673\n",
      "====> Epoch: 695 Average train loss: 90.8884\n",
      "====> Validation set loss: 94.5323\n",
      "====> Validation set kl: 26.2278\n",
      "Epoch: 696 [  100/50000 ( 0%)]  \tLoss:   89.437706\trec:   63.890545\tkl:   25.547157\n",
      "Epoch: 696 [10100/50000 (20%)]  \tLoss:   93.626892\trec:   66.994553\tkl:   26.632336\n",
      "Epoch: 696 [20100/50000 (40%)]  \tLoss:   93.558121\trec:   67.132286\tkl:   26.425837\n",
      "Epoch: 696 [30100/50000 (60%)]  \tLoss:   92.132614\trec:   65.598816\tkl:   26.533796\n",
      "Epoch: 696 [40100/50000 (80%)]  \tLoss:   94.208130\trec:   67.745934\tkl:   26.462196\n",
      "====> Epoch: 696 Average train loss: 90.8771\n",
      "====> Validation set loss: 94.5291\n",
      "====> Validation set kl: 26.0198\n",
      "Epoch: 697 [  100/50000 ( 0%)]  \tLoss:   88.776505\trec:   63.087692\tkl:   25.688803\n",
      "Epoch: 697 [10100/50000 (20%)]  \tLoss:   88.354355\trec:   62.609509\tkl:   25.744848\n",
      "Epoch: 697 [20100/50000 (40%)]  \tLoss:   89.440651\trec:   62.432236\tkl:   27.008419\n",
      "Epoch: 697 [30100/50000 (60%)]  \tLoss:   90.719864\trec:   65.060585\tkl:   25.659275\n",
      "Epoch: 697 [40100/50000 (80%)]  \tLoss:   91.002716\trec:   64.586327\tkl:   26.416382\n",
      "====> Epoch: 697 Average train loss: 90.8617\n",
      "====> Validation set loss: 94.5769\n",
      "====> Validation set kl: 26.3119\n",
      "Epoch: 698 [  100/50000 ( 0%)]  \tLoss:   94.127518\trec:   66.645241\tkl:   27.482275\n",
      "Epoch: 698 [10100/50000 (20%)]  \tLoss:   90.140724\trec:   64.440720\tkl:   25.699999\n",
      "Epoch: 698 [20100/50000 (40%)]  \tLoss:   90.181343\trec:   63.442566\tkl:   26.738783\n",
      "Epoch: 698 [30100/50000 (60%)]  \tLoss:   90.193016\trec:   64.283142\tkl:   25.909874\n",
      "Epoch: 698 [40100/50000 (80%)]  \tLoss:   93.413795\trec:   67.338776\tkl:   26.075024\n",
      "====> Epoch: 698 Average train loss: 90.8466\n",
      "====> Validation set loss: 94.5380\n",
      "====> Validation set kl: 26.1299\n",
      "Epoch: 699 [  100/50000 ( 0%)]  \tLoss:   88.818810\trec:   61.740551\tkl:   27.078251\n",
      "Epoch: 699 [10100/50000 (20%)]  \tLoss:   90.298500\trec:   64.497856\tkl:   25.800642\n",
      "Epoch: 699 [20100/50000 (40%)]  \tLoss:   92.601250\trec:   65.012428\tkl:   27.588814\n",
      "Epoch: 699 [30100/50000 (60%)]  \tLoss:   90.211464\trec:   64.023537\tkl:   26.187927\n",
      "Epoch: 699 [40100/50000 (80%)]  \tLoss:   92.609177\trec:   66.281250\tkl:   26.327929\n",
      "====> Epoch: 699 Average train loss: 90.8613\n",
      "====> Validation set loss: 94.5531\n",
      "====> Validation set kl: 26.3898\n",
      "Epoch: 700 [  100/50000 ( 0%)]  \tLoss:   92.846443\trec:   65.931587\tkl:   26.914858\n",
      "Epoch: 700 [10100/50000 (20%)]  \tLoss:   86.123024\trec:   60.179726\tkl:   25.943298\n",
      "Epoch: 700 [20100/50000 (40%)]  \tLoss:   88.243614\trec:   62.281704\tkl:   25.961914\n",
      "Epoch: 700 [30100/50000 (60%)]  \tLoss:   92.996582\trec:   66.679970\tkl:   26.316608\n",
      "Epoch: 700 [40100/50000 (80%)]  \tLoss:   90.434380\trec:   64.013359\tkl:   26.421024\n",
      "====> Epoch: 700 Average train loss: 90.8351\n",
      "====> Validation set loss: 94.4977\n",
      "====> Validation set kl: 26.2848\n",
      "Epoch: 701 [  100/50000 ( 0%)]  \tLoss:   86.366394\trec:   60.953987\tkl:   25.412409\n",
      "Epoch: 701 [10100/50000 (20%)]  \tLoss:   91.822662\trec:   65.265488\tkl:   26.557177\n",
      "Epoch: 701 [20100/50000 (40%)]  \tLoss:   89.584785\trec:   63.933613\tkl:   25.651171\n",
      "Epoch: 701 [30100/50000 (60%)]  \tLoss:   90.137291\trec:   64.487877\tkl:   25.649410\n",
      "Epoch: 701 [40100/50000 (80%)]  \tLoss:   87.962967\trec:   62.263626\tkl:   25.699335\n",
      "====> Epoch: 701 Average train loss: 90.8543\n",
      "====> Validation set loss: 94.5008\n",
      "====> Validation set kl: 26.2145\n",
      "Epoch: 702 [  100/50000 ( 0%)]  \tLoss:   92.658035\trec:   66.602081\tkl:   26.055952\n",
      "Epoch: 702 [10100/50000 (20%)]  \tLoss:   93.067947\trec:   67.231697\tkl:   25.836246\n",
      "Epoch: 702 [20100/50000 (40%)]  \tLoss:   92.011894\trec:   66.352654\tkl:   25.659235\n",
      "Epoch: 702 [30100/50000 (60%)]  \tLoss:   92.138702\trec:   65.975723\tkl:   26.162975\n",
      "Epoch: 702 [40100/50000 (80%)]  \tLoss:   92.489334\trec:   65.437012\tkl:   27.052324\n",
      "====> Epoch: 702 Average train loss: 90.8444\n",
      "====> Validation set loss: 94.6788\n",
      "====> Validation set kl: 26.0968\n",
      "Epoch: 703 [  100/50000 ( 0%)]  \tLoss:   92.102608\trec:   64.589043\tkl:   27.513563\n",
      "Epoch: 703 [10100/50000 (20%)]  \tLoss:   90.939590\trec:   64.850792\tkl:   26.088791\n",
      "Epoch: 703 [20100/50000 (40%)]  \tLoss:   87.035400\trec:   61.576225\tkl:   25.459171\n",
      "Epoch: 703 [30100/50000 (60%)]  \tLoss:   90.605453\trec:   64.597237\tkl:   26.008224\n",
      "Epoch: 703 [40100/50000 (80%)]  \tLoss:   93.069580\trec:   66.280518\tkl:   26.789059\n",
      "====> Epoch: 703 Average train loss: 90.8411\n",
      "====> Validation set loss: 94.4755\n",
      "====> Validation set kl: 26.2068\n",
      "Epoch: 704 [  100/50000 ( 0%)]  \tLoss:   90.860039\trec:   65.365616\tkl:   25.494419\n",
      "Epoch: 704 [10100/50000 (20%)]  \tLoss:   94.356415\trec:   67.237602\tkl:   27.118814\n",
      "Epoch: 704 [20100/50000 (40%)]  \tLoss:   94.149315\trec:   67.303970\tkl:   26.845341\n",
      "Epoch: 704 [30100/50000 (60%)]  \tLoss:   84.699402\trec:   59.820957\tkl:   24.878447\n",
      "Epoch: 704 [40100/50000 (80%)]  \tLoss:   89.277847\trec:   63.331142\tkl:   25.946703\n",
      "====> Epoch: 704 Average train loss: 90.8530\n",
      "====> Validation set loss: 94.5267\n",
      "====> Validation set kl: 26.2898\n",
      "Epoch: 705 [  100/50000 ( 0%)]  \tLoss:   89.939354\trec:   62.747936\tkl:   27.191414\n",
      "Epoch: 705 [10100/50000 (20%)]  \tLoss:   90.576042\trec:   65.104111\tkl:   25.471931\n",
      "Epoch: 705 [20100/50000 (40%)]  \tLoss:   96.063942\trec:   68.841591\tkl:   27.222359\n",
      "Epoch: 705 [30100/50000 (60%)]  \tLoss:   92.696304\trec:   66.485306\tkl:   26.211006\n",
      "Epoch: 705 [40100/50000 (80%)]  \tLoss:   91.319214\trec:   65.110855\tkl:   26.208357\n",
      "====> Epoch: 705 Average train loss: 90.8598\n",
      "====> Validation set loss: 94.4944\n",
      "====> Validation set kl: 26.1840\n",
      "Epoch: 706 [  100/50000 ( 0%)]  \tLoss:   88.982697\trec:   63.538486\tkl:   25.444204\n",
      "Epoch: 706 [10100/50000 (20%)]  \tLoss:   92.526688\trec:   66.110985\tkl:   26.415703\n",
      "Epoch: 706 [20100/50000 (40%)]  \tLoss:   88.887436\trec:   63.768974\tkl:   25.118471\n",
      "Epoch: 706 [30100/50000 (60%)]  \tLoss:   88.581230\trec:   63.284206\tkl:   25.297026\n",
      "Epoch: 706 [40100/50000 (80%)]  \tLoss:   92.012817\trec:   65.389153\tkl:   26.623671\n",
      "====> Epoch: 706 Average train loss: 90.8504\n",
      "====> Validation set loss: 94.4989\n",
      "====> Validation set kl: 26.0249\n",
      "Epoch: 707 [  100/50000 ( 0%)]  \tLoss:   91.077850\trec:   64.818993\tkl:   26.258860\n",
      "Epoch: 707 [10100/50000 (20%)]  \tLoss:   92.507217\trec:   64.782288\tkl:   27.724928\n",
      "Epoch: 707 [20100/50000 (40%)]  \tLoss:   89.580605\trec:   63.902431\tkl:   25.678179\n",
      "Epoch: 707 [30100/50000 (60%)]  \tLoss:   90.907654\trec:   64.558105\tkl:   26.349543\n",
      "Epoch: 707 [40100/50000 (80%)]  \tLoss:   90.347031\trec:   64.069939\tkl:   26.277094\n",
      "====> Epoch: 707 Average train loss: 90.8353\n",
      "====> Validation set loss: 94.5165\n",
      "====> Validation set kl: 26.2263\n",
      "Epoch: 708 [  100/50000 ( 0%)]  \tLoss:   92.209274\trec:   64.998672\tkl:   27.210602\n",
      "Epoch: 708 [10100/50000 (20%)]  \tLoss:   88.104996\trec:   62.503052\tkl:   25.601952\n",
      "Epoch: 708 [20100/50000 (40%)]  \tLoss:   92.607460\trec:   66.145081\tkl:   26.462376\n",
      "Epoch: 708 [30100/50000 (60%)]  \tLoss:   93.935661\trec:   67.334404\tkl:   26.601265\n",
      "Epoch: 708 [40100/50000 (80%)]  \tLoss:   94.522713\trec:   67.641846\tkl:   26.880867\n",
      "====> Epoch: 708 Average train loss: 90.8135\n",
      "====> Validation set loss: 94.6233\n",
      "====> Validation set kl: 26.2851\n",
      "Epoch: 709 [  100/50000 ( 0%)]  \tLoss:   93.388283\trec:   66.067352\tkl:   27.320927\n",
      "Epoch: 709 [10100/50000 (20%)]  \tLoss:   90.781235\trec:   64.611282\tkl:   26.169958\n",
      "Epoch: 709 [20100/50000 (40%)]  \tLoss:   89.298424\trec:   63.638062\tkl:   25.660366\n",
      "Epoch: 709 [30100/50000 (60%)]  \tLoss:   90.939049\trec:   64.431885\tkl:   26.507160\n",
      "Epoch: 709 [40100/50000 (80%)]  \tLoss:   91.239052\trec:   64.171928\tkl:   27.067122\n",
      "====> Epoch: 709 Average train loss: 90.8105\n",
      "====> Validation set loss: 94.6010\n",
      "====> Validation set kl: 26.1358\n",
      "Epoch: 710 [  100/50000 ( 0%)]  \tLoss:   88.478317\trec:   62.622997\tkl:   25.855316\n",
      "Epoch: 710 [10100/50000 (20%)]  \tLoss:   93.470108\trec:   65.882263\tkl:   27.587839\n",
      "Epoch: 710 [20100/50000 (40%)]  \tLoss:   89.272217\trec:   62.425053\tkl:   26.847160\n",
      "Epoch: 710 [30100/50000 (60%)]  \tLoss:   91.907867\trec:   66.141823\tkl:   25.766043\n",
      "Epoch: 710 [40100/50000 (80%)]  \tLoss:   92.009941\trec:   66.425217\tkl:   25.584726\n",
      "====> Epoch: 710 Average train loss: 90.8332\n",
      "====> Validation set loss: 94.4689\n",
      "====> Validation set kl: 26.2378\n",
      "Epoch: 711 [  100/50000 ( 0%)]  \tLoss:   90.004112\trec:   64.376724\tkl:   25.627384\n",
      "Epoch: 711 [10100/50000 (20%)]  \tLoss:   92.958359\trec:   66.673851\tkl:   26.284504\n",
      "Epoch: 711 [20100/50000 (40%)]  \tLoss:   94.377060\trec:   68.358032\tkl:   26.019022\n",
      "Epoch: 711 [30100/50000 (60%)]  \tLoss:   91.627419\trec:   64.660133\tkl:   26.967279\n",
      "Epoch: 711 [40100/50000 (80%)]  \tLoss:   92.083633\trec:   65.167442\tkl:   26.916193\n",
      "====> Epoch: 711 Average train loss: 90.8179\n",
      "====> Validation set loss: 94.4257\n",
      "====> Validation set kl: 26.3384\n",
      "Epoch: 712 [  100/50000 ( 0%)]  \tLoss:   89.999939\trec:   64.086769\tkl:   25.913166\n",
      "Epoch: 712 [10100/50000 (20%)]  \tLoss:   90.030045\trec:   64.600304\tkl:   25.429745\n",
      "Epoch: 712 [20100/50000 (40%)]  \tLoss:   90.944008\trec:   63.869545\tkl:   27.074471\n",
      "Epoch: 712 [30100/50000 (60%)]  \tLoss:   91.911270\trec:   65.119186\tkl:   26.792089\n",
      "Epoch: 712 [40100/50000 (80%)]  \tLoss:   91.238678\trec:   64.885323\tkl:   26.353355\n",
      "====> Epoch: 712 Average train loss: 90.8134\n",
      "====> Validation set loss: 94.5026\n",
      "====> Validation set kl: 26.3027\n",
      "Epoch: 713 [  100/50000 ( 0%)]  \tLoss:   90.869240\trec:   64.582275\tkl:   26.286953\n",
      "Epoch: 713 [10100/50000 (20%)]  \tLoss:   90.579811\trec:   63.351971\tkl:   27.227844\n",
      "Epoch: 713 [20100/50000 (40%)]  \tLoss:   89.315193\trec:   63.685528\tkl:   25.629667\n",
      "Epoch: 713 [30100/50000 (60%)]  \tLoss:   90.090927\trec:   63.455437\tkl:   26.635490\n",
      "Epoch: 713 [40100/50000 (80%)]  \tLoss:   88.494843\trec:   62.706211\tkl:   25.788628\n",
      "====> Epoch: 713 Average train loss: 90.8037\n",
      "====> Validation set loss: 94.5261\n",
      "====> Validation set kl: 26.2770\n",
      "Epoch: 714 [  100/50000 ( 0%)]  \tLoss:   91.431496\trec:   65.201973\tkl:   26.229519\n",
      "Epoch: 714 [10100/50000 (20%)]  \tLoss:   91.184181\trec:   64.865654\tkl:   26.318521\n",
      "Epoch: 714 [20100/50000 (40%)]  \tLoss:   93.741104\trec:   66.999794\tkl:   26.741308\n",
      "Epoch: 714 [30100/50000 (60%)]  \tLoss:   88.268082\trec:   62.343124\tkl:   25.924955\n",
      "Epoch: 714 [40100/50000 (80%)]  \tLoss:   89.868126\trec:   64.381386\tkl:   25.486734\n",
      "====> Epoch: 714 Average train loss: 90.8028\n",
      "====> Validation set loss: 94.5700\n",
      "====> Validation set kl: 26.3813\n",
      "Epoch: 715 [  100/50000 ( 0%)]  \tLoss:   89.733688\trec:   63.813496\tkl:   25.920189\n",
      "Epoch: 715 [10100/50000 (20%)]  \tLoss:   89.606026\trec:   63.834454\tkl:   25.771574\n",
      "Epoch: 715 [20100/50000 (40%)]  \tLoss:   89.673126\trec:   63.385876\tkl:   26.287241\n",
      "Epoch: 715 [30100/50000 (60%)]  \tLoss:   90.970596\trec:   65.116936\tkl:   25.853661\n",
      "Epoch: 715 [40100/50000 (80%)]  \tLoss:   91.958817\trec:   65.625526\tkl:   26.333290\n",
      "====> Epoch: 715 Average train loss: 90.7986\n",
      "====> Validation set loss: 94.6357\n",
      "====> Validation set kl: 26.2779\n",
      "Epoch: 716 [  100/50000 ( 0%)]  \tLoss:   93.453102\trec:   66.509521\tkl:   26.943581\n",
      "Epoch: 716 [10100/50000 (20%)]  \tLoss:   91.179863\trec:   64.922134\tkl:   26.257729\n",
      "Epoch: 716 [20100/50000 (40%)]  \tLoss:   87.362717\trec:   62.119762\tkl:   25.242950\n",
      "Epoch: 716 [30100/50000 (60%)]  \tLoss:   89.993462\trec:   63.399483\tkl:   26.593981\n",
      "Epoch: 716 [40100/50000 (80%)]  \tLoss:   93.432961\trec:   67.354790\tkl:   26.078163\n",
      "====> Epoch: 716 Average train loss: 90.8085\n",
      "====> Validation set loss: 94.4947\n",
      "====> Validation set kl: 26.1805\n",
      "Epoch: 717 [  100/50000 ( 0%)]  \tLoss:   93.413086\trec:   66.339188\tkl:   27.073900\n",
      "Epoch: 717 [10100/50000 (20%)]  \tLoss:   90.424316\trec:   64.153702\tkl:   26.270615\n",
      "Epoch: 717 [20100/50000 (40%)]  \tLoss:   90.278954\trec:   64.487869\tkl:   25.791084\n",
      "Epoch: 717 [30100/50000 (60%)]  \tLoss:   88.172104\trec:   62.158901\tkl:   26.013212\n",
      "Epoch: 717 [40100/50000 (80%)]  \tLoss:   93.801147\trec:   66.585464\tkl:   27.215689\n",
      "====> Epoch: 717 Average train loss: 90.7960\n",
      "====> Validation set loss: 94.5844\n",
      "====> Validation set kl: 26.2307\n",
      "Epoch: 718 [  100/50000 ( 0%)]  \tLoss:   91.317642\trec:   65.203247\tkl:   26.114391\n",
      "Epoch: 718 [10100/50000 (20%)]  \tLoss:   88.142487\trec:   61.967937\tkl:   26.174551\n",
      "Epoch: 718 [20100/50000 (40%)]  \tLoss:   89.014336\trec:   62.873848\tkl:   26.140490\n",
      "Epoch: 718 [30100/50000 (60%)]  \tLoss:   92.542213\trec:   66.392609\tkl:   26.149609\n",
      "Epoch: 718 [40100/50000 (80%)]  \tLoss:   87.132812\trec:   61.915936\tkl:   25.216869\n",
      "====> Epoch: 718 Average train loss: 90.7850\n",
      "====> Validation set loss: 94.5722\n",
      "====> Validation set kl: 26.2790\n",
      "Epoch: 719 [  100/50000 ( 0%)]  \tLoss:   86.783615\trec:   60.990803\tkl:   25.792803\n",
      "Epoch: 719 [10100/50000 (20%)]  \tLoss:   92.564781\trec:   65.560371\tkl:   27.004416\n",
      "Epoch: 719 [20100/50000 (40%)]  \tLoss:   89.115479\trec:   62.846443\tkl:   26.269035\n",
      "Epoch: 719 [30100/50000 (60%)]  \tLoss:   92.871269\trec:   65.982819\tkl:   26.888451\n",
      "Epoch: 719 [40100/50000 (80%)]  \tLoss:   91.042671\trec:   64.760216\tkl:   26.282461\n",
      "====> Epoch: 719 Average train loss: 90.8102\n",
      "====> Validation set loss: 94.5709\n",
      "====> Validation set kl: 26.3086\n",
      "Epoch: 720 [  100/50000 ( 0%)]  \tLoss:   93.728592\trec:   66.567810\tkl:   27.160780\n",
      "Epoch: 720 [10100/50000 (20%)]  \tLoss:   91.431709\trec:   66.146736\tkl:   25.284973\n",
      "Epoch: 720 [20100/50000 (40%)]  \tLoss:   92.714355\trec:   65.967392\tkl:   26.746965\n",
      "Epoch: 720 [30100/50000 (60%)]  \tLoss:   87.996666\trec:   61.947044\tkl:   26.049622\n",
      "Epoch: 720 [40100/50000 (80%)]  \tLoss:   88.324028\trec:   62.948635\tkl:   25.375395\n",
      "====> Epoch: 720 Average train loss: 90.7809\n",
      "====> Validation set loss: 94.5061\n",
      "====> Validation set kl: 26.1603\n",
      "Epoch: 721 [  100/50000 ( 0%)]  \tLoss:   90.070694\trec:   63.311813\tkl:   26.758877\n",
      "Epoch: 721 [10100/50000 (20%)]  \tLoss:   93.096275\trec:   66.416794\tkl:   26.679482\n",
      "Epoch: 721 [20100/50000 (40%)]  \tLoss:   94.282829\trec:   67.603432\tkl:   26.679394\n",
      "Epoch: 721 [30100/50000 (60%)]  \tLoss:   90.727791\trec:   64.869156\tkl:   25.858635\n",
      "Epoch: 721 [40100/50000 (80%)]  \tLoss:   90.555916\trec:   64.672134\tkl:   25.883780\n",
      "====> Epoch: 721 Average train loss: 90.7901\n",
      "====> Validation set loss: 94.5058\n",
      "====> Validation set kl: 26.3794\n",
      "Epoch: 722 [  100/50000 ( 0%)]  \tLoss:   90.594688\trec:   64.234428\tkl:   26.360256\n",
      "Epoch: 722 [10100/50000 (20%)]  \tLoss:   90.284782\trec:   63.520088\tkl:   26.764694\n",
      "Epoch: 722 [20100/50000 (40%)]  \tLoss:   88.320702\trec:   62.971905\tkl:   25.348801\n",
      "Epoch: 722 [30100/50000 (60%)]  \tLoss:   92.835915\trec:   66.727776\tkl:   26.108147\n",
      "Epoch: 722 [40100/50000 (80%)]  \tLoss:   86.346169\trec:   60.881653\tkl:   25.464521\n",
      "====> Epoch: 722 Average train loss: 90.7958\n",
      "====> Validation set loss: 94.5473\n",
      "====> Validation set kl: 26.3234\n",
      "Epoch: 723 [  100/50000 ( 0%)]  \tLoss:   88.857582\trec:   62.842957\tkl:   26.014627\n",
      "Epoch: 723 [10100/50000 (20%)]  \tLoss:   87.518944\trec:   61.397377\tkl:   26.121565\n",
      "Epoch: 723 [20100/50000 (40%)]  \tLoss:   91.534767\trec:   65.037956\tkl:   26.496811\n",
      "Epoch: 723 [30100/50000 (60%)]  \tLoss:   90.985039\trec:   64.266685\tkl:   26.718359\n",
      "Epoch: 723 [40100/50000 (80%)]  \tLoss:   92.662910\trec:   65.992645\tkl:   26.670265\n",
      "====> Epoch: 723 Average train loss: 90.7950\n",
      "====> Validation set loss: 94.5670\n",
      "====> Validation set kl: 26.4834\n",
      "Epoch: 724 [  100/50000 ( 0%)]  \tLoss:   91.121811\trec:   64.406372\tkl:   26.715445\n",
      "Epoch: 724 [10100/50000 (20%)]  \tLoss:   91.277153\trec:   65.221832\tkl:   26.055328\n",
      "Epoch: 724 [20100/50000 (40%)]  \tLoss:   92.645653\trec:   65.860527\tkl:   26.785118\n",
      "Epoch: 724 [30100/50000 (60%)]  \tLoss:   90.254936\trec:   63.838512\tkl:   26.416426\n",
      "Epoch: 724 [40100/50000 (80%)]  \tLoss:   87.712242\trec:   62.841221\tkl:   24.871019\n",
      "====> Epoch: 724 Average train loss: 90.7579\n",
      "====> Validation set loss: 94.6518\n",
      "====> Validation set kl: 26.3041\n",
      "Epoch: 725 [  100/50000 ( 0%)]  \tLoss:   88.419029\trec:   62.041248\tkl:   26.377785\n",
      "Epoch: 725 [10100/50000 (20%)]  \tLoss:   92.497658\trec:   65.120987\tkl:   27.376665\n",
      "Epoch: 725 [20100/50000 (40%)]  \tLoss:   85.742920\trec:   60.141735\tkl:   25.601179\n",
      "Epoch: 725 [30100/50000 (60%)]  \tLoss:   89.663895\trec:   63.177666\tkl:   26.486233\n",
      "Epoch: 725 [40100/50000 (80%)]  \tLoss:   85.706696\trec:   59.935757\tkl:   25.770947\n",
      "====> Epoch: 725 Average train loss: 90.7722\n",
      "====> Validation set loss: 94.5718\n",
      "====> Validation set kl: 26.2788\n",
      "Epoch: 726 [  100/50000 ( 0%)]  \tLoss:   90.693550\trec:   63.899216\tkl:   26.794333\n",
      "Epoch: 726 [10100/50000 (20%)]  \tLoss:   92.155113\trec:   65.451096\tkl:   26.704020\n",
      "Epoch: 726 [20100/50000 (40%)]  \tLoss:   87.833359\trec:   62.440273\tkl:   25.393091\n",
      "Epoch: 726 [30100/50000 (60%)]  \tLoss:   90.884872\trec:   64.167236\tkl:   26.717636\n",
      "Epoch: 726 [40100/50000 (80%)]  \tLoss:   92.674698\trec:   65.930603\tkl:   26.744091\n",
      "====> Epoch: 726 Average train loss: 90.7909\n",
      "====> Validation set loss: 94.4739\n",
      "====> Validation set kl: 26.2169\n",
      "Epoch: 727 [  100/50000 ( 0%)]  \tLoss:   87.931656\trec:   62.016190\tkl:   25.915462\n",
      "Epoch: 727 [10100/50000 (20%)]  \tLoss:   89.716309\trec:   63.323681\tkl:   26.392630\n",
      "Epoch: 727 [20100/50000 (40%)]  \tLoss:   90.831474\trec:   65.069527\tkl:   25.761946\n",
      "Epoch: 727 [30100/50000 (60%)]  \tLoss:   89.186089\trec:   63.508068\tkl:   25.678022\n",
      "Epoch: 727 [40100/50000 (80%)]  \tLoss:   91.901756\trec:   65.195183\tkl:   26.706572\n",
      "====> Epoch: 727 Average train loss: 90.7696\n",
      "====> Validation set loss: 94.5113\n",
      "====> Validation set kl: 26.2048\n",
      "Epoch: 728 [  100/50000 ( 0%)]  \tLoss:   90.827202\trec:   65.339905\tkl:   25.487297\n",
      "Epoch: 728 [10100/50000 (20%)]  \tLoss:   86.747978\trec:   61.299023\tkl:   25.448957\n",
      "Epoch: 728 [20100/50000 (40%)]  \tLoss:   89.096107\trec:   63.325272\tkl:   25.770844\n",
      "Epoch: 728 [30100/50000 (60%)]  \tLoss:   88.404922\trec:   62.312489\tkl:   26.092430\n",
      "Epoch: 728 [40100/50000 (80%)]  \tLoss:   89.623360\trec:   63.772976\tkl:   25.850386\n",
      "====> Epoch: 728 Average train loss: 90.7756\n",
      "====> Validation set loss: 94.5888\n",
      "====> Validation set kl: 26.2803\n",
      "Epoch: 729 [  100/50000 ( 0%)]  \tLoss:   92.488976\trec:   66.070168\tkl:   26.418806\n",
      "Epoch: 729 [10100/50000 (20%)]  \tLoss:   87.428307\trec:   61.235687\tkl:   26.192621\n",
      "Epoch: 729 [20100/50000 (40%)]  \tLoss:   94.557442\trec:   67.777985\tkl:   26.779448\n",
      "Epoch: 729 [30100/50000 (60%)]  \tLoss:   91.076485\trec:   65.012039\tkl:   26.064438\n",
      "Epoch: 729 [40100/50000 (80%)]  \tLoss:   91.976387\trec:   65.136162\tkl:   26.840221\n",
      "====> Epoch: 729 Average train loss: 90.7518\n",
      "====> Validation set loss: 94.4918\n",
      "====> Validation set kl: 26.2660\n",
      "Epoch: 730 [  100/50000 ( 0%)]  \tLoss:   88.795776\trec:   63.199909\tkl:   25.595865\n",
      "Epoch: 730 [10100/50000 (20%)]  \tLoss:   96.190201\trec:   68.904648\tkl:   27.285553\n",
      "Epoch: 730 [20100/50000 (40%)]  \tLoss:   90.804237\trec:   64.012520\tkl:   26.791710\n",
      "Epoch: 730 [30100/50000 (60%)]  \tLoss:   91.651329\trec:   65.664207\tkl:   25.987114\n",
      "Epoch: 730 [40100/50000 (80%)]  \tLoss:   93.692558\trec:   66.391121\tkl:   27.301432\n",
      "====> Epoch: 730 Average train loss: 90.7719\n",
      "====> Validation set loss: 94.5144\n",
      "====> Validation set kl: 26.0195\n",
      "Epoch: 731 [  100/50000 ( 0%)]  \tLoss:   88.145981\trec:   62.638611\tkl:   25.507372\n",
      "Epoch: 731 [10100/50000 (20%)]  \tLoss:   94.115845\trec:   67.884933\tkl:   26.230915\n",
      "Epoch: 731 [20100/50000 (40%)]  \tLoss:   89.451279\trec:   63.087059\tkl:   26.364220\n",
      "Epoch: 731 [30100/50000 (60%)]  \tLoss:   90.178276\trec:   64.297638\tkl:   25.880644\n",
      "Epoch: 731 [40100/50000 (80%)]  \tLoss:   89.147247\trec:   63.131523\tkl:   26.015724\n",
      "====> Epoch: 731 Average train loss: 90.7575\n",
      "====> Validation set loss: 94.5903\n",
      "====> Validation set kl: 26.2647\n",
      "Epoch: 732 [  100/50000 ( 0%)]  \tLoss:   87.392761\trec:   61.123402\tkl:   26.269360\n",
      "Epoch: 732 [10100/50000 (20%)]  \tLoss:   90.450722\trec:   64.498985\tkl:   25.951742\n",
      "Epoch: 732 [20100/50000 (40%)]  \tLoss:   91.304443\trec:   64.319870\tkl:   26.984575\n",
      "Epoch: 732 [30100/50000 (60%)]  \tLoss:   92.631882\trec:   65.873764\tkl:   26.758121\n",
      "Epoch: 732 [40100/50000 (80%)]  \tLoss:   91.139374\trec:   64.828537\tkl:   26.310839\n",
      "====> Epoch: 732 Average train loss: 90.7576\n",
      "====> Validation set loss: 94.4815\n",
      "====> Validation set kl: 26.2057\n",
      "Epoch: 733 [  100/50000 ( 0%)]  \tLoss:   91.411179\trec:   64.935570\tkl:   26.475613\n",
      "Epoch: 733 [10100/50000 (20%)]  \tLoss:   89.875977\trec:   63.664246\tkl:   26.211733\n",
      "Epoch: 733 [20100/50000 (40%)]  \tLoss:   93.029259\trec:   66.138580\tkl:   26.890678\n",
      "Epoch: 733 [30100/50000 (60%)]  \tLoss:   88.226456\trec:   62.632938\tkl:   25.593513\n",
      "Epoch: 733 [40100/50000 (80%)]  \tLoss:   88.791412\trec:   63.223236\tkl:   25.568180\n",
      "====> Epoch: 733 Average train loss: 90.7609\n",
      "====> Validation set loss: 94.4846\n",
      "====> Validation set kl: 26.1419\n",
      "Epoch: 734 [  100/50000 ( 0%)]  \tLoss:   90.155220\trec:   64.015350\tkl:   26.139870\n",
      "Epoch: 734 [10100/50000 (20%)]  \tLoss:   88.160515\trec:   62.027946\tkl:   26.132570\n",
      "Epoch: 734 [20100/50000 (40%)]  \tLoss:   88.654037\trec:   62.297958\tkl:   26.356087\n",
      "Epoch: 734 [30100/50000 (60%)]  \tLoss:   91.395859\trec:   64.724449\tkl:   26.671404\n",
      "Epoch: 734 [40100/50000 (80%)]  \tLoss:   87.536308\trec:   61.548096\tkl:   25.988216\n",
      "====> Epoch: 734 Average train loss: 90.7553\n",
      "====> Validation set loss: 94.5312\n",
      "====> Validation set kl: 26.2819\n",
      "Epoch: 735 [  100/50000 ( 0%)]  \tLoss:   91.989166\trec:   65.047989\tkl:   26.941174\n",
      "Epoch: 735 [10100/50000 (20%)]  \tLoss:   93.827164\trec:   67.441254\tkl:   26.385910\n",
      "Epoch: 735 [20100/50000 (40%)]  \tLoss:   92.522911\trec:   65.486137\tkl:   27.036764\n",
      "Epoch: 735 [30100/50000 (60%)]  \tLoss:   88.314568\trec:   62.835281\tkl:   25.479282\n",
      "Epoch: 735 [40100/50000 (80%)]  \tLoss:   92.750877\trec:   66.285118\tkl:   26.465759\n",
      "====> Epoch: 735 Average train loss: 90.7588\n",
      "====> Validation set loss: 94.4637\n",
      "====> Validation set kl: 26.1705\n",
      "Epoch: 736 [  100/50000 ( 0%)]  \tLoss:   93.080887\trec:   67.066940\tkl:   26.013945\n",
      "Epoch: 736 [10100/50000 (20%)]  \tLoss:   90.316734\trec:   63.755318\tkl:   26.561420\n",
      "Epoch: 736 [20100/50000 (40%)]  \tLoss:   95.283134\trec:   68.065567\tkl:   27.217560\n",
      "Epoch: 736 [30100/50000 (60%)]  \tLoss:   90.371895\trec:   64.690498\tkl:   25.681393\n",
      "Epoch: 736 [40100/50000 (80%)]  \tLoss:   92.173172\trec:   65.754028\tkl:   26.419142\n",
      "====> Epoch: 736 Average train loss: 90.7670\n",
      "====> Validation set loss: 94.5427\n",
      "====> Validation set kl: 26.3357\n",
      "Epoch: 737 [  100/50000 ( 0%)]  \tLoss:   89.403847\trec:   63.439037\tkl:   25.964804\n",
      "Epoch: 737 [10100/50000 (20%)]  \tLoss:   86.995331\trec:   61.189671\tkl:   25.805660\n",
      "Epoch: 737 [20100/50000 (40%)]  \tLoss:   89.496025\trec:   63.707958\tkl:   25.788069\n",
      "Epoch: 737 [30100/50000 (60%)]  \tLoss:   94.055748\trec:   67.759796\tkl:   26.295956\n",
      "Epoch: 737 [40100/50000 (80%)]  \tLoss:   94.586334\trec:   67.797394\tkl:   26.788942\n",
      "====> Epoch: 737 Average train loss: 90.7465\n",
      "====> Validation set loss: 94.3585\n",
      "====> Validation set kl: 26.1647\n",
      "Epoch: 738 [  100/50000 ( 0%)]  \tLoss:   90.578842\trec:   63.706406\tkl:   26.872446\n",
      "Epoch: 738 [10100/50000 (20%)]  \tLoss:   90.902908\trec:   64.368378\tkl:   26.534531\n",
      "Epoch: 738 [20100/50000 (40%)]  \tLoss:   90.409767\trec:   63.927822\tkl:   26.481943\n",
      "Epoch: 738 [30100/50000 (60%)]  \tLoss:   91.137848\trec:   64.481544\tkl:   26.656303\n",
      "Epoch: 738 [40100/50000 (80%)]  \tLoss:   93.811523\trec:   67.891510\tkl:   25.920017\n",
      "====> Epoch: 738 Average train loss: 90.7227\n",
      "====> Validation set loss: 94.5115\n",
      "====> Validation set kl: 26.4319\n",
      "Epoch: 739 [  100/50000 ( 0%)]  \tLoss:   89.025925\trec:   63.006477\tkl:   26.019445\n",
      "Epoch: 739 [10100/50000 (20%)]  \tLoss:   90.283043\trec:   64.541740\tkl:   25.741308\n",
      "Epoch: 739 [20100/50000 (40%)]  \tLoss:   88.678337\trec:   63.280659\tkl:   25.397684\n",
      "Epoch: 739 [30100/50000 (60%)]  \tLoss:   90.078575\trec:   64.119972\tkl:   25.958605\n",
      "Epoch: 739 [40100/50000 (80%)]  \tLoss:   92.592751\trec:   65.672508\tkl:   26.920246\n",
      "====> Epoch: 739 Average train loss: 90.7355\n",
      "====> Validation set loss: 94.4524\n",
      "====> Validation set kl: 26.2276\n",
      "Epoch: 740 [  100/50000 ( 0%)]  \tLoss:   90.001831\trec:   63.864120\tkl:   26.137718\n",
      "Epoch: 740 [10100/50000 (20%)]  \tLoss:   89.054703\trec:   64.042603\tkl:   25.012098\n",
      "Epoch: 740 [20100/50000 (40%)]  \tLoss:   93.792389\trec:   67.191856\tkl:   26.600538\n",
      "Epoch: 740 [30100/50000 (60%)]  \tLoss:   91.756699\trec:   65.694550\tkl:   26.062143\n",
      "Epoch: 740 [40100/50000 (80%)]  \tLoss:   91.499344\trec:   64.590050\tkl:   26.909294\n",
      "====> Epoch: 740 Average train loss: 90.7331\n",
      "====> Validation set loss: 94.7486\n",
      "====> Validation set kl: 26.2527\n",
      "Epoch: 741 [  100/50000 ( 0%)]  \tLoss:   90.034157\trec:   63.900761\tkl:   26.133398\n",
      "Epoch: 741 [10100/50000 (20%)]  \tLoss:   89.208183\trec:   63.378212\tkl:   25.829969\n",
      "Epoch: 741 [20100/50000 (40%)]  \tLoss:   92.265205\trec:   65.808281\tkl:   26.456923\n",
      "Epoch: 741 [30100/50000 (60%)]  \tLoss:   91.499619\trec:   64.736946\tkl:   26.762669\n",
      "Epoch: 741 [40100/50000 (80%)]  \tLoss:   92.740837\trec:   66.578117\tkl:   26.162724\n",
      "====> Epoch: 741 Average train loss: 90.7562\n",
      "====> Validation set loss: 94.5124\n",
      "====> Validation set kl: 26.2713\n",
      "Epoch: 742 [  100/50000 ( 0%)]  \tLoss:   92.249107\trec:   64.890411\tkl:   27.358698\n",
      "Epoch: 742 [10100/50000 (20%)]  \tLoss:   90.231941\trec:   63.730946\tkl:   26.500996\n",
      "Epoch: 742 [20100/50000 (40%)]  \tLoss:   89.162636\trec:   63.335369\tkl:   25.827267\n",
      "Epoch: 742 [30100/50000 (60%)]  \tLoss:   90.204330\trec:   64.380264\tkl:   25.824072\n",
      "Epoch: 742 [40100/50000 (80%)]  \tLoss:   86.783432\trec:   61.476757\tkl:   25.306681\n",
      "====> Epoch: 742 Average train loss: 90.7403\n",
      "====> Validation set loss: 94.5807\n",
      "====> Validation set kl: 26.2966\n",
      "Epoch: 743 [  100/50000 ( 0%)]  \tLoss:   87.372261\trec:   61.463093\tkl:   25.909170\n",
      "Epoch: 743 [10100/50000 (20%)]  \tLoss:   90.609589\trec:   64.831177\tkl:   25.778408\n",
      "Epoch: 743 [20100/50000 (40%)]  \tLoss:   90.273224\trec:   63.878803\tkl:   26.394421\n",
      "Epoch: 743 [30100/50000 (60%)]  \tLoss:   89.290474\trec:   63.441521\tkl:   25.848957\n",
      "Epoch: 743 [40100/50000 (80%)]  \tLoss:   95.254852\trec:   68.227165\tkl:   27.027683\n",
      "====> Epoch: 743 Average train loss: 90.7346\n",
      "====> Validation set loss: 94.4547\n",
      "====> Validation set kl: 26.1655\n",
      "Epoch: 744 [  100/50000 ( 0%)]  \tLoss:   88.577263\trec:   63.572842\tkl:   25.004423\n",
      "Epoch: 744 [10100/50000 (20%)]  \tLoss:   95.861717\trec:   68.251778\tkl:   27.609941\n",
      "Epoch: 744 [20100/50000 (40%)]  \tLoss:   88.339355\trec:   62.633236\tkl:   25.706120\n",
      "Epoch: 744 [30100/50000 (60%)]  \tLoss:   90.863571\trec:   64.721924\tkl:   26.141649\n",
      "Epoch: 744 [40100/50000 (80%)]  \tLoss:   93.204590\trec:   66.497604\tkl:   26.706989\n",
      "====> Epoch: 744 Average train loss: 90.7475\n",
      "====> Validation set loss: 94.7012\n",
      "====> Validation set kl: 26.2861\n",
      "Epoch: 745 [  100/50000 ( 0%)]  \tLoss:   86.344215\trec:   61.046780\tkl:   25.297432\n",
      "Epoch: 745 [10100/50000 (20%)]  \tLoss:   88.532700\trec:   62.670300\tkl:   25.862400\n",
      "Epoch: 745 [20100/50000 (40%)]  \tLoss:   86.411972\trec:   60.597996\tkl:   25.813969\n",
      "Epoch: 745 [30100/50000 (60%)]  \tLoss:   87.343246\trec:   62.024002\tkl:   25.319248\n",
      "Epoch: 745 [40100/50000 (80%)]  \tLoss:   91.556267\trec:   64.467247\tkl:   27.089020\n",
      "====> Epoch: 745 Average train loss: 90.7342\n",
      "====> Validation set loss: 94.5025\n",
      "====> Validation set kl: 26.2517\n",
      "Epoch: 746 [  100/50000 ( 0%)]  \tLoss:   90.498062\trec:   64.033295\tkl:   26.464775\n",
      "Epoch: 746 [10100/50000 (20%)]  \tLoss:   90.984184\trec:   65.244682\tkl:   25.739504\n",
      "Epoch: 746 [20100/50000 (40%)]  \tLoss:   86.823486\trec:   61.640018\tkl:   25.183468\n",
      "Epoch: 746 [30100/50000 (60%)]  \tLoss:   90.842735\trec:   64.083359\tkl:   26.759378\n",
      "Epoch: 746 [40100/50000 (80%)]  \tLoss:   96.407486\trec:   69.611076\tkl:   26.796413\n",
      "====> Epoch: 746 Average train loss: 90.7264\n",
      "====> Validation set loss: 94.5091\n",
      "====> Validation set kl: 26.2761\n",
      "Epoch: 747 [  100/50000 ( 0%)]  \tLoss:   89.649933\trec:   63.737724\tkl:   25.912209\n",
      "Epoch: 747 [10100/50000 (20%)]  \tLoss:   92.449997\trec:   66.410576\tkl:   26.039425\n",
      "Epoch: 747 [20100/50000 (40%)]  \tLoss:   92.115311\trec:   65.460678\tkl:   26.654636\n",
      "Epoch: 747 [30100/50000 (60%)]  \tLoss:   88.371147\trec:   62.074009\tkl:   26.297146\n",
      "Epoch: 747 [40100/50000 (80%)]  \tLoss:   89.962830\trec:   63.294941\tkl:   26.667894\n",
      "====> Epoch: 747 Average train loss: 90.7064\n",
      "====> Validation set loss: 94.5150\n",
      "====> Validation set kl: 26.3754\n",
      "Epoch: 748 [  100/50000 ( 0%)]  \tLoss:   89.992706\trec:   63.309868\tkl:   26.682837\n",
      "Epoch: 748 [10100/50000 (20%)]  \tLoss:   89.908302\trec:   63.372372\tkl:   26.535927\n",
      "Epoch: 748 [20100/50000 (40%)]  \tLoss:   86.051773\trec:   60.893257\tkl:   25.158522\n",
      "Epoch: 748 [30100/50000 (60%)]  \tLoss:   91.911446\trec:   65.771011\tkl:   26.140434\n",
      "Epoch: 748 [40100/50000 (80%)]  \tLoss:   83.153496\trec:   58.202244\tkl:   24.951244\n",
      "====> Epoch: 748 Average train loss: 90.6885\n",
      "====> Validation set loss: 94.4353\n",
      "====> Validation set kl: 26.1855\n",
      "Epoch: 749 [  100/50000 ( 0%)]  \tLoss:   90.069237\trec:   63.842529\tkl:   26.226704\n",
      "Epoch: 749 [10100/50000 (20%)]  \tLoss:   88.368454\trec:   62.823357\tkl:   25.545095\n",
      "Epoch: 749 [20100/50000 (40%)]  \tLoss:   92.560486\trec:   65.599831\tkl:   26.960649\n",
      "Epoch: 749 [30100/50000 (60%)]  \tLoss:   88.713478\trec:   63.603764\tkl:   25.109715\n",
      "Epoch: 749 [40100/50000 (80%)]  \tLoss:   87.173950\trec:   62.328007\tkl:   24.845943\n",
      "====> Epoch: 749 Average train loss: 90.7037\n",
      "====> Validation set loss: 94.5366\n",
      "====> Validation set kl: 26.2231\n",
      "Epoch: 750 [  100/50000 ( 0%)]  \tLoss:   87.485947\trec:   62.222195\tkl:   25.263746\n",
      "Epoch: 750 [10100/50000 (20%)]  \tLoss:   90.901375\trec:   64.941971\tkl:   25.959402\n",
      "Epoch: 750 [20100/50000 (40%)]  \tLoss:   89.056114\trec:   63.447083\tkl:   25.609028\n",
      "Epoch: 750 [30100/50000 (60%)]  \tLoss:   88.678749\trec:   63.535114\tkl:   25.143633\n",
      "Epoch: 750 [40100/50000 (80%)]  \tLoss:   89.134834\trec:   62.029148\tkl:   27.105682\n",
      "====> Epoch: 750 Average train loss: 90.7285\n",
      "====> Validation set loss: 94.4809\n",
      "====> Validation set kl: 26.0106\n",
      "Epoch: 751 [  100/50000 ( 0%)]  \tLoss:   88.577225\trec:   63.025478\tkl:   25.551748\n",
      "Epoch: 751 [10100/50000 (20%)]  \tLoss:   88.337051\trec:   62.786209\tkl:   25.550837\n",
      "Epoch: 751 [20100/50000 (40%)]  \tLoss:   88.704605\trec:   62.941219\tkl:   25.763384\n",
      "Epoch: 751 [30100/50000 (60%)]  \tLoss:   92.296700\trec:   66.096710\tkl:   26.199986\n",
      "Epoch: 751 [40100/50000 (80%)]  \tLoss:   91.382713\trec:   64.490204\tkl:   26.892513\n",
      "====> Epoch: 751 Average train loss: 90.6943\n",
      "====> Validation set loss: 94.4937\n",
      "====> Validation set kl: 26.1420\n",
      "Epoch: 752 [  100/50000 ( 0%)]  \tLoss:   88.475639\trec:   62.166759\tkl:   26.308878\n",
      "Epoch: 752 [10100/50000 (20%)]  \tLoss:   90.501923\trec:   64.310448\tkl:   26.191475\n",
      "Epoch: 752 [20100/50000 (40%)]  \tLoss:   90.475563\trec:   64.097649\tkl:   26.377916\n",
      "Epoch: 752 [30100/50000 (60%)]  \tLoss:   90.339470\trec:   63.097900\tkl:   27.241568\n",
      "Epoch: 752 [40100/50000 (80%)]  \tLoss:   87.605339\trec:   61.345898\tkl:   26.259443\n",
      "====> Epoch: 752 Average train loss: 90.7005\n",
      "====> Validation set loss: 94.4857\n",
      "====> Validation set kl: 26.3336\n",
      "Epoch: 753 [  100/50000 ( 0%)]  \tLoss:   90.491524\trec:   64.072998\tkl:   26.418514\n",
      "Epoch: 753 [10100/50000 (20%)]  \tLoss:   90.544838\trec:   64.493057\tkl:   26.051781\n",
      "Epoch: 753 [20100/50000 (40%)]  \tLoss:   89.749199\trec:   62.594578\tkl:   27.154619\n",
      "Epoch: 753 [30100/50000 (60%)]  \tLoss:   89.513443\trec:   64.010056\tkl:   25.503387\n",
      "Epoch: 753 [40100/50000 (80%)]  \tLoss:   90.084656\trec:   63.493652\tkl:   26.591007\n",
      "====> Epoch: 753 Average train loss: 90.7151\n",
      "====> Validation set loss: 94.4597\n",
      "====> Validation set kl: 26.4165\n",
      "Epoch: 754 [  100/50000 ( 0%)]  \tLoss:   90.429543\trec:   64.009377\tkl:   26.420162\n",
      "Epoch: 754 [10100/50000 (20%)]  \tLoss:   93.113495\trec:   66.142242\tkl:   26.971245\n",
      "Epoch: 754 [20100/50000 (40%)]  \tLoss:   90.593727\trec:   64.009262\tkl:   26.584469\n",
      "Epoch: 754 [30100/50000 (60%)]  \tLoss:   88.210739\trec:   61.424961\tkl:   26.785780\n",
      "Epoch: 754 [40100/50000 (80%)]  \tLoss:   87.847847\trec:   62.171776\tkl:   25.676069\n",
      "====> Epoch: 754 Average train loss: 90.6921\n",
      "====> Validation set loss: 94.4659\n",
      "====> Validation set kl: 26.3162\n",
      "Epoch: 755 [  100/50000 ( 0%)]  \tLoss:   92.009415\trec:   65.374504\tkl:   26.634911\n",
      "Epoch: 755 [10100/50000 (20%)]  \tLoss:   93.489151\trec:   66.665779\tkl:   26.823366\n",
      "Epoch: 755 [20100/50000 (40%)]  \tLoss:   88.398796\trec:   62.082108\tkl:   26.316689\n",
      "Epoch: 755 [30100/50000 (60%)]  \tLoss:   92.108826\trec:   65.281006\tkl:   26.827814\n",
      "Epoch: 755 [40100/50000 (80%)]  \tLoss:   89.670143\trec:   63.655766\tkl:   26.014378\n",
      "====> Epoch: 755 Average train loss: 90.7164\n",
      "====> Validation set loss: 94.4871\n",
      "====> Validation set kl: 26.2088\n",
      "Epoch: 756 [  100/50000 ( 0%)]  \tLoss:   96.272194\trec:   68.839699\tkl:   27.432499\n",
      "Epoch: 756 [10100/50000 (20%)]  \tLoss:   91.625000\trec:   65.032562\tkl:   26.592438\n",
      "Epoch: 756 [20100/50000 (40%)]  \tLoss:   90.887497\trec:   64.035034\tkl:   26.852463\n",
      "Epoch: 756 [30100/50000 (60%)]  \tLoss:   85.453651\trec:   60.533394\tkl:   24.920258\n",
      "Epoch: 756 [40100/50000 (80%)]  \tLoss:   91.571251\trec:   64.684074\tkl:   26.887173\n",
      "====> Epoch: 756 Average train loss: 90.6884\n",
      "====> Validation set loss: 94.5422\n",
      "====> Validation set kl: 26.3326\n",
      "Epoch: 757 [  100/50000 ( 0%)]  \tLoss:   93.406822\trec:   66.312141\tkl:   27.094685\n",
      "Epoch: 757 [10100/50000 (20%)]  \tLoss:   87.331421\trec:   60.707996\tkl:   26.623432\n",
      "Epoch: 757 [20100/50000 (40%)]  \tLoss:   90.695740\trec:   64.617188\tkl:   26.078560\n",
      "Epoch: 757 [30100/50000 (60%)]  \tLoss:   91.074600\trec:   65.166786\tkl:   25.907812\n",
      "Epoch: 757 [40100/50000 (80%)]  \tLoss:   87.655251\trec:   62.332314\tkl:   25.322945\n",
      "====> Epoch: 757 Average train loss: 90.6856\n",
      "====> Validation set loss: 94.4261\n",
      "====> Validation set kl: 26.2177\n",
      "Epoch: 758 [  100/50000 ( 0%)]  \tLoss:   90.650078\trec:   65.214386\tkl:   25.435688\n",
      "Epoch: 758 [10100/50000 (20%)]  \tLoss:   90.953156\trec:   63.927246\tkl:   27.025909\n",
      "Epoch: 758 [20100/50000 (40%)]  \tLoss:   90.399345\trec:   63.699776\tkl:   26.699572\n",
      "Epoch: 758 [30100/50000 (60%)]  \tLoss:   87.952499\trec:   62.307068\tkl:   25.645430\n",
      "Epoch: 758 [40100/50000 (80%)]  \tLoss:   91.768867\trec:   64.913139\tkl:   26.855726\n",
      "====> Epoch: 758 Average train loss: 90.6934\n",
      "====> Validation set loss: 94.5051\n",
      "====> Validation set kl: 26.3026\n",
      "Epoch: 759 [  100/50000 ( 0%)]  \tLoss:   88.794464\trec:   62.132053\tkl:   26.662405\n",
      "Epoch: 759 [10100/50000 (20%)]  \tLoss:   91.065170\trec:   64.314316\tkl:   26.750854\n",
      "Epoch: 759 [20100/50000 (40%)]  \tLoss:   91.705719\trec:   65.329689\tkl:   26.376040\n",
      "Epoch: 759 [30100/50000 (60%)]  \tLoss:   91.033203\trec:   65.037544\tkl:   25.995653\n",
      "Epoch: 759 [40100/50000 (80%)]  \tLoss:   86.892067\trec:   60.873295\tkl:   26.018774\n",
      "====> Epoch: 759 Average train loss: 90.6861\n",
      "====> Validation set loss: 94.5839\n",
      "====> Validation set kl: 26.1904\n",
      "Epoch: 760 [  100/50000 ( 0%)]  \tLoss:   94.332458\trec:   67.229668\tkl:   27.102787\n",
      "Epoch: 760 [10100/50000 (20%)]  \tLoss:   87.901535\trec:   61.956169\tkl:   25.945360\n",
      "Epoch: 760 [20100/50000 (40%)]  \tLoss:   85.718887\trec:   59.655300\tkl:   26.063589\n",
      "Epoch: 760 [30100/50000 (60%)]  \tLoss:   94.031784\trec:   67.624969\tkl:   26.406818\n",
      "Epoch: 760 [40100/50000 (80%)]  \tLoss:   86.876053\trec:   60.857536\tkl:   26.018513\n",
      "====> Epoch: 760 Average train loss: 90.7055\n",
      "====> Validation set loss: 94.3578\n",
      "====> Validation set kl: 26.3107\n",
      "Epoch: 761 [  100/50000 ( 0%)]  \tLoss:   91.845955\trec:   63.822346\tkl:   28.023603\n",
      "Epoch: 761 [10100/50000 (20%)]  \tLoss:   90.548126\trec:   63.985580\tkl:   26.562538\n",
      "Epoch: 761 [20100/50000 (40%)]  \tLoss:   93.540291\trec:   67.014023\tkl:   26.526268\n",
      "Epoch: 761 [30100/50000 (60%)]  \tLoss:   89.651627\trec:   63.261322\tkl:   26.390310\n",
      "Epoch: 761 [40100/50000 (80%)]  \tLoss:   90.586998\trec:   64.057419\tkl:   26.529587\n",
      "====> Epoch: 761 Average train loss: 90.6410\n",
      "====> Validation set loss: 94.5817\n",
      "====> Validation set kl: 26.2164\n",
      "Epoch: 762 [  100/50000 ( 0%)]  \tLoss:   88.815544\trec:   62.214218\tkl:   26.601322\n",
      "Epoch: 762 [10100/50000 (20%)]  \tLoss:   87.965225\trec:   62.413246\tkl:   25.551979\n",
      "Epoch: 762 [20100/50000 (40%)]  \tLoss:   88.404587\trec:   61.626072\tkl:   26.778515\n",
      "Epoch: 762 [30100/50000 (60%)]  \tLoss:   88.651207\trec:   62.375198\tkl:   26.276009\n",
      "Epoch: 762 [40100/50000 (80%)]  \tLoss:   89.422470\trec:   62.917126\tkl:   26.505341\n",
      "====> Epoch: 762 Average train loss: 90.6869\n",
      "====> Validation set loss: 94.4392\n",
      "====> Validation set kl: 26.4370\n",
      "Epoch: 763 [  100/50000 ( 0%)]  \tLoss:   88.846245\trec:   62.211010\tkl:   26.635237\n",
      "Epoch: 763 [10100/50000 (20%)]  \tLoss:   94.012169\trec:   67.559822\tkl:   26.452343\n",
      "Epoch: 763 [20100/50000 (40%)]  \tLoss:   88.756599\trec:   62.831074\tkl:   25.925524\n",
      "Epoch: 763 [30100/50000 (60%)]  \tLoss:   90.201698\trec:   63.674793\tkl:   26.526903\n",
      "Epoch: 763 [40100/50000 (80%)]  \tLoss:   91.500694\trec:   65.029030\tkl:   26.471663\n",
      "====> Epoch: 763 Average train loss: 90.6579\n",
      "====> Validation set loss: 94.5312\n",
      "====> Validation set kl: 26.5280\n",
      "Epoch: 764 [  100/50000 ( 0%)]  \tLoss:   90.289764\trec:   63.926609\tkl:   26.363152\n",
      "Epoch: 764 [10100/50000 (20%)]  \tLoss:   89.415688\trec:   63.150085\tkl:   26.265602\n",
      "Epoch: 764 [20100/50000 (40%)]  \tLoss:   88.038712\trec:   62.197002\tkl:   25.841713\n",
      "Epoch: 764 [30100/50000 (60%)]  \tLoss:   91.527184\trec:   64.840431\tkl:   26.686762\n",
      "Epoch: 764 [40100/50000 (80%)]  \tLoss:   91.103584\trec:   64.888374\tkl:   26.215202\n",
      "====> Epoch: 764 Average train loss: 90.6896\n",
      "====> Validation set loss: 94.4533\n",
      "====> Validation set kl: 26.2293\n",
      "Epoch: 765 [  100/50000 ( 0%)]  \tLoss:   90.226250\trec:   64.674438\tkl:   25.551805\n",
      "Epoch: 765 [10100/50000 (20%)]  \tLoss:   92.624695\trec:   65.584999\tkl:   27.039696\n",
      "Epoch: 765 [20100/50000 (40%)]  \tLoss:   97.330162\trec:   69.899712\tkl:   27.430456\n",
      "Epoch: 765 [30100/50000 (60%)]  \tLoss:   92.454567\trec:   65.745308\tkl:   26.709257\n",
      "Epoch: 765 [40100/50000 (80%)]  \tLoss:   91.306259\trec:   65.079453\tkl:   26.226809\n",
      "====> Epoch: 765 Average train loss: 90.6427\n",
      "====> Validation set loss: 94.4380\n",
      "====> Validation set kl: 26.1853\n",
      "Epoch: 766 [  100/50000 ( 0%)]  \tLoss:   92.437935\trec:   66.064667\tkl:   26.373270\n",
      "Epoch: 766 [10100/50000 (20%)]  \tLoss:   85.598190\trec:   60.894882\tkl:   24.703310\n",
      "Epoch: 766 [20100/50000 (40%)]  \tLoss:   90.150917\trec:   64.452370\tkl:   25.698547\n",
      "Epoch: 766 [30100/50000 (60%)]  \tLoss:   89.059303\trec:   63.251087\tkl:   25.808218\n",
      "Epoch: 766 [40100/50000 (80%)]  \tLoss:   90.269936\trec:   64.421227\tkl:   25.848711\n",
      "====> Epoch: 766 Average train loss: 90.6623\n",
      "====> Validation set loss: 94.4873\n",
      "====> Validation set kl: 26.5772\n",
      "Epoch: 767 [  100/50000 ( 0%)]  \tLoss:   90.523514\trec:   64.236809\tkl:   26.286703\n",
      "Epoch: 767 [10100/50000 (20%)]  \tLoss:   91.695564\trec:   64.420547\tkl:   27.275015\n",
      "Epoch: 767 [20100/50000 (40%)]  \tLoss:   88.889656\trec:   63.422253\tkl:   25.467400\n",
      "Epoch: 767 [30100/50000 (60%)]  \tLoss:   88.323257\trec:   62.274246\tkl:   26.049015\n",
      "Epoch: 767 [40100/50000 (80%)]  \tLoss:   93.661942\trec:   66.955215\tkl:   26.706722\n",
      "====> Epoch: 767 Average train loss: 90.6631\n",
      "====> Validation set loss: 94.5328\n",
      "====> Validation set kl: 26.2160\n",
      "Epoch: 768 [  100/50000 ( 0%)]  \tLoss:   86.797401\trec:   60.760803\tkl:   26.036596\n",
      "Epoch: 768 [10100/50000 (20%)]  \tLoss:   85.122772\trec:   59.408085\tkl:   25.714691\n",
      "Epoch: 768 [20100/50000 (40%)]  \tLoss:   90.587456\trec:   64.124863\tkl:   26.462597\n",
      "Epoch: 768 [30100/50000 (60%)]  \tLoss:   90.874214\trec:   64.501587\tkl:   26.372631\n",
      "Epoch: 768 [40100/50000 (80%)]  \tLoss:   91.364532\trec:   64.704056\tkl:   26.660471\n",
      "====> Epoch: 768 Average train loss: 90.6450\n",
      "====> Validation set loss: 94.3967\n",
      "====> Validation set kl: 26.3248\n",
      "Epoch: 769 [  100/50000 ( 0%)]  \tLoss:   89.272087\trec:   64.122505\tkl:   25.149574\n",
      "Epoch: 769 [10100/50000 (20%)]  \tLoss:   92.625351\trec:   65.861229\tkl:   26.764118\n",
      "Epoch: 769 [20100/50000 (40%)]  \tLoss:   90.121796\trec:   63.354233\tkl:   26.767569\n",
      "Epoch: 769 [30100/50000 (60%)]  \tLoss:   91.089928\trec:   64.618362\tkl:   26.471567\n",
      "Epoch: 769 [40100/50000 (80%)]  \tLoss:   87.751938\trec:   61.879608\tkl:   25.872335\n",
      "====> Epoch: 769 Average train loss: 90.6695\n",
      "====> Validation set loss: 94.3882\n",
      "====> Validation set kl: 26.3615\n",
      "Epoch: 770 [  100/50000 ( 0%)]  \tLoss:   92.446640\trec:   65.636398\tkl:   26.810244\n",
      "Epoch: 770 [10100/50000 (20%)]  \tLoss:   92.163887\trec:   66.091721\tkl:   26.072170\n",
      "Epoch: 770 [20100/50000 (40%)]  \tLoss:   92.430824\trec:   66.511650\tkl:   25.919176\n",
      "Epoch: 770 [30100/50000 (60%)]  \tLoss:   92.802315\trec:   65.931633\tkl:   26.870678\n",
      "Epoch: 770 [40100/50000 (80%)]  \tLoss:   86.620171\trec:   61.525555\tkl:   25.094624\n",
      "====> Epoch: 770 Average train loss: 90.6705\n",
      "====> Validation set loss: 94.5425\n",
      "====> Validation set kl: 26.4091\n",
      "Epoch: 771 [  100/50000 ( 0%)]  \tLoss:   89.643150\trec:   63.158154\tkl:   26.484999\n",
      "Epoch: 771 [10100/50000 (20%)]  \tLoss:   91.040955\trec:   63.583908\tkl:   27.457045\n",
      "Epoch: 771 [20100/50000 (40%)]  \tLoss:   92.501846\trec:   65.401505\tkl:   27.100344\n",
      "Epoch: 771 [30100/50000 (60%)]  \tLoss:   89.454102\trec:   63.128124\tkl:   26.325981\n",
      "Epoch: 771 [40100/50000 (80%)]  \tLoss:   88.628693\trec:   62.279839\tkl:   26.348856\n",
      "====> Epoch: 771 Average train loss: 90.6371\n",
      "====> Validation set loss: 94.5917\n",
      "====> Validation set kl: 26.2135\n",
      "Epoch: 772 [  100/50000 ( 0%)]  \tLoss:   93.267334\trec:   67.038963\tkl:   26.228367\n",
      "Epoch: 772 [10100/50000 (20%)]  \tLoss:   90.165359\trec:   63.170853\tkl:   26.994503\n",
      "Epoch: 772 [20100/50000 (40%)]  \tLoss:   87.581345\trec:   62.302948\tkl:   25.278400\n",
      "Epoch: 772 [30100/50000 (60%)]  \tLoss:   89.160645\trec:   62.696655\tkl:   26.463993\n",
      "Epoch: 772 [40100/50000 (80%)]  \tLoss:   88.361717\trec:   62.327969\tkl:   26.033749\n",
      "====> Epoch: 772 Average train loss: 90.6485\n",
      "====> Validation set loss: 94.3751\n",
      "====> Validation set kl: 26.2234\n",
      "Epoch: 773 [  100/50000 ( 0%)]  \tLoss:   88.027237\trec:   61.767410\tkl:   26.259827\n",
      "Epoch: 773 [10100/50000 (20%)]  \tLoss:   92.156967\trec:   65.124229\tkl:   27.032742\n",
      "Epoch: 773 [20100/50000 (40%)]  \tLoss:   86.122955\trec:   59.927879\tkl:   26.195080\n",
      "Epoch: 773 [30100/50000 (60%)]  \tLoss:   92.111977\trec:   65.575905\tkl:   26.536074\n",
      "Epoch: 773 [40100/50000 (80%)]  \tLoss:   93.022003\trec:   66.689148\tkl:   26.332851\n",
      "====> Epoch: 773 Average train loss: 90.6251\n",
      "====> Validation set loss: 94.3956\n",
      "====> Validation set kl: 26.3958\n",
      "Epoch: 774 [  100/50000 ( 0%)]  \tLoss:   92.010536\trec:   65.529984\tkl:   26.480549\n",
      "Epoch: 774 [10100/50000 (20%)]  \tLoss:   91.850060\trec:   64.565453\tkl:   27.284601\n",
      "Epoch: 774 [20100/50000 (40%)]  \tLoss:   91.629219\trec:   65.058922\tkl:   26.570297\n",
      "Epoch: 774 [30100/50000 (60%)]  \tLoss:   91.625221\trec:   65.154053\tkl:   26.471169\n",
      "Epoch: 774 [40100/50000 (80%)]  \tLoss:   89.036278\trec:   63.063934\tkl:   25.972345\n",
      "====> Epoch: 774 Average train loss: 90.6251\n",
      "====> Validation set loss: 94.5295\n",
      "====> Validation set kl: 26.1288\n",
      "Epoch: 775 [  100/50000 ( 0%)]  \tLoss:   91.119766\trec:   64.798759\tkl:   26.321003\n",
      "Epoch: 775 [10100/50000 (20%)]  \tLoss:   93.877693\trec:   67.273064\tkl:   26.604631\n",
      "Epoch: 775 [20100/50000 (40%)]  \tLoss:   96.869591\trec:   68.991417\tkl:   27.878176\n",
      "Epoch: 775 [30100/50000 (60%)]  \tLoss:   92.818550\trec:   66.523293\tkl:   26.295256\n",
      "Epoch: 775 [40100/50000 (80%)]  \tLoss:   88.880959\trec:   62.849297\tkl:   26.031654\n",
      "====> Epoch: 775 Average train loss: 90.6334\n",
      "====> Validation set loss: 94.4631\n",
      "====> Validation set kl: 26.2970\n",
      "Epoch: 776 [  100/50000 ( 0%)]  \tLoss:   88.156738\trec:   62.794193\tkl:   25.362549\n",
      "Epoch: 776 [10100/50000 (20%)]  \tLoss:   90.333771\trec:   63.892178\tkl:   26.441589\n",
      "Epoch: 776 [20100/50000 (40%)]  \tLoss:   88.564995\trec:   62.988445\tkl:   25.576555\n",
      "Epoch: 776 [30100/50000 (60%)]  \tLoss:   87.541245\trec:   62.228298\tkl:   25.312948\n",
      "Epoch: 776 [40100/50000 (80%)]  \tLoss:   96.238594\trec:   69.369347\tkl:   26.869246\n",
      "====> Epoch: 776 Average train loss: 90.6471\n",
      "====> Validation set loss: 94.5590\n",
      "====> Validation set kl: 26.4319\n",
      "Epoch: 777 [  100/50000 ( 0%)]  \tLoss:   88.437218\trec:   62.624435\tkl:   25.812780\n",
      "Epoch: 777 [10100/50000 (20%)]  \tLoss:   92.188141\trec:   66.006828\tkl:   26.181318\n",
      "Epoch: 777 [20100/50000 (40%)]  \tLoss:   92.811417\trec:   66.206245\tkl:   26.605165\n",
      "Epoch: 777 [30100/50000 (60%)]  \tLoss:   88.206444\trec:   62.409832\tkl:   25.796610\n",
      "Epoch: 777 [40100/50000 (80%)]  \tLoss:   91.221596\trec:   65.479675\tkl:   25.741920\n",
      "====> Epoch: 777 Average train loss: 90.6374\n",
      "====> Validation set loss: 94.3797\n",
      "====> Validation set kl: 26.4001\n",
      "Epoch: 778 [  100/50000 ( 0%)]  \tLoss:   92.536774\trec:   66.285645\tkl:   26.251129\n",
      "Epoch: 778 [10100/50000 (20%)]  \tLoss:   94.359840\trec:   67.687622\tkl:   26.672220\n",
      "Epoch: 778 [20100/50000 (40%)]  \tLoss:   90.765640\trec:   64.093803\tkl:   26.671841\n",
      "Epoch: 778 [30100/50000 (60%)]  \tLoss:   90.721626\trec:   63.503487\tkl:   27.218142\n",
      "Epoch: 778 [40100/50000 (80%)]  \tLoss:   87.997108\trec:   62.078480\tkl:   25.918631\n",
      "====> Epoch: 778 Average train loss: 90.6030\n",
      "====> Validation set loss: 94.5362\n",
      "====> Validation set kl: 26.2036\n",
      "Epoch: 779 [  100/50000 ( 0%)]  \tLoss:   87.584961\trec:   61.986137\tkl:   25.598825\n",
      "Epoch: 779 [10100/50000 (20%)]  \tLoss:   89.744537\trec:   63.198475\tkl:   26.546064\n",
      "Epoch: 779 [20100/50000 (40%)]  \tLoss:   91.407036\trec:   64.480621\tkl:   26.926414\n",
      "Epoch: 779 [30100/50000 (60%)]  \tLoss:   94.465157\trec:   67.572502\tkl:   26.892658\n",
      "Epoch: 779 [40100/50000 (80%)]  \tLoss:   94.791901\trec:   67.516266\tkl:   27.275637\n",
      "====> Epoch: 779 Average train loss: 90.6460\n",
      "====> Validation set loss: 94.6494\n",
      "====> Validation set kl: 26.7422\n",
      "Epoch: 780 [  100/50000 ( 0%)]  \tLoss:   92.721344\trec:   66.364128\tkl:   26.357222\n",
      "Epoch: 780 [10100/50000 (20%)]  \tLoss:   86.796158\trec:   60.918991\tkl:   25.877165\n",
      "Epoch: 780 [20100/50000 (40%)]  \tLoss:   88.080391\trec:   62.567402\tkl:   25.512983\n",
      "Epoch: 780 [30100/50000 (60%)]  \tLoss:   92.626938\trec:   66.086006\tkl:   26.540937\n",
      "Epoch: 780 [40100/50000 (80%)]  \tLoss:   91.237846\trec:   64.930229\tkl:   26.307621\n",
      "====> Epoch: 780 Average train loss: 90.6217\n",
      "====> Validation set loss: 94.5441\n",
      "====> Validation set kl: 26.2501\n",
      "Epoch: 781 [  100/50000 ( 0%)]  \tLoss:   89.865341\trec:   63.854321\tkl:   26.011023\n",
      "Epoch: 781 [10100/50000 (20%)]  \tLoss:   87.579460\trec:   61.364906\tkl:   26.214552\n",
      "Epoch: 781 [20100/50000 (40%)]  \tLoss:   89.666870\trec:   63.102898\tkl:   26.563974\n",
      "Epoch: 781 [30100/50000 (60%)]  \tLoss:   93.782928\trec:   67.417305\tkl:   26.365623\n",
      "Epoch: 781 [40100/50000 (80%)]  \tLoss:   88.826462\trec:   62.969803\tkl:   25.856657\n",
      "====> Epoch: 781 Average train loss: 90.6335\n",
      "====> Validation set loss: 94.4476\n",
      "====> Validation set kl: 26.4236\n",
      "Epoch: 782 [  100/50000 ( 0%)]  \tLoss:   90.451012\trec:   64.371895\tkl:   26.079119\n",
      "Epoch: 782 [10100/50000 (20%)]  \tLoss:   91.022957\trec:   65.193062\tkl:   25.829895\n",
      "Epoch: 782 [20100/50000 (40%)]  \tLoss:   89.504372\trec:   63.051624\tkl:   26.452749\n",
      "Epoch: 782 [30100/50000 (60%)]  \tLoss:   89.864723\trec:   63.666061\tkl:   26.198666\n",
      "Epoch: 782 [40100/50000 (80%)]  \tLoss:   88.829872\trec:   62.890488\tkl:   25.939386\n",
      "====> Epoch: 782 Average train loss: 90.6126\n",
      "====> Validation set loss: 94.5377\n",
      "====> Validation set kl: 26.2688\n",
      "Epoch: 783 [  100/50000 ( 0%)]  \tLoss:   90.383728\trec:   64.154167\tkl:   26.229567\n",
      "Epoch: 783 [10100/50000 (20%)]  \tLoss:   89.344078\trec:   62.905319\tkl:   26.438757\n",
      "Epoch: 783 [20100/50000 (40%)]  \tLoss:   91.125031\trec:   63.839622\tkl:   27.285408\n",
      "Epoch: 783 [30100/50000 (60%)]  \tLoss:   91.855942\trec:   65.297455\tkl:   26.558493\n",
      "Epoch: 783 [40100/50000 (80%)]  \tLoss:   87.417763\trec:   61.030037\tkl:   26.387726\n",
      "====> Epoch: 783 Average train loss: 90.6116\n",
      "====> Validation set loss: 94.4509\n",
      "====> Validation set kl: 26.1273\n",
      "Epoch: 784 [  100/50000 ( 0%)]  \tLoss:   89.975632\trec:   63.662930\tkl:   26.312702\n",
      "Epoch: 784 [10100/50000 (20%)]  \tLoss:   94.188133\trec:   67.530685\tkl:   26.657454\n",
      "Epoch: 784 [20100/50000 (40%)]  \tLoss:   86.138687\trec:   59.283726\tkl:   26.854967\n",
      "Epoch: 784 [30100/50000 (60%)]  \tLoss:   89.558014\trec:   63.500866\tkl:   26.057148\n",
      "Epoch: 784 [40100/50000 (80%)]  \tLoss:   91.921104\trec:   64.665497\tkl:   27.255602\n",
      "====> Epoch: 784 Average train loss: 90.6029\n",
      "====> Validation set loss: 94.5083\n",
      "====> Validation set kl: 26.2370\n",
      "Epoch: 785 [  100/50000 ( 0%)]  \tLoss:   92.107391\trec:   66.149033\tkl:   25.958357\n",
      "Epoch: 785 [10100/50000 (20%)]  \tLoss:   91.893669\trec:   64.989746\tkl:   26.903921\n",
      "Epoch: 785 [20100/50000 (40%)]  \tLoss:   90.660782\trec:   63.645859\tkl:   27.014919\n",
      "Epoch: 785 [30100/50000 (60%)]  \tLoss:   95.064667\trec:   68.298508\tkl:   26.766161\n",
      "Epoch: 785 [40100/50000 (80%)]  \tLoss:   87.324181\trec:   61.848484\tkl:   25.475691\n",
      "====> Epoch: 785 Average train loss: 90.6143\n",
      "====> Validation set loss: 94.4866\n",
      "====> Validation set kl: 26.3244\n",
      "Epoch: 786 [  100/50000 ( 0%)]  \tLoss:   90.838654\trec:   65.235870\tkl:   25.602787\n",
      "Epoch: 786 [10100/50000 (20%)]  \tLoss:   94.765854\trec:   67.923996\tkl:   26.841860\n",
      "Epoch: 786 [20100/50000 (40%)]  \tLoss:   87.608269\trec:   61.926693\tkl:   25.681574\n",
      "Epoch: 786 [30100/50000 (60%)]  \tLoss:   90.504532\trec:   64.437286\tkl:   26.067251\n",
      "Epoch: 786 [40100/50000 (80%)]  \tLoss:   90.541267\trec:   63.777832\tkl:   26.763432\n",
      "====> Epoch: 786 Average train loss: 90.6217\n",
      "====> Validation set loss: 94.5382\n",
      "====> Validation set kl: 26.3939\n",
      "Epoch: 787 [  100/50000 ( 0%)]  \tLoss:   96.127457\trec:   69.135437\tkl:   26.992023\n",
      "Epoch: 787 [10100/50000 (20%)]  \tLoss:   90.699203\trec:   64.715797\tkl:   25.983406\n",
      "Epoch: 787 [20100/50000 (40%)]  \tLoss:   92.349930\trec:   65.867989\tkl:   26.481943\n",
      "Epoch: 787 [30100/50000 (60%)]  \tLoss:   91.083961\trec:   64.659904\tkl:   26.424067\n",
      "Epoch: 787 [40100/50000 (80%)]  \tLoss:   90.209511\trec:   64.991348\tkl:   25.218159\n",
      "====> Epoch: 787 Average train loss: 90.6196\n",
      "====> Validation set loss: 94.5187\n",
      "====> Validation set kl: 26.2456\n",
      "Epoch: 788 [  100/50000 ( 0%)]  \tLoss:   88.103500\trec:   61.948837\tkl:   26.154665\n",
      "Epoch: 788 [10100/50000 (20%)]  \tLoss:   91.586296\trec:   64.996162\tkl:   26.590132\n",
      "Epoch: 788 [20100/50000 (40%)]  \tLoss:   95.252686\trec:   67.909004\tkl:   27.343681\n",
      "Epoch: 788 [30100/50000 (60%)]  \tLoss:   87.332558\trec:   60.837284\tkl:   26.495270\n",
      "Epoch: 788 [40100/50000 (80%)]  \tLoss:   89.843552\trec:   63.703579\tkl:   26.139980\n",
      "====> Epoch: 788 Average train loss: 90.6001\n",
      "====> Validation set loss: 94.5119\n",
      "====> Validation set kl: 26.1575\n",
      "Epoch: 789 [  100/50000 ( 0%)]  \tLoss:   91.080017\trec:   63.525879\tkl:   27.554140\n",
      "Epoch: 789 [10100/50000 (20%)]  \tLoss:   90.879791\trec:   63.565228\tkl:   27.314568\n",
      "Epoch: 789 [20100/50000 (40%)]  \tLoss:   91.996910\trec:   64.655815\tkl:   27.341087\n",
      "Epoch: 789 [30100/50000 (60%)]  \tLoss:   88.746811\trec:   62.826923\tkl:   25.919888\n",
      "Epoch: 789 [40100/50000 (80%)]  \tLoss:   90.869858\trec:   64.114967\tkl:   26.754887\n",
      "====> Epoch: 789 Average train loss: 90.5846\n",
      "====> Validation set loss: 94.5654\n",
      "====> Validation set kl: 26.2989\n",
      "Epoch: 790 [  100/50000 ( 0%)]  \tLoss:   94.880554\trec:   68.206894\tkl:   26.673662\n",
      "Epoch: 790 [10100/50000 (20%)]  \tLoss:   91.138687\trec:   64.874001\tkl:   26.264692\n",
      "Epoch: 790 [20100/50000 (40%)]  \tLoss:   87.308556\trec:   61.873650\tkl:   25.434902\n",
      "Epoch: 790 [30100/50000 (60%)]  \tLoss:   91.007622\trec:   64.991379\tkl:   26.016247\n",
      "Epoch: 790 [40100/50000 (80%)]  \tLoss:   90.493950\trec:   63.852371\tkl:   26.641581\n",
      "====> Epoch: 790 Average train loss: 90.6049\n",
      "====> Validation set loss: 94.5853\n",
      "====> Validation set kl: 26.3889\n",
      "Epoch: 791 [  100/50000 ( 0%)]  \tLoss:   89.364624\trec:   63.180912\tkl:   26.183716\n",
      "Epoch: 791 [10100/50000 (20%)]  \tLoss:   90.666603\trec:   64.319603\tkl:   26.346992\n",
      "Epoch: 791 [20100/50000 (40%)]  \tLoss:   89.976318\trec:   63.218731\tkl:   26.757586\n",
      "Epoch: 791 [30100/50000 (60%)]  \tLoss:   87.527168\trec:   61.778358\tkl:   25.748814\n",
      "Epoch: 791 [40100/50000 (80%)]  \tLoss:   93.404236\trec:   66.747856\tkl:   26.656374\n",
      "====> Epoch: 791 Average train loss: 90.5849\n",
      "====> Validation set loss: 94.4484\n",
      "====> Validation set kl: 26.1057\n",
      "Epoch: 792 [  100/50000 ( 0%)]  \tLoss:   92.438004\trec:   65.699883\tkl:   26.738129\n",
      "Epoch: 792 [10100/50000 (20%)]  \tLoss:   89.564194\trec:   63.183586\tkl:   26.380615\n",
      "Epoch: 792 [20100/50000 (40%)]  \tLoss:   92.439064\trec:   65.346138\tkl:   27.092915\n",
      "Epoch: 792 [30100/50000 (60%)]  \tLoss:   94.537285\trec:   68.625801\tkl:   25.911482\n",
      "Epoch: 792 [40100/50000 (80%)]  \tLoss:   94.428467\trec:   68.044914\tkl:   26.383553\n",
      "====> Epoch: 792 Average train loss: 90.5972\n",
      "====> Validation set loss: 94.4807\n",
      "====> Validation set kl: 26.3241\n",
      "Epoch: 793 [  100/50000 ( 0%)]  \tLoss:   90.376114\trec:   64.487495\tkl:   25.888618\n",
      "Epoch: 793 [10100/50000 (20%)]  \tLoss:   90.409706\trec:   64.688110\tkl:   25.721588\n",
      "Epoch: 793 [20100/50000 (40%)]  \tLoss:   93.227989\trec:   66.321327\tkl:   26.906664\n",
      "Epoch: 793 [30100/50000 (60%)]  \tLoss:   88.220505\trec:   62.299809\tkl:   25.920694\n",
      "Epoch: 793 [40100/50000 (80%)]  \tLoss:   91.615059\trec:   65.379761\tkl:   26.235291\n",
      "====> Epoch: 793 Average train loss: 90.5748\n",
      "====> Validation set loss: 94.5566\n",
      "====> Validation set kl: 26.3464\n",
      "Epoch: 794 [  100/50000 ( 0%)]  \tLoss:   89.842476\trec:   63.684608\tkl:   26.157867\n",
      "Epoch: 794 [10100/50000 (20%)]  \tLoss:   90.885040\trec:   64.158875\tkl:   26.726164\n",
      "Epoch: 794 [20100/50000 (40%)]  \tLoss:   90.510193\trec:   64.374634\tkl:   26.135559\n",
      "Epoch: 794 [30100/50000 (60%)]  \tLoss:   93.608200\trec:   66.948273\tkl:   26.659929\n",
      "Epoch: 794 [40100/50000 (80%)]  \tLoss:   93.646347\trec:   66.408096\tkl:   27.238251\n",
      "====> Epoch: 794 Average train loss: 90.5850\n",
      "====> Validation set loss: 94.5551\n",
      "====> Validation set kl: 26.5954\n",
      "Epoch: 795 [  100/50000 ( 0%)]  \tLoss:   90.962189\trec:   64.199989\tkl:   26.762197\n",
      "Epoch: 795 [10100/50000 (20%)]  \tLoss:   90.055679\trec:   64.088066\tkl:   25.967611\n",
      "Epoch: 795 [20100/50000 (40%)]  \tLoss:   93.228218\trec:   66.337326\tkl:   26.890898\n",
      "Epoch: 795 [30100/50000 (60%)]  \tLoss:   91.398331\trec:   65.587296\tkl:   25.811035\n",
      "Epoch: 795 [40100/50000 (80%)]  \tLoss:   90.727066\trec:   64.500084\tkl:   26.226986\n",
      "====> Epoch: 795 Average train loss: 90.5964\n",
      "====> Validation set loss: 94.5181\n",
      "====> Validation set kl: 26.3937\n",
      "Epoch: 796 [  100/50000 ( 0%)]  \tLoss:   89.593826\trec:   62.641891\tkl:   26.951933\n",
      "Epoch: 796 [10100/50000 (20%)]  \tLoss:   93.755859\trec:   66.260216\tkl:   27.495640\n",
      "Epoch: 796 [20100/50000 (40%)]  \tLoss:   88.213318\trec:   62.502197\tkl:   25.711117\n",
      "Epoch: 796 [30100/50000 (60%)]  \tLoss:   90.035172\trec:   63.493622\tkl:   26.541552\n",
      "Epoch: 796 [40100/50000 (80%)]  \tLoss:   90.686432\trec:   65.210381\tkl:   25.476051\n",
      "====> Epoch: 796 Average train loss: 90.5978\n",
      "====> Validation set loss: 94.4476\n",
      "====> Validation set kl: 26.3045\n",
      "Epoch: 797 [  100/50000 ( 0%)]  \tLoss:   91.488632\trec:   64.640770\tkl:   26.847866\n",
      "Epoch: 797 [10100/50000 (20%)]  \tLoss:   86.523552\trec:   60.651566\tkl:   25.871988\n",
      "Epoch: 797 [20100/50000 (40%)]  \tLoss:   87.218697\trec:   61.718639\tkl:   25.500055\n",
      "Epoch: 797 [30100/50000 (60%)]  \tLoss:   88.755737\trec:   61.852264\tkl:   26.903477\n",
      "Epoch: 797 [40100/50000 (80%)]  \tLoss:   91.878120\trec:   65.092934\tkl:   26.785191\n",
      "====> Epoch: 797 Average train loss: 90.5803\n",
      "====> Validation set loss: 94.3495\n",
      "====> Validation set kl: 26.1979\n",
      "Epoch: 798 [  100/50000 ( 0%)]  \tLoss:   93.639168\trec:   66.727150\tkl:   26.912018\n",
      "Epoch: 798 [10100/50000 (20%)]  \tLoss:   89.262146\trec:   63.334183\tkl:   25.927969\n",
      "Epoch: 798 [20100/50000 (40%)]  \tLoss:   92.583702\trec:   65.378426\tkl:   27.205271\n",
      "Epoch: 798 [30100/50000 (60%)]  \tLoss:   92.065300\trec:   66.790329\tkl:   25.274967\n",
      "Epoch: 798 [40100/50000 (80%)]  \tLoss:   89.319199\trec:   62.916748\tkl:   26.402452\n",
      "====> Epoch: 798 Average train loss: 90.5880\n",
      "====> Validation set loss: 94.4910\n",
      "====> Validation set kl: 26.4269\n",
      "Epoch: 799 [  100/50000 ( 0%)]  \tLoss:   91.489487\trec:   64.881783\tkl:   26.607700\n",
      "Epoch: 799 [10100/50000 (20%)]  \tLoss:   90.727676\trec:   64.666054\tkl:   26.061625\n",
      "Epoch: 799 [20100/50000 (40%)]  \tLoss:   94.586967\trec:   67.081055\tkl:   27.505915\n",
      "Epoch: 799 [30100/50000 (60%)]  \tLoss:   92.067436\trec:   65.596169\tkl:   26.471270\n",
      "Epoch: 799 [40100/50000 (80%)]  \tLoss:   90.512070\trec:   64.303490\tkl:   26.208578\n",
      "====> Epoch: 799 Average train loss: 90.5876\n",
      "====> Validation set loss: 94.5263\n",
      "====> Validation set kl: 26.2693\n",
      "Epoch: 800 [  100/50000 ( 0%)]  \tLoss:   88.495193\trec:   61.929623\tkl:   26.565575\n",
      "Epoch: 800 [10100/50000 (20%)]  \tLoss:   90.982788\trec:   65.103722\tkl:   25.879072\n",
      "Epoch: 800 [20100/50000 (40%)]  \tLoss:   90.579483\trec:   63.727909\tkl:   26.851574\n",
      "Epoch: 800 [30100/50000 (60%)]  \tLoss:   90.453369\trec:   64.173424\tkl:   26.279943\n",
      "Epoch: 800 [40100/50000 (80%)]  \tLoss:   91.344597\trec:   64.896278\tkl:   26.448320\n",
      "====> Epoch: 800 Average train loss: 90.5701\n",
      "====> Validation set loss: 94.4786\n",
      "====> Validation set kl: 26.2871\n",
      "Epoch: 801 [  100/50000 ( 0%)]  \tLoss:   91.311928\trec:   64.137062\tkl:   27.174877\n",
      "Epoch: 801 [10100/50000 (20%)]  \tLoss:   91.257637\trec:   64.262909\tkl:   26.994728\n",
      "Epoch: 801 [20100/50000 (40%)]  \tLoss:   87.480919\trec:   61.869129\tkl:   25.611782\n",
      "Epoch: 801 [30100/50000 (60%)]  \tLoss:   90.576500\trec:   64.557350\tkl:   26.019156\n",
      "Epoch: 801 [40100/50000 (80%)]  \tLoss:   93.760887\trec:   66.559959\tkl:   27.200930\n",
      "====> Epoch: 801 Average train loss: 90.5750\n",
      "====> Validation set loss: 94.4704\n",
      "====> Validation set kl: 26.3965\n",
      "Epoch: 802 [  100/50000 ( 0%)]  \tLoss:   85.177544\trec:   60.114491\tkl:   25.063057\n",
      "Epoch: 802 [10100/50000 (20%)]  \tLoss:   89.306580\trec:   63.256775\tkl:   26.049809\n",
      "Epoch: 802 [20100/50000 (40%)]  \tLoss:   92.319092\trec:   64.883179\tkl:   27.435913\n",
      "Epoch: 802 [30100/50000 (60%)]  \tLoss:   89.216324\trec:   63.078217\tkl:   26.138115\n",
      "Epoch: 802 [40100/50000 (80%)]  \tLoss:   89.032814\trec:   63.004105\tkl:   26.028706\n",
      "====> Epoch: 802 Average train loss: 90.5856\n",
      "====> Validation set loss: 94.5336\n",
      "====> Validation set kl: 26.1251\n",
      "Epoch: 803 [  100/50000 ( 0%)]  \tLoss:   93.429459\trec:   66.832169\tkl:   26.597298\n",
      "Epoch: 803 [10100/50000 (20%)]  \tLoss:   88.109810\trec:   62.213142\tkl:   25.896671\n",
      "Epoch: 803 [20100/50000 (40%)]  \tLoss:   95.549507\trec:   68.688286\tkl:   26.861217\n",
      "Epoch: 803 [30100/50000 (60%)]  \tLoss:   89.893280\trec:   63.351917\tkl:   26.541365\n",
      "Epoch: 803 [40100/50000 (80%)]  \tLoss:   88.202957\trec:   62.157005\tkl:   26.045952\n",
      "====> Epoch: 803 Average train loss: 90.5714\n",
      "====> Validation set loss: 94.5592\n",
      "====> Validation set kl: 26.2975\n",
      "Epoch: 804 [  100/50000 ( 0%)]  \tLoss:   90.074814\trec:   63.843330\tkl:   26.231482\n",
      "Epoch: 804 [10100/50000 (20%)]  \tLoss:   91.319626\trec:   64.616524\tkl:   26.703100\n",
      "Epoch: 804 [20100/50000 (40%)]  \tLoss:   89.136940\trec:   62.851562\tkl:   26.285383\n",
      "Epoch: 804 [30100/50000 (60%)]  \tLoss:   93.164932\trec:   65.431725\tkl:   27.733202\n",
      "Epoch: 804 [40100/50000 (80%)]  \tLoss:   92.743279\trec:   65.805092\tkl:   26.938189\n",
      "====> Epoch: 804 Average train loss: 90.5366\n",
      "====> Validation set loss: 94.4541\n",
      "====> Validation set kl: 26.2470\n",
      "Epoch: 805 [  100/50000 ( 0%)]  \tLoss:   87.768295\trec:   61.482128\tkl:   26.286167\n",
      "Epoch: 805 [10100/50000 (20%)]  \tLoss:   91.838562\trec:   65.452713\tkl:   26.385849\n",
      "Epoch: 805 [20100/50000 (40%)]  \tLoss:   89.795425\trec:   63.771740\tkl:   26.023691\n",
      "Epoch: 805 [30100/50000 (60%)]  \tLoss:   88.587120\trec:   63.145638\tkl:   25.441477\n",
      "Epoch: 805 [40100/50000 (80%)]  \tLoss:   89.646187\trec:   64.414589\tkl:   25.231598\n",
      "====> Epoch: 805 Average train loss: 90.5716\n",
      "====> Validation set loss: 94.4544\n",
      "====> Validation set kl: 26.1921\n",
      "Epoch: 806 [  100/50000 ( 0%)]  \tLoss:   90.261177\trec:   63.694138\tkl:   26.567038\n",
      "Epoch: 806 [10100/50000 (20%)]  \tLoss:   88.712654\trec:   62.972649\tkl:   25.740004\n",
      "Epoch: 806 [20100/50000 (40%)]  \tLoss:   87.130722\trec:   60.509758\tkl:   26.620958\n",
      "Epoch: 806 [30100/50000 (60%)]  \tLoss:   90.137611\trec:   63.239632\tkl:   26.897987\n",
      "Epoch: 806 [40100/50000 (80%)]  \tLoss:   92.808891\trec:   66.168892\tkl:   26.639999\n",
      "====> Epoch: 806 Average train loss: 90.5561\n",
      "====> Validation set loss: 94.4164\n",
      "====> Validation set kl: 26.2731\n",
      "Epoch: 807 [  100/50000 ( 0%)]  \tLoss:   85.107750\trec:   60.196316\tkl:   24.911440\n",
      "Epoch: 807 [10100/50000 (20%)]  \tLoss:   85.737755\trec:   59.902634\tkl:   25.835117\n",
      "Epoch: 807 [20100/50000 (40%)]  \tLoss:   90.380798\trec:   63.687622\tkl:   26.693178\n",
      "Epoch: 807 [30100/50000 (60%)]  \tLoss:   86.942734\trec:   60.472870\tkl:   26.469862\n",
      "Epoch: 807 [40100/50000 (80%)]  \tLoss:   83.732292\trec:   58.655994\tkl:   25.076296\n",
      "====> Epoch: 807 Average train loss: 90.5504\n",
      "====> Validation set loss: 94.5668\n",
      "====> Validation set kl: 26.4814\n",
      "Epoch: 808 [  100/50000 ( 0%)]  \tLoss:   90.554863\trec:   64.012009\tkl:   26.542856\n",
      "Epoch: 808 [10100/50000 (20%)]  \tLoss:   89.311134\trec:   62.246426\tkl:   27.064709\n",
      "Epoch: 808 [20100/50000 (40%)]  \tLoss:   88.843178\trec:   63.876255\tkl:   24.966923\n",
      "Epoch: 808 [30100/50000 (60%)]  \tLoss:   90.607498\trec:   63.690174\tkl:   26.917320\n",
      "Epoch: 808 [40100/50000 (80%)]  \tLoss:   87.683121\trec:   61.574829\tkl:   26.108290\n",
      "====> Epoch: 808 Average train loss: 90.5436\n",
      "====> Validation set loss: 94.4604\n",
      "====> Validation set kl: 26.3389\n",
      "Epoch: 809 [  100/50000 ( 0%)]  \tLoss:   88.167358\trec:   62.726406\tkl:   25.440954\n",
      "Epoch: 809 [10100/50000 (20%)]  \tLoss:   89.834084\trec:   64.011383\tkl:   25.822689\n",
      "Epoch: 809 [20100/50000 (40%)]  \tLoss:   89.166069\trec:   62.248638\tkl:   26.917440\n",
      "Epoch: 809 [30100/50000 (60%)]  \tLoss:   89.745506\trec:   63.423531\tkl:   26.321978\n",
      "Epoch: 809 [40100/50000 (80%)]  \tLoss:   89.846191\trec:   63.687458\tkl:   26.158728\n",
      "====> Epoch: 809 Average train loss: 90.5349\n",
      "====> Validation set loss: 94.5253\n",
      "====> Validation set kl: 26.2922\n",
      "Epoch: 810 [  100/50000 ( 0%)]  \tLoss:   89.220253\trec:   63.196011\tkl:   26.024240\n",
      "Epoch: 810 [10100/50000 (20%)]  \tLoss:   87.716904\trec:   61.915009\tkl:   25.801893\n",
      "Epoch: 810 [20100/50000 (40%)]  \tLoss:   94.802162\trec:   67.392029\tkl:   27.410141\n",
      "Epoch: 810 [30100/50000 (60%)]  \tLoss:   92.526962\trec:   66.792999\tkl:   25.733961\n",
      "Epoch: 810 [40100/50000 (80%)]  \tLoss:   90.169937\trec:   63.658073\tkl:   26.511869\n",
      "====> Epoch: 810 Average train loss: 90.5653\n",
      "====> Validation set loss: 94.4155\n",
      "====> Validation set kl: 26.3250\n",
      "Epoch: 811 [  100/50000 ( 0%)]  \tLoss:   87.367950\trec:   62.204296\tkl:   25.163656\n",
      "Epoch: 811 [10100/50000 (20%)]  \tLoss:   88.165222\trec:   61.886723\tkl:   26.278503\n",
      "Epoch: 811 [20100/50000 (40%)]  \tLoss:   91.026749\trec:   64.653831\tkl:   26.372911\n",
      "Epoch: 811 [30100/50000 (60%)]  \tLoss:   88.769539\trec:   63.203320\tkl:   25.566219\n",
      "Epoch: 811 [40100/50000 (80%)]  \tLoss:   91.891403\trec:   64.390968\tkl:   27.500444\n",
      "====> Epoch: 811 Average train loss: 90.5671\n",
      "====> Validation set loss: 94.5449\n",
      "====> Validation set kl: 26.3561\n",
      "Epoch: 812 [  100/50000 ( 0%)]  \tLoss:   88.973846\trec:   63.439556\tkl:   25.534294\n",
      "Epoch: 812 [10100/50000 (20%)]  \tLoss:   91.710831\trec:   66.083504\tkl:   25.627321\n",
      "Epoch: 812 [20100/50000 (40%)]  \tLoss:   91.044197\trec:   64.583885\tkl:   26.460318\n",
      "Epoch: 812 [30100/50000 (60%)]  \tLoss:   89.176979\trec:   62.728943\tkl:   26.448036\n",
      "Epoch: 812 [40100/50000 (80%)]  \tLoss:   91.849487\trec:   65.090538\tkl:   26.758949\n",
      "====> Epoch: 812 Average train loss: 90.5384\n",
      "====> Validation set loss: 94.5311\n",
      "====> Validation set kl: 26.3417\n",
      "Epoch: 813 [  100/50000 ( 0%)]  \tLoss:   90.653877\trec:   64.184364\tkl:   26.469509\n",
      "Epoch: 813 [10100/50000 (20%)]  \tLoss:   85.769814\trec:   60.242207\tkl:   25.527609\n",
      "Epoch: 813 [20100/50000 (40%)]  \tLoss:   87.439911\trec:   61.923996\tkl:   25.515909\n",
      "Epoch: 813 [30100/50000 (60%)]  \tLoss:   94.064598\trec:   66.469215\tkl:   27.595383\n",
      "Epoch: 813 [40100/50000 (80%)]  \tLoss:   90.905502\trec:   64.254379\tkl:   26.651123\n",
      "====> Epoch: 813 Average train loss: 90.5453\n",
      "====> Validation set loss: 94.4371\n",
      "====> Validation set kl: 26.3523\n",
      "Epoch: 814 [  100/50000 ( 0%)]  \tLoss:   92.382378\trec:   65.693359\tkl:   26.689018\n",
      "Epoch: 814 [10100/50000 (20%)]  \tLoss:   94.835876\trec:   67.729446\tkl:   27.106424\n",
      "Epoch: 814 [20100/50000 (40%)]  \tLoss:   93.084747\trec:   66.633156\tkl:   26.451591\n",
      "Epoch: 814 [30100/50000 (60%)]  \tLoss:   91.446877\trec:   65.017929\tkl:   26.428947\n",
      "Epoch: 814 [40100/50000 (80%)]  \tLoss:   89.113106\trec:   63.255322\tkl:   25.857779\n",
      "====> Epoch: 814 Average train loss: 90.5444\n",
      "====> Validation set loss: 94.5471\n",
      "====> Validation set kl: 26.1939\n",
      "Epoch: 815 [  100/50000 ( 0%)]  \tLoss:   91.223358\trec:   65.293465\tkl:   25.929892\n",
      "Epoch: 815 [10100/50000 (20%)]  \tLoss:   95.252449\trec:   69.092094\tkl:   26.160358\n",
      "Epoch: 815 [20100/50000 (40%)]  \tLoss:   89.330719\trec:   63.105301\tkl:   26.225420\n",
      "Epoch: 815 [30100/50000 (60%)]  \tLoss:   97.334610\trec:   70.297554\tkl:   27.037058\n",
      "Epoch: 815 [40100/50000 (80%)]  \tLoss:   91.310112\trec:   65.047173\tkl:   26.262943\n",
      "====> Epoch: 815 Average train loss: 90.5532\n",
      "====> Validation set loss: 94.4538\n",
      "====> Validation set kl: 26.4831\n",
      "Epoch: 816 [  100/50000 ( 0%)]  \tLoss:   91.748795\trec:   64.688087\tkl:   27.060705\n",
      "Epoch: 816 [10100/50000 (20%)]  \tLoss:   93.821236\trec:   67.147057\tkl:   26.674177\n",
      "Epoch: 816 [20100/50000 (40%)]  \tLoss:   93.005203\trec:   66.035065\tkl:   26.970139\n",
      "Epoch: 816 [30100/50000 (60%)]  \tLoss:   89.389351\trec:   63.424129\tkl:   25.965227\n",
      "Epoch: 816 [40100/50000 (80%)]  \tLoss:   89.876930\trec:   64.046097\tkl:   25.830839\n",
      "====> Epoch: 816 Average train loss: 90.5406\n",
      "====> Validation set loss: 94.4983\n",
      "====> Validation set kl: 26.4307\n",
      "Epoch: 817 [  100/50000 ( 0%)]  \tLoss:   88.565651\trec:   62.716545\tkl:   25.849104\n",
      "Epoch: 817 [10100/50000 (20%)]  \tLoss:   86.949654\trec:   61.469170\tkl:   25.480488\n",
      "Epoch: 817 [20100/50000 (40%)]  \tLoss:   92.258522\trec:   66.277557\tkl:   25.980968\n",
      "Epoch: 817 [30100/50000 (60%)]  \tLoss:   89.460487\trec:   62.497429\tkl:   26.963058\n",
      "Epoch: 817 [40100/50000 (80%)]  \tLoss:   92.195625\trec:   65.048691\tkl:   27.146938\n",
      "====> Epoch: 817 Average train loss: 90.5304\n",
      "====> Validation set loss: 94.4394\n",
      "====> Validation set kl: 26.1787\n",
      "Epoch: 818 [  100/50000 ( 0%)]  \tLoss:   89.152603\trec:   62.720085\tkl:   26.432516\n",
      "Epoch: 818 [10100/50000 (20%)]  \tLoss:   90.249168\trec:   64.172867\tkl:   26.076298\n",
      "Epoch: 818 [20100/50000 (40%)]  \tLoss:   91.615044\trec:   64.714684\tkl:   26.900364\n",
      "Epoch: 818 [30100/50000 (60%)]  \tLoss:   92.910866\trec:   67.263046\tkl:   25.647821\n",
      "Epoch: 818 [40100/50000 (80%)]  \tLoss:   90.231178\trec:   64.439888\tkl:   25.791286\n",
      "====> Epoch: 818 Average train loss: 90.5392\n",
      "====> Validation set loss: 94.3219\n",
      "====> Validation set kl: 26.2600\n",
      "Epoch: 819 [  100/50000 ( 0%)]  \tLoss:   90.459244\trec:   64.231552\tkl:   26.227697\n",
      "Epoch: 819 [10100/50000 (20%)]  \tLoss:   90.456970\trec:   63.423046\tkl:   27.033920\n",
      "Epoch: 819 [20100/50000 (40%)]  \tLoss:   91.145752\trec:   64.419960\tkl:   26.725792\n",
      "Epoch: 819 [30100/50000 (60%)]  \tLoss:   93.628334\trec:   67.431267\tkl:   26.197079\n",
      "Epoch: 819 [40100/50000 (80%)]  \tLoss:   96.207695\trec:   69.407478\tkl:   26.800209\n",
      "====> Epoch: 819 Average train loss: 90.5371\n",
      "====> Validation set loss: 94.4771\n",
      "====> Validation set kl: 26.3120\n",
      "Epoch: 820 [  100/50000 ( 0%)]  \tLoss:   88.814781\trec:   63.014030\tkl:   25.800756\n",
      "Epoch: 820 [10100/50000 (20%)]  \tLoss:   91.216553\trec:   64.553146\tkl:   26.663403\n",
      "Epoch: 820 [20100/50000 (40%)]  \tLoss:   90.110962\trec:   63.876202\tkl:   26.234764\n",
      "Epoch: 820 [30100/50000 (60%)]  \tLoss:   91.663490\trec:   65.331505\tkl:   26.331993\n",
      "Epoch: 820 [40100/50000 (80%)]  \tLoss:   93.759949\trec:   66.962929\tkl:   26.797018\n",
      "====> Epoch: 820 Average train loss: 90.5152\n",
      "====> Validation set loss: 94.4100\n",
      "====> Validation set kl: 26.2513\n",
      "Epoch: 821 [  100/50000 ( 0%)]  \tLoss:   89.631866\trec:   63.508358\tkl:   26.123505\n",
      "Epoch: 821 [10100/50000 (20%)]  \tLoss:   88.027756\trec:   62.308403\tkl:   25.719353\n",
      "Epoch: 821 [20100/50000 (40%)]  \tLoss:   90.011719\trec:   63.225986\tkl:   26.785734\n",
      "Epoch: 821 [30100/50000 (60%)]  \tLoss:   88.374596\trec:   62.558472\tkl:   25.816130\n",
      "Epoch: 821 [40100/50000 (80%)]  \tLoss:   94.031647\trec:   66.979820\tkl:   27.051828\n",
      "====> Epoch: 821 Average train loss: 90.5250\n",
      "====> Validation set loss: 94.4151\n",
      "====> Validation set kl: 26.4509\n",
      "Epoch: 822 [  100/50000 ( 0%)]  \tLoss:   92.910645\trec:   64.876846\tkl:   28.033804\n",
      "Epoch: 822 [10100/50000 (20%)]  \tLoss:   90.381332\trec:   63.615891\tkl:   26.765442\n",
      "Epoch: 822 [20100/50000 (40%)]  \tLoss:   92.904785\trec:   65.956871\tkl:   26.947910\n",
      "Epoch: 822 [30100/50000 (60%)]  \tLoss:   88.652466\trec:   62.488770\tkl:   26.163700\n",
      "Epoch: 822 [40100/50000 (80%)]  \tLoss:   89.282379\trec:   63.641033\tkl:   25.641342\n",
      "====> Epoch: 822 Average train loss: 90.5213\n",
      "====> Validation set loss: 94.4284\n",
      "====> Validation set kl: 26.2161\n",
      "Epoch: 823 [  100/50000 ( 0%)]  \tLoss:   92.513084\trec:   65.906631\tkl:   26.606455\n",
      "Epoch: 823 [10100/50000 (20%)]  \tLoss:   87.585953\trec:   62.294392\tkl:   25.291561\n",
      "Epoch: 823 [20100/50000 (40%)]  \tLoss:   85.638474\trec:   60.319511\tkl:   25.318962\n",
      "Epoch: 823 [30100/50000 (60%)]  \tLoss:   90.770973\trec:   64.255241\tkl:   26.515734\n",
      "Epoch: 823 [40100/50000 (80%)]  \tLoss:   91.669304\trec:   64.790894\tkl:   26.878405\n",
      "====> Epoch: 823 Average train loss: 90.5056\n",
      "====> Validation set loss: 94.3930\n",
      "====> Validation set kl: 26.2098\n",
      "Epoch: 824 [  100/50000 ( 0%)]  \tLoss:   89.023094\trec:   63.567791\tkl:   25.455299\n",
      "Epoch: 824 [10100/50000 (20%)]  \tLoss:   92.131020\trec:   64.610100\tkl:   27.520926\n",
      "Epoch: 824 [20100/50000 (40%)]  \tLoss:   88.749863\trec:   61.776875\tkl:   26.972982\n",
      "Epoch: 824 [30100/50000 (60%)]  \tLoss:   94.128075\trec:   67.218369\tkl:   26.909704\n",
      "Epoch: 824 [40100/50000 (80%)]  \tLoss:   94.165367\trec:   67.255455\tkl:   26.909912\n",
      "====> Epoch: 824 Average train loss: 90.4951\n",
      "====> Validation set loss: 94.3406\n",
      "====> Validation set kl: 26.1938\n",
      "Epoch: 825 [  100/50000 ( 0%)]  \tLoss:   96.973495\trec:   70.098671\tkl:   26.874819\n",
      "Epoch: 825 [10100/50000 (20%)]  \tLoss:   88.734352\trec:   62.155209\tkl:   26.579145\n",
      "Epoch: 825 [20100/50000 (40%)]  \tLoss:   91.431992\trec:   63.753578\tkl:   27.678408\n",
      "Epoch: 825 [30100/50000 (60%)]  \tLoss:   89.586632\trec:   64.117828\tkl:   25.468798\n",
      "Epoch: 825 [40100/50000 (80%)]  \tLoss:   92.879295\trec:   66.717857\tkl:   26.161442\n",
      "====> Epoch: 825 Average train loss: 90.5102\n",
      "====> Validation set loss: 94.4640\n",
      "====> Validation set kl: 26.2617\n",
      "Epoch: 826 [  100/50000 ( 0%)]  \tLoss:   93.228256\trec:   66.571678\tkl:   26.656576\n",
      "Epoch: 826 [10100/50000 (20%)]  \tLoss:   88.933083\trec:   61.512352\tkl:   27.420736\n",
      "Epoch: 826 [20100/50000 (40%)]  \tLoss:   92.221542\trec:   65.253052\tkl:   26.968493\n",
      "Epoch: 826 [30100/50000 (60%)]  \tLoss:   92.721992\trec:   65.991280\tkl:   26.730707\n",
      "Epoch: 826 [40100/50000 (80%)]  \tLoss:   89.434822\trec:   63.454479\tkl:   25.980347\n",
      "====> Epoch: 826 Average train loss: 90.5161\n",
      "====> Validation set loss: 94.3802\n",
      "====> Validation set kl: 26.4934\n",
      "Epoch: 827 [  100/50000 ( 0%)]  \tLoss:   92.098045\trec:   65.384247\tkl:   26.713802\n",
      "Epoch: 827 [10100/50000 (20%)]  \tLoss:   91.334129\trec:   64.484390\tkl:   26.849735\n",
      "Epoch: 827 [20100/50000 (40%)]  \tLoss:   89.434975\trec:   63.017929\tkl:   26.417049\n",
      "Epoch: 827 [30100/50000 (60%)]  \tLoss:   92.070702\trec:   65.706963\tkl:   26.363735\n",
      "Epoch: 827 [40100/50000 (80%)]  \tLoss:   88.543533\trec:   62.094738\tkl:   26.448799\n",
      "====> Epoch: 827 Average train loss: 90.5035\n",
      "====> Validation set loss: 94.5015\n",
      "====> Validation set kl: 26.3361\n",
      "Epoch: 828 [  100/50000 ( 0%)]  \tLoss:   88.501305\trec:   62.761032\tkl:   25.740269\n",
      "Epoch: 828 [10100/50000 (20%)]  \tLoss:   92.371452\trec:   65.453926\tkl:   26.917524\n",
      "Epoch: 828 [20100/50000 (40%)]  \tLoss:   87.708595\trec:   62.276794\tkl:   25.431797\n",
      "Epoch: 828 [30100/50000 (60%)]  \tLoss:   93.366997\trec:   66.647476\tkl:   26.719519\n",
      "Epoch: 828 [40100/50000 (80%)]  \tLoss:   89.299606\trec:   63.234089\tkl:   26.065517\n",
      "====> Epoch: 828 Average train loss: 90.5010\n",
      "====> Validation set loss: 94.4691\n",
      "====> Validation set kl: 26.5204\n",
      "Epoch: 829 [  100/50000 ( 0%)]  \tLoss:   91.337265\trec:   64.617584\tkl:   26.719683\n",
      "Epoch: 829 [10100/50000 (20%)]  \tLoss:   88.022949\trec:   61.290203\tkl:   26.732744\n",
      "Epoch: 829 [20100/50000 (40%)]  \tLoss:   88.764542\trec:   62.471905\tkl:   26.292633\n",
      "Epoch: 829 [30100/50000 (60%)]  \tLoss:   95.948814\trec:   68.848038\tkl:   27.100782\n",
      "Epoch: 829 [40100/50000 (80%)]  \tLoss:   92.656387\trec:   65.410957\tkl:   27.245424\n",
      "====> Epoch: 829 Average train loss: 90.4849\n",
      "====> Validation set loss: 94.4091\n",
      "====> Validation set kl: 26.4362\n",
      "Epoch: 830 [  100/50000 ( 0%)]  \tLoss:   90.569000\trec:   64.847198\tkl:   25.721802\n",
      "Epoch: 830 [10100/50000 (20%)]  \tLoss:   90.255310\trec:   64.699394\tkl:   25.555922\n",
      "Epoch: 830 [20100/50000 (40%)]  \tLoss:   92.559685\trec:   65.348358\tkl:   27.211327\n",
      "Epoch: 830 [30100/50000 (60%)]  \tLoss:   90.356270\trec:   63.023987\tkl:   27.332275\n",
      "Epoch: 830 [40100/50000 (80%)]  \tLoss:   89.438614\trec:   63.843594\tkl:   25.595022\n",
      "====> Epoch: 830 Average train loss: 90.4867\n",
      "====> Validation set loss: 94.4008\n",
      "====> Validation set kl: 26.2514\n",
      "Epoch: 831 [  100/50000 ( 0%)]  \tLoss:   93.828796\trec:   67.295883\tkl:   26.532915\n",
      "Epoch: 831 [10100/50000 (20%)]  \tLoss:   88.575211\trec:   62.194931\tkl:   26.380285\n",
      "Epoch: 831 [20100/50000 (40%)]  \tLoss:   92.193413\trec:   65.892899\tkl:   26.300520\n",
      "Epoch: 831 [30100/50000 (60%)]  \tLoss:   92.280693\trec:   64.893913\tkl:   27.386780\n",
      "Epoch: 831 [40100/50000 (80%)]  \tLoss:   93.132675\trec:   65.960670\tkl:   27.171997\n",
      "====> Epoch: 831 Average train loss: 90.5193\n",
      "====> Validation set loss: 94.5000\n",
      "====> Validation set kl: 26.3217\n",
      "Epoch: 832 [  100/50000 ( 0%)]  \tLoss:   94.869713\trec:   67.646164\tkl:   27.223555\n",
      "Epoch: 832 [10100/50000 (20%)]  \tLoss:   89.096451\trec:   62.591244\tkl:   26.505209\n",
      "Epoch: 832 [20100/50000 (40%)]  \tLoss:   93.647217\trec:   67.560371\tkl:   26.086843\n",
      "Epoch: 832 [30100/50000 (60%)]  \tLoss:   91.624237\trec:   64.347656\tkl:   27.276577\n",
      "Epoch: 832 [40100/50000 (80%)]  \tLoss:   90.132156\trec:   63.557346\tkl:   26.574808\n",
      "====> Epoch: 832 Average train loss: 90.4903\n",
      "====> Validation set loss: 94.4604\n",
      "====> Validation set kl: 26.5151\n",
      "Epoch: 833 [  100/50000 ( 0%)]  \tLoss:   89.026558\trec:   62.409031\tkl:   26.617531\n",
      "Epoch: 833 [10100/50000 (20%)]  \tLoss:   95.157349\trec:   68.328667\tkl:   26.828684\n",
      "Epoch: 833 [20100/50000 (40%)]  \tLoss:   89.963661\trec:   64.105171\tkl:   25.858482\n",
      "Epoch: 833 [30100/50000 (60%)]  \tLoss:   94.023865\trec:   66.624191\tkl:   27.399675\n",
      "Epoch: 833 [40100/50000 (80%)]  \tLoss:   89.963730\trec:   63.328682\tkl:   26.635054\n",
      "====> Epoch: 833 Average train loss: 90.5083\n",
      "====> Validation set loss: 94.3653\n",
      "====> Validation set kl: 26.3410\n",
      "Epoch: 834 [  100/50000 ( 0%)]  \tLoss:   92.796349\trec:   66.358330\tkl:   26.438015\n",
      "Epoch: 834 [10100/50000 (20%)]  \tLoss:   88.572891\trec:   62.549805\tkl:   26.023081\n",
      "Epoch: 834 [20100/50000 (40%)]  \tLoss:   91.250183\trec:   64.034485\tkl:   27.215698\n",
      "Epoch: 834 [30100/50000 (60%)]  \tLoss:   89.492378\trec:   63.633934\tkl:   25.858444\n",
      "Epoch: 834 [40100/50000 (80%)]  \tLoss:   91.057060\trec:   64.438774\tkl:   26.618282\n",
      "====> Epoch: 834 Average train loss: 90.4824\n",
      "====> Validation set loss: 94.4467\n",
      "====> Validation set kl: 26.3153\n",
      "Epoch: 835 [  100/50000 ( 0%)]  \tLoss:   91.738632\trec:   65.624985\tkl:   26.113651\n",
      "Epoch: 835 [10100/50000 (20%)]  \tLoss:   87.759239\trec:   62.411133\tkl:   25.348104\n",
      "Epoch: 835 [20100/50000 (40%)]  \tLoss:   89.897362\trec:   63.519588\tkl:   26.377768\n",
      "Epoch: 835 [30100/50000 (60%)]  \tLoss:   90.712906\trec:   65.135933\tkl:   25.576982\n",
      "Epoch: 835 [40100/50000 (80%)]  \tLoss:   90.337143\trec:   63.288944\tkl:   27.048201\n",
      "====> Epoch: 835 Average train loss: 90.4816\n",
      "====> Validation set loss: 94.5840\n",
      "====> Validation set kl: 26.4637\n",
      "Epoch: 836 [  100/50000 ( 0%)]  \tLoss:   91.341866\trec:   63.806419\tkl:   27.535444\n",
      "Epoch: 836 [10100/50000 (20%)]  \tLoss:   87.057671\trec:   61.532173\tkl:   25.525503\n",
      "Epoch: 836 [20100/50000 (40%)]  \tLoss:   87.803009\trec:   62.257500\tkl:   25.545502\n",
      "Epoch: 836 [30100/50000 (60%)]  \tLoss:   93.427750\trec:   66.487297\tkl:   26.940453\n",
      "Epoch: 836 [40100/50000 (80%)]  \tLoss:   85.922958\trec:   60.725834\tkl:   25.197123\n",
      "====> Epoch: 836 Average train loss: 90.4828\n",
      "====> Validation set loss: 94.4867\n",
      "====> Validation set kl: 26.4199\n",
      "Epoch: 837 [  100/50000 ( 0%)]  \tLoss:   90.941032\trec:   63.940174\tkl:   27.000856\n",
      "Epoch: 837 [10100/50000 (20%)]  \tLoss:   91.416122\trec:   65.422462\tkl:   25.993664\n",
      "Epoch: 837 [20100/50000 (40%)]  \tLoss:   91.050156\trec:   63.866581\tkl:   27.183578\n",
      "Epoch: 837 [30100/50000 (60%)]  \tLoss:   90.279449\trec:   64.580444\tkl:   25.699001\n",
      "Epoch: 837 [40100/50000 (80%)]  \tLoss:   85.075623\trec:   59.684002\tkl:   25.391626\n",
      "====> Epoch: 837 Average train loss: 90.4768\n",
      "====> Validation set loss: 94.4862\n",
      "====> Validation set kl: 26.4275\n",
      "Epoch: 838 [  100/50000 ( 0%)]  \tLoss:   90.746460\trec:   63.790195\tkl:   26.956264\n",
      "Epoch: 838 [10100/50000 (20%)]  \tLoss:   91.402809\trec:   65.070030\tkl:   26.332785\n",
      "Epoch: 838 [20100/50000 (40%)]  \tLoss:   95.981728\trec:   69.075768\tkl:   26.905956\n",
      "Epoch: 838 [30100/50000 (60%)]  \tLoss:   91.578857\trec:   64.309837\tkl:   27.269020\n",
      "Epoch: 838 [40100/50000 (80%)]  \tLoss:   91.904892\trec:   65.639236\tkl:   26.265652\n",
      "====> Epoch: 838 Average train loss: 90.4580\n",
      "====> Validation set loss: 94.5446\n",
      "====> Validation set kl: 26.3491\n",
      "Epoch: 839 [  100/50000 ( 0%)]  \tLoss:   89.105255\trec:   63.062527\tkl:   26.042725\n",
      "Epoch: 839 [10100/50000 (20%)]  \tLoss:   95.902946\trec:   68.282906\tkl:   27.620033\n",
      "Epoch: 839 [20100/50000 (40%)]  \tLoss:   93.861336\trec:   66.715645\tkl:   27.145691\n",
      "Epoch: 839 [30100/50000 (60%)]  \tLoss:   94.163445\trec:   66.955338\tkl:   27.208105\n",
      "Epoch: 839 [40100/50000 (80%)]  \tLoss:   94.660622\trec:   68.973877\tkl:   25.686747\n",
      "====> Epoch: 839 Average train loss: 90.5013\n",
      "====> Validation set loss: 94.4930\n",
      "====> Validation set kl: 26.4963\n",
      "Epoch: 840 [  100/50000 ( 0%)]  \tLoss:   89.969910\trec:   63.680897\tkl:   26.289013\n",
      "Epoch: 840 [10100/50000 (20%)]  \tLoss:   91.690437\trec:   64.662628\tkl:   27.027809\n",
      "Epoch: 840 [20100/50000 (40%)]  \tLoss:   87.078171\trec:   61.932068\tkl:   25.146105\n",
      "Epoch: 840 [30100/50000 (60%)]  \tLoss:   89.641289\trec:   63.001415\tkl:   26.639872\n",
      "Epoch: 840 [40100/50000 (80%)]  \tLoss:   89.471840\trec:   63.288963\tkl:   26.182880\n",
      "====> Epoch: 840 Average train loss: 90.4816\n",
      "====> Validation set loss: 94.4789\n",
      "====> Validation set kl: 26.3829\n",
      "Epoch: 841 [  100/50000 ( 0%)]  \tLoss:   91.382050\trec:   65.092766\tkl:   26.289286\n",
      "Epoch: 841 [10100/50000 (20%)]  \tLoss:   91.633446\trec:   64.931900\tkl:   26.701542\n",
      "Epoch: 841 [20100/50000 (40%)]  \tLoss:   92.428612\trec:   66.541374\tkl:   25.887238\n",
      "Epoch: 841 [30100/50000 (60%)]  \tLoss:   91.194801\trec:   64.041016\tkl:   27.153778\n",
      "Epoch: 841 [40100/50000 (80%)]  \tLoss:   90.561493\trec:   63.706989\tkl:   26.854500\n",
      "====> Epoch: 841 Average train loss: 90.4560\n",
      "====> Validation set loss: 94.4304\n",
      "====> Validation set kl: 26.3022\n",
      "Epoch: 842 [  100/50000 ( 0%)]  \tLoss:   93.366432\trec:   67.206261\tkl:   26.160177\n",
      "Epoch: 842 [10100/50000 (20%)]  \tLoss:   90.511894\trec:   64.383453\tkl:   26.128447\n",
      "Epoch: 842 [20100/50000 (40%)]  \tLoss:   92.184380\trec:   66.558868\tkl:   25.625519\n",
      "Epoch: 842 [30100/50000 (60%)]  \tLoss:   98.879135\trec:   71.033096\tkl:   27.846045\n",
      "Epoch: 842 [40100/50000 (80%)]  \tLoss:   92.134010\trec:   65.737793\tkl:   26.396215\n",
      "====> Epoch: 842 Average train loss: 90.4644\n",
      "====> Validation set loss: 94.4298\n",
      "====> Validation set kl: 26.3892\n",
      "Epoch: 843 [  100/50000 ( 0%)]  \tLoss:   89.411163\trec:   63.249912\tkl:   26.161247\n",
      "Epoch: 843 [10100/50000 (20%)]  \tLoss:   87.114922\trec:   61.174217\tkl:   25.940701\n",
      "Epoch: 843 [20100/50000 (40%)]  \tLoss:   92.199783\trec:   65.485260\tkl:   26.714521\n",
      "Epoch: 843 [30100/50000 (60%)]  \tLoss:   88.198006\trec:   63.237724\tkl:   24.960279\n",
      "Epoch: 843 [40100/50000 (80%)]  \tLoss:   95.199120\trec:   68.330574\tkl:   26.868542\n",
      "====> Epoch: 843 Average train loss: 90.4508\n",
      "====> Validation set loss: 94.5074\n",
      "====> Validation set kl: 26.2617\n",
      "Epoch: 844 [  100/50000 ( 0%)]  \tLoss:   91.659225\trec:   65.828835\tkl:   25.830393\n",
      "Epoch: 844 [10100/50000 (20%)]  \tLoss:   86.478218\trec:   60.771553\tkl:   25.706667\n",
      "Epoch: 844 [20100/50000 (40%)]  \tLoss:   95.311661\trec:   68.541367\tkl:   26.770294\n",
      "Epoch: 844 [30100/50000 (60%)]  \tLoss:   93.230156\trec:   66.982010\tkl:   26.248148\n",
      "Epoch: 844 [40100/50000 (80%)]  \tLoss:   92.904572\trec:   65.975838\tkl:   26.928732\n",
      "====> Epoch: 844 Average train loss: 90.4630\n",
      "====> Validation set loss: 94.4859\n",
      "====> Validation set kl: 26.3656\n",
      "Epoch: 845 [  100/50000 ( 0%)]  \tLoss:   86.744804\trec:   60.585030\tkl:   26.159769\n",
      "Epoch: 845 [10100/50000 (20%)]  \tLoss:   90.621368\trec:   63.639339\tkl:   26.982021\n",
      "Epoch: 845 [20100/50000 (40%)]  \tLoss:   88.691093\trec:   63.126469\tkl:   25.564621\n",
      "Epoch: 845 [30100/50000 (60%)]  \tLoss:   92.135620\trec:   65.065300\tkl:   27.070316\n",
      "Epoch: 845 [40100/50000 (80%)]  \tLoss:   88.079567\trec:   62.315487\tkl:   25.764084\n",
      "====> Epoch: 845 Average train loss: 90.4731\n",
      "====> Validation set loss: 94.5073\n",
      "====> Validation set kl: 26.2572\n",
      "Epoch: 846 [  100/50000 ( 0%)]  \tLoss:   94.276031\trec:   67.241386\tkl:   27.034643\n",
      "Epoch: 846 [10100/50000 (20%)]  \tLoss:   89.575378\trec:   63.204006\tkl:   26.371372\n",
      "Epoch: 846 [20100/50000 (40%)]  \tLoss:   91.267929\trec:   64.546631\tkl:   26.721298\n",
      "Epoch: 846 [30100/50000 (60%)]  \tLoss:   91.833084\trec:   65.941353\tkl:   25.891733\n",
      "Epoch: 846 [40100/50000 (80%)]  \tLoss:   87.097389\trec:   61.442207\tkl:   25.655182\n",
      "====> Epoch: 846 Average train loss: 90.4660\n",
      "====> Validation set loss: 94.4947\n",
      "====> Validation set kl: 26.5791\n",
      "Epoch: 847 [  100/50000 ( 0%)]  \tLoss:   88.685547\trec:   62.085583\tkl:   26.599966\n",
      "Epoch: 847 [10100/50000 (20%)]  \tLoss:   89.451309\trec:   63.362537\tkl:   26.088768\n",
      "Epoch: 847 [20100/50000 (40%)]  \tLoss:   90.975067\trec:   64.138306\tkl:   26.836760\n",
      "Epoch: 847 [30100/50000 (60%)]  \tLoss:   96.354004\trec:   68.725288\tkl:   27.628716\n",
      "Epoch: 847 [40100/50000 (80%)]  \tLoss:   91.311310\trec:   64.964493\tkl:   26.346815\n",
      "====> Epoch: 847 Average train loss: 90.4451\n",
      "====> Validation set loss: 94.4194\n",
      "====> Validation set kl: 26.2706\n",
      "Epoch: 848 [  100/50000 ( 0%)]  \tLoss:   88.549545\trec:   62.395096\tkl:   26.154453\n",
      "Epoch: 848 [10100/50000 (20%)]  \tLoss:   92.160309\trec:   65.425751\tkl:   26.734560\n",
      "Epoch: 848 [20100/50000 (40%)]  \tLoss:   87.387833\trec:   61.889618\tkl:   25.498209\n",
      "Epoch: 848 [30100/50000 (60%)]  \tLoss:   87.218536\trec:   62.217606\tkl:   25.000927\n",
      "Epoch: 848 [40100/50000 (80%)]  \tLoss:   90.404175\trec:   63.878403\tkl:   26.525774\n",
      "====> Epoch: 848 Average train loss: 90.4592\n",
      "====> Validation set loss: 94.4950\n",
      "====> Validation set kl: 26.4454\n",
      "Epoch: 849 [  100/50000 ( 0%)]  \tLoss:   90.667755\trec:   63.501865\tkl:   27.165886\n",
      "Epoch: 849 [10100/50000 (20%)]  \tLoss:   89.305153\trec:   62.721725\tkl:   26.583429\n",
      "Epoch: 849 [20100/50000 (40%)]  \tLoss:   91.000175\trec:   64.587242\tkl:   26.412924\n",
      "Epoch: 849 [30100/50000 (60%)]  \tLoss:   92.735626\trec:   65.739685\tkl:   26.995934\n",
      "Epoch: 849 [40100/50000 (80%)]  \tLoss:   89.291969\trec:   62.968071\tkl:   26.323900\n",
      "====> Epoch: 849 Average train loss: 90.4676\n",
      "====> Validation set loss: 94.4743\n",
      "====> Validation set kl: 26.5432\n",
      "Epoch: 850 [  100/50000 ( 0%)]  \tLoss:   90.609444\trec:   63.820793\tkl:   26.788649\n",
      "Epoch: 850 [10100/50000 (20%)]  \tLoss:   92.148903\trec:   66.305809\tkl:   25.843096\n",
      "Epoch: 850 [20100/50000 (40%)]  \tLoss:   91.245483\trec:   65.223778\tkl:   26.021706\n",
      "Epoch: 850 [30100/50000 (60%)]  \tLoss:   89.510529\trec:   62.947376\tkl:   26.563150\n",
      "Epoch: 850 [40100/50000 (80%)]  \tLoss:   93.186798\trec:   66.298706\tkl:   26.888083\n",
      "====> Epoch: 850 Average train loss: 90.4616\n",
      "====> Validation set loss: 94.3894\n",
      "====> Validation set kl: 26.3220\n",
      "Epoch: 851 [  100/50000 ( 0%)]  \tLoss:   93.611343\trec:   66.454826\tkl:   27.156517\n",
      "Epoch: 851 [10100/50000 (20%)]  \tLoss:   87.621735\trec:   61.896233\tkl:   25.725502\n",
      "Epoch: 851 [20100/50000 (40%)]  \tLoss:   90.673653\trec:   64.124878\tkl:   26.548769\n",
      "Epoch: 851 [30100/50000 (60%)]  \tLoss:   93.508316\trec:   67.067841\tkl:   26.440477\n",
      "Epoch: 851 [40100/50000 (80%)]  \tLoss:   92.741615\trec:   65.204041\tkl:   27.537582\n",
      "====> Epoch: 851 Average train loss: 90.4609\n",
      "====> Validation set loss: 94.4236\n",
      "====> Validation set kl: 26.4153\n",
      "Epoch: 852 [  100/50000 ( 0%)]  \tLoss:   86.795311\trec:   61.025410\tkl:   25.769907\n",
      "Epoch: 852 [10100/50000 (20%)]  \tLoss:   91.281349\trec:   64.398674\tkl:   26.882666\n",
      "Epoch: 852 [20100/50000 (40%)]  \tLoss:   95.069000\trec:   68.603935\tkl:   26.465073\n",
      "Epoch: 852 [30100/50000 (60%)]  \tLoss:   93.123520\trec:   66.492432\tkl:   26.631090\n",
      "Epoch: 852 [40100/50000 (80%)]  \tLoss:   87.307899\trec:   61.800575\tkl:   25.507320\n",
      "====> Epoch: 852 Average train loss: 90.4659\n",
      "====> Validation set loss: 94.4602\n",
      "====> Validation set kl: 26.4773\n",
      "Epoch: 853 [  100/50000 ( 0%)]  \tLoss:   88.757637\trec:   62.198364\tkl:   26.559267\n",
      "Epoch: 853 [10100/50000 (20%)]  \tLoss:   90.268143\trec:   63.382858\tkl:   26.885288\n",
      "Epoch: 853 [20100/50000 (40%)]  \tLoss:   89.132042\trec:   63.771442\tkl:   25.360596\n",
      "Epoch: 853 [30100/50000 (60%)]  \tLoss:   90.170807\trec:   63.725719\tkl:   26.445089\n",
      "Epoch: 853 [40100/50000 (80%)]  \tLoss:   90.797508\trec:   63.936714\tkl:   26.860796\n",
      "====> Epoch: 853 Average train loss: 90.4211\n",
      "====> Validation set loss: 94.3823\n",
      "====> Validation set kl: 26.2219\n",
      "Epoch: 854 [  100/50000 ( 0%)]  \tLoss:   86.053162\trec:   60.481945\tkl:   25.571220\n",
      "Epoch: 854 [10100/50000 (20%)]  \tLoss:   96.576569\trec:   68.524292\tkl:   28.052275\n",
      "Epoch: 854 [20100/50000 (40%)]  \tLoss:   90.850975\trec:   64.331223\tkl:   26.519758\n",
      "Epoch: 854 [30100/50000 (60%)]  \tLoss:   89.450241\trec:   63.174236\tkl:   26.276003\n",
      "Epoch: 854 [40100/50000 (80%)]  \tLoss:   89.474884\trec:   63.712910\tkl:   25.761976\n",
      "====> Epoch: 854 Average train loss: 90.4473\n",
      "====> Validation set loss: 94.4326\n",
      "====> Validation set kl: 26.3883\n",
      "Epoch: 855 [  100/50000 ( 0%)]  \tLoss:   89.179916\trec:   62.706127\tkl:   26.473789\n",
      "Epoch: 855 [10100/50000 (20%)]  \tLoss:   89.372612\trec:   63.442253\tkl:   25.930367\n",
      "Epoch: 855 [20100/50000 (40%)]  \tLoss:   89.115028\trec:   62.493446\tkl:   26.621584\n",
      "Epoch: 855 [30100/50000 (60%)]  \tLoss:   91.586502\trec:   65.392448\tkl:   26.194057\n",
      "Epoch: 855 [40100/50000 (80%)]  \tLoss:   89.276756\trec:   63.068493\tkl:   26.208256\n",
      "====> Epoch: 855 Average train loss: 90.4419\n",
      "====> Validation set loss: 94.4674\n",
      "====> Validation set kl: 26.3135\n",
      "Epoch: 856 [  100/50000 ( 0%)]  \tLoss:   86.827606\trec:   61.163479\tkl:   25.664127\n",
      "Epoch: 856 [10100/50000 (20%)]  \tLoss:   92.557228\trec:   65.915909\tkl:   26.641323\n",
      "Epoch: 856 [20100/50000 (40%)]  \tLoss:   89.533730\trec:   62.833344\tkl:   26.700386\n",
      "Epoch: 856 [30100/50000 (60%)]  \tLoss:   88.842422\trec:   62.686493\tkl:   26.155924\n",
      "Epoch: 856 [40100/50000 (80%)]  \tLoss:   86.721748\trec:   61.179951\tkl:   25.541800\n",
      "====> Epoch: 856 Average train loss: 90.4506\n",
      "====> Validation set loss: 94.5069\n",
      "====> Validation set kl: 26.3314\n",
      "Epoch: 857 [  100/50000 ( 0%)]  \tLoss:   86.069588\trec:   60.995110\tkl:   25.074474\n",
      "Epoch: 857 [10100/50000 (20%)]  \tLoss:   93.702286\trec:   66.563889\tkl:   27.138393\n",
      "Epoch: 857 [20100/50000 (40%)]  \tLoss:   88.279793\trec:   62.401699\tkl:   25.878092\n",
      "Epoch: 857 [30100/50000 (60%)]  \tLoss:   93.409393\trec:   67.204460\tkl:   26.204931\n",
      "Epoch: 857 [40100/50000 (80%)]  \tLoss:   93.339394\trec:   66.362457\tkl:   26.976938\n",
      "====> Epoch: 857 Average train loss: 90.4238\n",
      "====> Validation set loss: 94.4430\n",
      "====> Validation set kl: 26.3821\n",
      "Epoch: 858 [  100/50000 ( 0%)]  \tLoss:   92.849503\trec:   65.788254\tkl:   27.061249\n",
      "Epoch: 858 [10100/50000 (20%)]  \tLoss:   90.375908\trec:   64.598381\tkl:   25.777527\n",
      "Epoch: 858 [20100/50000 (40%)]  \tLoss:   91.079811\trec:   64.576019\tkl:   26.503796\n",
      "Epoch: 858 [30100/50000 (60%)]  \tLoss:   88.162506\trec:   62.093670\tkl:   26.068838\n",
      "Epoch: 858 [40100/50000 (80%)]  \tLoss:   94.343666\trec:   66.526558\tkl:   27.817112\n",
      "====> Epoch: 858 Average train loss: 90.4583\n",
      "====> Validation set loss: 94.5630\n",
      "====> Validation set kl: 26.3098\n",
      "Epoch: 859 [  100/50000 ( 0%)]  \tLoss:   91.339821\trec:   64.881340\tkl:   26.458488\n",
      "Epoch: 859 [10100/50000 (20%)]  \tLoss:   88.436890\trec:   62.377720\tkl:   26.059170\n",
      "Epoch: 859 [20100/50000 (40%)]  \tLoss:   86.843971\trec:   61.096878\tkl:   25.747097\n",
      "Epoch: 859 [30100/50000 (60%)]  \tLoss:   91.569199\trec:   65.993484\tkl:   25.575714\n",
      "Epoch: 859 [40100/50000 (80%)]  \tLoss:   90.875626\trec:   65.014755\tkl:   25.860867\n",
      "====> Epoch: 859 Average train loss: 90.4380\n",
      "====> Validation set loss: 94.5281\n",
      "====> Validation set kl: 26.2744\n",
      "Epoch: 860 [  100/50000 ( 0%)]  \tLoss:   90.734184\trec:   63.912045\tkl:   26.822144\n",
      "Epoch: 860 [10100/50000 (20%)]  \tLoss:   88.673111\trec:   62.565414\tkl:   26.107700\n",
      "Epoch: 860 [20100/50000 (40%)]  \tLoss:   91.646042\trec:   65.138046\tkl:   26.507998\n",
      "Epoch: 860 [30100/50000 (60%)]  \tLoss:   89.149796\trec:   63.199333\tkl:   25.950459\n",
      "Epoch: 860 [40100/50000 (80%)]  \tLoss:   89.887970\trec:   64.014656\tkl:   25.873310\n",
      "====> Epoch: 860 Average train loss: 90.4306\n",
      "====> Validation set loss: 94.4149\n",
      "====> Validation set kl: 26.2871\n",
      "Epoch: 861 [  100/50000 ( 0%)]  \tLoss:   94.833298\trec:   67.694534\tkl:   27.138763\n",
      "Epoch: 861 [10100/50000 (20%)]  \tLoss:   88.881470\trec:   63.094666\tkl:   25.786806\n",
      "Epoch: 861 [20100/50000 (40%)]  \tLoss:   90.700699\trec:   64.790176\tkl:   25.910532\n",
      "Epoch: 861 [30100/50000 (60%)]  \tLoss:   87.620621\trec:   61.247177\tkl:   26.373447\n",
      "Epoch: 861 [40100/50000 (80%)]  \tLoss:   93.919403\trec:   66.434807\tkl:   27.484602\n",
      "====> Epoch: 861 Average train loss: 90.4161\n",
      "====> Validation set loss: 94.4466\n",
      "====> Validation set kl: 26.4761\n",
      "Epoch: 862 [  100/50000 ( 0%)]  \tLoss:   89.998886\trec:   64.084770\tkl:   25.914106\n",
      "Epoch: 862 [10100/50000 (20%)]  \tLoss:   88.570000\trec:   62.313000\tkl:   26.257002\n",
      "Epoch: 862 [20100/50000 (40%)]  \tLoss:   91.850739\trec:   65.604141\tkl:   26.246595\n",
      "Epoch: 862 [30100/50000 (60%)]  \tLoss:   86.766090\trec:   61.839447\tkl:   24.926643\n",
      "Epoch: 862 [40100/50000 (80%)]  \tLoss:   88.816269\trec:   62.007851\tkl:   26.808418\n",
      "====> Epoch: 862 Average train loss: 90.4246\n",
      "====> Validation set loss: 94.4699\n",
      "====> Validation set kl: 26.3894\n",
      "Epoch: 863 [  100/50000 ( 0%)]  \tLoss:   88.684013\trec:   62.301640\tkl:   26.382372\n",
      "Epoch: 863 [10100/50000 (20%)]  \tLoss:   85.082703\trec:   59.806721\tkl:   25.275984\n",
      "Epoch: 863 [20100/50000 (40%)]  \tLoss:   89.719467\trec:   62.926952\tkl:   26.792519\n",
      "Epoch: 863 [30100/50000 (60%)]  \tLoss:   89.706841\trec:   62.969353\tkl:   26.737490\n",
      "Epoch: 863 [40100/50000 (80%)]  \tLoss:   91.901680\trec:   64.430351\tkl:   27.471325\n",
      "====> Epoch: 863 Average train loss: 90.4239\n",
      "====> Validation set loss: 94.4054\n",
      "====> Validation set kl: 26.3996\n",
      "Epoch: 864 [  100/50000 ( 0%)]  \tLoss:   90.282455\trec:   64.256096\tkl:   26.026361\n",
      "Epoch: 864 [10100/50000 (20%)]  \tLoss:   89.862831\trec:   63.077675\tkl:   26.785152\n",
      "Epoch: 864 [20100/50000 (40%)]  \tLoss:   91.393883\trec:   63.950790\tkl:   27.443094\n",
      "Epoch: 864 [30100/50000 (60%)]  \tLoss:   90.251450\trec:   63.568802\tkl:   26.682652\n",
      "Epoch: 864 [40100/50000 (80%)]  \tLoss:   92.481842\trec:   65.111076\tkl:   27.370768\n",
      "====> Epoch: 864 Average train loss: 90.4131\n",
      "====> Validation set loss: 94.4263\n",
      "====> Validation set kl: 26.2794\n",
      "Epoch: 865 [  100/50000 ( 0%)]  \tLoss:   88.584976\trec:   62.708435\tkl:   25.876543\n",
      "Epoch: 865 [10100/50000 (20%)]  \tLoss:   90.941689\trec:   63.896175\tkl:   27.045509\n",
      "Epoch: 865 [20100/50000 (40%)]  \tLoss:   88.570816\trec:   62.452106\tkl:   26.118713\n",
      "Epoch: 865 [30100/50000 (60%)]  \tLoss:   90.112862\trec:   63.246372\tkl:   26.866491\n",
      "Epoch: 865 [40100/50000 (80%)]  \tLoss:   91.533974\trec:   64.785339\tkl:   26.748634\n",
      "====> Epoch: 865 Average train loss: 90.4437\n",
      "====> Validation set loss: 94.5128\n",
      "====> Validation set kl: 26.4661\n",
      "Epoch: 866 [  100/50000 ( 0%)]  \tLoss:   82.604744\trec:   57.982891\tkl:   24.621855\n",
      "Epoch: 866 [10100/50000 (20%)]  \tLoss:   94.287605\trec:   67.375801\tkl:   26.911798\n",
      "Epoch: 866 [20100/50000 (40%)]  \tLoss:   94.176941\trec:   66.900574\tkl:   27.276369\n",
      "Epoch: 866 [30100/50000 (60%)]  \tLoss:   89.529839\trec:   63.018646\tkl:   26.511190\n",
      "Epoch: 866 [40100/50000 (80%)]  \tLoss:   89.371033\trec:   63.036106\tkl:   26.334929\n",
      "====> Epoch: 866 Average train loss: 90.4263\n",
      "====> Validation set loss: 94.3609\n",
      "====> Validation set kl: 26.5539\n",
      "Epoch: 867 [  100/50000 ( 0%)]  \tLoss:   94.149620\trec:   66.039413\tkl:   28.110203\n",
      "Epoch: 867 [10100/50000 (20%)]  \tLoss:   87.339180\trec:   61.309784\tkl:   26.029398\n",
      "Epoch: 867 [20100/50000 (40%)]  \tLoss:   90.253067\trec:   63.998379\tkl:   26.254692\n",
      "Epoch: 867 [30100/50000 (60%)]  \tLoss:   92.300995\trec:   65.303970\tkl:   26.997021\n",
      "Epoch: 867 [40100/50000 (80%)]  \tLoss:   89.881889\trec:   64.044533\tkl:   25.837360\n",
      "====> Epoch: 867 Average train loss: 90.4239\n",
      "====> Validation set loss: 94.5170\n",
      "====> Validation set kl: 26.3418\n",
      "Epoch: 868 [  100/50000 ( 0%)]  \tLoss:   91.232361\trec:   64.142448\tkl:   27.089916\n",
      "Epoch: 868 [10100/50000 (20%)]  \tLoss:   90.707809\trec:   63.436993\tkl:   27.270817\n",
      "Epoch: 868 [20100/50000 (40%)]  \tLoss:   88.415817\trec:   62.453815\tkl:   25.962006\n",
      "Epoch: 868 [30100/50000 (60%)]  \tLoss:   92.903961\trec:   66.131348\tkl:   26.772617\n",
      "Epoch: 868 [40100/50000 (80%)]  \tLoss:   92.357124\trec:   65.233963\tkl:   27.123159\n",
      "====> Epoch: 868 Average train loss: 90.3984\n",
      "====> Validation set loss: 94.4347\n",
      "====> Validation set kl: 26.5017\n",
      "Epoch: 869 [  100/50000 ( 0%)]  \tLoss:   94.266640\trec:   67.214966\tkl:   27.051672\n",
      "Epoch: 869 [10100/50000 (20%)]  \tLoss:   88.096992\trec:   62.414486\tkl:   25.682499\n",
      "Epoch: 869 [20100/50000 (40%)]  \tLoss:   88.792076\trec:   62.395477\tkl:   26.396601\n",
      "Epoch: 869 [30100/50000 (60%)]  \tLoss:   89.803864\trec:   63.263729\tkl:   26.540136\n",
      "Epoch: 869 [40100/50000 (80%)]  \tLoss:   91.724449\trec:   64.765938\tkl:   26.958515\n",
      "====> Epoch: 869 Average train loss: 90.3825\n",
      "====> Validation set loss: 94.3831\n",
      "====> Validation set kl: 26.2901\n",
      "Epoch: 870 [  100/50000 ( 0%)]  \tLoss:   90.933395\trec:   64.032478\tkl:   26.900919\n",
      "Epoch: 870 [10100/50000 (20%)]  \tLoss:   87.243309\trec:   60.878056\tkl:   26.365252\n",
      "Epoch: 870 [20100/50000 (40%)]  \tLoss:   86.733986\trec:   60.854740\tkl:   25.879246\n",
      "Epoch: 870 [30100/50000 (60%)]  \tLoss:   89.455292\trec:   63.621166\tkl:   25.834127\n",
      "Epoch: 870 [40100/50000 (80%)]  \tLoss:   88.930878\trec:   62.755428\tkl:   26.175444\n",
      "====> Epoch: 870 Average train loss: 90.4236\n",
      "====> Validation set loss: 94.4633\n",
      "====> Validation set kl: 26.3543\n",
      "Epoch: 871 [  100/50000 ( 0%)]  \tLoss:   89.645142\trec:   62.601727\tkl:   27.043417\n",
      "Epoch: 871 [10100/50000 (20%)]  \tLoss:   90.824303\trec:   63.916584\tkl:   26.907719\n",
      "Epoch: 871 [20100/50000 (40%)]  \tLoss:   88.581902\trec:   61.992458\tkl:   26.589445\n",
      "Epoch: 871 [30100/50000 (60%)]  \tLoss:   97.160751\trec:   69.660164\tkl:   27.500586\n",
      "Epoch: 871 [40100/50000 (80%)]  \tLoss:   93.914070\trec:   67.044449\tkl:   26.869621\n",
      "====> Epoch: 871 Average train loss: 90.3707\n",
      "====> Validation set loss: 94.4052\n",
      "====> Validation set kl: 26.3091\n",
      "Epoch: 872 [  100/50000 ( 0%)]  \tLoss:   90.684929\trec:   63.529316\tkl:   27.155617\n",
      "Epoch: 872 [10100/50000 (20%)]  \tLoss:   88.815430\trec:   63.118416\tkl:   25.697012\n",
      "Epoch: 872 [20100/50000 (40%)]  \tLoss:   91.504158\trec:   64.533005\tkl:   26.971151\n",
      "Epoch: 872 [30100/50000 (60%)]  \tLoss:   80.473595\trec:   56.479778\tkl:   23.993813\n",
      "Epoch: 872 [40100/50000 (80%)]  \tLoss:   89.992294\trec:   63.396057\tkl:   26.596237\n",
      "====> Epoch: 872 Average train loss: 90.3883\n",
      "====> Validation set loss: 94.4544\n",
      "====> Validation set kl: 26.4529\n",
      "Epoch: 873 [  100/50000 ( 0%)]  \tLoss:   87.048904\trec:   61.250313\tkl:   25.798594\n",
      "Epoch: 873 [10100/50000 (20%)]  \tLoss:   91.304688\trec:   64.486031\tkl:   26.818657\n",
      "Epoch: 873 [20100/50000 (40%)]  \tLoss:   92.993423\trec:   66.646690\tkl:   26.346731\n",
      "Epoch: 873 [30100/50000 (60%)]  \tLoss:   87.865211\trec:   61.552391\tkl:   26.312819\n",
      "Epoch: 873 [40100/50000 (80%)]  \tLoss:   89.314568\trec:   63.596069\tkl:   25.718506\n",
      "====> Epoch: 873 Average train loss: 90.4143\n",
      "====> Validation set loss: 94.3631\n",
      "====> Validation set kl: 26.3607\n",
      "Epoch: 874 [  100/50000 ( 0%)]  \tLoss:   90.833374\trec:   63.934414\tkl:   26.898960\n",
      "Epoch: 874 [10100/50000 (20%)]  \tLoss:   92.199669\trec:   65.857529\tkl:   26.342142\n",
      "Epoch: 874 [20100/50000 (40%)]  \tLoss:   90.613640\trec:   64.531136\tkl:   26.082506\n",
      "Epoch: 874 [30100/50000 (60%)]  \tLoss:   92.815323\trec:   65.563988\tkl:   27.251335\n",
      "Epoch: 874 [40100/50000 (80%)]  \tLoss:   89.873962\trec:   63.536629\tkl:   26.337332\n",
      "====> Epoch: 874 Average train loss: 90.3913\n",
      "====> Validation set loss: 94.3755\n",
      "====> Validation set kl: 26.1963\n",
      "Epoch: 875 [  100/50000 ( 0%)]  \tLoss:   91.482086\trec:   65.195656\tkl:   26.286425\n",
      "Epoch: 875 [10100/50000 (20%)]  \tLoss:   89.652039\trec:   64.136261\tkl:   25.515783\n",
      "Epoch: 875 [20100/50000 (40%)]  \tLoss:   89.926445\trec:   64.157005\tkl:   25.769432\n",
      "Epoch: 875 [30100/50000 (60%)]  \tLoss:   87.566101\trec:   61.589775\tkl:   25.976330\n",
      "Epoch: 875 [40100/50000 (80%)]  \tLoss:   90.175232\trec:   63.944267\tkl:   26.230961\n",
      "====> Epoch: 875 Average train loss: 90.3950\n",
      "====> Validation set loss: 94.5137\n",
      "====> Validation set kl: 26.3322\n",
      "Epoch: 876 [  100/50000 ( 0%)]  \tLoss:   90.414078\trec:   63.709599\tkl:   26.704481\n",
      "Epoch: 876 [10100/50000 (20%)]  \tLoss:   89.143692\trec:   63.119610\tkl:   26.024084\n",
      "Epoch: 876 [20100/50000 (40%)]  \tLoss:   88.161659\trec:   61.644726\tkl:   26.516930\n",
      "Epoch: 876 [30100/50000 (60%)]  \tLoss:   91.207085\trec:   65.768127\tkl:   25.438959\n",
      "Epoch: 876 [40100/50000 (80%)]  \tLoss:   90.727097\trec:   64.470871\tkl:   26.256226\n",
      "====> Epoch: 876 Average train loss: 90.3753\n",
      "====> Validation set loss: 94.5399\n",
      "====> Validation set kl: 26.5499\n",
      "Epoch: 877 [  100/50000 ( 0%)]  \tLoss:   91.734726\trec:   65.101463\tkl:   26.633261\n",
      "Epoch: 877 [10100/50000 (20%)]  \tLoss:   90.001793\trec:   63.590321\tkl:   26.411472\n",
      "Epoch: 877 [20100/50000 (40%)]  \tLoss:   92.830292\trec:   66.196472\tkl:   26.633823\n",
      "Epoch: 877 [30100/50000 (60%)]  \tLoss:   87.940468\trec:   62.015514\tkl:   25.924948\n",
      "Epoch: 877 [40100/50000 (80%)]  \tLoss:   91.156120\trec:   64.432068\tkl:   26.724054\n",
      "====> Epoch: 877 Average train loss: 90.3828\n",
      "====> Validation set loss: 94.4637\n",
      "====> Validation set kl: 26.2454\n",
      "Epoch: 878 [  100/50000 ( 0%)]  \tLoss:   87.249237\trec:   61.478542\tkl:   25.770687\n",
      "Epoch: 878 [10100/50000 (20%)]  \tLoss:   85.650955\trec:   60.213135\tkl:   25.437822\n",
      "Epoch: 878 [20100/50000 (40%)]  \tLoss:   89.684135\trec:   63.683308\tkl:   26.000828\n",
      "Epoch: 878 [30100/50000 (60%)]  \tLoss:   92.733261\trec:   66.539825\tkl:   26.193441\n",
      "Epoch: 878 [40100/50000 (80%)]  \tLoss:   88.991287\trec:   62.006668\tkl:   26.984619\n",
      "====> Epoch: 878 Average train loss: 90.3842\n",
      "====> Validation set loss: 94.4580\n",
      "====> Validation set kl: 26.6280\n",
      "Epoch: 879 [  100/50000 ( 0%)]  \tLoss:   87.566406\trec:   60.703308\tkl:   26.863092\n",
      "Epoch: 879 [10100/50000 (20%)]  \tLoss:   90.040817\trec:   63.631374\tkl:   26.409443\n",
      "Epoch: 879 [20100/50000 (40%)]  \tLoss:   91.818459\trec:   64.832573\tkl:   26.985884\n",
      "Epoch: 879 [30100/50000 (60%)]  \tLoss:   86.156967\trec:   61.400715\tkl:   24.756250\n",
      "Epoch: 879 [40100/50000 (80%)]  \tLoss:   87.325653\trec:   61.403332\tkl:   25.922321\n",
      "====> Epoch: 879 Average train loss: 90.3770\n",
      "====> Validation set loss: 94.4998\n",
      "====> Validation set kl: 26.4857\n",
      "Epoch: 880 [  100/50000 ( 0%)]  \tLoss:   90.802773\trec:   64.120796\tkl:   26.681974\n",
      "Epoch: 880 [10100/50000 (20%)]  \tLoss:   84.575172\trec:   58.808727\tkl:   25.766441\n",
      "Epoch: 880 [20100/50000 (40%)]  \tLoss:   89.824936\trec:   63.393906\tkl:   26.431034\n",
      "Epoch: 880 [30100/50000 (60%)]  \tLoss:   92.271561\trec:   65.705254\tkl:   26.566305\n",
      "Epoch: 880 [40100/50000 (80%)]  \tLoss:   88.278885\trec:   62.859516\tkl:   25.419373\n",
      "====> Epoch: 880 Average train loss: 90.3826\n",
      "====> Validation set loss: 94.5036\n",
      "====> Validation set kl: 26.1567\n",
      "Epoch: 881 [  100/50000 ( 0%)]  \tLoss:   88.694901\trec:   63.279739\tkl:   25.415157\n",
      "Epoch: 881 [10100/50000 (20%)]  \tLoss:   91.312614\trec:   64.744148\tkl:   26.568468\n",
      "Epoch: 881 [20100/50000 (40%)]  \tLoss:   92.150131\trec:   65.329308\tkl:   26.820835\n",
      "Epoch: 881 [30100/50000 (60%)]  \tLoss:   93.325249\trec:   66.043221\tkl:   27.282028\n",
      "Epoch: 881 [40100/50000 (80%)]  \tLoss:   92.587227\trec:   65.746964\tkl:   26.840261\n",
      "====> Epoch: 881 Average train loss: 90.4039\n",
      "====> Validation set loss: 94.4325\n",
      "====> Validation set kl: 26.4365\n",
      "Epoch: 882 [  100/50000 ( 0%)]  \tLoss:   88.537422\trec:   62.518826\tkl:   26.018593\n",
      "Epoch: 882 [10100/50000 (20%)]  \tLoss:   92.699913\trec:   65.661049\tkl:   27.038858\n",
      "Epoch: 882 [20100/50000 (40%)]  \tLoss:   88.833618\trec:   63.311054\tkl:   25.522570\n",
      "Epoch: 882 [30100/50000 (60%)]  \tLoss:   91.722794\trec:   65.687424\tkl:   26.035370\n",
      "Epoch: 882 [40100/50000 (80%)]  \tLoss:   93.666000\trec:   66.542358\tkl:   27.123644\n",
      "====> Epoch: 882 Average train loss: 90.3695\n",
      "====> Validation set loss: 94.4513\n",
      "====> Validation set kl: 26.4777\n",
      "Epoch: 883 [  100/50000 ( 0%)]  \tLoss:   88.271988\trec:   62.091465\tkl:   26.180527\n",
      "Epoch: 883 [10100/50000 (20%)]  \tLoss:   88.403687\trec:   62.709324\tkl:   25.694363\n",
      "Epoch: 883 [20100/50000 (40%)]  \tLoss:   90.516121\trec:   63.977165\tkl:   26.538952\n",
      "Epoch: 883 [30100/50000 (60%)]  \tLoss:   92.745331\trec:   65.496277\tkl:   27.249058\n",
      "Epoch: 883 [40100/50000 (80%)]  \tLoss:   88.916870\trec:   61.841190\tkl:   27.075678\n",
      "====> Epoch: 883 Average train loss: 90.3644\n",
      "====> Validation set loss: 94.4767\n",
      "====> Validation set kl: 26.4891\n",
      "Epoch: 884 [  100/50000 ( 0%)]  \tLoss:   87.464439\trec:   61.486839\tkl:   25.977600\n",
      "Epoch: 884 [10100/50000 (20%)]  \tLoss:   87.557617\trec:   61.361015\tkl:   26.196604\n",
      "Epoch: 884 [20100/50000 (40%)]  \tLoss:   91.491432\trec:   65.268234\tkl:   26.223200\n",
      "Epoch: 884 [30100/50000 (60%)]  \tLoss:   88.413818\trec:   62.581951\tkl:   25.831865\n",
      "Epoch: 884 [40100/50000 (80%)]  \tLoss:   91.230942\trec:   64.402184\tkl:   26.828766\n",
      "====> Epoch: 884 Average train loss: 90.3620\n",
      "====> Validation set loss: 94.4214\n",
      "====> Validation set kl: 26.4314\n",
      "Epoch: 885 [  100/50000 ( 0%)]  \tLoss:   91.166779\trec:   64.966576\tkl:   26.200205\n",
      "Epoch: 885 [10100/50000 (20%)]  \tLoss:   89.803642\trec:   64.170059\tkl:   25.633581\n",
      "Epoch: 885 [20100/50000 (40%)]  \tLoss:   90.445869\trec:   63.921219\tkl:   26.524645\n",
      "Epoch: 885 [30100/50000 (60%)]  \tLoss:   89.376808\trec:   63.583923\tkl:   25.792883\n",
      "Epoch: 885 [40100/50000 (80%)]  \tLoss:   91.939804\trec:   65.184204\tkl:   26.755606\n",
      "====> Epoch: 885 Average train loss: 90.3769\n",
      "====> Validation set loss: 94.3960\n",
      "====> Validation set kl: 26.2714\n",
      "Epoch: 886 [  100/50000 ( 0%)]  \tLoss:   92.537674\trec:   65.956772\tkl:   26.580908\n",
      "Epoch: 886 [10100/50000 (20%)]  \tLoss:   92.131638\trec:   65.294678\tkl:   26.836962\n",
      "Epoch: 886 [20100/50000 (40%)]  \tLoss:   89.651855\trec:   63.248665\tkl:   26.403185\n",
      "Epoch: 886 [30100/50000 (60%)]  \tLoss:   90.433266\trec:   63.691635\tkl:   26.741634\n",
      "Epoch: 886 [40100/50000 (80%)]  \tLoss:   91.001862\trec:   64.329178\tkl:   26.672688\n",
      "====> Epoch: 886 Average train loss: 90.3760\n",
      "====> Validation set loss: 94.4954\n",
      "====> Validation set kl: 26.6575\n",
      "Epoch: 887 [  100/50000 ( 0%)]  \tLoss:   92.095772\trec:   64.722939\tkl:   27.372829\n",
      "Epoch: 887 [10100/50000 (20%)]  \tLoss:   90.093864\trec:   63.985073\tkl:   26.108799\n",
      "Epoch: 887 [20100/50000 (40%)]  \tLoss:   89.060097\trec:   62.695808\tkl:   26.364286\n",
      "Epoch: 887 [30100/50000 (60%)]  \tLoss:   93.526558\trec:   66.841263\tkl:   26.685293\n",
      "Epoch: 887 [40100/50000 (80%)]  \tLoss:   91.843300\trec:   65.853760\tkl:   25.989540\n",
      "====> Epoch: 887 Average train loss: 90.3542\n",
      "====> Validation set loss: 94.4136\n",
      "====> Validation set kl: 26.3217\n",
      "Epoch: 888 [  100/50000 ( 0%)]  \tLoss:   87.399292\trec:   61.519543\tkl:   25.879753\n",
      "Epoch: 888 [10100/50000 (20%)]  \tLoss:   86.102676\trec:   59.895988\tkl:   26.206684\n",
      "Epoch: 888 [20100/50000 (40%)]  \tLoss:   90.118065\trec:   62.921970\tkl:   27.196089\n",
      "Epoch: 888 [30100/50000 (60%)]  \tLoss:   91.552895\trec:   64.787926\tkl:   26.764973\n",
      "Epoch: 888 [40100/50000 (80%)]  \tLoss:   93.891930\trec:   66.823784\tkl:   27.068146\n",
      "====> Epoch: 888 Average train loss: 90.3574\n",
      "====> Validation set loss: 94.3253\n",
      "====> Validation set kl: 26.3492\n",
      "Epoch: 889 [  100/50000 ( 0%)]  \tLoss:   95.421043\trec:   68.379028\tkl:   27.042011\n",
      "Epoch: 889 [10100/50000 (20%)]  \tLoss:   91.644371\trec:   64.942078\tkl:   26.702297\n",
      "Epoch: 889 [20100/50000 (40%)]  \tLoss:   86.130417\trec:   60.072758\tkl:   26.057663\n",
      "Epoch: 889 [30100/50000 (60%)]  \tLoss:   89.404037\trec:   63.442616\tkl:   25.961422\n",
      "Epoch: 889 [40100/50000 (80%)]  \tLoss:   92.245438\trec:   65.526489\tkl:   26.718946\n",
      "====> Epoch: 889 Average train loss: 90.3603\n",
      "====> Validation set loss: 94.3995\n",
      "====> Validation set kl: 26.3801\n",
      "Epoch: 890 [  100/50000 ( 0%)]  \tLoss:   89.367500\trec:   63.025452\tkl:   26.342045\n",
      "Epoch: 890 [10100/50000 (20%)]  \tLoss:   88.438156\trec:   61.932976\tkl:   26.505175\n",
      "Epoch: 890 [20100/50000 (40%)]  \tLoss:   90.949471\trec:   64.078995\tkl:   26.870474\n",
      "Epoch: 890 [30100/50000 (60%)]  \tLoss:   90.233711\trec:   64.148857\tkl:   26.084850\n",
      "Epoch: 890 [40100/50000 (80%)]  \tLoss:   87.666306\trec:   61.738205\tkl:   25.928102\n",
      "====> Epoch: 890 Average train loss: 90.3438\n",
      "====> Validation set loss: 94.4336\n",
      "====> Validation set kl: 26.5826\n",
      "Epoch: 891 [  100/50000 ( 0%)]  \tLoss:   88.564041\trec:   62.616650\tkl:   25.947390\n",
      "Epoch: 891 [10100/50000 (20%)]  \tLoss:   90.624184\trec:   64.275124\tkl:   26.349062\n",
      "Epoch: 891 [20100/50000 (40%)]  \tLoss:   89.225388\trec:   63.595879\tkl:   25.629517\n",
      "Epoch: 891 [30100/50000 (60%)]  \tLoss:   89.141335\trec:   62.789539\tkl:   26.351797\n",
      "Epoch: 891 [40100/50000 (80%)]  \tLoss:   84.995018\trec:   59.477612\tkl:   25.517408\n",
      "====> Epoch: 891 Average train loss: 90.3894\n",
      "====> Validation set loss: 94.4478\n",
      "====> Validation set kl: 26.3789\n",
      "Epoch: 892 [  100/50000 ( 0%)]  \tLoss:   88.602570\trec:   62.893417\tkl:   25.709146\n",
      "Epoch: 892 [10100/50000 (20%)]  \tLoss:   83.270134\trec:   58.514450\tkl:   24.755684\n",
      "Epoch: 892 [20100/50000 (40%)]  \tLoss:   87.503777\trec:   61.521275\tkl:   25.982506\n",
      "Epoch: 892 [30100/50000 (60%)]  \tLoss:   89.331772\trec:   62.704960\tkl:   26.626822\n",
      "Epoch: 892 [40100/50000 (80%)]  \tLoss:   88.874306\trec:   63.247898\tkl:   25.626408\n",
      "====> Epoch: 892 Average train loss: 90.3396\n",
      "====> Validation set loss: 94.4333\n",
      "====> Validation set kl: 26.5734\n",
      "Epoch: 893 [  100/50000 ( 0%)]  \tLoss:   92.768250\trec:   65.423065\tkl:   27.345184\n",
      "Epoch: 893 [10100/50000 (20%)]  \tLoss:   93.184761\trec:   65.822838\tkl:   27.361917\n",
      "Epoch: 893 [20100/50000 (40%)]  \tLoss:   92.779373\trec:   65.639595\tkl:   27.139780\n",
      "Epoch: 893 [30100/50000 (60%)]  \tLoss:   91.474503\trec:   65.073036\tkl:   26.401463\n",
      "Epoch: 893 [40100/50000 (80%)]  \tLoss:   88.371544\trec:   61.561871\tkl:   26.809671\n",
      "====> Epoch: 893 Average train loss: 90.3381\n",
      "====> Validation set loss: 94.5273\n",
      "====> Validation set kl: 26.4482\n",
      "Epoch: 894 [  100/50000 ( 0%)]  \tLoss:   89.562531\trec:   62.214794\tkl:   27.347734\n",
      "Epoch: 894 [10100/50000 (20%)]  \tLoss:   92.577309\trec:   65.596100\tkl:   26.981218\n",
      "Epoch: 894 [20100/50000 (40%)]  \tLoss:   90.057030\trec:   64.250977\tkl:   25.806049\n",
      "Epoch: 894 [30100/50000 (60%)]  \tLoss:   87.555176\trec:   61.922527\tkl:   25.632648\n",
      "Epoch: 894 [40100/50000 (80%)]  \tLoss:   90.991798\trec:   64.953995\tkl:   26.037802\n",
      "====> Epoch: 894 Average train loss: 90.3554\n",
      "====> Validation set loss: 94.4739\n",
      "====> Validation set kl: 26.5481\n",
      "Epoch: 895 [  100/50000 ( 0%)]  \tLoss:   91.085274\trec:   63.602680\tkl:   27.482597\n",
      "Epoch: 895 [10100/50000 (20%)]  \tLoss:   92.947456\trec:   66.794624\tkl:   26.152826\n",
      "Epoch: 895 [20100/50000 (40%)]  \tLoss:   88.557480\trec:   62.162754\tkl:   26.394728\n",
      "Epoch: 895 [30100/50000 (60%)]  \tLoss:   90.236000\trec:   62.991550\tkl:   27.244453\n",
      "Epoch: 895 [40100/50000 (80%)]  \tLoss:   90.212486\trec:   64.289307\tkl:   25.923180\n",
      "====> Epoch: 895 Average train loss: 90.3457\n",
      "====> Validation set loss: 94.4408\n",
      "====> Validation set kl: 26.2909\n",
      "Epoch: 896 [  100/50000 ( 0%)]  \tLoss:   91.220634\trec:   65.028389\tkl:   26.192238\n",
      "Epoch: 896 [10100/50000 (20%)]  \tLoss:   90.749870\trec:   63.893505\tkl:   26.856367\n",
      "Epoch: 896 [20100/50000 (40%)]  \tLoss:   90.602608\trec:   64.028130\tkl:   26.574471\n",
      "Epoch: 896 [30100/50000 (60%)]  \tLoss:   92.754868\trec:   66.008125\tkl:   26.746750\n",
      "Epoch: 896 [40100/50000 (80%)]  \tLoss:   88.579552\trec:   62.518513\tkl:   26.061029\n",
      "====> Epoch: 896 Average train loss: 90.3462\n",
      "====> Validation set loss: 94.5397\n",
      "====> Validation set kl: 26.4549\n",
      "Epoch: 897 [  100/50000 ( 0%)]  \tLoss:   93.461861\trec:   66.496231\tkl:   26.965630\n",
      "Epoch: 897 [10100/50000 (20%)]  \tLoss:   88.452072\trec:   62.903728\tkl:   25.548344\n",
      "Epoch: 897 [20100/50000 (40%)]  \tLoss:   93.199684\trec:   66.464325\tkl:   26.735361\n",
      "Epoch: 897 [30100/50000 (60%)]  \tLoss:   93.195747\trec:   66.093582\tkl:   27.102167\n",
      "Epoch: 897 [40100/50000 (80%)]  \tLoss:   90.329491\trec:   64.371513\tkl:   25.957973\n",
      "====> Epoch: 897 Average train loss: 90.3566\n",
      "====> Validation set loss: 94.4614\n",
      "====> Validation set kl: 26.4017\n",
      "Epoch: 898 [  100/50000 ( 0%)]  \tLoss:   91.174210\trec:   64.769478\tkl:   26.404726\n",
      "Epoch: 898 [10100/50000 (20%)]  \tLoss:   90.141441\trec:   63.544598\tkl:   26.596846\n",
      "Epoch: 898 [20100/50000 (40%)]  \tLoss:   90.829178\trec:   64.332840\tkl:   26.496334\n",
      "Epoch: 898 [30100/50000 (60%)]  \tLoss:   91.744736\trec:   64.992027\tkl:   26.752707\n",
      "Epoch: 898 [40100/50000 (80%)]  \tLoss:   90.481979\trec:   64.268097\tkl:   26.213879\n",
      "====> Epoch: 898 Average train loss: 90.3457\n",
      "====> Validation set loss: 94.3793\n",
      "====> Validation set kl: 26.3582\n",
      "Epoch: 899 [  100/50000 ( 0%)]  \tLoss:   90.015144\trec:   63.572380\tkl:   26.442760\n",
      "Epoch: 899 [10100/50000 (20%)]  \tLoss:   89.406601\trec:   63.186489\tkl:   26.220106\n",
      "Epoch: 899 [20100/50000 (40%)]  \tLoss:   90.708260\trec:   64.094086\tkl:   26.614170\n",
      "Epoch: 899 [30100/50000 (60%)]  \tLoss:   90.399315\trec:   64.058487\tkl:   26.340828\n",
      "Epoch: 899 [40100/50000 (80%)]  \tLoss:   92.463837\trec:   65.943199\tkl:   26.520641\n",
      "====> Epoch: 899 Average train loss: 90.3341\n",
      "====> Validation set loss: 94.3538\n",
      "====> Validation set kl: 26.5970\n",
      "Epoch: 900 [  100/50000 ( 0%)]  \tLoss:   87.678406\trec:   62.040985\tkl:   25.637419\n",
      "Epoch: 900 [10100/50000 (20%)]  \tLoss:   90.432671\trec:   64.051483\tkl:   26.381191\n",
      "Epoch: 900 [20100/50000 (40%)]  \tLoss:   91.597572\trec:   65.217918\tkl:   26.379658\n",
      "Epoch: 900 [30100/50000 (60%)]  \tLoss:   88.888336\trec:   62.220287\tkl:   26.668051\n",
      "Epoch: 900 [40100/50000 (80%)]  \tLoss:   87.596733\trec:   61.216908\tkl:   26.379829\n",
      "====> Epoch: 900 Average train loss: 90.3398\n",
      "====> Validation set loss: 94.4303\n",
      "====> Validation set kl: 26.5156\n",
      "Epoch: 901 [  100/50000 ( 0%)]  \tLoss:   91.010078\trec:   64.334366\tkl:   26.675707\n",
      "Epoch: 901 [10100/50000 (20%)]  \tLoss:   95.683594\trec:   67.975525\tkl:   27.708061\n",
      "Epoch: 901 [20100/50000 (40%)]  \tLoss:   85.235184\trec:   59.411423\tkl:   25.823757\n",
      "Epoch: 901 [30100/50000 (60%)]  \tLoss:   95.102814\trec:   67.644577\tkl:   27.458227\n",
      "Epoch: 901 [40100/50000 (80%)]  \tLoss:   92.186401\trec:   65.616241\tkl:   26.570164\n",
      "====> Epoch: 901 Average train loss: 90.3376\n",
      "====> Validation set loss: 94.4879\n",
      "====> Validation set kl: 26.2737\n",
      "Epoch: 902 [  100/50000 ( 0%)]  \tLoss:   93.125015\trec:   66.802681\tkl:   26.322330\n",
      "Epoch: 902 [10100/50000 (20%)]  \tLoss:   88.223045\trec:   62.510662\tkl:   25.712378\n",
      "Epoch: 902 [20100/50000 (40%)]  \tLoss:   91.210358\trec:   64.939560\tkl:   26.270798\n",
      "Epoch: 902 [30100/50000 (60%)]  \tLoss:   92.254044\trec:   65.322418\tkl:   26.931625\n",
      "Epoch: 902 [40100/50000 (80%)]  \tLoss:   91.010658\trec:   64.804352\tkl:   26.206303\n",
      "====> Epoch: 902 Average train loss: 90.3268\n",
      "====> Validation set loss: 94.4467\n",
      "====> Validation set kl: 26.3718\n",
      "Epoch: 903 [  100/50000 ( 0%)]  \tLoss:   92.216454\trec:   64.381027\tkl:   27.835426\n",
      "Epoch: 903 [10100/50000 (20%)]  \tLoss:   86.177216\trec:   60.264118\tkl:   25.913097\n",
      "Epoch: 903 [20100/50000 (40%)]  \tLoss:   87.948036\trec:   62.776737\tkl:   25.171299\n",
      "Epoch: 903 [30100/50000 (60%)]  \tLoss:   93.684784\trec:   67.039246\tkl:   26.645542\n",
      "Epoch: 903 [40100/50000 (80%)]  \tLoss:   88.229118\trec:   62.151020\tkl:   26.078094\n",
      "====> Epoch: 903 Average train loss: 90.3571\n",
      "====> Validation set loss: 94.4401\n",
      "====> Validation set kl: 26.4542\n",
      "Epoch: 904 [  100/50000 ( 0%)]  \tLoss:   92.372208\trec:   65.018349\tkl:   27.353855\n",
      "Epoch: 904 [10100/50000 (20%)]  \tLoss:   91.946602\trec:   65.775490\tkl:   26.171116\n",
      "Epoch: 904 [20100/50000 (40%)]  \tLoss:   91.393883\trec:   64.295456\tkl:   27.098429\n",
      "Epoch: 904 [30100/50000 (60%)]  \tLoss:   89.649422\trec:   63.232899\tkl:   26.416523\n",
      "Epoch: 904 [40100/50000 (80%)]  \tLoss:   86.338432\trec:   60.743340\tkl:   25.595097\n",
      "====> Epoch: 904 Average train loss: 90.3432\n",
      "====> Validation set loss: 94.3595\n",
      "====> Validation set kl: 26.2203\n",
      "Epoch: 905 [  100/50000 ( 0%)]  \tLoss:   92.235352\trec:   65.167740\tkl:   27.067606\n",
      "Epoch: 905 [10100/50000 (20%)]  \tLoss:   92.588493\trec:   65.913261\tkl:   26.675232\n",
      "Epoch: 905 [20100/50000 (40%)]  \tLoss:   89.066322\trec:   63.068584\tkl:   25.997742\n",
      "Epoch: 905 [30100/50000 (60%)]  \tLoss:   91.940796\trec:   65.542381\tkl:   26.398420\n",
      "Epoch: 905 [40100/50000 (80%)]  \tLoss:   87.298004\trec:   61.764004\tkl:   25.534008\n",
      "====> Epoch: 905 Average train loss: 90.3238\n",
      "====> Validation set loss: 94.5493\n",
      "====> Validation set kl: 26.3306\n",
      "Epoch: 906 [  100/50000 ( 0%)]  \tLoss:   89.955147\trec:   64.006416\tkl:   25.948732\n",
      "Epoch: 906 [10100/50000 (20%)]  \tLoss:   90.076538\trec:   64.456291\tkl:   25.620249\n",
      "Epoch: 906 [20100/50000 (40%)]  \tLoss:   95.200806\trec:   68.845215\tkl:   26.355598\n",
      "Epoch: 906 [30100/50000 (60%)]  \tLoss:   88.161804\trec:   62.498203\tkl:   25.663601\n",
      "Epoch: 906 [40100/50000 (80%)]  \tLoss:   89.746208\trec:   62.857616\tkl:   26.888590\n",
      "====> Epoch: 906 Average train loss: 90.3348\n",
      "====> Validation set loss: 94.4863\n",
      "====> Validation set kl: 26.4094\n",
      "Epoch: 907 [  100/50000 ( 0%)]  \tLoss:   90.748833\trec:   63.475780\tkl:   27.273056\n",
      "Epoch: 907 [10100/50000 (20%)]  \tLoss:   89.977539\trec:   63.305820\tkl:   26.671715\n",
      "Epoch: 907 [20100/50000 (40%)]  \tLoss:   86.813065\trec:   61.509773\tkl:   25.303291\n",
      "Epoch: 907 [30100/50000 (60%)]  \tLoss:   92.220886\trec:   64.766090\tkl:   27.454794\n",
      "Epoch: 907 [40100/50000 (80%)]  \tLoss:   89.016479\trec:   63.961220\tkl:   25.055267\n",
      "====> Epoch: 907 Average train loss: 90.3259\n",
      "====> Validation set loss: 94.3921\n",
      "====> Validation set kl: 26.4516\n",
      "Epoch: 908 [  100/50000 ( 0%)]  \tLoss:   89.348228\trec:   63.076893\tkl:   26.271338\n",
      "Epoch: 908 [10100/50000 (20%)]  \tLoss:   90.949272\trec:   64.203476\tkl:   26.745806\n",
      "Epoch: 908 [20100/50000 (40%)]  \tLoss:   89.871254\trec:   63.340618\tkl:   26.530642\n",
      "Epoch: 908 [30100/50000 (60%)]  \tLoss:   92.112831\trec:   65.421249\tkl:   26.691586\n",
      "Epoch: 908 [40100/50000 (80%)]  \tLoss:   92.896309\trec:   66.713684\tkl:   26.182621\n",
      "====> Epoch: 908 Average train loss: 90.3052\n",
      "====> Validation set loss: 94.3968\n",
      "====> Validation set kl: 26.2629\n",
      "Epoch: 909 [  100/50000 ( 0%)]  \tLoss:   88.827362\trec:   62.628960\tkl:   26.198404\n",
      "Epoch: 909 [10100/50000 (20%)]  \tLoss:   88.691772\trec:   62.129158\tkl:   26.562614\n",
      "Epoch: 909 [20100/50000 (40%)]  \tLoss:   89.709717\trec:   63.674129\tkl:   26.035583\n",
      "Epoch: 909 [30100/50000 (60%)]  \tLoss:   91.270195\trec:   64.355888\tkl:   26.914307\n",
      "Epoch: 909 [40100/50000 (80%)]  \tLoss:   87.392372\trec:   62.022888\tkl:   25.369484\n",
      "====> Epoch: 909 Average train loss: 90.3100\n",
      "====> Validation set loss: 94.5152\n",
      "====> Validation set kl: 26.4748\n",
      "Epoch: 910 [  100/50000 ( 0%)]  \tLoss:   92.179413\trec:   65.589561\tkl:   26.589855\n",
      "Epoch: 910 [10100/50000 (20%)]  \tLoss:   90.891518\trec:   63.217323\tkl:   27.674194\n",
      "Epoch: 910 [20100/50000 (40%)]  \tLoss:   94.787354\trec:   68.351242\tkl:   26.436113\n",
      "Epoch: 910 [30100/50000 (60%)]  \tLoss:   89.755211\trec:   64.163506\tkl:   25.591703\n",
      "Epoch: 910 [40100/50000 (80%)]  \tLoss:   89.220154\trec:   62.534363\tkl:   26.685791\n",
      "====> Epoch: 910 Average train loss: 90.3266\n",
      "====> Validation set loss: 94.4751\n",
      "====> Validation set kl: 26.4939\n",
      "Epoch: 911 [  100/50000 ( 0%)]  \tLoss:   90.967957\trec:   64.691650\tkl:   26.276306\n",
      "Epoch: 911 [10100/50000 (20%)]  \tLoss:   90.713142\trec:   63.774143\tkl:   26.938995\n",
      "Epoch: 911 [20100/50000 (40%)]  \tLoss:   87.112282\trec:   61.343662\tkl:   25.768620\n",
      "Epoch: 911 [30100/50000 (60%)]  \tLoss:   91.302490\trec:   65.244942\tkl:   26.057543\n",
      "Epoch: 911 [40100/50000 (80%)]  \tLoss:   90.315445\trec:   63.697491\tkl:   26.617958\n",
      "====> Epoch: 911 Average train loss: 90.2951\n",
      "====> Validation set loss: 94.3786\n",
      "====> Validation set kl: 26.2367\n",
      "Epoch: 912 [  100/50000 ( 0%)]  \tLoss:   89.823074\trec:   63.865273\tkl:   25.957804\n",
      "Epoch: 912 [10100/50000 (20%)]  \tLoss:   86.301094\trec:   61.656467\tkl:   24.644629\n",
      "Epoch: 912 [20100/50000 (40%)]  \tLoss:   89.633553\trec:   63.312012\tkl:   26.321547\n",
      "Epoch: 912 [30100/50000 (60%)]  \tLoss:   92.354477\trec:   65.345879\tkl:   27.008606\n",
      "Epoch: 912 [40100/50000 (80%)]  \tLoss:   90.762810\trec:   63.949062\tkl:   26.813755\n",
      "====> Epoch: 912 Average train loss: 90.2990\n",
      "====> Validation set loss: 94.2927\n",
      "====> Validation set kl: 26.3029\n",
      "Epoch: 913 [  100/50000 ( 0%)]  \tLoss:   87.188164\trec:   60.594696\tkl:   26.593468\n",
      "Epoch: 913 [10100/50000 (20%)]  \tLoss:   89.962013\trec:   63.511864\tkl:   26.450146\n",
      "Epoch: 913 [20100/50000 (40%)]  \tLoss:   90.418221\trec:   64.394547\tkl:   26.023668\n",
      "Epoch: 913 [30100/50000 (60%)]  \tLoss:   91.645096\trec:   65.218521\tkl:   26.426569\n",
      "Epoch: 913 [40100/50000 (80%)]  \tLoss:   89.726006\trec:   63.837124\tkl:   25.888884\n",
      "====> Epoch: 913 Average train loss: 90.2896\n",
      "====> Validation set loss: 94.4417\n",
      "====> Validation set kl: 26.4096\n",
      "Epoch: 914 [  100/50000 ( 0%)]  \tLoss:   91.348610\trec:   64.816765\tkl:   26.531839\n",
      "Epoch: 914 [10100/50000 (20%)]  \tLoss:   89.759842\trec:   63.517284\tkl:   26.242561\n",
      "Epoch: 914 [20100/50000 (40%)]  \tLoss:   87.831200\trec:   61.921497\tkl:   25.909704\n",
      "Epoch: 914 [30100/50000 (60%)]  \tLoss:   94.124062\trec:   66.092529\tkl:   28.031528\n",
      "Epoch: 914 [40100/50000 (80%)]  \tLoss:   91.124138\trec:   64.849869\tkl:   26.274271\n",
      "====> Epoch: 914 Average train loss: 90.2866\n",
      "====> Validation set loss: 94.4376\n",
      "====> Validation set kl: 26.4683\n",
      "Epoch: 915 [  100/50000 ( 0%)]  \tLoss:   93.138168\trec:   66.312286\tkl:   26.825890\n",
      "Epoch: 915 [10100/50000 (20%)]  \tLoss:   92.736359\trec:   66.177742\tkl:   26.558615\n",
      "Epoch: 915 [20100/50000 (40%)]  \tLoss:   85.043503\trec:   59.927761\tkl:   25.115742\n",
      "Epoch: 915 [30100/50000 (60%)]  \tLoss:   87.597397\trec:   61.088116\tkl:   26.509291\n",
      "Epoch: 915 [40100/50000 (80%)]  \tLoss:   88.080704\trec:   61.687607\tkl:   26.393097\n",
      "====> Epoch: 915 Average train loss: 90.3103\n",
      "====> Validation set loss: 94.4466\n",
      "====> Validation set kl: 26.4376\n",
      "Epoch: 916 [  100/50000 ( 0%)]  \tLoss:   93.916519\trec:   66.581848\tkl:   27.334667\n",
      "Epoch: 916 [10100/50000 (20%)]  \tLoss:   88.911636\trec:   62.483093\tkl:   26.428541\n",
      "Epoch: 916 [20100/50000 (40%)]  \tLoss:   90.858536\trec:   65.037849\tkl:   25.820679\n",
      "Epoch: 916 [30100/50000 (60%)]  \tLoss:   92.983856\trec:   64.957207\tkl:   28.026648\n",
      "Epoch: 916 [40100/50000 (80%)]  \tLoss:   92.596329\trec:   65.771622\tkl:   26.824701\n",
      "====> Epoch: 916 Average train loss: 90.2905\n",
      "====> Validation set loss: 94.3146\n",
      "====> Validation set kl: 26.1586\n",
      "Epoch: 917 [  100/50000 ( 0%)]  \tLoss:   87.862762\trec:   63.065243\tkl:   24.797516\n",
      "Epoch: 917 [10100/50000 (20%)]  \tLoss:   91.488045\trec:   64.856133\tkl:   26.631914\n",
      "Epoch: 917 [20100/50000 (40%)]  \tLoss:   89.100693\trec:   62.457134\tkl:   26.643559\n",
      "Epoch: 917 [30100/50000 (60%)]  \tLoss:   90.450226\trec:   63.843464\tkl:   26.606760\n",
      "Epoch: 917 [40100/50000 (80%)]  \tLoss:   88.608055\trec:   62.752548\tkl:   25.855505\n",
      "====> Epoch: 917 Average train loss: 90.3175\n",
      "====> Validation set loss: 94.4534\n",
      "====> Validation set kl: 26.4131\n",
      "Epoch: 918 [  100/50000 ( 0%)]  \tLoss:   88.514168\trec:   62.395954\tkl:   26.118210\n",
      "Epoch: 918 [10100/50000 (20%)]  \tLoss:   91.365410\trec:   64.775246\tkl:   26.590168\n",
      "Epoch: 918 [20100/50000 (40%)]  \tLoss:   87.551666\trec:   61.076317\tkl:   26.475353\n",
      "Epoch: 918 [30100/50000 (60%)]  \tLoss:   89.166679\trec:   62.886982\tkl:   26.279697\n",
      "Epoch: 918 [40100/50000 (80%)]  \tLoss:   92.120537\trec:   64.771782\tkl:   27.348747\n",
      "====> Epoch: 918 Average train loss: 90.2797\n",
      "====> Validation set loss: 94.4704\n",
      "====> Validation set kl: 26.4665\n",
      "Epoch: 919 [  100/50000 ( 0%)]  \tLoss:   90.678711\trec:   64.436638\tkl:   26.242067\n",
      "Epoch: 919 [10100/50000 (20%)]  \tLoss:   89.718445\trec:   63.728344\tkl:   25.990105\n",
      "Epoch: 919 [20100/50000 (40%)]  \tLoss:   89.257378\trec:   62.929626\tkl:   26.327759\n",
      "Epoch: 919 [30100/50000 (60%)]  \tLoss:   91.009644\trec:   64.892570\tkl:   26.117077\n",
      "Epoch: 919 [40100/50000 (80%)]  \tLoss:   92.688911\trec:   66.068085\tkl:   26.620827\n",
      "====> Epoch: 919 Average train loss: 90.2906\n",
      "====> Validation set loss: 94.4254\n",
      "====> Validation set kl: 26.4887\n",
      "Epoch: 920 [  100/50000 ( 0%)]  \tLoss:   91.409492\trec:   64.349640\tkl:   27.059858\n",
      "Epoch: 920 [10100/50000 (20%)]  \tLoss:   88.954132\trec:   63.085945\tkl:   25.868183\n",
      "Epoch: 920 [20100/50000 (40%)]  \tLoss:   91.883827\trec:   65.682556\tkl:   26.201269\n",
      "Epoch: 920 [30100/50000 (60%)]  \tLoss:   90.696541\trec:   64.250534\tkl:   26.446007\n",
      "Epoch: 920 [40100/50000 (80%)]  \tLoss:   94.326218\trec:   67.467491\tkl:   26.858725\n",
      "====> Epoch: 920 Average train loss: 90.2847\n",
      "====> Validation set loss: 94.4159\n",
      "====> Validation set kl: 26.4628\n",
      "Epoch: 921 [  100/50000 ( 0%)]  \tLoss:   87.761856\trec:   62.185585\tkl:   25.576269\n",
      "Epoch: 921 [10100/50000 (20%)]  \tLoss:   94.314812\trec:   67.337723\tkl:   26.977091\n",
      "Epoch: 921 [20100/50000 (40%)]  \tLoss:   95.618095\trec:   69.323616\tkl:   26.294479\n",
      "Epoch: 921 [30100/50000 (60%)]  \tLoss:   89.142235\trec:   62.958664\tkl:   26.183569\n",
      "Epoch: 921 [40100/50000 (80%)]  \tLoss:   92.366577\trec:   65.226265\tkl:   27.140312\n",
      "====> Epoch: 921 Average train loss: 90.2960\n",
      "====> Validation set loss: 94.4228\n",
      "====> Validation set kl: 26.4505\n",
      "Epoch: 922 [  100/50000 ( 0%)]  \tLoss:   89.982582\trec:   63.006062\tkl:   26.976522\n",
      "Epoch: 922 [10100/50000 (20%)]  \tLoss:   91.784996\trec:   65.126045\tkl:   26.658958\n",
      "Epoch: 922 [20100/50000 (40%)]  \tLoss:   91.521851\trec:   64.517471\tkl:   27.004381\n",
      "Epoch: 922 [30100/50000 (60%)]  \tLoss:   87.417175\trec:   61.478928\tkl:   25.938244\n",
      "Epoch: 922 [40100/50000 (80%)]  \tLoss:   87.588730\trec:   61.453388\tkl:   26.135336\n",
      "====> Epoch: 922 Average train loss: 90.2720\n",
      "====> Validation set loss: 94.3919\n",
      "====> Validation set kl: 26.2742\n",
      "Epoch: 923 [  100/50000 ( 0%)]  \tLoss:   90.981003\trec:   64.833611\tkl:   26.147400\n",
      "Epoch: 923 [10100/50000 (20%)]  \tLoss:   91.507050\trec:   65.071968\tkl:   26.435080\n",
      "Epoch: 923 [20100/50000 (40%)]  \tLoss:   91.216240\trec:   64.449455\tkl:   26.766788\n",
      "Epoch: 923 [30100/50000 (60%)]  \tLoss:   94.851357\trec:   67.657051\tkl:   27.194302\n",
      "Epoch: 923 [40100/50000 (80%)]  \tLoss:   92.483292\trec:   66.199562\tkl:   26.283728\n",
      "====> Epoch: 923 Average train loss: 90.2795\n",
      "====> Validation set loss: 94.3837\n",
      "====> Validation set kl: 26.3735\n",
      "Epoch: 924 [  100/50000 ( 0%)]  \tLoss:   88.318619\trec:   61.457611\tkl:   26.861008\n",
      "Epoch: 924 [10100/50000 (20%)]  \tLoss:   89.382446\trec:   62.752026\tkl:   26.630424\n",
      "Epoch: 924 [20100/50000 (40%)]  \tLoss:   93.022758\trec:   66.428513\tkl:   26.594250\n",
      "Epoch: 924 [30100/50000 (60%)]  \tLoss:   88.851990\trec:   62.806660\tkl:   26.045336\n",
      "Epoch: 924 [40100/50000 (80%)]  \tLoss:   89.647926\trec:   62.907009\tkl:   26.740913\n",
      "====> Epoch: 924 Average train loss: 90.2982\n",
      "====> Validation set loss: 94.4425\n",
      "====> Validation set kl: 26.4826\n",
      "Epoch: 925 [  100/50000 ( 0%)]  \tLoss:   91.449493\trec:   63.967232\tkl:   27.482260\n",
      "Epoch: 925 [10100/50000 (20%)]  \tLoss:   88.343719\trec:   62.470360\tkl:   25.873356\n",
      "Epoch: 925 [20100/50000 (40%)]  \tLoss:   89.759399\trec:   63.002762\tkl:   26.756639\n",
      "Epoch: 925 [30100/50000 (60%)]  \tLoss:   95.468849\trec:   68.866295\tkl:   26.602551\n",
      "Epoch: 925 [40100/50000 (80%)]  \tLoss:   93.696632\trec:   66.695580\tkl:   27.001047\n",
      "====> Epoch: 925 Average train loss: 90.2581\n",
      "====> Validation set loss: 94.4191\n",
      "====> Validation set kl: 26.3942\n",
      "Epoch: 926 [  100/50000 ( 0%)]  \tLoss:   91.945312\trec:   65.653587\tkl:   26.291727\n",
      "Epoch: 926 [10100/50000 (20%)]  \tLoss:   91.398415\trec:   64.746330\tkl:   26.652079\n",
      "Epoch: 926 [20100/50000 (40%)]  \tLoss:   89.009773\trec:   62.512650\tkl:   26.497126\n",
      "Epoch: 926 [30100/50000 (60%)]  \tLoss:   90.353256\trec:   64.242569\tkl:   26.110687\n",
      "Epoch: 926 [40100/50000 (80%)]  \tLoss:   87.017380\trec:   61.727032\tkl:   25.290348\n",
      "====> Epoch: 926 Average train loss: 90.2681\n",
      "====> Validation set loss: 94.4039\n",
      "====> Validation set kl: 26.3985\n",
      "Epoch: 927 [  100/50000 ( 0%)]  \tLoss:   88.561577\trec:   62.167973\tkl:   26.393602\n",
      "Epoch: 927 [10100/50000 (20%)]  \tLoss:   93.706215\trec:   66.840927\tkl:   26.865290\n",
      "Epoch: 927 [20100/50000 (40%)]  \tLoss:   90.939766\trec:   64.406197\tkl:   26.533566\n",
      "Epoch: 927 [30100/50000 (60%)]  \tLoss:   90.410995\trec:   64.201096\tkl:   26.209894\n",
      "Epoch: 927 [40100/50000 (80%)]  \tLoss:   91.049370\trec:   64.260216\tkl:   26.789162\n",
      "====> Epoch: 927 Average train loss: 90.2944\n",
      "====> Validation set loss: 94.3665\n",
      "====> Validation set kl: 26.3905\n",
      "Epoch: 928 [  100/50000 ( 0%)]  \tLoss:   90.644882\trec:   64.417343\tkl:   26.227539\n",
      "Epoch: 928 [10100/50000 (20%)]  \tLoss:   92.112381\trec:   65.269150\tkl:   26.843231\n",
      "Epoch: 928 [20100/50000 (40%)]  \tLoss:   87.877274\trec:   62.536987\tkl:   25.340290\n",
      "Epoch: 928 [30100/50000 (60%)]  \tLoss:   92.302711\trec:   65.680855\tkl:   26.621853\n",
      "Epoch: 928 [40100/50000 (80%)]  \tLoss:   91.955727\trec:   64.627472\tkl:   27.328260\n",
      "====> Epoch: 928 Average train loss: 90.2747\n",
      "====> Validation set loss: 94.4545\n",
      "====> Validation set kl: 26.3083\n",
      "Epoch: 929 [  100/50000 ( 0%)]  \tLoss:   87.123482\trec:   61.183727\tkl:   25.939758\n",
      "Epoch: 929 [10100/50000 (20%)]  \tLoss:   89.576385\trec:   63.123692\tkl:   26.452692\n",
      "Epoch: 929 [20100/50000 (40%)]  \tLoss:   90.359528\trec:   64.336334\tkl:   26.023203\n",
      "Epoch: 929 [30100/50000 (60%)]  \tLoss:   89.304604\trec:   63.080391\tkl:   26.224220\n",
      "Epoch: 929 [40100/50000 (80%)]  \tLoss:   86.525703\trec:   60.212582\tkl:   26.313120\n",
      "====> Epoch: 929 Average train loss: 90.2672\n",
      "====> Validation set loss: 94.4106\n",
      "====> Validation set kl: 26.2861\n",
      "Epoch: 930 [  100/50000 ( 0%)]  \tLoss:   88.330528\trec:   62.202591\tkl:   26.127939\n",
      "Epoch: 930 [10100/50000 (20%)]  \tLoss:   93.691879\trec:   66.793114\tkl:   26.898769\n",
      "Epoch: 930 [20100/50000 (40%)]  \tLoss:   90.692421\trec:   63.856941\tkl:   26.835476\n",
      "Epoch: 930 [30100/50000 (60%)]  \tLoss:   93.278809\trec:   65.994507\tkl:   27.284302\n",
      "Epoch: 930 [40100/50000 (80%)]  \tLoss:   94.438591\trec:   67.062050\tkl:   27.376539\n",
      "====> Epoch: 930 Average train loss: 90.2708\n",
      "====> Validation set loss: 94.3621\n",
      "====> Validation set kl: 26.5150\n",
      "Epoch: 931 [  100/50000 ( 0%)]  \tLoss:   89.116005\trec:   62.434021\tkl:   26.681984\n",
      "Epoch: 931 [10100/50000 (20%)]  \tLoss:   88.595192\trec:   62.816406\tkl:   25.778784\n",
      "Epoch: 931 [20100/50000 (40%)]  \tLoss:   92.640045\trec:   66.004654\tkl:   26.635393\n",
      "Epoch: 931 [30100/50000 (60%)]  \tLoss:   89.237282\trec:   62.613018\tkl:   26.624271\n",
      "Epoch: 931 [40100/50000 (80%)]  \tLoss:   89.026558\trec:   62.437614\tkl:   26.588947\n",
      "====> Epoch: 931 Average train loss: 90.2578\n",
      "====> Validation set loss: 94.5070\n",
      "====> Validation set kl: 26.5050\n",
      "Epoch: 932 [  100/50000 ( 0%)]  \tLoss:   90.079201\trec:   63.300854\tkl:   26.778339\n",
      "Epoch: 932 [10100/50000 (20%)]  \tLoss:   91.373993\trec:   64.785713\tkl:   26.588284\n",
      "Epoch: 932 [20100/50000 (40%)]  \tLoss:   90.409767\trec:   64.514900\tkl:   25.894865\n",
      "Epoch: 932 [30100/50000 (60%)]  \tLoss:   91.585915\trec:   64.619133\tkl:   26.966784\n",
      "Epoch: 932 [40100/50000 (80%)]  \tLoss:   89.957184\trec:   63.761982\tkl:   26.195204\n",
      "====> Epoch: 932 Average train loss: 90.2688\n",
      "====> Validation set loss: 94.4467\n",
      "====> Validation set kl: 26.3335\n",
      "Epoch: 933 [  100/50000 ( 0%)]  \tLoss:   87.862534\trec:   61.975506\tkl:   25.887028\n",
      "Epoch: 933 [10100/50000 (20%)]  \tLoss:   91.789040\trec:   64.766701\tkl:   27.022348\n",
      "Epoch: 933 [20100/50000 (40%)]  \tLoss:   86.814484\trec:   60.935417\tkl:   25.879065\n",
      "Epoch: 933 [30100/50000 (60%)]  \tLoss:   89.598595\trec:   64.065025\tkl:   25.533562\n",
      "Epoch: 933 [40100/50000 (80%)]  \tLoss:   87.438354\trec:   61.896355\tkl:   25.542002\n",
      "====> Epoch: 933 Average train loss: 90.2565\n",
      "====> Validation set loss: 94.5051\n",
      "====> Validation set kl: 26.6075\n",
      "Epoch: 934 [  100/50000 ( 0%)]  \tLoss:   87.411339\trec:   61.403454\tkl:   26.007883\n",
      "Epoch: 934 [10100/50000 (20%)]  \tLoss:   88.060860\trec:   61.836258\tkl:   26.224602\n",
      "Epoch: 934 [20100/50000 (40%)]  \tLoss:   91.124664\trec:   64.432579\tkl:   26.692085\n",
      "Epoch: 934 [30100/50000 (60%)]  \tLoss:   91.660645\trec:   64.434616\tkl:   27.226023\n",
      "Epoch: 934 [40100/50000 (80%)]  \tLoss:   88.017036\trec:   62.054325\tkl:   25.962715\n",
      "====> Epoch: 934 Average train loss: 90.2831\n",
      "====> Validation set loss: 94.4826\n",
      "====> Validation set kl: 26.3816\n",
      "Epoch: 935 [  100/50000 ( 0%)]  \tLoss:   91.372925\trec:   64.929718\tkl:   26.443213\n",
      "Epoch: 935 [10100/50000 (20%)]  \tLoss:   92.106384\trec:   65.264801\tkl:   26.841581\n",
      "Epoch: 935 [20100/50000 (40%)]  \tLoss:   93.106941\trec:   66.372299\tkl:   26.734646\n",
      "Epoch: 935 [30100/50000 (60%)]  \tLoss:   90.623322\trec:   64.668816\tkl:   25.954512\n",
      "Epoch: 935 [40100/50000 (80%)]  \tLoss:   90.459160\trec:   64.395454\tkl:   26.063705\n",
      "====> Epoch: 935 Average train loss: 90.2641\n",
      "====> Validation set loss: 94.4218\n",
      "====> Validation set kl: 26.5008\n",
      "Epoch: 936 [  100/50000 ( 0%)]  \tLoss:   96.368698\trec:   68.808006\tkl:   27.560690\n",
      "Epoch: 936 [10100/50000 (20%)]  \tLoss:   90.939743\trec:   64.821213\tkl:   26.118534\n",
      "Epoch: 936 [20100/50000 (40%)]  \tLoss:   91.036552\trec:   64.745888\tkl:   26.290663\n",
      "Epoch: 936 [30100/50000 (60%)]  \tLoss:   92.159279\trec:   66.412354\tkl:   25.746916\n",
      "Epoch: 936 [40100/50000 (80%)]  \tLoss:   88.585739\trec:   62.870888\tkl:   25.714848\n",
      "====> Epoch: 936 Average train loss: 90.2781\n",
      "====> Validation set loss: 94.4214\n",
      "====> Validation set kl: 26.4473\n",
      "Epoch: 937 [  100/50000 ( 0%)]  \tLoss:   87.322212\trec:   61.251083\tkl:   26.071133\n",
      "Epoch: 937 [10100/50000 (20%)]  \tLoss:   88.862389\trec:   62.325741\tkl:   26.536650\n",
      "Epoch: 937 [20100/50000 (40%)]  \tLoss:   91.308914\trec:   64.636536\tkl:   26.672380\n",
      "Epoch: 937 [30100/50000 (60%)]  \tLoss:   88.446953\trec:   62.096539\tkl:   26.350420\n",
      "Epoch: 937 [40100/50000 (80%)]  \tLoss:   87.092735\trec:   60.607792\tkl:   26.484941\n",
      "====> Epoch: 937 Average train loss: 90.2750\n",
      "====> Validation set loss: 94.4125\n",
      "====> Validation set kl: 26.4982\n",
      "Epoch: 938 [  100/50000 ( 0%)]  \tLoss:   85.506447\trec:   59.415478\tkl:   26.090965\n",
      "Epoch: 938 [10100/50000 (20%)]  \tLoss:   86.899292\trec:   61.738319\tkl:   25.160980\n",
      "Epoch: 938 [20100/50000 (40%)]  \tLoss:   91.658875\trec:   64.280838\tkl:   27.378035\n",
      "Epoch: 938 [30100/50000 (60%)]  \tLoss:   91.248886\trec:   64.183609\tkl:   27.065269\n",
      "Epoch: 938 [40100/50000 (80%)]  \tLoss:   89.230591\trec:   62.720303\tkl:   26.510290\n",
      "====> Epoch: 938 Average train loss: 90.2429\n",
      "====> Validation set loss: 94.4086\n",
      "====> Validation set kl: 26.4553\n",
      "Epoch: 939 [  100/50000 ( 0%)]  \tLoss:   89.914490\trec:   63.697441\tkl:   26.217045\n",
      "Epoch: 939 [10100/50000 (20%)]  \tLoss:   92.515976\trec:   66.038025\tkl:   26.477953\n",
      "Epoch: 939 [20100/50000 (40%)]  \tLoss:   84.158058\trec:   58.371128\tkl:   25.786928\n",
      "Epoch: 939 [30100/50000 (60%)]  \tLoss:   91.223618\trec:   64.696892\tkl:   26.526730\n",
      "Epoch: 939 [40100/50000 (80%)]  \tLoss:   88.714493\trec:   61.866875\tkl:   26.847616\n",
      "====> Epoch: 939 Average train loss: 90.2540\n",
      "====> Validation set loss: 94.4393\n",
      "====> Validation set kl: 26.5665\n",
      "Epoch: 940 [  100/50000 ( 0%)]  \tLoss:   89.338165\trec:   62.399490\tkl:   26.938667\n",
      "Epoch: 940 [10100/50000 (20%)]  \tLoss:   89.835846\trec:   63.370647\tkl:   26.465197\n",
      "Epoch: 940 [20100/50000 (40%)]  \tLoss:   95.911171\trec:   69.031929\tkl:   26.879242\n",
      "Epoch: 940 [30100/50000 (60%)]  \tLoss:   87.902328\trec:   61.674320\tkl:   26.228012\n",
      "Epoch: 940 [40100/50000 (80%)]  \tLoss:   87.633377\trec:   61.568760\tkl:   26.064619\n",
      "====> Epoch: 940 Average train loss: 90.2596\n",
      "====> Validation set loss: 94.4574\n",
      "====> Validation set kl: 26.5058\n",
      "Epoch: 941 [  100/50000 ( 0%)]  \tLoss:   90.843552\trec:   64.071266\tkl:   26.772282\n",
      "Epoch: 941 [10100/50000 (20%)]  \tLoss:   90.754890\trec:   64.681076\tkl:   26.073812\n",
      "Epoch: 941 [20100/50000 (40%)]  \tLoss:   91.471756\trec:   64.894844\tkl:   26.576918\n",
      "Epoch: 941 [30100/50000 (60%)]  \tLoss:   90.476357\trec:   63.945076\tkl:   26.531281\n",
      "Epoch: 941 [40100/50000 (80%)]  \tLoss:   92.738564\trec:   66.272026\tkl:   26.466539\n",
      "====> Epoch: 941 Average train loss: 90.2300\n",
      "====> Validation set loss: 94.3805\n",
      "====> Validation set kl: 26.4297\n",
      "Epoch: 942 [  100/50000 ( 0%)]  \tLoss:   90.170181\trec:   63.881550\tkl:   26.288635\n",
      "Epoch: 942 [10100/50000 (20%)]  \tLoss:   89.387184\trec:   63.506916\tkl:   25.880262\n",
      "Epoch: 942 [20100/50000 (40%)]  \tLoss:   90.979721\trec:   63.471966\tkl:   27.507761\n",
      "Epoch: 942 [30100/50000 (60%)]  \tLoss:   88.369217\trec:   61.734447\tkl:   26.634773\n",
      "Epoch: 942 [40100/50000 (80%)]  \tLoss:   91.331978\trec:   64.820358\tkl:   26.511620\n",
      "====> Epoch: 942 Average train loss: 90.2406\n",
      "====> Validation set loss: 94.3883\n",
      "====> Validation set kl: 26.4150\n",
      "Epoch: 943 [  100/50000 ( 0%)]  \tLoss:   84.400650\trec:   58.770027\tkl:   25.630625\n",
      "Epoch: 943 [10100/50000 (20%)]  \tLoss:   85.284966\trec:   58.962162\tkl:   26.322805\n",
      "Epoch: 943 [20100/50000 (40%)]  \tLoss:   86.287575\trec:   59.969677\tkl:   26.317902\n",
      "Epoch: 943 [30100/50000 (60%)]  \tLoss:   88.577248\trec:   63.100849\tkl:   25.476400\n",
      "Epoch: 943 [40100/50000 (80%)]  \tLoss:   90.292549\trec:   64.031082\tkl:   26.261467\n",
      "====> Epoch: 943 Average train loss: 90.2342\n",
      "====> Validation set loss: 94.5418\n",
      "====> Validation set kl: 26.4207\n",
      "Epoch: 944 [  100/50000 ( 0%)]  \tLoss:   88.663818\trec:   63.377548\tkl:   25.286268\n",
      "Epoch: 944 [10100/50000 (20%)]  \tLoss:   89.402290\trec:   63.311325\tkl:   26.090963\n",
      "Epoch: 944 [20100/50000 (40%)]  \tLoss:   87.973282\trec:   62.566833\tkl:   25.406450\n",
      "Epoch: 944 [30100/50000 (60%)]  \tLoss:   91.655182\trec:   64.696777\tkl:   26.958405\n",
      "Epoch: 944 [40100/50000 (80%)]  \tLoss:   88.771431\trec:   61.936054\tkl:   26.835381\n",
      "====> Epoch: 944 Average train loss: 90.2228\n",
      "====> Validation set loss: 94.3601\n",
      "====> Validation set kl: 26.5489\n",
      "Epoch: 945 [  100/50000 ( 0%)]  \tLoss:   91.323219\trec:   65.548782\tkl:   25.774443\n",
      "Epoch: 945 [10100/50000 (20%)]  \tLoss:   89.927711\trec:   63.846352\tkl:   26.081360\n",
      "Epoch: 945 [20100/50000 (40%)]  \tLoss:   92.210587\trec:   65.366089\tkl:   26.844486\n",
      "Epoch: 945 [30100/50000 (60%)]  \tLoss:   87.619080\trec:   61.226849\tkl:   26.392233\n",
      "Epoch: 945 [40100/50000 (80%)]  \tLoss:   91.295250\trec:   64.509392\tkl:   26.785854\n",
      "====> Epoch: 945 Average train loss: 90.2477\n",
      "====> Validation set loss: 94.3791\n",
      "====> Validation set kl: 26.1942\n",
      "Epoch: 946 [  100/50000 ( 0%)]  \tLoss:   87.834038\trec:   61.989040\tkl:   25.845001\n",
      "Epoch: 946 [10100/50000 (20%)]  \tLoss:   92.301041\trec:   65.237213\tkl:   27.063828\n",
      "Epoch: 946 [20100/50000 (40%)]  \tLoss:   89.664650\trec:   63.502731\tkl:   26.161917\n",
      "Epoch: 946 [30100/50000 (60%)]  \tLoss:   87.501915\trec:   62.158825\tkl:   25.343082\n",
      "Epoch: 946 [40100/50000 (80%)]  \tLoss:   86.787392\trec:   61.209476\tkl:   25.577915\n",
      "====> Epoch: 946 Average train loss: 90.2363\n",
      "====> Validation set loss: 94.4232\n",
      "====> Validation set kl: 26.4927\n",
      "Epoch: 947 [  100/50000 ( 0%)]  \tLoss:   87.909119\trec:   61.381084\tkl:   26.528038\n",
      "Epoch: 947 [10100/50000 (20%)]  \tLoss:   89.716850\trec:   63.391151\tkl:   26.325701\n",
      "Epoch: 947 [20100/50000 (40%)]  \tLoss:   92.775963\trec:   65.866020\tkl:   26.909943\n",
      "Epoch: 947 [30100/50000 (60%)]  \tLoss:   86.070175\trec:   60.697441\tkl:   25.372734\n",
      "Epoch: 947 [40100/50000 (80%)]  \tLoss:   86.095390\trec:   60.190586\tkl:   25.904806\n",
      "====> Epoch: 947 Average train loss: 90.2414\n",
      "====> Validation set loss: 94.4199\n",
      "====> Validation set kl: 26.4407\n",
      "Epoch: 948 [  100/50000 ( 0%)]  \tLoss:   89.877579\trec:   63.483017\tkl:   26.394560\n",
      "Epoch: 948 [10100/50000 (20%)]  \tLoss:   86.020332\trec:   60.703938\tkl:   25.316391\n",
      "Epoch: 948 [20100/50000 (40%)]  \tLoss:   90.830925\trec:   64.900932\tkl:   25.929989\n",
      "Epoch: 948 [30100/50000 (60%)]  \tLoss:   91.702736\trec:   64.595230\tkl:   27.107504\n",
      "Epoch: 948 [40100/50000 (80%)]  \tLoss:   91.966537\trec:   65.295883\tkl:   26.670662\n",
      "====> Epoch: 948 Average train loss: 90.2408\n",
      "====> Validation set loss: 94.3217\n",
      "====> Validation set kl: 26.3115\n",
      "Epoch: 949 [  100/50000 ( 0%)]  \tLoss:   88.819374\trec:   62.320400\tkl:   26.498980\n",
      "Epoch: 949 [10100/50000 (20%)]  \tLoss:   90.439499\trec:   63.565281\tkl:   26.874220\n",
      "Epoch: 949 [20100/50000 (40%)]  \tLoss:   87.032196\trec:   61.355320\tkl:   25.676876\n",
      "Epoch: 949 [30100/50000 (60%)]  \tLoss:   90.569069\trec:   63.972031\tkl:   26.597038\n",
      "Epoch: 949 [40100/50000 (80%)]  \tLoss:   92.665054\trec:   65.810356\tkl:   26.854702\n",
      "====> Epoch: 949 Average train loss: 90.2278\n",
      "====> Validation set loss: 94.4626\n",
      "====> Validation set kl: 26.4134\n",
      "Epoch: 950 [  100/50000 ( 0%)]  \tLoss:   86.092499\trec:   61.477547\tkl:   24.614946\n",
      "Epoch: 950 [10100/50000 (20%)]  \tLoss:   91.260635\trec:   65.000359\tkl:   26.260281\n",
      "Epoch: 950 [20100/50000 (40%)]  \tLoss:   91.740585\trec:   65.170738\tkl:   26.569838\n",
      "Epoch: 950 [30100/50000 (60%)]  \tLoss:   90.139999\trec:   62.969860\tkl:   27.170135\n",
      "Epoch: 950 [40100/50000 (80%)]  \tLoss:   91.577080\trec:   64.838898\tkl:   26.738176\n",
      "====> Epoch: 950 Average train loss: 90.2324\n",
      "====> Validation set loss: 94.4316\n",
      "====> Validation set kl: 26.6571\n",
      "Epoch: 951 [  100/50000 ( 0%)]  \tLoss:   85.491219\trec:   59.021893\tkl:   26.469326\n",
      "Epoch: 951 [10100/50000 (20%)]  \tLoss:   92.068855\trec:   64.343925\tkl:   27.724932\n",
      "Epoch: 951 [20100/50000 (40%)]  \tLoss:   91.965172\trec:   65.870834\tkl:   26.094343\n",
      "Epoch: 951 [30100/50000 (60%)]  \tLoss:   88.446266\trec:   62.526184\tkl:   25.920088\n",
      "Epoch: 951 [40100/50000 (80%)]  \tLoss:   87.979156\trec:   62.826618\tkl:   25.152536\n",
      "====> Epoch: 951 Average train loss: 90.2135\n",
      "====> Validation set loss: 94.4216\n",
      "====> Validation set kl: 26.4346\n",
      "Epoch: 952 [  100/50000 ( 0%)]  \tLoss:   89.115334\trec:   62.310936\tkl:   26.804396\n",
      "Epoch: 952 [10100/50000 (20%)]  \tLoss:   88.240463\trec:   61.749599\tkl:   26.490864\n",
      "Epoch: 952 [20100/50000 (40%)]  \tLoss:   88.863998\trec:   62.599960\tkl:   26.264038\n",
      "Epoch: 952 [30100/50000 (60%)]  \tLoss:   90.168091\trec:   63.904949\tkl:   26.263144\n",
      "Epoch: 952 [40100/50000 (80%)]  \tLoss:   89.914703\trec:   63.659870\tkl:   26.254829\n",
      "====> Epoch: 952 Average train loss: 90.2245\n",
      "====> Validation set loss: 94.3747\n",
      "====> Validation set kl: 26.3061\n",
      "Epoch: 953 [  100/50000 ( 0%)]  \tLoss:   93.796875\trec:   66.380943\tkl:   27.415924\n",
      "Epoch: 953 [10100/50000 (20%)]  \tLoss:   90.156807\trec:   62.996922\tkl:   27.159882\n",
      "Epoch: 953 [20100/50000 (40%)]  \tLoss:   89.887085\trec:   63.299938\tkl:   26.587149\n",
      "Epoch: 953 [30100/50000 (60%)]  \tLoss:   92.610489\trec:   65.754852\tkl:   26.855629\n",
      "Epoch: 953 [40100/50000 (80%)]  \tLoss:   91.021469\trec:   64.442749\tkl:   26.578726\n",
      "====> Epoch: 953 Average train loss: 90.2268\n",
      "====> Validation set loss: 94.4533\n",
      "====> Validation set kl: 26.4521\n",
      "Epoch: 954 [  100/50000 ( 0%)]  \tLoss:   85.144218\trec:   60.144878\tkl:   24.999342\n",
      "Epoch: 954 [10100/50000 (20%)]  \tLoss:   94.895508\trec:   67.500832\tkl:   27.394672\n",
      "Epoch: 954 [20100/50000 (40%)]  \tLoss:   90.690834\trec:   64.166382\tkl:   26.524452\n",
      "Epoch: 954 [30100/50000 (60%)]  \tLoss:   87.617897\trec:   61.824970\tkl:   25.792929\n",
      "Epoch: 954 [40100/50000 (80%)]  \tLoss:   92.714272\trec:   66.217361\tkl:   26.496914\n",
      "====> Epoch: 954 Average train loss: 90.2052\n",
      "====> Validation set loss: 94.4153\n",
      "====> Validation set kl: 26.5172\n",
      "Epoch: 955 [  100/50000 ( 0%)]  \tLoss:   90.624611\trec:   63.317974\tkl:   27.306641\n",
      "Epoch: 955 [10100/50000 (20%)]  \tLoss:   91.896561\trec:   65.495224\tkl:   26.401335\n",
      "Epoch: 955 [20100/50000 (40%)]  \tLoss:   90.765816\trec:   64.752686\tkl:   26.013132\n",
      "Epoch: 955 [30100/50000 (60%)]  \tLoss:   90.241989\trec:   64.332069\tkl:   25.909925\n",
      "Epoch: 955 [40100/50000 (80%)]  \tLoss:   88.378716\trec:   62.487190\tkl:   25.891525\n",
      "====> Epoch: 955 Average train loss: 90.2156\n",
      "====> Validation set loss: 94.3999\n",
      "====> Validation set kl: 26.4775\n",
      "Epoch: 956 [  100/50000 ( 0%)]  \tLoss:   88.240036\trec:   61.815769\tkl:   26.424269\n",
      "Epoch: 956 [10100/50000 (20%)]  \tLoss:   92.962379\trec:   66.467041\tkl:   26.495346\n",
      "Epoch: 956 [20100/50000 (40%)]  \tLoss:   92.262794\trec:   65.317383\tkl:   26.945404\n",
      "Epoch: 956 [30100/50000 (60%)]  \tLoss:   87.108139\trec:   61.241577\tkl:   25.866570\n",
      "Epoch: 956 [40100/50000 (80%)]  \tLoss:   93.702408\trec:   66.783432\tkl:   26.918982\n",
      "====> Epoch: 956 Average train loss: 90.2291\n",
      "====> Validation set loss: 94.3441\n",
      "====> Validation set kl: 26.3143\n",
      "Epoch: 957 [  100/50000 ( 0%)]  \tLoss:   92.553024\trec:   65.805733\tkl:   26.747284\n",
      "Epoch: 957 [10100/50000 (20%)]  \tLoss:   89.699783\trec:   63.202217\tkl:   26.497566\n",
      "Epoch: 957 [20100/50000 (40%)]  \tLoss:   87.015099\trec:   60.778790\tkl:   26.236303\n",
      "Epoch: 957 [30100/50000 (60%)]  \tLoss:   87.954063\trec:   61.912136\tkl:   26.041918\n",
      "Epoch: 957 [40100/50000 (80%)]  \tLoss:   90.981941\trec:   64.350494\tkl:   26.631447\n",
      "====> Epoch: 957 Average train loss: 90.2104\n",
      "====> Validation set loss: 94.4134\n",
      "====> Validation set kl: 26.5516\n",
      "Epoch: 958 [  100/50000 ( 0%)]  \tLoss:   95.400620\trec:   68.021019\tkl:   27.379606\n",
      "Epoch: 958 [10100/50000 (20%)]  \tLoss:   86.831444\trec:   60.825039\tkl:   26.006401\n",
      "Epoch: 958 [20100/50000 (40%)]  \tLoss:   89.521416\trec:   63.052498\tkl:   26.468912\n",
      "Epoch: 958 [30100/50000 (60%)]  \tLoss:   90.056099\trec:   63.583084\tkl:   26.473017\n",
      "Epoch: 958 [40100/50000 (80%)]  \tLoss:   90.942986\trec:   63.732880\tkl:   27.210110\n",
      "====> Epoch: 958 Average train loss: 90.2167\n",
      "====> Validation set loss: 94.4454\n",
      "====> Validation set kl: 26.5655\n",
      "Epoch: 959 [  100/50000 ( 0%)]  \tLoss:   90.598534\trec:   63.606434\tkl:   26.992100\n",
      "Epoch: 959 [10100/50000 (20%)]  \tLoss:   88.280464\trec:   62.094978\tkl:   26.185486\n",
      "Epoch: 959 [20100/50000 (40%)]  \tLoss:   88.567772\trec:   62.410858\tkl:   26.156916\n",
      "Epoch: 959 [30100/50000 (60%)]  \tLoss:   87.470490\trec:   61.785404\tkl:   25.685087\n",
      "Epoch: 959 [40100/50000 (80%)]  \tLoss:   87.061348\trec:   61.036457\tkl:   26.024893\n",
      "====> Epoch: 959 Average train loss: 90.2124\n",
      "====> Validation set loss: 94.4353\n",
      "====> Validation set kl: 26.3212\n",
      "Epoch: 960 [  100/50000 ( 0%)]  \tLoss:   88.678253\trec:   62.578491\tkl:   26.099760\n",
      "Epoch: 960 [10100/50000 (20%)]  \tLoss:   85.471756\trec:   60.000454\tkl:   25.471298\n",
      "Epoch: 960 [20100/50000 (40%)]  \tLoss:   90.604263\trec:   63.431541\tkl:   27.172726\n",
      "Epoch: 960 [30100/50000 (60%)]  \tLoss:   91.184883\trec:   64.063278\tkl:   27.121605\n",
      "Epoch: 960 [40100/50000 (80%)]  \tLoss:   91.165756\trec:   64.608162\tkl:   26.557600\n",
      "====> Epoch: 960 Average train loss: 90.2071\n",
      "====> Validation set loss: 94.3608\n",
      "====> Validation set kl: 26.3948\n",
      "Epoch: 961 [  100/50000 ( 0%)]  \tLoss:   86.733711\trec:   62.269157\tkl:   24.464550\n",
      "Epoch: 961 [10100/50000 (20%)]  \tLoss:   89.689087\trec:   63.869442\tkl:   25.819649\n",
      "Epoch: 961 [20100/50000 (40%)]  \tLoss:   88.497421\trec:   63.034565\tkl:   25.462858\n",
      "Epoch: 961 [30100/50000 (60%)]  \tLoss:   91.256264\trec:   64.938622\tkl:   26.317646\n",
      "Epoch: 961 [40100/50000 (80%)]  \tLoss:   89.896736\trec:   64.139153\tkl:   25.757587\n",
      "====> Epoch: 961 Average train loss: 90.1966\n",
      "====> Validation set loss: 94.4335\n",
      "====> Validation set kl: 26.4470\n",
      "Epoch: 962 [  100/50000 ( 0%)]  \tLoss:   89.985809\trec:   63.771976\tkl:   26.213835\n",
      "Epoch: 962 [10100/50000 (20%)]  \tLoss:   91.807495\trec:   65.635918\tkl:   26.171574\n",
      "Epoch: 962 [20100/50000 (40%)]  \tLoss:   87.260818\trec:   60.749531\tkl:   26.511288\n",
      "Epoch: 962 [30100/50000 (60%)]  \tLoss:   88.172714\trec:   61.495609\tkl:   26.677099\n",
      "Epoch: 962 [40100/50000 (80%)]  \tLoss:   87.042374\trec:   60.917950\tkl:   26.124426\n",
      "====> Epoch: 962 Average train loss: 90.1979\n",
      "====> Validation set loss: 94.4565\n",
      "====> Validation set kl: 26.5228\n",
      "Epoch: 963 [  100/50000 ( 0%)]  \tLoss:   85.861938\trec:   59.703842\tkl:   26.158100\n",
      "Epoch: 963 [10100/50000 (20%)]  \tLoss:   89.211014\trec:   62.979519\tkl:   26.231491\n",
      "Epoch: 963 [20100/50000 (40%)]  \tLoss:   87.895065\trec:   62.839844\tkl:   25.055223\n",
      "Epoch: 963 [30100/50000 (60%)]  \tLoss:   87.451004\trec:   61.476395\tkl:   25.974606\n",
      "Epoch: 963 [40100/50000 (80%)]  \tLoss:   93.790230\trec:   66.390213\tkl:   27.400019\n",
      "====> Epoch: 963 Average train loss: 90.2080\n",
      "====> Validation set loss: 94.3911\n",
      "====> Validation set kl: 26.4033\n",
      "Epoch: 964 [  100/50000 ( 0%)]  \tLoss:   91.427261\trec:   63.928646\tkl:   27.498623\n",
      "Epoch: 964 [10100/50000 (20%)]  \tLoss:   92.567413\trec:   64.944092\tkl:   27.623312\n",
      "Epoch: 964 [20100/50000 (40%)]  \tLoss:   90.281525\trec:   63.267147\tkl:   27.014372\n",
      "Epoch: 964 [30100/50000 (60%)]  \tLoss:   89.249672\trec:   62.834808\tkl:   26.414867\n",
      "Epoch: 964 [40100/50000 (80%)]  \tLoss:   87.944374\trec:   62.174187\tkl:   25.770185\n",
      "====> Epoch: 964 Average train loss: 90.1751\n",
      "====> Validation set loss: 94.4916\n",
      "====> Validation set kl: 26.3528\n",
      "Epoch: 965 [  100/50000 ( 0%)]  \tLoss:   89.293716\trec:   62.701336\tkl:   26.592384\n",
      "Epoch: 965 [10100/50000 (20%)]  \tLoss:   88.738953\trec:   62.817146\tkl:   25.921804\n",
      "Epoch: 965 [20100/50000 (40%)]  \tLoss:   86.189552\trec:   60.693039\tkl:   25.496508\n",
      "Epoch: 965 [30100/50000 (60%)]  \tLoss:   96.036446\trec:   68.527351\tkl:   27.509096\n",
      "Epoch: 965 [40100/50000 (80%)]  \tLoss:   94.725945\trec:   68.109299\tkl:   26.616648\n",
      "====> Epoch: 965 Average train loss: 90.2066\n",
      "====> Validation set loss: 94.5072\n",
      "====> Validation set kl: 26.5320\n",
      "Epoch: 966 [  100/50000 ( 0%)]  \tLoss:   90.553619\trec:   63.278660\tkl:   27.274960\n",
      "Epoch: 966 [10100/50000 (20%)]  \tLoss:   88.634277\trec:   62.471062\tkl:   26.163208\n",
      "Epoch: 966 [20100/50000 (40%)]  \tLoss:   89.721481\trec:   62.594490\tkl:   27.126987\n",
      "Epoch: 966 [30100/50000 (60%)]  \tLoss:   90.093155\trec:   63.433796\tkl:   26.659357\n",
      "Epoch: 966 [40100/50000 (80%)]  \tLoss:   92.146660\trec:   64.855644\tkl:   27.291010\n",
      "====> Epoch: 966 Average train loss: 90.2034\n",
      "====> Validation set loss: 94.3030\n",
      "====> Validation set kl: 26.3470\n",
      "Epoch: 967 [  100/50000 ( 0%)]  \tLoss:   87.716949\trec:   61.931580\tkl:   25.785376\n",
      "Epoch: 967 [10100/50000 (20%)]  \tLoss:   92.899620\trec:   65.560341\tkl:   27.339277\n",
      "Epoch: 967 [20100/50000 (40%)]  \tLoss:   91.291275\trec:   65.012840\tkl:   26.278439\n",
      "Epoch: 967 [30100/50000 (60%)]  \tLoss:   85.825874\trec:   60.444920\tkl:   25.380957\n",
      "Epoch: 967 [40100/50000 (80%)]  \tLoss:   93.572731\trec:   66.975304\tkl:   26.597435\n",
      "====> Epoch: 967 Average train loss: 90.2103\n",
      "====> Validation set loss: 94.4543\n",
      "====> Validation set kl: 26.5408\n",
      "Epoch: 968 [  100/50000 ( 0%)]  \tLoss:   92.811249\trec:   65.189888\tkl:   27.621361\n",
      "Epoch: 968 [10100/50000 (20%)]  \tLoss:   94.386894\trec:   66.953285\tkl:   27.433613\n",
      "Epoch: 968 [20100/50000 (40%)]  \tLoss:   86.014069\trec:   59.977997\tkl:   26.036072\n",
      "Epoch: 968 [30100/50000 (60%)]  \tLoss:   91.555214\trec:   63.800095\tkl:   27.755114\n",
      "Epoch: 968 [40100/50000 (80%)]  \tLoss:   88.421646\trec:   62.340115\tkl:   26.081530\n",
      "====> Epoch: 968 Average train loss: 90.1973\n",
      "====> Validation set loss: 94.4606\n",
      "====> Validation set kl: 26.5409\n",
      "Epoch: 969 [  100/50000 ( 0%)]  \tLoss:   89.611153\trec:   63.392109\tkl:   26.219048\n",
      "Epoch: 969 [10100/50000 (20%)]  \tLoss:   89.385948\trec:   63.005966\tkl:   26.379982\n",
      "Epoch: 969 [20100/50000 (40%)]  \tLoss:   88.606812\trec:   62.113564\tkl:   26.493252\n",
      "Epoch: 969 [30100/50000 (60%)]  \tLoss:   88.725067\trec:   62.219067\tkl:   26.506002\n",
      "Epoch: 969 [40100/50000 (80%)]  \tLoss:   93.144547\trec:   66.298363\tkl:   26.846188\n",
      "====> Epoch: 969 Average train loss: 90.1890\n",
      "====> Validation set loss: 94.3922\n",
      "====> Validation set kl: 26.4180\n",
      "Epoch: 970 [  100/50000 ( 0%)]  \tLoss:   87.667442\trec:   62.188301\tkl:   25.479143\n",
      "Epoch: 970 [10100/50000 (20%)]  \tLoss:   87.175499\trec:   61.530666\tkl:   25.644827\n",
      "Epoch: 970 [20100/50000 (40%)]  \tLoss:   89.140182\trec:   63.090534\tkl:   26.049650\n",
      "Epoch: 970 [30100/50000 (60%)]  \tLoss:   92.107246\trec:   65.712135\tkl:   26.395102\n",
      "Epoch: 970 [40100/50000 (80%)]  \tLoss:   86.203377\trec:   61.315098\tkl:   24.888283\n",
      "====> Epoch: 970 Average train loss: 90.2067\n",
      "====> Validation set loss: 94.3455\n",
      "====> Validation set kl: 26.3730\n",
      "Epoch: 971 [  100/50000 ( 0%)]  \tLoss:   92.069832\trec:   64.938263\tkl:   27.131571\n",
      "Epoch: 971 [10100/50000 (20%)]  \tLoss:   90.473602\trec:   64.669693\tkl:   25.803913\n",
      "Epoch: 971 [20100/50000 (40%)]  \tLoss:   87.811363\trec:   61.294876\tkl:   26.516493\n",
      "Epoch: 971 [30100/50000 (60%)]  \tLoss:   93.456154\trec:   66.896278\tkl:   26.559872\n",
      "Epoch: 971 [40100/50000 (80%)]  \tLoss:   94.292488\trec:   67.499626\tkl:   26.792860\n",
      "====> Epoch: 971 Average train loss: 90.1738\n",
      "====> Validation set loss: 94.3332\n",
      "====> Validation set kl: 26.4187\n",
      "Epoch: 972 [  100/50000 ( 0%)]  \tLoss:   89.121742\trec:   62.945889\tkl:   26.175856\n",
      "Epoch: 972 [10100/50000 (20%)]  \tLoss:   87.088982\trec:   61.357685\tkl:   25.731300\n",
      "Epoch: 972 [20100/50000 (40%)]  \tLoss:   91.052208\trec:   64.860558\tkl:   26.191647\n",
      "Epoch: 972 [30100/50000 (60%)]  \tLoss:   91.046814\trec:   64.143806\tkl:   26.903006\n",
      "Epoch: 972 [40100/50000 (80%)]  \tLoss:   88.267830\trec:   62.031136\tkl:   26.236694\n",
      "====> Epoch: 972 Average train loss: 90.1694\n",
      "====> Validation set loss: 94.3725\n",
      "====> Validation set kl: 26.5305\n",
      "Epoch: 973 [  100/50000 ( 0%)]  \tLoss:   88.855385\trec:   62.222401\tkl:   26.632992\n",
      "Epoch: 973 [10100/50000 (20%)]  \tLoss:   90.247360\trec:   63.016148\tkl:   27.231215\n",
      "Epoch: 973 [20100/50000 (40%)]  \tLoss:   88.648384\trec:   63.180664\tkl:   25.467724\n",
      "Epoch: 973 [30100/50000 (60%)]  \tLoss:   89.649178\trec:   62.991165\tkl:   26.658016\n",
      "Epoch: 973 [40100/50000 (80%)]  \tLoss:   85.866730\trec:   59.969471\tkl:   25.897257\n",
      "====> Epoch: 973 Average train loss: 90.1819\n",
      "====> Validation set loss: 94.3285\n",
      "====> Validation set kl: 26.4343\n",
      "Epoch: 974 [  100/50000 ( 0%)]  \tLoss:   89.233452\trec:   63.414555\tkl:   25.818905\n",
      "Epoch: 974 [10100/50000 (20%)]  \tLoss:   89.715645\trec:   64.145599\tkl:   25.570047\n",
      "Epoch: 974 [20100/50000 (40%)]  \tLoss:   89.944717\trec:   63.896198\tkl:   26.048517\n",
      "Epoch: 974 [30100/50000 (60%)]  \tLoss:   85.900291\trec:   60.100716\tkl:   25.799570\n",
      "Epoch: 974 [40100/50000 (80%)]  \tLoss:   90.531227\trec:   63.578064\tkl:   26.953165\n",
      "====> Epoch: 974 Average train loss: 90.1965\n",
      "====> Validation set loss: 94.3953\n",
      "====> Validation set kl: 26.4109\n",
      "Epoch: 975 [  100/50000 ( 0%)]  \tLoss:   91.506264\trec:   64.264267\tkl:   27.241997\n",
      "Epoch: 975 [10100/50000 (20%)]  \tLoss:   87.958862\trec:   61.695847\tkl:   26.263021\n",
      "Epoch: 975 [20100/50000 (40%)]  \tLoss:   91.036278\trec:   64.256279\tkl:   26.779997\n",
      "Epoch: 975 [30100/50000 (60%)]  \tLoss:   92.685043\trec:   66.131783\tkl:   26.553261\n",
      "Epoch: 975 [40100/50000 (80%)]  \tLoss:   93.280861\trec:   67.064850\tkl:   26.216007\n",
      "====> Epoch: 975 Average train loss: 90.1654\n",
      "====> Validation set loss: 94.2686\n",
      "====> Validation set kl: 26.3159\n",
      "Epoch: 976 [  100/50000 ( 0%)]  \tLoss:   92.423569\trec:   65.427643\tkl:   26.995924\n",
      "Epoch: 976 [10100/50000 (20%)]  \tLoss:   87.492828\trec:   62.745296\tkl:   24.747528\n",
      "Epoch: 976 [20100/50000 (40%)]  \tLoss:   92.100914\trec:   64.697823\tkl:   27.403086\n",
      "Epoch: 976 [30100/50000 (60%)]  \tLoss:   96.335426\trec:   69.526939\tkl:   26.808485\n",
      "Epoch: 976 [40100/50000 (80%)]  \tLoss:   87.654274\trec:   61.735146\tkl:   25.919132\n",
      "====> Epoch: 976 Average train loss: 90.1715\n",
      "====> Validation set loss: 94.3922\n",
      "====> Validation set kl: 26.3693\n",
      "Epoch: 977 [  100/50000 ( 0%)]  \tLoss:   88.664276\trec:   63.444866\tkl:   25.219404\n",
      "Epoch: 977 [10100/50000 (20%)]  \tLoss:   90.323212\trec:   63.279198\tkl:   27.044010\n",
      "Epoch: 977 [20100/50000 (40%)]  \tLoss:   92.307671\trec:   65.618446\tkl:   26.689226\n",
      "Epoch: 977 [30100/50000 (60%)]  \tLoss:   90.598663\trec:   63.999138\tkl:   26.599518\n",
      "Epoch: 977 [40100/50000 (80%)]  \tLoss:   90.810646\trec:   64.451241\tkl:   26.359404\n",
      "====> Epoch: 977 Average train loss: 90.1803\n",
      "====> Validation set loss: 94.5172\n",
      "====> Validation set kl: 26.4942\n",
      "Epoch: 978 [  100/50000 ( 0%)]  \tLoss:   87.746666\trec:   62.349522\tkl:   25.397150\n",
      "Epoch: 978 [10100/50000 (20%)]  \tLoss:   90.185661\trec:   63.854343\tkl:   26.331312\n",
      "Epoch: 978 [20100/50000 (40%)]  \tLoss:   86.396065\trec:   60.800789\tkl:   25.595276\n",
      "Epoch: 978 [30100/50000 (60%)]  \tLoss:   92.625557\trec:   66.469948\tkl:   26.155613\n",
      "Epoch: 978 [40100/50000 (80%)]  \tLoss:   87.043877\trec:   60.661911\tkl:   26.381960\n",
      "====> Epoch: 978 Average train loss: 90.1680\n",
      "====> Validation set loss: 94.3996\n",
      "====> Validation set kl: 26.5759\n",
      "Epoch: 979 [  100/50000 ( 0%)]  \tLoss:   89.477814\trec:   62.685795\tkl:   26.792021\n",
      "Epoch: 979 [10100/50000 (20%)]  \tLoss:   91.798347\trec:   64.571945\tkl:   27.226408\n",
      "Epoch: 979 [20100/50000 (40%)]  \tLoss:   90.121193\trec:   63.450493\tkl:   26.670700\n",
      "Epoch: 979 [30100/50000 (60%)]  \tLoss:   91.776756\trec:   66.090034\tkl:   25.686714\n",
      "Epoch: 979 [40100/50000 (80%)]  \tLoss:   89.121910\trec:   62.571560\tkl:   26.550350\n",
      "====> Epoch: 979 Average train loss: 90.1569\n",
      "====> Validation set loss: 94.4741\n",
      "====> Validation set kl: 26.5676\n",
      "Epoch: 980 [  100/50000 ( 0%)]  \tLoss:   90.732422\trec:   64.475899\tkl:   26.256517\n",
      "Epoch: 980 [10100/50000 (20%)]  \tLoss:   91.208672\trec:   64.918365\tkl:   26.290306\n",
      "Epoch: 980 [20100/50000 (40%)]  \tLoss:   93.880470\trec:   66.364998\tkl:   27.515472\n",
      "Epoch: 980 [30100/50000 (60%)]  \tLoss:   86.476036\trec:   61.287270\tkl:   25.188761\n",
      "Epoch: 980 [40100/50000 (80%)]  \tLoss:   91.119804\trec:   64.907516\tkl:   26.212282\n",
      "====> Epoch: 980 Average train loss: 90.1712\n",
      "====> Validation set loss: 94.3862\n",
      "====> Validation set kl: 26.3252\n",
      "Epoch: 981 [  100/50000 ( 0%)]  \tLoss:   93.343483\trec:   66.712799\tkl:   26.630686\n",
      "Epoch: 981 [10100/50000 (20%)]  \tLoss:   86.852127\trec:   60.602158\tkl:   26.249968\n",
      "Epoch: 981 [20100/50000 (40%)]  \tLoss:   88.308563\trec:   63.156601\tkl:   25.151960\n",
      "Epoch: 981 [30100/50000 (60%)]  \tLoss:   93.054337\trec:   66.626831\tkl:   26.427494\n",
      "Epoch: 981 [40100/50000 (80%)]  \tLoss:   92.659790\trec:   66.024879\tkl:   26.634914\n",
      "====> Epoch: 981 Average train loss: 90.1549\n",
      "====> Validation set loss: 94.5097\n",
      "====> Validation set kl: 26.5186\n",
      "Epoch: 982 [  100/50000 ( 0%)]  \tLoss:   91.142319\trec:   64.849663\tkl:   26.292650\n",
      "Epoch: 982 [10100/50000 (20%)]  \tLoss:   90.243301\trec:   63.445370\tkl:   26.797926\n",
      "Epoch: 982 [20100/50000 (40%)]  \tLoss:   90.440254\trec:   64.223885\tkl:   26.216370\n",
      "Epoch: 982 [30100/50000 (60%)]  \tLoss:   87.499901\trec:   61.096367\tkl:   26.403534\n",
      "Epoch: 982 [40100/50000 (80%)]  \tLoss:   90.837456\trec:   63.713261\tkl:   27.124193\n",
      "====> Epoch: 982 Average train loss: 90.1499\n",
      "====> Validation set loss: 94.5317\n",
      "====> Validation set kl: 26.3478\n",
      "Epoch: 983 [  100/50000 ( 0%)]  \tLoss:   90.695435\trec:   64.194542\tkl:   26.500898\n",
      "Epoch: 983 [10100/50000 (20%)]  \tLoss:   92.843727\trec:   65.922890\tkl:   26.920834\n",
      "Epoch: 983 [20100/50000 (40%)]  \tLoss:   89.233337\trec:   62.339359\tkl:   26.893978\n",
      "Epoch: 983 [30100/50000 (60%)]  \tLoss:   86.395065\trec:   60.542538\tkl:   25.852530\n",
      "Epoch: 983 [40100/50000 (80%)]  \tLoss:   90.412422\trec:   63.771595\tkl:   26.640825\n",
      "====> Epoch: 983 Average train loss: 90.1781\n",
      "====> Validation set loss: 94.4020\n",
      "====> Validation set kl: 26.5099\n",
      "Epoch: 984 [  100/50000 ( 0%)]  \tLoss:   86.527130\trec:   60.948910\tkl:   25.578213\n",
      "Epoch: 984 [10100/50000 (20%)]  \tLoss:   87.902290\trec:   61.210697\tkl:   26.691593\n",
      "Epoch: 984 [20100/50000 (40%)]  \tLoss:   85.480309\trec:   59.695435\tkl:   25.784878\n",
      "Epoch: 984 [30100/50000 (60%)]  \tLoss:   93.509377\trec:   66.766594\tkl:   26.742783\n",
      "Epoch: 984 [40100/50000 (80%)]  \tLoss:   94.722153\trec:   67.608078\tkl:   27.114079\n",
      "====> Epoch: 984 Average train loss: 90.1639\n",
      "====> Validation set loss: 94.3914\n",
      "====> Validation set kl: 26.5733\n",
      "Epoch: 985 [  100/50000 ( 0%)]  \tLoss:   89.403496\trec:   63.049541\tkl:   26.353956\n",
      "Epoch: 985 [10100/50000 (20%)]  \tLoss:   89.146103\trec:   63.917500\tkl:   25.228603\n",
      "Epoch: 985 [20100/50000 (40%)]  \tLoss:   87.779587\trec:   61.993778\tkl:   25.785805\n",
      "Epoch: 985 [30100/50000 (60%)]  \tLoss:   90.510056\trec:   63.450623\tkl:   27.059431\n",
      "Epoch: 985 [40100/50000 (80%)]  \tLoss:   91.352402\trec:   64.917725\tkl:   26.434677\n",
      "====> Epoch: 985 Average train loss: 90.1553\n",
      "====> Validation set loss: 94.4535\n",
      "====> Validation set kl: 26.5193\n",
      "Epoch: 986 [  100/50000 ( 0%)]  \tLoss:   93.381355\trec:   66.302383\tkl:   27.078966\n",
      "Epoch: 986 [10100/50000 (20%)]  \tLoss:   87.625191\trec:   62.049587\tkl:   25.575602\n",
      "Epoch: 986 [20100/50000 (40%)]  \tLoss:   88.289780\trec:   62.357067\tkl:   25.932713\n",
      "Epoch: 986 [30100/50000 (60%)]  \tLoss:   95.073586\trec:   67.753227\tkl:   27.320349\n",
      "Epoch: 986 [40100/50000 (80%)]  \tLoss:   87.590523\trec:   62.117077\tkl:   25.473444\n",
      "====> Epoch: 986 Average train loss: 90.1682\n",
      "====> Validation set loss: 94.4442\n",
      "====> Validation set kl: 26.4908\n",
      "Epoch: 987 [  100/50000 ( 0%)]  \tLoss:   90.997459\trec:   64.165520\tkl:   26.831938\n",
      "Epoch: 987 [10100/50000 (20%)]  \tLoss:   92.254585\trec:   65.689034\tkl:   26.565552\n",
      "Epoch: 987 [20100/50000 (40%)]  \tLoss:   88.662148\trec:   63.164207\tkl:   25.497938\n",
      "Epoch: 987 [30100/50000 (60%)]  \tLoss:   89.753517\trec:   63.860065\tkl:   25.893452\n",
      "Epoch: 987 [40100/50000 (80%)]  \tLoss:   87.760651\trec:   61.311039\tkl:   26.449612\n",
      "====> Epoch: 987 Average train loss: 90.1548\n",
      "====> Validation set loss: 94.2921\n",
      "====> Validation set kl: 26.3174\n",
      "Epoch: 988 [  100/50000 ( 0%)]  \tLoss:   88.200722\trec:   62.216778\tkl:   25.983950\n",
      "Epoch: 988 [10100/50000 (20%)]  \tLoss:   90.132881\trec:   63.659931\tkl:   26.472948\n",
      "Epoch: 988 [20100/50000 (40%)]  \tLoss:   88.404236\trec:   62.235119\tkl:   26.169121\n",
      "Epoch: 988 [30100/50000 (60%)]  \tLoss:   89.673882\trec:   63.401737\tkl:   26.272148\n",
      "Epoch: 988 [40100/50000 (80%)]  \tLoss:   91.841133\trec:   64.890923\tkl:   26.950199\n",
      "====> Epoch: 988 Average train loss: 90.1403\n",
      "====> Validation set loss: 94.3660\n",
      "====> Validation set kl: 26.4730\n",
      "Epoch: 989 [  100/50000 ( 0%)]  \tLoss:   92.826591\trec:   65.712029\tkl:   27.114563\n",
      "Epoch: 989 [10100/50000 (20%)]  \tLoss:   87.809311\trec:   62.155766\tkl:   25.653543\n",
      "Epoch: 989 [20100/50000 (40%)]  \tLoss:   90.211075\trec:   64.432892\tkl:   25.778185\n",
      "Epoch: 989 [30100/50000 (60%)]  \tLoss:   88.876717\trec:   63.158642\tkl:   25.718071\n",
      "Epoch: 989 [40100/50000 (80%)]  \tLoss:   90.991211\trec:   64.418510\tkl:   26.572697\n",
      "====> Epoch: 989 Average train loss: 90.1520\n",
      "====> Validation set loss: 94.5412\n",
      "====> Validation set kl: 26.5466\n",
      "Epoch: 990 [  100/50000 ( 0%)]  \tLoss:   85.750229\trec:   60.396282\tkl:   25.353951\n",
      "Epoch: 990 [10100/50000 (20%)]  \tLoss:   91.669441\trec:   64.524994\tkl:   27.144449\n",
      "Epoch: 990 [20100/50000 (40%)]  \tLoss:   90.691505\trec:   63.603924\tkl:   27.087578\n",
      "Epoch: 990 [30100/50000 (60%)]  \tLoss:   89.290840\trec:   62.742050\tkl:   26.548788\n",
      "Epoch: 990 [40100/50000 (80%)]  \tLoss:   91.360504\trec:   64.574745\tkl:   26.785757\n",
      "====> Epoch: 990 Average train loss: 90.1538\n",
      "====> Validation set loss: 94.4600\n",
      "====> Validation set kl: 26.5048\n",
      "Epoch: 991 [  100/50000 ( 0%)]  \tLoss:   88.910759\trec:   63.126137\tkl:   25.784628\n",
      "Epoch: 991 [10100/50000 (20%)]  \tLoss:   90.262459\trec:   63.633896\tkl:   26.628567\n",
      "Epoch: 991 [20100/50000 (40%)]  \tLoss:   89.260269\trec:   63.024643\tkl:   26.235630\n",
      "Epoch: 991 [30100/50000 (60%)]  \tLoss:   94.226852\trec:   67.038345\tkl:   27.188503\n",
      "Epoch: 991 [40100/50000 (80%)]  \tLoss:   92.508980\trec:   65.609192\tkl:   26.899788\n",
      "====> Epoch: 991 Average train loss: 90.1314\n",
      "====> Validation set loss: 94.4739\n",
      "====> Validation set kl: 26.4360\n",
      "Epoch: 992 [  100/50000 ( 0%)]  \tLoss:   93.178680\trec:   66.638313\tkl:   26.540365\n",
      "Epoch: 992 [10100/50000 (20%)]  \tLoss:   88.105698\trec:   61.527714\tkl:   26.577993\n",
      "Epoch: 992 [20100/50000 (40%)]  \tLoss:   93.329041\trec:   66.338310\tkl:   26.990726\n",
      "Epoch: 992 [30100/50000 (60%)]  \tLoss:   89.672241\trec:   63.327347\tkl:   26.344902\n",
      "Epoch: 992 [40100/50000 (80%)]  \tLoss:   89.549171\trec:   63.251274\tkl:   26.297895\n",
      "====> Epoch: 992 Average train loss: 90.1548\n",
      "====> Validation set loss: 94.4720\n",
      "====> Validation set kl: 26.6357\n",
      "Epoch: 993 [  100/50000 ( 0%)]  \tLoss:   92.090118\trec:   64.243553\tkl:   27.846565\n",
      "Epoch: 993 [10100/50000 (20%)]  \tLoss:   91.834831\trec:   65.224815\tkl:   26.610016\n",
      "Epoch: 993 [20100/50000 (40%)]  \tLoss:   89.578751\trec:   63.474346\tkl:   26.104404\n",
      "Epoch: 993 [30100/50000 (60%)]  \tLoss:   92.675148\trec:   65.691414\tkl:   26.983727\n",
      "Epoch: 993 [40100/50000 (80%)]  \tLoss:   93.933456\trec:   66.888947\tkl:   27.044502\n",
      "====> Epoch: 993 Average train loss: 90.1383\n",
      "====> Validation set loss: 94.4427\n",
      "====> Validation set kl: 26.5865\n",
      "Epoch: 994 [  100/50000 ( 0%)]  \tLoss:   93.212189\trec:   66.093071\tkl:   27.119114\n",
      "Epoch: 994 [10100/50000 (20%)]  \tLoss:   90.279732\trec:   62.707748\tkl:   27.571983\n",
      "Epoch: 994 [20100/50000 (40%)]  \tLoss:   89.343765\trec:   63.010731\tkl:   26.333038\n",
      "Epoch: 994 [30100/50000 (60%)]  \tLoss:   86.329887\trec:   61.035995\tkl:   25.293894\n",
      "Epoch: 994 [40100/50000 (80%)]  \tLoss:   92.451500\trec:   65.541237\tkl:   26.910263\n",
      "====> Epoch: 994 Average train loss: 90.1451\n",
      "====> Validation set loss: 94.4868\n",
      "====> Validation set kl: 26.5681\n",
      "Epoch: 995 [  100/50000 ( 0%)]  \tLoss:   88.633316\trec:   62.312847\tkl:   26.320473\n",
      "Epoch: 995 [10100/50000 (20%)]  \tLoss:   91.248962\trec:   64.190720\tkl:   27.058247\n",
      "Epoch: 995 [20100/50000 (40%)]  \tLoss:   90.000748\trec:   63.227783\tkl:   26.772968\n",
      "Epoch: 995 [30100/50000 (60%)]  \tLoss:   87.996475\trec:   61.585419\tkl:   26.411055\n",
      "Epoch: 995 [40100/50000 (80%)]  \tLoss:   90.035126\trec:   62.830273\tkl:   27.204851\n",
      "====> Epoch: 995 Average train loss: 90.1118\n",
      "====> Validation set loss: 94.4434\n",
      "====> Validation set kl: 26.4648\n",
      "Epoch: 996 [  100/50000 ( 0%)]  \tLoss:   94.691238\trec:   67.135010\tkl:   27.556227\n",
      "Epoch: 996 [10100/50000 (20%)]  \tLoss:   88.540848\trec:   62.264343\tkl:   26.276501\n",
      "Epoch: 996 [20100/50000 (40%)]  \tLoss:   86.874680\trec:   61.748455\tkl:   25.126217\n",
      "Epoch: 996 [30100/50000 (60%)]  \tLoss:   90.457085\trec:   64.014870\tkl:   26.442219\n",
      "Epoch: 996 [40100/50000 (80%)]  \tLoss:   87.065353\trec:   61.282112\tkl:   25.783234\n",
      "====> Epoch: 996 Average train loss: 90.1490\n",
      "====> Validation set loss: 94.4015\n",
      "====> Validation set kl: 26.4670\n",
      "Epoch: 997 [  100/50000 ( 0%)]  \tLoss:   88.860054\trec:   62.086750\tkl:   26.773308\n",
      "Epoch: 997 [10100/50000 (20%)]  \tLoss:   92.324181\trec:   65.010857\tkl:   27.313314\n",
      "Epoch: 997 [20100/50000 (40%)]  \tLoss:   88.324646\trec:   62.387753\tkl:   25.936897\n",
      "Epoch: 997 [30100/50000 (60%)]  \tLoss:   90.160858\trec:   64.350822\tkl:   25.810032\n",
      "Epoch: 997 [40100/50000 (80%)]  \tLoss:   88.803337\trec:   62.453548\tkl:   26.349792\n",
      "====> Epoch: 997 Average train loss: 90.1283\n",
      "====> Validation set loss: 94.3761\n",
      "====> Validation set kl: 26.3621\n",
      "Epoch: 998 [  100/50000 ( 0%)]  \tLoss:   89.899902\trec:   64.269020\tkl:   25.630873\n",
      "Epoch: 998 [10100/50000 (20%)]  \tLoss:   91.106285\trec:   64.505165\tkl:   26.601120\n",
      "Epoch: 998 [20100/50000 (40%)]  \tLoss:   91.570908\trec:   63.798855\tkl:   27.772053\n",
      "Epoch: 998 [30100/50000 (60%)]  \tLoss:   88.948921\trec:   63.662041\tkl:   25.286884\n",
      "Epoch: 998 [40100/50000 (80%)]  \tLoss:   87.614639\trec:   61.816277\tkl:   25.798357\n",
      "====> Epoch: 998 Average train loss: 90.1610\n",
      "====> Validation set loss: 94.3220\n",
      "====> Validation set kl: 26.4185\n",
      "Epoch: 999 [  100/50000 ( 0%)]  \tLoss:   88.221802\trec:   61.783085\tkl:   26.438721\n",
      "Epoch: 999 [10100/50000 (20%)]  \tLoss:   90.754410\trec:   64.676544\tkl:   26.077873\n",
      "Epoch: 999 [20100/50000 (40%)]  \tLoss:   91.073944\trec:   63.602852\tkl:   27.471090\n",
      "Epoch: 999 [30100/50000 (60%)]  \tLoss:   86.848419\trec:   61.049683\tkl:   25.798729\n",
      "Epoch: 999 [40100/50000 (80%)]  \tLoss:   91.985313\trec:   65.236259\tkl:   26.749052\n",
      "====> Epoch: 999 Average train loss: 90.1251\n",
      "====> Validation set loss: 94.3813\n",
      "====> Validation set kl: 26.5045\n",
      "Epoch: 1000 [  100/50000 ( 0%)]  \tLoss:   88.713493\trec:   61.736729\tkl:   26.976767\n",
      "Epoch: 1000 [10100/50000 (20%)]  \tLoss:   89.167587\trec:   62.959393\tkl:   26.208193\n",
      "Epoch: 1000 [20100/50000 (40%)]  \tLoss:   91.851723\trec:   64.724991\tkl:   27.126740\n",
      "Epoch: 1000 [30100/50000 (60%)]  \tLoss:   88.498955\trec:   61.931316\tkl:   26.567635\n",
      "Epoch: 1000 [40100/50000 (80%)]  \tLoss:   88.816650\trec:   61.985615\tkl:   26.831032\n",
      "====> Epoch: 1000 Average train loss: 90.1386\n",
      "====> Validation set loss: 94.3911\n",
      "====> Validation set kl: 26.6038\n",
      "====> Validation set loss: 94.4430\n",
      "====> Validation set kl: 26.5923\n",
      "Computing log-likelihood on test set\n",
      "Progress: 0.00%\n",
      "Progress: 10.00%\n",
      "Progress: 20.00%\n",
      "Progress: 30.00%\n",
      "Progress: 40.00%\n",
      "Progress: 50.00%\n",
      "Progress: 60.00%\n",
      "Progress: 70.00%\n",
      "Progress: 80.00%\n",
      "Progress: 90.00%\n",
      "====> Test set loss: 93.5985\n",
      "====> Test set kl: 26.5209\n",
      "====> Test set log-likelihood: 89.0247\n"
     ]
    }
   ],
   "source": [
    "%run main_experiment_VAE.py --manual_seed 55158"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "k10_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
