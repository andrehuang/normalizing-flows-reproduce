{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8rHen723nbw",
    "outputId": "84696431-5ac4-416d-c73a-c2927b289033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xljZuUC23yUE",
    "outputId": "1965dc21-9f96-4cc2-c2ab-986af28f69b1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Kied2gs4Be_",
    "outputId": "1cbbb9dc-f551-4f5b-ee33-3a884d643877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "cd \"/content/drive/MyDrive/Colab Notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYaXVCAE4JKS",
    "outputId": "29346cbc-0ddb-4e8d-a2db-b47e40a76d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch: 378 [  100/50000 ( 0%)]  \tLoss:   91.598564\trec:   64.974586\tkl:   26.623972\n",
      "Epoch: 378 [10100/50000 (20%)]  \tLoss:   93.282997\trec:   67.848183\tkl:   25.434816\n",
      "Epoch: 378 [20100/50000 (40%)]  \tLoss:   91.736336\trec:   67.784958\tkl:   23.951380\n",
      "Epoch: 378 [30100/50000 (60%)]  \tLoss:   94.999237\trec:   69.775169\tkl:   25.224070\n",
      "Epoch: 378 [40100/50000 (80%)]  \tLoss:   93.443085\trec:   67.785851\tkl:   25.657228\n",
      "====> Epoch: 378 Average train loss: 94.1395\n",
      "====> Validation set loss: 97.1685\n",
      "====> Validation set kl: 24.6464\n",
      "Epoch: 379 [  100/50000 ( 0%)]  \tLoss:   90.551208\trec:   66.359901\tkl:   24.191309\n",
      "Epoch: 379 [10100/50000 (20%)]  \tLoss:   94.918282\trec:   69.884415\tkl:   25.033871\n",
      "Epoch: 379 [20100/50000 (40%)]  \tLoss:   98.163368\trec:   71.090508\tkl:   27.072853\n",
      "Epoch: 379 [30100/50000 (60%)]  \tLoss:   92.293739\trec:   67.305473\tkl:   24.988264\n",
      "Epoch: 379 [40100/50000 (80%)]  \tLoss:   92.706284\trec:   67.068329\tkl:   25.637957\n",
      "====> Epoch: 379 Average train loss: 94.2708\n",
      "====> Validation set loss: 96.3631\n",
      "====> Validation set kl: 25.6773\n",
      "Epoch: 380 [  100/50000 ( 0%)]  \tLoss:   91.975204\trec:   66.549484\tkl:   25.425722\n",
      "Epoch: 380 [10100/50000 (20%)]  \tLoss:   96.460365\trec:   70.631767\tkl:   25.828604\n",
      "Epoch: 380 [20100/50000 (40%)]  \tLoss:   96.995918\trec:   71.187263\tkl:   25.808649\n",
      "Epoch: 380 [30100/50000 (60%)]  \tLoss:   96.177948\trec:   70.456642\tkl:   25.721308\n",
      "Epoch: 380 [40100/50000 (80%)]  \tLoss:   91.131561\trec:   66.288582\tkl:   24.842974\n",
      "====> Epoch: 380 Average train loss: 94.7042\n",
      "====> Validation set loss: 97.2061\n",
      "====> Validation set kl: 25.4314\n",
      "Epoch: 381 [  100/50000 ( 0%)]  \tLoss:   91.721489\trec:   67.146942\tkl:   24.574553\n",
      "Epoch: 381 [10100/50000 (20%)]  \tLoss:   95.363747\trec:   69.245529\tkl:   26.118221\n",
      "Epoch: 381 [20100/50000 (40%)]  \tLoss:   93.559555\trec:   68.251701\tkl:   25.307863\n",
      "Epoch: 381 [30100/50000 (60%)]  \tLoss:   94.273438\trec:   68.390770\tkl:   25.882666\n",
      "Epoch: 381 [40100/50000 (80%)]  \tLoss:   88.586014\trec:   64.967751\tkl:   23.618265\n",
      "====> Epoch: 381 Average train loss: 94.3108\n",
      "====> Validation set loss: 96.4397\n",
      "====> Validation set kl: 25.0866\n",
      "Epoch: 382 [  100/50000 ( 0%)]  \tLoss:   96.112183\trec:   70.318672\tkl:   25.793520\n",
      "Epoch: 382 [10100/50000 (20%)]  \tLoss:   95.031715\trec:   68.560173\tkl:   26.471548\n",
      "Epoch: 382 [20100/50000 (40%)]  \tLoss:   98.552017\trec:   73.003944\tkl:   25.548075\n",
      "Epoch: 382 [30100/50000 (60%)]  \tLoss:   98.082458\trec:   72.045296\tkl:   26.037157\n",
      "Epoch: 382 [40100/50000 (80%)]  \tLoss:   93.725990\trec:   69.252800\tkl:   24.473200\n",
      "====> Epoch: 382 Average train loss: 94.3215\n",
      "====> Validation set loss: 96.6037\n",
      "====> Validation set kl: 25.8409\n",
      "Epoch: 383 [  100/50000 ( 0%)]  \tLoss:   96.695564\trec:   69.633995\tkl:   27.061569\n",
      "Epoch: 383 [10100/50000 (20%)]  \tLoss:   97.249596\trec:   70.887390\tkl:   26.362206\n",
      "Epoch: 383 [20100/50000 (40%)]  \tLoss:   96.192940\trec:   70.268303\tkl:   25.924635\n",
      "Epoch: 383 [30100/50000 (60%)]  \tLoss:   98.431953\trec:   72.681808\tkl:   25.750145\n",
      "Epoch: 383 [40100/50000 (80%)]  \tLoss:   88.114929\trec:   62.669209\tkl:   25.445719\n",
      "====> Epoch: 383 Average train loss: 94.6367\n",
      "====> Validation set loss: 97.3255\n",
      "====> Validation set kl: 25.6454\n",
      "Epoch: 384 [  100/50000 ( 0%)]  \tLoss:   97.113152\trec:   71.723122\tkl:   25.390028\n",
      "Epoch: 384 [10100/50000 (20%)]  \tLoss:   94.866859\trec:   68.676003\tkl:   26.190859\n",
      "Epoch: 384 [20100/50000 (40%)]  \tLoss:   95.683136\trec:   69.994942\tkl:   25.688194\n",
      "Epoch: 384 [30100/50000 (60%)]  \tLoss:   90.442741\trec:   65.872818\tkl:   24.569923\n",
      "Epoch: 384 [40100/50000 (80%)]  \tLoss:   94.634666\trec:   68.788742\tkl:   25.845926\n",
      "====> Epoch: 384 Average train loss: 94.3011\n",
      "====> Validation set loss: 96.4836\n",
      "====> Validation set kl: 25.6064\n",
      "Epoch: 385 [  100/50000 ( 0%)]  \tLoss:   97.790466\trec:   71.900558\tkl:   25.889915\n",
      "Epoch: 385 [10100/50000 (20%)]  \tLoss:   92.036964\trec:   67.131660\tkl:   24.905300\n",
      "Epoch: 385 [20100/50000 (40%)]  \tLoss:   91.861786\trec:   66.970024\tkl:   24.891760\n",
      "Epoch: 385 [30100/50000 (60%)]  \tLoss:   96.046181\trec:   70.133049\tkl:   25.913137\n",
      "Epoch: 385 [40100/50000 (80%)]  \tLoss:   95.440216\trec:   68.636795\tkl:   26.803419\n",
      "====> Epoch: 385 Average train loss: 94.3563\n",
      "====> Validation set loss: 97.0018\n",
      "====> Validation set kl: 25.8277\n",
      "Epoch: 386 [  100/50000 ( 0%)]  \tLoss:   91.871925\trec:   66.711090\tkl:   25.160835\n",
      "Epoch: 386 [10100/50000 (20%)]  \tLoss:   94.571953\trec:   69.132706\tkl:   25.439247\n",
      "Epoch: 386 [20100/50000 (40%)]  \tLoss:   89.750587\trec:   64.330826\tkl:   25.419760\n",
      "Epoch: 386 [30100/50000 (60%)]  \tLoss:   95.553619\trec:   69.835075\tkl:   25.718542\n",
      "Epoch: 386 [40100/50000 (80%)]  \tLoss:   92.888374\trec:   67.775421\tkl:   25.112955\n",
      "====> Epoch: 386 Average train loss: 94.0769\n",
      "====> Validation set loss: 96.2661\n",
      "====> Validation set kl: 25.6163\n",
      "Epoch: 387 [  100/50000 ( 0%)]  \tLoss:   91.872810\trec:   66.620476\tkl:   25.252337\n",
      "Epoch: 387 [10100/50000 (20%)]  \tLoss:   92.877861\trec:   67.370781\tkl:   25.507072\n",
      "Epoch: 387 [20100/50000 (40%)]  \tLoss:   92.874374\trec:   67.153328\tkl:   25.721039\n",
      "Epoch: 387 [30100/50000 (60%)]  \tLoss:   96.510445\trec:   70.256638\tkl:   26.253811\n",
      "Epoch: 387 [40100/50000 (80%)]  \tLoss:   95.581238\trec:   68.415062\tkl:   27.166176\n",
      "====> Epoch: 387 Average train loss: 94.1941\n",
      "====> Validation set loss: 96.6902\n",
      "====> Validation set kl: 25.3588\n",
      "Epoch: 388 [  100/50000 ( 0%)]  \tLoss:   96.223961\trec:   70.260887\tkl:   25.963081\n",
      "Epoch: 388 [10100/50000 (20%)]  \tLoss:   95.570122\trec:   69.886787\tkl:   25.683342\n",
      "Epoch: 388 [20100/50000 (40%)]  \tLoss:   95.721756\trec:   69.397942\tkl:   26.323820\n",
      "Epoch: 388 [30100/50000 (60%)]  \tLoss:   91.988747\trec:   66.685020\tkl:   25.303730\n",
      "Epoch: 388 [40100/50000 (80%)]  \tLoss:   90.899933\trec:   65.948654\tkl:   24.951277\n",
      "====> Epoch: 388 Average train loss: 94.1477\n",
      "====> Validation set loss: 96.3747\n",
      "====> Validation set kl: 25.5595\n",
      "Epoch: 389 [  100/50000 ( 0%)]  \tLoss:   92.495796\trec:   66.910149\tkl:   25.585657\n",
      "Epoch: 389 [10100/50000 (20%)]  \tLoss:   92.240311\trec:   67.971695\tkl:   24.268610\n",
      "Epoch: 389 [20100/50000 (40%)]  \tLoss:   94.977745\trec:   69.267265\tkl:   25.710476\n",
      "Epoch: 389 [30100/50000 (60%)]  \tLoss:   91.524986\trec:   66.505783\tkl:   25.019209\n",
      "Epoch: 389 [40100/50000 (80%)]  \tLoss:   94.595093\trec:   68.566696\tkl:   26.028399\n",
      "====> Epoch: 389 Average train loss: 93.9816\n",
      "====> Validation set loss: 97.5498\n",
      "====> Validation set kl: 24.6214\n",
      "Epoch: 390 [  100/50000 ( 0%)]  \tLoss:   94.799644\trec:   69.747169\tkl:   25.052475\n",
      "Epoch: 390 [10100/50000 (20%)]  \tLoss:   97.993126\trec:   71.888161\tkl:   26.104965\n",
      "Epoch: 390 [20100/50000 (40%)]  \tLoss:   92.997429\trec:   67.490471\tkl:   25.506954\n",
      "Epoch: 390 [30100/50000 (60%)]  \tLoss:   94.450577\trec:   68.799004\tkl:   25.651569\n",
      "Epoch: 390 [40100/50000 (80%)]  \tLoss:   88.610504\trec:   64.439102\tkl:   24.171406\n",
      "====> Epoch: 390 Average train loss: 94.1551\n",
      "====> Validation set loss: 96.6949\n",
      "====> Validation set kl: 25.8491\n",
      "Epoch: 391 [  100/50000 ( 0%)]  \tLoss:   92.271027\trec:   66.692253\tkl:   25.578773\n",
      "Epoch: 391 [10100/50000 (20%)]  \tLoss:   96.259987\trec:   70.146034\tkl:   26.113956\n",
      "Epoch: 391 [20100/50000 (40%)]  \tLoss:   94.516640\trec:   69.253914\tkl:   25.262720\n",
      "Epoch: 391 [30100/50000 (60%)]  \tLoss:   90.585213\trec:   65.589844\tkl:   24.995369\n",
      "Epoch: 391 [40100/50000 (80%)]  \tLoss:   97.619568\trec:   71.491234\tkl:   26.128334\n",
      "====> Epoch: 391 Average train loss: 94.0820\n",
      "====> Validation set loss: 96.2146\n",
      "====> Validation set kl: 25.3965\n",
      "Epoch: 392 [  100/50000 ( 0%)]  \tLoss:   93.803444\trec:   68.404259\tkl:   25.399187\n",
      "Epoch: 392 [10100/50000 (20%)]  \tLoss:   94.025757\trec:   69.017433\tkl:   25.008329\n",
      "Epoch: 392 [20100/50000 (40%)]  \tLoss:   91.505318\trec:   66.506050\tkl:   24.999268\n",
      "Epoch: 392 [30100/50000 (60%)]  \tLoss:   98.112091\trec:   72.291199\tkl:   25.820885\n",
      "Epoch: 392 [40100/50000 (80%)]  \tLoss:   93.376251\trec:   68.357765\tkl:   25.018486\n",
      "====> Epoch: 392 Average train loss: 94.0578\n",
      "====> Validation set loss: 97.2683\n",
      "====> Validation set kl: 25.8381\n",
      "Epoch: 393 [  100/50000 ( 0%)]  \tLoss:   90.540291\trec:   65.308151\tkl:   25.232143\n",
      "Epoch: 393 [10100/50000 (20%)]  \tLoss:   90.348404\trec:   65.870636\tkl:   24.477774\n",
      "Epoch: 393 [20100/50000 (40%)]  \tLoss:   96.579041\trec:   71.276245\tkl:   25.302792\n",
      "Epoch: 393 [30100/50000 (60%)]  \tLoss:   91.476318\trec:   66.297974\tkl:   25.178337\n",
      "Epoch: 393 [40100/50000 (80%)]  \tLoss:   87.704903\trec:   62.928780\tkl:   24.776117\n",
      "====> Epoch: 393 Average train loss: 94.0503\n",
      "====> Validation set loss: 96.7817\n",
      "====> Validation set kl: 25.4987\n",
      "Epoch: 394 [  100/50000 ( 0%)]  \tLoss:   94.271591\trec:   67.485138\tkl:   26.786455\n",
      "Epoch: 394 [10100/50000 (20%)]  \tLoss:   95.422478\trec:   70.010429\tkl:   25.412050\n",
      "Epoch: 394 [20100/50000 (40%)]  \tLoss:   92.008232\trec:   67.074638\tkl:   24.933596\n",
      "Epoch: 394 [30100/50000 (60%)]  \tLoss:   92.331772\trec:   66.722099\tkl:   25.609673\n",
      "Epoch: 394 [40100/50000 (80%)]  \tLoss:   93.282120\trec:   68.299400\tkl:   24.982714\n",
      "====> Epoch: 394 Average train loss: 93.6892\n",
      "====> Validation set loss: 96.5702\n",
      "====> Validation set kl: 25.2830\n",
      "Epoch: 395 [  100/50000 ( 0%)]  \tLoss:   96.218445\trec:   70.393219\tkl:   25.825232\n",
      "Epoch: 395 [10100/50000 (20%)]  \tLoss:   99.723648\trec:   73.792908\tkl:   25.930742\n",
      "Epoch: 395 [20100/50000 (40%)]  \tLoss:   96.278946\trec:   71.109016\tkl:   25.169924\n",
      "Epoch: 395 [30100/50000 (60%)]  \tLoss:   96.481323\trec:   70.807426\tkl:   25.673891\n",
      "Epoch: 395 [40100/50000 (80%)]  \tLoss:   92.447166\trec:   67.416763\tkl:   25.030407\n",
      "====> Epoch: 395 Average train loss: 93.8850\n",
      "====> Validation set loss: 96.0784\n",
      "====> Validation set kl: 25.1551\n",
      "Epoch: 396 [  100/50000 ( 0%)]  \tLoss:   94.949448\trec:   69.825439\tkl:   25.124008\n",
      "Epoch: 396 [10100/50000 (20%)]  \tLoss:   95.265343\trec:   69.332451\tkl:   25.932890\n",
      "Epoch: 396 [20100/50000 (40%)]  \tLoss:   91.474167\trec:   66.890045\tkl:   24.584126\n",
      "Epoch: 396 [30100/50000 (60%)]  \tLoss:   93.565506\trec:   68.348679\tkl:   25.216824\n",
      "Epoch: 396 [40100/50000 (80%)]  \tLoss:   95.575165\trec:   70.376740\tkl:   25.198427\n",
      "====> Epoch: 396 Average train loss: 94.2001\n",
      "====> Validation set loss: 96.6980\n",
      "====> Validation set kl: 25.4925\n",
      "Epoch: 397 [  100/50000 ( 0%)]  \tLoss:   91.702461\trec:   67.395020\tkl:   24.307438\n",
      "Epoch: 397 [10100/50000 (20%)]  \tLoss:   92.332809\trec:   66.946388\tkl:   25.386427\n",
      "Epoch: 397 [20100/50000 (40%)]  \tLoss:   99.592262\trec:   74.632774\tkl:   24.959486\n",
      "Epoch: 397 [30100/50000 (60%)]  \tLoss:   93.829109\trec:   68.924561\tkl:   24.904549\n",
      "Epoch: 397 [40100/50000 (80%)]  \tLoss:   95.773399\trec:   70.333946\tkl:   25.439457\n",
      "====> Epoch: 397 Average train loss: 94.3698\n",
      "====> Validation set loss: 96.3310\n",
      "====> Validation set kl: 25.2714\n",
      "Epoch: 398 [  100/50000 ( 0%)]  \tLoss:   94.988716\trec:   69.975456\tkl:   25.013262\n",
      "Epoch: 398 [10100/50000 (20%)]  \tLoss:   96.789375\trec:   70.928635\tkl:   25.860739\n",
      "Epoch: 398 [20100/50000 (40%)]  \tLoss:   91.489677\trec:   65.628487\tkl:   25.861189\n",
      "Epoch: 398 [30100/50000 (60%)]  \tLoss:   92.922302\trec:   67.333839\tkl:   25.588469\n",
      "Epoch: 398 [40100/50000 (80%)]  \tLoss:   97.953979\trec:   72.452812\tkl:   25.501171\n",
      "====> Epoch: 398 Average train loss: 94.4532\n",
      "====> Validation set loss: 97.3406\n",
      "====> Validation set kl: 25.7329\n",
      "Epoch: 399 [  100/50000 ( 0%)]  \tLoss:   95.917488\trec:   69.442795\tkl:   26.474699\n",
      "Epoch: 399 [10100/50000 (20%)]  \tLoss:   93.435074\trec:   68.800133\tkl:   24.634939\n",
      "Epoch: 399 [20100/50000 (40%)]  \tLoss:   94.205406\trec:   69.470245\tkl:   24.735161\n",
      "Epoch: 399 [30100/50000 (60%)]  \tLoss:   98.111404\trec:   71.631157\tkl:   26.480247\n",
      "Epoch: 399 [40100/50000 (80%)]  \tLoss:   92.311943\trec:   67.267670\tkl:   25.044275\n",
      "====> Epoch: 399 Average train loss: 94.4027\n",
      "====> Validation set loss: 96.4298\n",
      "====> Validation set kl: 25.5994\n",
      "Epoch: 400 [  100/50000 ( 0%)]  \tLoss:   96.183006\trec:   70.118790\tkl:   26.064219\n",
      "Epoch: 400 [10100/50000 (20%)]  \tLoss:   94.131004\trec:   68.592781\tkl:   25.538225\n",
      "Epoch: 400 [20100/50000 (40%)]  \tLoss:   90.205482\trec:   65.625778\tkl:   24.579704\n",
      "Epoch: 400 [30100/50000 (60%)]  \tLoss:   99.296440\trec:   72.331123\tkl:   26.965322\n",
      "Epoch: 400 [40100/50000 (80%)]  \tLoss:   98.151245\trec:   72.176521\tkl:   25.974722\n",
      "====> Epoch: 400 Average train loss: 94.0807\n",
      "====> Validation set loss: 96.2995\n",
      "====> Validation set kl: 25.2885\n",
      "Epoch: 401 [  100/50000 ( 0%)]  \tLoss:   96.919060\trec:   71.394684\tkl:   25.524380\n",
      "Epoch: 401 [10100/50000 (20%)]  \tLoss:   93.122498\trec:   67.712051\tkl:   25.410454\n",
      "Epoch: 401 [20100/50000 (40%)]  \tLoss:   91.720093\trec:   67.058784\tkl:   24.661312\n",
      "Epoch: 401 [30100/50000 (60%)]  \tLoss:   93.459389\trec:   68.628586\tkl:   24.830811\n",
      "Epoch: 401 [40100/50000 (80%)]  \tLoss:   91.801857\trec:   66.781548\tkl:   25.020300\n",
      "====> Epoch: 401 Average train loss: 94.9931\n",
      "====> Validation set loss: 97.2206\n",
      "====> Validation set kl: 25.8189\n",
      "Epoch: 402 [  100/50000 ( 0%)]  \tLoss:   96.434410\trec:   70.752129\tkl:   25.682285\n",
      "Epoch: 402 [10100/50000 (20%)]  \tLoss:   95.444237\trec:   69.217033\tkl:   26.227200\n",
      "Epoch: 402 [20100/50000 (40%)]  \tLoss:   97.201813\trec:   71.028236\tkl:   26.173578\n",
      "Epoch: 402 [30100/50000 (60%)]  \tLoss:   95.513687\trec:   70.441620\tkl:   25.072065\n",
      "Epoch: 402 [40100/50000 (80%)]  \tLoss:   97.800537\trec:   70.864586\tkl:   26.935947\n",
      "====> Epoch: 402 Average train loss: 94.4014\n",
      "====> Validation set loss: 96.6813\n",
      "====> Validation set kl: 25.2078\n",
      "Epoch: 403 [  100/50000 ( 0%)]  \tLoss:   95.960251\trec:   70.378960\tkl:   25.581289\n",
      "Epoch: 403 [10100/50000 (20%)]  \tLoss:   97.700562\trec:   72.116890\tkl:   25.583681\n",
      "Epoch: 403 [20100/50000 (40%)]  \tLoss:   94.307068\trec:   69.822334\tkl:   24.484732\n",
      "Epoch: 403 [30100/50000 (60%)]  \tLoss:   97.523979\trec:   72.001244\tkl:   25.522741\n",
      "Epoch: 403 [40100/50000 (80%)]  \tLoss:   98.164482\trec:   71.701988\tkl:   26.462490\n",
      "====> Epoch: 403 Average train loss: 94.5240\n",
      "====> Validation set loss: 97.0648\n",
      "====> Validation set kl: 25.0869\n",
      "Epoch: 404 [  100/50000 ( 0%)]  \tLoss:   93.228073\trec:   68.153740\tkl:   25.074331\n",
      "Epoch: 404 [10100/50000 (20%)]  \tLoss:   88.886505\trec:   64.092888\tkl:   24.793615\n",
      "Epoch: 404 [20100/50000 (40%)]  \tLoss:   91.963066\trec:   66.944435\tkl:   25.018627\n",
      "Epoch: 404 [30100/50000 (60%)]  \tLoss:   98.957596\trec:   73.266106\tkl:   25.691498\n",
      "Epoch: 404 [40100/50000 (80%)]  \tLoss:   91.927994\trec:   66.770584\tkl:   25.157410\n",
      "====> Epoch: 404 Average train loss: 94.5988\n",
      "====> Validation set loss: 97.3570\n",
      "====> Validation set kl: 25.4735\n",
      "Epoch: 405 [  100/50000 ( 0%)]  \tLoss:   94.268631\trec:   68.826973\tkl:   25.441652\n",
      "Epoch: 405 [10100/50000 (20%)]  \tLoss:   95.395332\trec:   69.947296\tkl:   25.448042\n",
      "Epoch: 405 [20100/50000 (40%)]  \tLoss:   93.871948\trec:   67.499870\tkl:   26.372084\n",
      "Epoch: 405 [30100/50000 (60%)]  \tLoss:   91.451851\trec:   66.579254\tkl:   24.872593\n",
      "Epoch: 405 [40100/50000 (80%)]  \tLoss:   90.813377\trec:   65.644547\tkl:   25.168827\n",
      "====> Epoch: 405 Average train loss: 94.5045\n",
      "====> Validation set loss: 96.8560\n",
      "====> Validation set kl: 26.4808\n",
      "Epoch: 406 [  100/50000 ( 0%)]  \tLoss:   92.208672\trec:   67.128448\tkl:   25.080219\n",
      "Epoch: 406 [10100/50000 (20%)]  \tLoss:   94.748848\trec:   69.065811\tkl:   25.683031\n",
      "Epoch: 406 [20100/50000 (40%)]  \tLoss:   91.764160\trec:   66.316818\tkl:   25.447340\n",
      "Epoch: 406 [30100/50000 (60%)]  \tLoss:   94.805458\trec:   69.494034\tkl:   25.311419\n",
      "Epoch: 406 [40100/50000 (80%)]  \tLoss:   94.167068\trec:   67.855255\tkl:   26.311806\n",
      "====> Epoch: 406 Average train loss: 94.2323\n",
      "====> Validation set loss: 96.7877\n",
      "====> Validation set kl: 25.6918\n",
      "Epoch: 407 [  100/50000 ( 0%)]  \tLoss:   97.184196\trec:   71.549614\tkl:   25.634579\n",
      "Epoch: 407 [10100/50000 (20%)]  \tLoss:   93.268036\trec:   68.312370\tkl:   24.955664\n",
      "Epoch: 407 [20100/50000 (40%)]  \tLoss:   93.516045\trec:   69.441101\tkl:   24.074942\n",
      "Epoch: 407 [30100/50000 (60%)]  \tLoss:   94.844666\trec:   68.504387\tkl:   26.340284\n",
      "Epoch: 407 [40100/50000 (80%)]  \tLoss:   91.742836\trec:   66.415962\tkl:   25.326874\n",
      "====> Epoch: 407 Average train loss: 94.2879\n",
      "====> Validation set loss: 97.2678\n",
      "====> Validation set kl: 25.5919\n",
      "Epoch: 408 [  100/50000 ( 0%)]  \tLoss:   96.189522\trec:   70.341797\tkl:   25.847725\n",
      "Epoch: 408 [10100/50000 (20%)]  \tLoss:   94.152344\trec:   68.307793\tkl:   25.844545\n",
      "Epoch: 408 [20100/50000 (40%)]  \tLoss:   92.548317\trec:   66.887344\tkl:   25.660976\n",
      "Epoch: 408 [30100/50000 (60%)]  \tLoss:   94.674194\trec:   69.480881\tkl:   25.193319\n",
      "Epoch: 408 [40100/50000 (80%)]  \tLoss:   90.390038\trec:   65.593376\tkl:   24.796658\n",
      "====> Epoch: 408 Average train loss: 94.6080\n",
      "====> Validation set loss: 97.5030\n",
      "====> Validation set kl: 25.5025\n",
      "Epoch: 409 [  100/50000 ( 0%)]  \tLoss:   89.972267\trec:   65.727203\tkl:   24.245060\n",
      "Epoch: 409 [10100/50000 (20%)]  \tLoss:   91.910416\trec:   66.336670\tkl:   25.573751\n",
      "Epoch: 409 [20100/50000 (40%)]  \tLoss:   92.473595\trec:   67.627075\tkl:   24.846516\n",
      "Epoch: 409 [30100/50000 (60%)]  \tLoss:   93.947021\trec:   68.665970\tkl:   25.281050\n",
      "Epoch: 409 [40100/50000 (80%)]  \tLoss:   95.530174\trec:   69.172859\tkl:   26.357311\n",
      "====> Epoch: 409 Average train loss: 94.2162\n",
      "====> Validation set loss: 96.7693\n",
      "====> Validation set kl: 25.0121\n",
      "Epoch: 410 [  100/50000 ( 0%)]  \tLoss:   92.149666\trec:   67.189110\tkl:   24.960556\n",
      "Epoch: 410 [10100/50000 (20%)]  \tLoss:   97.086784\trec:   69.776810\tkl:   27.309973\n",
      "Epoch: 410 [20100/50000 (40%)]  \tLoss:   95.350937\trec:   69.356392\tkl:   25.994541\n",
      "Epoch: 410 [30100/50000 (60%)]  \tLoss:   92.239395\trec:   66.459435\tkl:   25.779964\n",
      "Epoch: 410 [40100/50000 (80%)]  \tLoss:   95.596062\trec:   70.639816\tkl:   24.956247\n",
      "====> Epoch: 410 Average train loss: 94.1176\n",
      "====> Validation set loss: 96.7447\n",
      "====> Validation set kl: 25.5254\n",
      "Epoch: 411 [  100/50000 ( 0%)]  \tLoss:   93.554649\trec:   68.277870\tkl:   25.276773\n",
      "Epoch: 411 [10100/50000 (20%)]  \tLoss:   94.595528\trec:   69.058250\tkl:   25.537279\n",
      "Epoch: 411 [20100/50000 (40%)]  \tLoss:   96.561554\trec:   69.845955\tkl:   26.715597\n",
      "Epoch: 411 [30100/50000 (60%)]  \tLoss:   98.227501\trec:   72.089104\tkl:   26.138390\n",
      "Epoch: 411 [40100/50000 (80%)]  \tLoss:   95.585548\trec:   69.912636\tkl:   25.672909\n",
      "====> Epoch: 411 Average train loss: 94.6238\n",
      "====> Validation set loss: 96.9175\n",
      "====> Validation set kl: 25.7505\n",
      "Epoch: 412 [  100/50000 ( 0%)]  \tLoss:   94.360977\trec:   69.171295\tkl:   25.189672\n",
      "Epoch: 412 [10100/50000 (20%)]  \tLoss:   98.968506\trec:   73.781418\tkl:   25.187086\n",
      "Epoch: 412 [20100/50000 (40%)]  \tLoss:   93.338905\trec:   68.121284\tkl:   25.217611\n",
      "Epoch: 412 [30100/50000 (60%)]  \tLoss:   96.062134\trec:   69.945374\tkl:   26.116764\n",
      "Epoch: 412 [40100/50000 (80%)]  \tLoss:   93.107880\trec:   67.269981\tkl:   25.837893\n",
      "====> Epoch: 412 Average train loss: 94.1752\n",
      "====> Validation set loss: 96.4414\n",
      "====> Validation set kl: 25.7724\n",
      "Epoch: 413 [  100/50000 ( 0%)]  \tLoss:   94.509483\trec:   68.185997\tkl:   26.323486\n",
      "Epoch: 413 [10100/50000 (20%)]  \tLoss:   95.151627\trec:   69.193817\tkl:   25.957815\n",
      "Epoch: 413 [20100/50000 (40%)]  \tLoss:   95.553375\trec:   69.714256\tkl:   25.839119\n",
      "Epoch: 413 [30100/50000 (60%)]  \tLoss:   96.057793\trec:   69.700752\tkl:   26.357035\n",
      "Epoch: 413 [40100/50000 (80%)]  \tLoss:   95.736961\trec:   69.848083\tkl:   25.888878\n",
      "====> Epoch: 413 Average train loss: 93.8877\n",
      "====> Validation set loss: 96.4441\n",
      "====> Validation set kl: 25.3576\n",
      "Epoch: 414 [  100/50000 ( 0%)]  \tLoss:   94.691017\trec:   69.585709\tkl:   25.105305\n",
      "Epoch: 414 [10100/50000 (20%)]  \tLoss:   96.528198\trec:   70.376869\tkl:   26.151333\n",
      "Epoch: 414 [20100/50000 (40%)]  \tLoss:   95.883369\trec:   69.923706\tkl:   25.959663\n",
      "Epoch: 414 [30100/50000 (60%)]  \tLoss:   92.302696\trec:   66.066650\tkl:   26.236034\n",
      "Epoch: 414 [40100/50000 (80%)]  \tLoss:   91.709419\trec:   66.974266\tkl:   24.735155\n",
      "====> Epoch: 414 Average train loss: 94.1450\n",
      "====> Validation set loss: 96.0228\n",
      "====> Validation set kl: 25.2970\n",
      "Epoch: 415 [  100/50000 ( 0%)]  \tLoss:   96.219322\trec:   70.319717\tkl:   25.899601\n",
      "Epoch: 415 [10100/50000 (20%)]  \tLoss:   94.139587\trec:   68.329742\tkl:   25.809843\n",
      "Epoch: 415 [20100/50000 (40%)]  \tLoss:   97.575493\trec:   71.044693\tkl:   26.530802\n",
      "Epoch: 415 [30100/50000 (60%)]  \tLoss:   95.777306\trec:   70.097794\tkl:   25.679501\n",
      "Epoch: 415 [40100/50000 (80%)]  \tLoss:   93.090065\trec:   66.568085\tkl:   26.521980\n",
      "====> Epoch: 415 Average train loss: 94.0513\n",
      "====> Validation set loss: 96.2989\n",
      "====> Validation set kl: 25.6465\n",
      "Epoch: 416 [  100/50000 ( 0%)]  \tLoss:   88.929604\trec:   63.793484\tkl:   25.136127\n",
      "Epoch: 416 [10100/50000 (20%)]  \tLoss:   93.501495\trec:   67.192703\tkl:   26.308786\n",
      "Epoch: 416 [20100/50000 (40%)]  \tLoss:   94.938446\trec:   68.571686\tkl:   26.366760\n",
      "Epoch: 416 [30100/50000 (60%)]  \tLoss:   93.815681\trec:   68.583046\tkl:   25.232641\n",
      "Epoch: 416 [40100/50000 (80%)]  \tLoss:   90.285164\trec:   65.232819\tkl:   25.052341\n",
      "====> Epoch: 416 Average train loss: 93.8912\n",
      "====> Validation set loss: 96.8650\n",
      "====> Validation set kl: 26.0299\n",
      "Epoch: 417 [  100/50000 ( 0%)]  \tLoss:   89.628693\trec:   64.667511\tkl:   24.961182\n",
      "Epoch: 417 [10100/50000 (20%)]  \tLoss:   95.004196\trec:   69.186729\tkl:   25.817465\n",
      "Epoch: 417 [20100/50000 (40%)]  \tLoss:   97.709946\trec:   71.210167\tkl:   26.499784\n",
      "Epoch: 417 [30100/50000 (60%)]  \tLoss:   94.232124\trec:   68.810371\tkl:   25.421755\n",
      "Epoch: 417 [40100/50000 (80%)]  \tLoss:   95.939728\trec:   71.402863\tkl:   24.536867\n",
      "====> Epoch: 417 Average train loss: 93.7825\n",
      "====> Validation set loss: 96.4742\n",
      "====> Validation set kl: 25.5537\n",
      "Epoch: 418 [  100/50000 ( 0%)]  \tLoss:   96.635773\trec:   70.524452\tkl:   26.111320\n",
      "Epoch: 418 [10100/50000 (20%)]  \tLoss:   97.034531\trec:   71.301170\tkl:   25.733356\n",
      "Epoch: 418 [20100/50000 (40%)]  \tLoss:   93.137070\trec:   67.692627\tkl:   25.444443\n",
      "Epoch: 418 [30100/50000 (60%)]  \tLoss:   98.742088\trec:   72.596306\tkl:   26.145779\n",
      "Epoch: 418 [40100/50000 (80%)]  \tLoss:   97.888229\trec:   71.037262\tkl:   26.850969\n",
      "====> Epoch: 418 Average train loss: 94.9074\n",
      "====> Validation set loss: 97.8129\n",
      "====> Validation set kl: 26.1199\n",
      "Epoch: 419 [  100/50000 ( 0%)]  \tLoss:   90.174431\trec:   64.465050\tkl:   25.709381\n",
      "Epoch: 419 [10100/50000 (20%)]  \tLoss:   96.303101\trec:   69.518890\tkl:   26.784216\n",
      "Epoch: 419 [20100/50000 (40%)]  \tLoss:   92.375076\trec:   66.816170\tkl:   25.558910\n",
      "Epoch: 419 [30100/50000 (60%)]  \tLoss:   94.164650\trec:   68.802116\tkl:   25.362528\n",
      "Epoch: 419 [40100/50000 (80%)]  \tLoss:   90.078156\trec:   65.793602\tkl:   24.284550\n",
      "====> Epoch: 419 Average train loss: 94.3808\n",
      "====> Validation set loss: 96.8935\n",
      "====> Validation set kl: 25.4103\n",
      "Epoch: 420 [  100/50000 ( 0%)]  \tLoss:   93.436768\trec:   67.816994\tkl:   25.619774\n",
      "Epoch: 420 [10100/50000 (20%)]  \tLoss:   95.993614\trec:   71.047020\tkl:   24.946596\n",
      "Epoch: 420 [20100/50000 (40%)]  \tLoss:   91.433540\trec:   66.940468\tkl:   24.493078\n",
      "Epoch: 420 [30100/50000 (60%)]  \tLoss:   92.340416\trec:   65.714531\tkl:   26.625883\n",
      "Epoch: 420 [40100/50000 (80%)]  \tLoss:   95.608994\trec:   68.070229\tkl:   27.538765\n",
      "====> Epoch: 420 Average train loss: 95.2347\n",
      "====> Validation set loss: 97.2845\n",
      "====> Validation set kl: 26.0501\n",
      "Epoch: 421 [  100/50000 ( 0%)]  \tLoss:   95.612129\trec:   69.633766\tkl:   25.978354\n",
      "Epoch: 421 [10100/50000 (20%)]  \tLoss:   94.026245\trec:   68.278122\tkl:   25.748135\n",
      "Epoch: 421 [20100/50000 (40%)]  \tLoss:   93.402695\trec:   67.363693\tkl:   26.039003\n",
      "Epoch: 421 [30100/50000 (60%)]  \tLoss:   97.314484\trec:   70.760979\tkl:   26.553503\n",
      "Epoch: 421 [40100/50000 (80%)]  \tLoss:   95.619179\trec:   69.602791\tkl:   26.016392\n",
      "====> Epoch: 421 Average train loss: 94.5637\n",
      "====> Validation set loss: 96.7411\n",
      "====> Validation set kl: 26.0033\n",
      "Epoch: 422 [  100/50000 ( 0%)]  \tLoss:   98.088066\trec:   70.829811\tkl:   27.258247\n",
      "Epoch: 422 [10100/50000 (20%)]  \tLoss:   90.515800\trec:   64.805893\tkl:   25.709909\n",
      "Epoch: 422 [20100/50000 (40%)]  \tLoss:   94.188477\trec:   68.582375\tkl:   25.606106\n",
      "Epoch: 422 [30100/50000 (60%)]  \tLoss:   96.012184\trec:   68.786530\tkl:   27.225657\n",
      "Epoch: 422 [40100/50000 (80%)]  \tLoss:   99.543129\trec:   73.821342\tkl:   25.721794\n",
      "====> Epoch: 422 Average train loss: 95.0792\n",
      "====> Validation set loss: 97.4746\n",
      "====> Validation set kl: 25.9913\n",
      "Epoch: 423 [  100/50000 ( 0%)]  \tLoss:   94.925156\trec:   68.780548\tkl:   26.144604\n",
      "Epoch: 423 [10100/50000 (20%)]  \tLoss:   93.477379\trec:   67.917305\tkl:   25.560083\n",
      "Epoch: 423 [20100/50000 (40%)]  \tLoss:   96.480522\trec:   69.285973\tkl:   27.194555\n",
      "Epoch: 423 [30100/50000 (60%)]  \tLoss:   93.644547\trec:   68.156433\tkl:   25.488119\n",
      "Epoch: 423 [40100/50000 (80%)]  \tLoss:  100.043640\trec:   73.464371\tkl:   26.579271\n",
      "====> Epoch: 423 Average train loss: 95.0855\n",
      "====> Validation set loss: 98.0888\n",
      "====> Validation set kl: 26.3795\n",
      "Epoch: 424 [  100/50000 ( 0%)]  \tLoss:   97.925087\trec:   70.649940\tkl:   27.275146\n",
      "Epoch: 424 [10100/50000 (20%)]  \tLoss:   95.601677\trec:   69.172821\tkl:   26.428854\n",
      "Epoch: 424 [20100/50000 (40%)]  \tLoss:   98.733437\trec:   71.763618\tkl:   26.969809\n",
      "Epoch: 424 [30100/50000 (60%)]  \tLoss:   93.373322\trec:   67.889084\tkl:   25.484238\n",
      "Epoch: 424 [40100/50000 (80%)]  \tLoss:   93.403214\trec:   67.397675\tkl:   26.005537\n",
      "====> Epoch: 424 Average train loss: 94.7699\n",
      "====> Validation set loss: 96.6914\n",
      "====> Validation set kl: 25.3855\n",
      "Epoch: 425 [  100/50000 ( 0%)]  \tLoss:   95.408669\trec:   69.422607\tkl:   25.986061\n",
      "Epoch: 425 [10100/50000 (20%)]  \tLoss:   93.118759\trec:   68.607033\tkl:   24.511726\n",
      "Epoch: 425 [20100/50000 (40%)]  \tLoss:   91.311676\trec:   65.946953\tkl:   25.364723\n",
      "Epoch: 425 [30100/50000 (60%)]  \tLoss:   94.652901\trec:   68.543419\tkl:   26.109480\n",
      "Epoch: 425 [40100/50000 (80%)]  \tLoss:   95.038918\trec:   68.809196\tkl:   26.229715\n",
      "====> Epoch: 425 Average train loss: 94.6853\n",
      "====> Validation set loss: 96.8934\n",
      "====> Validation set kl: 25.5331\n",
      "Epoch: 426 [  100/50000 ( 0%)]  \tLoss:   96.169609\trec:   70.557831\tkl:   25.611778\n",
      "Epoch: 426 [10100/50000 (20%)]  \tLoss:   91.327850\trec:   64.904785\tkl:   26.423071\n",
      "Epoch: 426 [20100/50000 (40%)]  \tLoss:   93.275848\trec:   68.642296\tkl:   24.633553\n",
      "Epoch: 426 [30100/50000 (60%)]  \tLoss:   95.455719\trec:   70.868774\tkl:   24.586943\n",
      "Epoch: 426 [40100/50000 (80%)]  \tLoss:   93.162209\trec:   65.741821\tkl:   27.420383\n",
      "====> Epoch: 426 Average train loss: 94.5442\n",
      "====> Validation set loss: 97.2629\n",
      "====> Validation set kl: 26.2203\n",
      "Epoch: 427 [  100/50000 ( 0%)]  \tLoss:   95.143921\trec:   67.830421\tkl:   27.313499\n",
      "Epoch: 427 [10100/50000 (20%)]  \tLoss:   91.104355\trec:   65.393852\tkl:   25.710503\n",
      "Epoch: 427 [20100/50000 (40%)]  \tLoss:   95.802521\trec:   69.760254\tkl:   26.042263\n",
      "Epoch: 427 [30100/50000 (60%)]  \tLoss:   94.626320\trec:   68.926933\tkl:   25.699379\n",
      "Epoch: 427 [40100/50000 (80%)]  \tLoss:   90.778816\trec:   65.391953\tkl:   25.386866\n",
      "====> Epoch: 427 Average train loss: 94.7825\n",
      "====> Validation set loss: 97.2204\n",
      "====> Validation set kl: 26.0347\n",
      "Epoch: 428 [  100/50000 ( 0%)]  \tLoss:   96.889862\trec:   70.632973\tkl:   26.256884\n",
      "Epoch: 428 [10100/50000 (20%)]  \tLoss:   91.091110\trec:   66.414383\tkl:   24.676725\n",
      "Epoch: 428 [20100/50000 (40%)]  \tLoss:   92.752167\trec:   67.753448\tkl:   24.998720\n",
      "Epoch: 428 [30100/50000 (60%)]  \tLoss:   93.351791\trec:   67.633354\tkl:   25.718439\n",
      "Epoch: 428 [40100/50000 (80%)]  \tLoss:   93.233406\trec:   69.607155\tkl:   23.626251\n",
      "====> Epoch: 428 Average train loss: 94.6696\n",
      "====> Validation set loss: 97.4327\n",
      "====> Validation set kl: 24.4980\n",
      "Epoch: 429 [  100/50000 ( 0%)]  \tLoss:   93.943642\trec:   70.254921\tkl:   23.688717\n",
      "Epoch: 429 [10100/50000 (20%)]  \tLoss:   92.196152\trec:   67.603218\tkl:   24.592932\n",
      "Epoch: 429 [20100/50000 (40%)]  \tLoss:   95.501442\trec:   69.067596\tkl:   26.433849\n",
      "Epoch: 429 [30100/50000 (60%)]  \tLoss:   97.802696\trec:   71.240196\tkl:   26.562500\n",
      "Epoch: 429 [40100/50000 (80%)]  \tLoss:   92.191589\trec:   66.687042\tkl:   25.504553\n",
      "====> Epoch: 429 Average train loss: 94.4736\n",
      "====> Validation set loss: 97.3465\n",
      "====> Validation set kl: 26.9184\n",
      "Epoch: 430 [  100/50000 ( 0%)]  \tLoss:   96.921013\trec:   69.572609\tkl:   27.348398\n",
      "Epoch: 430 [10100/50000 (20%)]  \tLoss:   95.558334\trec:   69.263756\tkl:   26.294575\n",
      "Epoch: 430 [20100/50000 (40%)]  \tLoss:   96.593712\trec:   70.932732\tkl:   25.660980\n",
      "Epoch: 430 [30100/50000 (60%)]  \tLoss:   95.698036\trec:   70.353683\tkl:   25.344351\n",
      "Epoch: 430 [40100/50000 (80%)]  \tLoss:   94.915321\trec:   69.293457\tkl:   25.621864\n",
      "====> Epoch: 430 Average train loss: 94.3752\n",
      "====> Validation set loss: 97.0006\n",
      "====> Validation set kl: 25.6170\n",
      "Epoch: 431 [  100/50000 ( 0%)]  \tLoss:   89.162491\trec:   63.977890\tkl:   25.184597\n",
      "Epoch: 431 [10100/50000 (20%)]  \tLoss:   91.800056\trec:   65.286812\tkl:   26.513248\n",
      "Epoch: 431 [20100/50000 (40%)]  \tLoss:   97.115150\trec:   70.957397\tkl:   26.157755\n",
      "Epoch: 431 [30100/50000 (60%)]  \tLoss:   95.502052\trec:   70.120361\tkl:   25.381689\n",
      "Epoch: 431 [40100/50000 (80%)]  \tLoss:   94.924438\trec:   70.159508\tkl:   24.764931\n",
      "====> Epoch: 431 Average train loss: 94.1377\n",
      "====> Validation set loss: 96.6380\n",
      "====> Validation set kl: 24.8599\n",
      "Epoch: 432 [  100/50000 ( 0%)]  \tLoss:   94.027802\trec:   69.538147\tkl:   24.489655\n",
      "Epoch: 432 [10100/50000 (20%)]  \tLoss:   94.903320\trec:   69.432785\tkl:   25.470533\n",
      "Epoch: 432 [20100/50000 (40%)]  \tLoss:   94.089912\trec:   67.987305\tkl:   26.102610\n",
      "Epoch: 432 [30100/50000 (60%)]  \tLoss:   93.749878\trec:   70.389969\tkl:   23.359909\n",
      "Epoch: 432 [40100/50000 (80%)]  \tLoss:   94.082436\trec:   68.976860\tkl:   25.105576\n",
      "====> Epoch: 432 Average train loss: 94.1961\n",
      "====> Validation set loss: 96.2691\n",
      "====> Validation set kl: 25.6732\n",
      "Epoch: 433 [  100/50000 ( 0%)]  \tLoss:   96.865288\trec:   70.949928\tkl:   25.915356\n",
      "Epoch: 433 [10100/50000 (20%)]  \tLoss:   94.613808\trec:   68.855812\tkl:   25.757996\n",
      "Epoch: 433 [20100/50000 (40%)]  \tLoss:   90.202919\trec:   65.623352\tkl:   24.579563\n",
      "Epoch: 433 [30100/50000 (60%)]  \tLoss:   92.180214\trec:   66.852570\tkl:   25.327644\n",
      "Epoch: 433 [40100/50000 (80%)]  \tLoss:   95.083458\trec:   69.020454\tkl:   26.063000\n",
      "====> Epoch: 433 Average train loss: 93.9516\n",
      "====> Validation set loss: 97.4151\n",
      "====> Validation set kl: 25.3174\n",
      "Epoch: 434 [  100/50000 ( 0%)]  \tLoss:   97.006447\trec:   70.712364\tkl:   26.294086\n",
      "Epoch: 434 [10100/50000 (20%)]  \tLoss:   91.558449\trec:   66.415894\tkl:   25.142551\n",
      "Epoch: 434 [20100/50000 (40%)]  \tLoss:   96.590942\trec:   70.354706\tkl:   26.236242\n",
      "Epoch: 434 [30100/50000 (60%)]  \tLoss:   95.095581\trec:   68.772339\tkl:   26.323242\n",
      "Epoch: 434 [40100/50000 (80%)]  \tLoss:   93.819809\trec:   67.744148\tkl:   26.075661\n",
      "====> Epoch: 434 Average train loss: 94.1997\n",
      "====> Validation set loss: 96.4467\n",
      "====> Validation set kl: 24.9937\n",
      "Epoch: 435 [  100/50000 ( 0%)]  \tLoss:   91.670525\trec:   68.176651\tkl:   23.493881\n",
      "Epoch: 435 [10100/50000 (20%)]  \tLoss:   94.551270\trec:   68.226715\tkl:   26.324551\n",
      "Epoch: 435 [20100/50000 (40%)]  \tLoss:   91.626221\trec:   65.409081\tkl:   26.217136\n",
      "Epoch: 435 [30100/50000 (60%)]  \tLoss:   91.606209\trec:   66.500168\tkl:   25.106043\n",
      "Epoch: 435 [40100/50000 (80%)]  \tLoss:   90.794998\trec:   66.069603\tkl:   24.725393\n",
      "====> Epoch: 435 Average train loss: 93.9700\n",
      "====> Validation set loss: 96.2965\n",
      "====> Validation set kl: 25.3116\n",
      "Epoch: 436 [  100/50000 ( 0%)]  \tLoss:   90.235191\trec:   65.966133\tkl:   24.269056\n",
      "Epoch: 436 [10100/50000 (20%)]  \tLoss:   93.217636\trec:   67.633209\tkl:   25.584431\n",
      "Epoch: 436 [20100/50000 (40%)]  \tLoss:   96.631279\trec:   69.813477\tkl:   26.817802\n",
      "Epoch: 436 [30100/50000 (60%)]  \tLoss:   97.942085\trec:   72.313194\tkl:   25.628901\n",
      "Epoch: 436 [40100/50000 (80%)]  \tLoss:   96.985321\trec:   70.748131\tkl:   26.237186\n",
      "====> Epoch: 436 Average train loss: 94.1272\n",
      "====> Validation set loss: 96.5204\n",
      "====> Validation set kl: 25.2436\n",
      "Epoch: 437 [  100/50000 ( 0%)]  \tLoss:   91.873848\trec:   67.963707\tkl:   23.910139\n",
      "Epoch: 437 [10100/50000 (20%)]  \tLoss:   94.356850\trec:   68.742798\tkl:   25.614061\n",
      "Epoch: 437 [20100/50000 (40%)]  \tLoss:  101.132301\trec:   73.995369\tkl:   27.136932\n",
      "Epoch: 437 [30100/50000 (60%)]  \tLoss:   90.569946\trec:   64.384872\tkl:   26.185080\n",
      "Epoch: 437 [40100/50000 (80%)]  \tLoss:   92.171700\trec:   67.425369\tkl:   24.746332\n",
      "====> Epoch: 437 Average train loss: 94.3076\n",
      "====> Validation set loss: 96.5600\n",
      "====> Validation set kl: 25.9437\n",
      "Epoch: 438 [  100/50000 ( 0%)]  \tLoss:   97.785721\trec:   71.926056\tkl:   25.859663\n",
      "Epoch: 438 [10100/50000 (20%)]  \tLoss:   94.563652\trec:   68.968445\tkl:   25.595207\n",
      "Epoch: 438 [20100/50000 (40%)]  \tLoss:   93.389137\trec:   67.250816\tkl:   26.138323\n",
      "Epoch: 438 [30100/50000 (60%)]  \tLoss:   98.539696\trec:   73.436226\tkl:   25.103468\n",
      "Epoch: 438 [40100/50000 (80%)]  \tLoss:   97.552711\trec:   71.452423\tkl:   26.100292\n",
      "====> Epoch: 438 Average train loss: 93.9363\n",
      "====> Validation set loss: 97.0559\n",
      "====> Validation set kl: 25.4916\n",
      "Epoch: 439 [  100/50000 ( 0%)]  \tLoss:   95.620064\trec:   70.049927\tkl:   25.570133\n",
      "Epoch: 439 [10100/50000 (20%)]  \tLoss:   93.939430\trec:   68.908318\tkl:   25.031107\n",
      "Epoch: 439 [20100/50000 (40%)]  \tLoss:   93.170479\trec:   67.733849\tkl:   25.436632\n",
      "Epoch: 439 [30100/50000 (60%)]  \tLoss:   90.675194\trec:   65.209663\tkl:   25.465532\n",
      "Epoch: 439 [40100/50000 (80%)]  \tLoss:   92.564278\trec:   67.150467\tkl:   25.413809\n",
      "====> Epoch: 439 Average train loss: 94.2127\n",
      "====> Validation set loss: 96.4374\n",
      "====> Validation set kl: 25.6606\n",
      "Epoch: 440 [  100/50000 ( 0%)]  \tLoss:   89.479073\trec:   64.314903\tkl:   25.164167\n",
      "Epoch: 440 [10100/50000 (20%)]  \tLoss:   92.182312\trec:   66.985420\tkl:   25.196892\n",
      "Epoch: 440 [20100/50000 (40%)]  \tLoss:   90.914513\trec:   65.751144\tkl:   25.163366\n",
      "Epoch: 440 [30100/50000 (60%)]  \tLoss:   96.506813\trec:   70.403366\tkl:   26.103456\n",
      "Epoch: 440 [40100/50000 (80%)]  \tLoss:   93.272682\trec:   68.184013\tkl:   25.088675\n",
      "====> Epoch: 440 Average train loss: 93.8267\n",
      "====> Validation set loss: 96.7760\n",
      "====> Validation set kl: 25.9259\n",
      "Epoch: 441 [  100/50000 ( 0%)]  \tLoss:   92.301987\trec:   67.075333\tkl:   25.226650\n",
      "Epoch: 441 [10100/50000 (20%)]  \tLoss:   92.827438\trec:   68.571304\tkl:   24.256130\n",
      "Epoch: 441 [20100/50000 (40%)]  \tLoss:   95.307831\trec:   70.086395\tkl:   25.221432\n",
      "Epoch: 441 [30100/50000 (60%)]  \tLoss:   92.937714\trec:   67.032761\tkl:   25.904945\n",
      "Epoch: 441 [40100/50000 (80%)]  \tLoss:   94.715561\trec:   70.281807\tkl:   24.433750\n",
      "====> Epoch: 441 Average train loss: 93.9497\n",
      "====> Validation set loss: 96.4722\n",
      "====> Validation set kl: 25.5418\n",
      "Epoch: 442 [  100/50000 ( 0%)]  \tLoss:   93.148628\trec:   68.391235\tkl:   24.757397\n",
      "Epoch: 442 [10100/50000 (20%)]  \tLoss:   93.191322\trec:   67.497803\tkl:   25.693516\n",
      "Epoch: 442 [20100/50000 (40%)]  \tLoss:   92.598595\trec:   67.706139\tkl:   24.892456\n",
      "Epoch: 442 [30100/50000 (60%)]  \tLoss:   91.502975\trec:   66.336533\tkl:   25.166445\n",
      "Epoch: 442 [40100/50000 (80%)]  \tLoss:   92.344635\trec:   66.213402\tkl:   26.131237\n",
      "====> Epoch: 442 Average train loss: 93.9338\n",
      "====> Validation set loss: 96.6938\n",
      "====> Validation set kl: 25.2744\n",
      "Epoch: 443 [  100/50000 ( 0%)]  \tLoss:   98.023277\trec:   71.986610\tkl:   26.036673\n",
      "Epoch: 443 [10100/50000 (20%)]  \tLoss:   95.985283\trec:   69.560226\tkl:   26.425060\n",
      "Epoch: 443 [20100/50000 (40%)]  \tLoss:   93.212158\trec:   67.714073\tkl:   25.498085\n",
      "Epoch: 443 [30100/50000 (60%)]  \tLoss:   95.773537\trec:   70.911003\tkl:   24.862524\n",
      "Epoch: 443 [40100/50000 (80%)]  \tLoss:   94.041817\trec:   68.503242\tkl:   25.538574\n",
      "====> Epoch: 443 Average train loss: 94.0924\n",
      "====> Validation set loss: 96.5694\n",
      "====> Validation set kl: 25.3291\n",
      "Epoch: 444 [  100/50000 ( 0%)]  \tLoss:   96.356010\trec:   70.426773\tkl:   25.929237\n",
      "Epoch: 444 [10100/50000 (20%)]  \tLoss:   95.064629\trec:   68.958710\tkl:   26.105917\n",
      "Epoch: 444 [20100/50000 (40%)]  \tLoss:   95.737907\trec:   69.536316\tkl:   26.201595\n",
      "Epoch: 444 [30100/50000 (60%)]  \tLoss:   92.717995\trec:   66.721275\tkl:   25.996717\n",
      "Epoch: 444 [40100/50000 (80%)]  \tLoss:   99.476532\trec:   72.377899\tkl:   27.098629\n",
      "====> Epoch: 444 Average train loss: 93.9018\n",
      "====> Validation set loss: 96.4672\n",
      "====> Validation set kl: 25.7293\n",
      "Epoch: 445 [  100/50000 ( 0%)]  \tLoss:   93.915367\trec:   67.802238\tkl:   26.113127\n",
      "Epoch: 445 [10100/50000 (20%)]  \tLoss:   90.080292\trec:   65.861694\tkl:   24.218596\n",
      "Epoch: 445 [20100/50000 (40%)]  \tLoss:   93.542152\trec:   67.926628\tkl:   25.615530\n",
      "Epoch: 445 [30100/50000 (60%)]  \tLoss:   91.054916\trec:   65.925880\tkl:   25.129042\n",
      "Epoch: 445 [40100/50000 (80%)]  \tLoss:   95.804276\trec:   68.982948\tkl:   26.821323\n",
      "====> Epoch: 445 Average train loss: 93.7300\n",
      "====> Validation set loss: 96.3830\n",
      "====> Validation set kl: 25.1319\n",
      "Epoch: 446 [  100/50000 ( 0%)]  \tLoss:   92.563103\trec:   67.703079\tkl:   24.860022\n",
      "Epoch: 446 [10100/50000 (20%)]  \tLoss:   88.093491\trec:   63.858067\tkl:   24.235432\n",
      "Epoch: 446 [20100/50000 (40%)]  \tLoss:   96.541061\trec:   70.367935\tkl:   26.173132\n",
      "Epoch: 446 [30100/50000 (60%)]  \tLoss:   91.538399\trec:   66.514236\tkl:   25.024155\n",
      "Epoch: 446 [40100/50000 (80%)]  \tLoss:   93.156464\trec:   66.941994\tkl:   26.214462\n",
      "====> Epoch: 446 Average train loss: 93.7490\n",
      "====> Validation set loss: 96.0552\n",
      "====> Validation set kl: 25.4849\n",
      "Epoch: 447 [  100/50000 ( 0%)]  \tLoss:   94.465942\trec:   69.236061\tkl:   25.229889\n",
      "Epoch: 447 [10100/50000 (20%)]  \tLoss:   95.083260\trec:   69.472466\tkl:   25.610788\n",
      "Epoch: 447 [20100/50000 (40%)]  \tLoss:   94.607788\trec:   69.833427\tkl:   24.774370\n",
      "Epoch: 447 [30100/50000 (60%)]  \tLoss:   97.889343\trec:   71.533440\tkl:   26.355909\n",
      "Epoch: 447 [40100/50000 (80%)]  \tLoss:   93.543068\trec:   68.134277\tkl:   25.408791\n",
      "====> Epoch: 447 Average train loss: 93.9727\n",
      "====> Validation set loss: 97.3163\n",
      "====> Validation set kl: 25.1526\n",
      "Epoch: 448 [  100/50000 ( 0%)]  \tLoss:   94.276016\trec:   69.213951\tkl:   25.062057\n",
      "Epoch: 448 [10100/50000 (20%)]  \tLoss:   98.046051\trec:   72.264381\tkl:   25.781670\n",
      "Epoch: 448 [20100/50000 (40%)]  \tLoss:   93.737793\trec:   67.465858\tkl:   26.271931\n",
      "Epoch: 448 [30100/50000 (60%)]  \tLoss:   90.517982\trec:   66.442482\tkl:   24.075510\n",
      "Epoch: 448 [40100/50000 (80%)]  \tLoss:   94.128433\trec:   69.368805\tkl:   24.759634\n",
      "====> Epoch: 448 Average train loss: 94.1095\n",
      "====> Validation set loss: 96.3690\n",
      "====> Validation set kl: 25.4336\n",
      "Epoch: 449 [  100/50000 ( 0%)]  \tLoss:   93.123856\trec:   66.902214\tkl:   26.221642\n",
      "Epoch: 449 [10100/50000 (20%)]  \tLoss:   92.694374\trec:   68.208076\tkl:   24.486288\n",
      "Epoch: 449 [20100/50000 (40%)]  \tLoss:   92.577675\trec:   67.719864\tkl:   24.857813\n",
      "Epoch: 449 [30100/50000 (60%)]  \tLoss:   93.461952\trec:   67.578445\tkl:   25.883507\n",
      "Epoch: 449 [40100/50000 (80%)]  \tLoss:   88.448013\trec:   64.011597\tkl:   24.436419\n",
      "====> Epoch: 449 Average train loss: 93.7287\n",
      "====> Validation set loss: 96.3000\n",
      "====> Validation set kl: 25.8133\n",
      "Epoch: 450 [  100/50000 ( 0%)]  \tLoss:   90.424210\trec:   64.739517\tkl:   25.684685\n",
      "Epoch: 450 [10100/50000 (20%)]  \tLoss:   94.649178\trec:   68.821579\tkl:   25.827595\n",
      "Epoch: 450 [20100/50000 (40%)]  \tLoss:   95.086128\trec:   69.067810\tkl:   26.018322\n",
      "Epoch: 450 [30100/50000 (60%)]  \tLoss:   94.444832\trec:   68.636505\tkl:   25.808327\n",
      "Epoch: 450 [40100/50000 (80%)]  \tLoss:   93.565445\trec:   67.391724\tkl:   26.173729\n",
      "====> Epoch: 450 Average train loss: 94.1469\n",
      "====> Validation set loss: 96.3688\n",
      "====> Validation set kl: 25.4885\n",
      "Epoch: 451 [  100/50000 ( 0%)]  \tLoss:   92.108513\trec:   67.068626\tkl:   25.039883\n",
      "Epoch: 451 [10100/50000 (20%)]  \tLoss:   95.306084\trec:   67.710312\tkl:   27.595774\n",
      "Epoch: 451 [20100/50000 (40%)]  \tLoss:   97.293617\trec:   71.149315\tkl:   26.144308\n",
      "Epoch: 451 [30100/50000 (60%)]  \tLoss:   89.924591\trec:   64.449287\tkl:   25.475294\n",
      "Epoch: 451 [40100/50000 (80%)]  \tLoss:   96.671356\trec:   70.105743\tkl:   26.565613\n",
      "====> Epoch: 451 Average train loss: 94.5794\n",
      "====> Validation set loss: 96.2322\n",
      "====> Validation set kl: 25.5320\n",
      "Epoch: 452 [  100/50000 ( 0%)]  \tLoss:   96.032303\trec:   69.709717\tkl:   26.322592\n",
      "Epoch: 452 [10100/50000 (20%)]  \tLoss:   99.552017\trec:   73.295074\tkl:   26.256941\n",
      "Epoch: 452 [20100/50000 (40%)]  \tLoss:   93.093925\trec:   67.225708\tkl:   25.868208\n",
      "Epoch: 452 [30100/50000 (60%)]  \tLoss:   93.365425\trec:   67.593582\tkl:   25.771847\n",
      "Epoch: 452 [40100/50000 (80%)]  \tLoss:   95.083183\trec:   69.279259\tkl:   25.803923\n",
      "====> Epoch: 452 Average train loss: 94.4696\n",
      "====> Validation set loss: 96.9062\n",
      "====> Validation set kl: 25.9149\n",
      "Epoch: 453 [  100/50000 ( 0%)]  \tLoss:   92.290443\trec:   67.161530\tkl:   25.128918\n",
      "Epoch: 453 [10100/50000 (20%)]  \tLoss:   92.745331\trec:   67.649956\tkl:   25.095373\n",
      "Epoch: 453 [20100/50000 (40%)]  \tLoss:   90.599731\trec:   65.065201\tkl:   25.534529\n",
      "Epoch: 453 [30100/50000 (60%)]  \tLoss:   92.525093\trec:   66.678421\tkl:   25.846670\n",
      "Epoch: 453 [40100/50000 (80%)]  \tLoss:   96.785934\trec:   71.271492\tkl:   25.514441\n",
      "====> Epoch: 453 Average train loss: 94.0660\n",
      "====> Validation set loss: 96.5950\n",
      "====> Validation set kl: 25.5259\n",
      "Epoch: 454 [  100/50000 ( 0%)]  \tLoss:   88.807175\trec:   63.961338\tkl:   24.845839\n",
      "Epoch: 454 [10100/50000 (20%)]  \tLoss:   98.251999\trec:   73.666359\tkl:   24.585644\n",
      "Epoch: 454 [20100/50000 (40%)]  \tLoss:   96.025620\trec:   69.897011\tkl:   26.128613\n",
      "Epoch: 454 [30100/50000 (60%)]  \tLoss:   96.561813\trec:   70.543449\tkl:   26.018372\n",
      "Epoch: 454 [40100/50000 (80%)]  \tLoss:   92.981674\trec:   68.002937\tkl:   24.978745\n",
      "====> Epoch: 454 Average train loss: 94.5455\n",
      "====> Validation set loss: 96.9622\n",
      "====> Validation set kl: 26.1363\n",
      "Epoch: 455 [  100/50000 ( 0%)]  \tLoss:   92.843643\trec:   66.849709\tkl:   25.993937\n",
      "Epoch: 455 [10100/50000 (20%)]  \tLoss:   90.399605\trec:   65.557999\tkl:   24.841606\n",
      "Epoch: 455 [20100/50000 (40%)]  \tLoss:   98.049347\trec:   67.009377\tkl:   31.039963\n",
      "Epoch: 455 [30100/50000 (60%)]  \tLoss:  100.538651\trec:   73.396416\tkl:   27.142225\n",
      "Epoch: 455 [40100/50000 (80%)]  \tLoss:   89.828003\trec:   64.365997\tkl:   25.462015\n",
      "====> Epoch: 455 Average train loss: 95.0833\n",
      "====> Validation set loss: 97.1355\n",
      "====> Validation set kl: 25.0591\n",
      "Epoch: 456 [  100/50000 ( 0%)]  \tLoss:   96.748535\trec:   71.205078\tkl:   25.543447\n",
      "Epoch: 456 [10100/50000 (20%)]  \tLoss:   92.278214\trec:   66.965118\tkl:   25.313093\n",
      "Epoch: 456 [20100/50000 (40%)]  \tLoss:   94.443085\trec:   68.365967\tkl:   26.077108\n",
      "Epoch: 456 [30100/50000 (60%)]  \tLoss:   96.925056\trec:   71.164413\tkl:   25.760641\n",
      "Epoch: 456 [40100/50000 (80%)]  \tLoss:   94.946060\trec:   69.575867\tkl:   25.370195\n",
      "====> Epoch: 456 Average train loss: 94.0069\n",
      "====> Validation set loss: 96.3700\n",
      "====> Validation set kl: 25.2511\n",
      "Epoch: 457 [  100/50000 ( 0%)]  \tLoss:   91.775116\trec:   66.712715\tkl:   25.062395\n",
      "Epoch: 457 [10100/50000 (20%)]  \tLoss:   97.624168\trec:   70.527931\tkl:   27.096239\n",
      "Epoch: 457 [20100/50000 (40%)]  \tLoss:   97.453270\trec:   71.486351\tkl:   25.966915\n",
      "Epoch: 457 [30100/50000 (60%)]  \tLoss:   95.241943\trec:   70.105164\tkl:   25.136774\n",
      "Epoch: 457 [40100/50000 (80%)]  \tLoss:   91.396461\trec:   65.042946\tkl:   26.353519\n",
      "====> Epoch: 457 Average train loss: 94.2078\n",
      "====> Validation set loss: 97.6917\n",
      "====> Validation set kl: 26.3768\n",
      "Epoch: 458 [  100/50000 ( 0%)]  \tLoss:   93.637566\trec:   67.477501\tkl:   26.160070\n",
      "Epoch: 458 [10100/50000 (20%)]  \tLoss:   95.671440\trec:   70.041512\tkl:   25.629930\n",
      "Epoch: 458 [20100/50000 (40%)]  \tLoss:   94.909500\trec:   69.132698\tkl:   25.776802\n",
      "Epoch: 458 [30100/50000 (60%)]  \tLoss:   93.124763\trec:   68.041626\tkl:   25.083138\n",
      "Epoch: 458 [40100/50000 (80%)]  \tLoss:   99.446075\trec:   71.121452\tkl:   28.324619\n",
      "====> Epoch: 458 Average train loss: 94.3144\n",
      "====> Validation set loss: 96.5356\n",
      "====> Validation set kl: 25.4473\n",
      "Epoch: 459 [  100/50000 ( 0%)]  \tLoss:   96.976982\trec:   70.683784\tkl:   26.293198\n",
      "Epoch: 459 [10100/50000 (20%)]  \tLoss:   99.271271\trec:   73.039688\tkl:   26.231581\n",
      "Epoch: 459 [20100/50000 (40%)]  \tLoss:   98.035210\trec:   71.943031\tkl:   26.092182\n",
      "Epoch: 459 [30100/50000 (60%)]  \tLoss:   94.195641\trec:   67.936401\tkl:   26.259239\n",
      "Epoch: 459 [40100/50000 (80%)]  \tLoss:   93.833473\trec:   68.528549\tkl:   25.304926\n",
      "====> Epoch: 459 Average train loss: 93.8697\n",
      "====> Validation set loss: 96.0712\n",
      "====> Validation set kl: 25.6268\n",
      "Epoch: 460 [  100/50000 ( 0%)]  \tLoss:   95.217361\trec:   69.162292\tkl:   26.055073\n",
      "Epoch: 460 [10100/50000 (20%)]  \tLoss:   90.230408\trec:   65.102715\tkl:   25.127693\n",
      "Epoch: 460 [20100/50000 (40%)]  \tLoss:   93.810524\trec:   67.719383\tkl:   26.091137\n",
      "Epoch: 460 [30100/50000 (60%)]  \tLoss:   92.593887\trec:   66.388649\tkl:   26.205231\n",
      "Epoch: 460 [40100/50000 (80%)]  \tLoss:   94.982750\trec:   69.984703\tkl:   24.998047\n",
      "====> Epoch: 460 Average train loss: 93.8192\n",
      "====> Validation set loss: 96.7977\n",
      "====> Validation set kl: 25.2146\n",
      "Epoch: 461 [  100/50000 ( 0%)]  \tLoss:   94.658386\trec:   69.103706\tkl:   25.554678\n",
      "Epoch: 461 [10100/50000 (20%)]  \tLoss:   90.832695\trec:   65.883995\tkl:   24.948696\n",
      "Epoch: 461 [20100/50000 (40%)]  \tLoss:   92.618980\trec:   67.792053\tkl:   24.826933\n",
      "Epoch: 461 [30100/50000 (60%)]  \tLoss:   91.928154\trec:   67.612732\tkl:   24.315416\n",
      "Epoch: 461 [40100/50000 (80%)]  \tLoss:   96.062950\trec:   70.201546\tkl:   25.861401\n",
      "====> Epoch: 461 Average train loss: 94.0630\n",
      "====> Validation set loss: 96.6001\n",
      "====> Validation set kl: 26.2225\n",
      "Epoch: 462 [  100/50000 ( 0%)]  \tLoss:   93.596725\trec:   67.507530\tkl:   26.089199\n",
      "Epoch: 462 [10100/50000 (20%)]  \tLoss:   93.023613\trec:   67.813904\tkl:   25.209707\n",
      "Epoch: 462 [20100/50000 (40%)]  \tLoss:   98.514862\trec:   72.434532\tkl:   26.080330\n",
      "Epoch: 462 [30100/50000 (60%)]  \tLoss:   93.286270\trec:   67.623222\tkl:   25.663052\n",
      "Epoch: 462 [40100/50000 (80%)]  \tLoss:   93.009140\trec:   66.878891\tkl:   26.130245\n",
      "====> Epoch: 462 Average train loss: 93.7011\n",
      "====> Validation set loss: 95.9083\n",
      "====> Validation set kl: 25.6017\n",
      "Epoch: 463 [  100/50000 ( 0%)]  \tLoss:   97.239029\trec:   70.678757\tkl:   26.560278\n",
      "Epoch: 463 [10100/50000 (20%)]  \tLoss:   94.295547\trec:   68.321831\tkl:   25.973711\n",
      "Epoch: 463 [20100/50000 (40%)]  \tLoss:   90.767090\trec:   65.382545\tkl:   25.384548\n",
      "Epoch: 463 [30100/50000 (60%)]  \tLoss:   92.944626\trec:   67.793259\tkl:   25.151375\n",
      "Epoch: 463 [40100/50000 (80%)]  \tLoss:   94.839157\trec:   69.293571\tkl:   25.545589\n",
      "====> Epoch: 463 Average train loss: 93.6636\n",
      "====> Validation set loss: 96.4918\n",
      "====> Validation set kl: 25.5059\n",
      "Epoch: 464 [  100/50000 ( 0%)]  \tLoss:   92.752831\trec:   67.059196\tkl:   25.693632\n",
      "Epoch: 464 [10100/50000 (20%)]  \tLoss:   96.023872\trec:   70.077827\tkl:   25.946041\n",
      "Epoch: 464 [20100/50000 (40%)]  \tLoss:   96.306999\trec:   70.673073\tkl:   25.633924\n",
      "Epoch: 464 [30100/50000 (60%)]  \tLoss:   90.892052\trec:   65.137123\tkl:   25.754930\n",
      "Epoch: 464 [40100/50000 (80%)]  \tLoss:   90.987495\trec:   66.406227\tkl:   24.581274\n",
      "====> Epoch: 464 Average train loss: 93.7103\n",
      "====> Validation set loss: 97.0230\n",
      "====> Validation set kl: 26.2289\n",
      "Epoch: 465 [  100/50000 ( 0%)]  \tLoss:   95.872765\trec:   69.256416\tkl:   26.616343\n",
      "Epoch: 465 [10100/50000 (20%)]  \tLoss:   92.324860\trec:   67.224762\tkl:   25.100094\n",
      "Epoch: 465 [20100/50000 (40%)]  \tLoss:   91.298195\trec:   66.071541\tkl:   25.226652\n",
      "Epoch: 465 [30100/50000 (60%)]  \tLoss:   99.914322\trec:   72.982964\tkl:   26.931355\n",
      "Epoch: 465 [40100/50000 (80%)]  \tLoss:   91.929916\trec:   66.374001\tkl:   25.555916\n",
      "====> Epoch: 465 Average train loss: 93.9629\n",
      "====> Validation set loss: 96.2133\n",
      "====> Validation set kl: 25.6331\n",
      "Epoch: 466 [  100/50000 ( 0%)]  \tLoss:   90.813454\trec:   65.148766\tkl:   25.664692\n",
      "Epoch: 466 [10100/50000 (20%)]  \tLoss:   90.888672\trec:   64.947487\tkl:   25.941175\n",
      "Epoch: 466 [20100/50000 (40%)]  \tLoss:   92.815826\trec:   67.685539\tkl:   25.130289\n",
      "Epoch: 466 [30100/50000 (60%)]  \tLoss:   95.350327\trec:   70.028877\tkl:   25.321457\n",
      "Epoch: 466 [40100/50000 (80%)]  \tLoss:   94.523514\trec:   68.643570\tkl:   25.879946\n",
      "====> Epoch: 466 Average train loss: 93.6052\n",
      "====> Validation set loss: 96.8933\n",
      "====> Validation set kl: 26.3718\n",
      "Epoch: 467 [  100/50000 ( 0%)]  \tLoss:   91.364975\trec:   64.947647\tkl:   26.417332\n",
      "Epoch: 467 [10100/50000 (20%)]  \tLoss:   97.232231\trec:   71.332664\tkl:   25.899567\n",
      "Epoch: 467 [20100/50000 (40%)]  \tLoss:   97.430580\trec:   70.957626\tkl:   26.472961\n",
      "Epoch: 467 [30100/50000 (60%)]  \tLoss:   90.803261\trec:   65.189468\tkl:   25.613789\n",
      "Epoch: 467 [40100/50000 (80%)]  \tLoss:   94.733803\trec:   69.337204\tkl:   25.396603\n",
      "====> Epoch: 467 Average train loss: 94.3442\n",
      "====> Validation set loss: 96.6035\n",
      "====> Validation set kl: 25.4956\n",
      "Epoch: 468 [  100/50000 ( 0%)]  \tLoss:   91.206993\trec:   66.449684\tkl:   24.757303\n",
      "Epoch: 468 [10100/50000 (20%)]  \tLoss:   94.731216\trec:   69.198906\tkl:   25.532314\n",
      "Epoch: 468 [20100/50000 (40%)]  \tLoss:   90.624352\trec:   65.668121\tkl:   24.956234\n",
      "Epoch: 468 [30100/50000 (60%)]  \tLoss:   95.141327\trec:   69.369240\tkl:   25.772085\n",
      "Epoch: 468 [40100/50000 (80%)]  \tLoss:   97.707115\trec:   71.565948\tkl:   26.141167\n",
      "====> Epoch: 468 Average train loss: 93.9763\n",
      "====> Validation set loss: 96.1977\n",
      "====> Validation set kl: 25.6872\n",
      "Epoch: 469 [  100/50000 ( 0%)]  \tLoss:   99.930504\trec:   72.923943\tkl:   27.006559\n",
      "Epoch: 469 [10100/50000 (20%)]  \tLoss:   91.713150\trec:   66.245857\tkl:   25.467297\n",
      "Epoch: 469 [20100/50000 (40%)]  \tLoss:   89.824623\trec:   65.398399\tkl:   24.426226\n",
      "Epoch: 469 [30100/50000 (60%)]  \tLoss:   98.980019\trec:   71.626701\tkl:   27.353323\n",
      "Epoch: 469 [40100/50000 (80%)]  \tLoss:   96.088036\trec:   69.081665\tkl:   27.006371\n",
      "====> Epoch: 469 Average train loss: 94.1230\n",
      "====> Validation set loss: 97.2475\n",
      "====> Validation set kl: 26.2108\n",
      "Epoch: 470 [  100/50000 ( 0%)]  \tLoss:   91.501152\trec:   65.672997\tkl:   25.828156\n",
      "Epoch: 470 [10100/50000 (20%)]  \tLoss:   94.413857\trec:   68.123589\tkl:   26.290260\n",
      "Epoch: 470 [20100/50000 (40%)]  \tLoss:   91.161095\trec:   65.286674\tkl:   25.874409\n",
      "Epoch: 470 [30100/50000 (60%)]  \tLoss:   94.525078\trec:   68.615845\tkl:   25.909227\n",
      "Epoch: 470 [40100/50000 (80%)]  \tLoss:   90.387466\trec:   65.088547\tkl:   25.298923\n",
      "====> Epoch: 470 Average train loss: 94.0419\n",
      "====> Validation set loss: 96.8962\n",
      "====> Validation set kl: 26.2190\n",
      "Epoch: 471 [  100/50000 ( 0%)]  \tLoss:   95.617126\trec:   68.429840\tkl:   27.187290\n",
      "Epoch: 471 [10100/50000 (20%)]  \tLoss:   92.621315\trec:   67.722778\tkl:   24.898533\n",
      "Epoch: 471 [20100/50000 (40%)]  \tLoss:   93.943886\trec:   68.338654\tkl:   25.605232\n",
      "Epoch: 471 [30100/50000 (60%)]  \tLoss:  100.783363\trec:   76.480766\tkl:   24.302595\n",
      "Epoch: 471 [40100/50000 (80%)]  \tLoss:   94.839981\trec:   68.680099\tkl:   26.159887\n",
      "====> Epoch: 471 Average train loss: 94.2492\n",
      "====> Validation set loss: 97.9113\n",
      "====> Validation set kl: 26.7441\n",
      "Epoch: 472 [  100/50000 ( 0%)]  \tLoss:   94.963142\trec:   67.877129\tkl:   27.086016\n",
      "Epoch: 472 [10100/50000 (20%)]  \tLoss:   92.672173\trec:   68.150581\tkl:   24.521593\n",
      "Epoch: 472 [20100/50000 (40%)]  \tLoss:   95.277748\trec:   68.889503\tkl:   26.388247\n",
      "Epoch: 472 [30100/50000 (60%)]  \tLoss:   92.889076\trec:   67.392830\tkl:   25.496254\n",
      "Epoch: 472 [40100/50000 (80%)]  \tLoss:   93.601791\trec:   67.477753\tkl:   26.124037\n",
      "====> Epoch: 472 Average train loss: 94.3006\n",
      "====> Validation set loss: 96.5664\n",
      "====> Validation set kl: 25.8101\n",
      "Epoch: 473 [  100/50000 ( 0%)]  \tLoss:   96.674187\trec:   70.227623\tkl:   26.446560\n",
      "Epoch: 473 [10100/50000 (20%)]  \tLoss:   97.433907\trec:   70.873650\tkl:   26.560255\n",
      "Epoch: 473 [20100/50000 (40%)]  \tLoss:   93.515106\trec:   67.640282\tkl:   25.874825\n",
      "Epoch: 473 [30100/50000 (60%)]  \tLoss:   94.912094\trec:   69.983414\tkl:   24.928688\n",
      "Epoch: 473 [40100/50000 (80%)]  \tLoss:   96.873024\trec:   71.015480\tkl:   25.857544\n",
      "====> Epoch: 473 Average train loss: 94.3757\n",
      "====> Validation set loss: 96.3373\n",
      "====> Validation set kl: 25.5179\n",
      "Epoch: 474 [  100/50000 ( 0%)]  \tLoss:   97.284294\trec:   70.853569\tkl:   26.430725\n",
      "Epoch: 474 [10100/50000 (20%)]  \tLoss:   94.815132\trec:   68.398781\tkl:   26.416351\n",
      "Epoch: 474 [20100/50000 (40%)]  \tLoss:   90.217598\trec:   65.235558\tkl:   24.982035\n",
      "Epoch: 474 [30100/50000 (60%)]  \tLoss:   95.949959\trec:   70.650391\tkl:   25.299568\n",
      "Epoch: 474 [40100/50000 (80%)]  \tLoss:   94.159843\trec:   67.993896\tkl:   26.165941\n",
      "====> Epoch: 474 Average train loss: 93.7223\n",
      "====> Validation set loss: 96.6060\n",
      "====> Validation set kl: 25.3591\n",
      "Epoch: 475 [  100/50000 ( 0%)]  \tLoss:   91.752960\trec:   66.223953\tkl:   25.529001\n",
      "Epoch: 475 [10100/50000 (20%)]  \tLoss:   97.103424\trec:   72.596748\tkl:   24.506681\n",
      "Epoch: 475 [20100/50000 (40%)]  \tLoss:   93.022980\trec:   67.376534\tkl:   25.646446\n",
      "Epoch: 475 [30100/50000 (60%)]  \tLoss:   94.944969\trec:   69.090096\tkl:   25.854870\n",
      "Epoch: 475 [40100/50000 (80%)]  \tLoss:   93.517944\trec:   68.522118\tkl:   24.995829\n",
      "====> Epoch: 475 Average train loss: 93.9146\n",
      "====> Validation set loss: 96.2913\n",
      "====> Validation set kl: 25.4765\n",
      "Epoch: 476 [  100/50000 ( 0%)]  \tLoss:   92.947052\trec:   67.213196\tkl:   25.733849\n",
      "Epoch: 476 [10100/50000 (20%)]  \tLoss:   91.768570\trec:   66.886177\tkl:   24.882387\n",
      "Epoch: 476 [20100/50000 (40%)]  \tLoss:   93.403885\trec:   67.199829\tkl:   26.204050\n",
      "Epoch: 476 [30100/50000 (60%)]  \tLoss:   98.832520\trec:   72.332703\tkl:   26.499819\n",
      "Epoch: 476 [40100/50000 (80%)]  \tLoss:   89.480347\trec:   64.691368\tkl:   24.788980\n",
      "====> Epoch: 476 Average train loss: 93.7982\n",
      "====> Validation set loss: 96.4346\n",
      "====> Validation set kl: 25.9198\n",
      "Epoch: 477 [  100/50000 ( 0%)]  \tLoss:   91.903526\trec:   67.165627\tkl:   24.737898\n",
      "Epoch: 477 [10100/50000 (20%)]  \tLoss:   94.530273\trec:   68.581192\tkl:   25.949081\n",
      "Epoch: 477 [20100/50000 (40%)]  \tLoss:   97.444168\trec:   71.303886\tkl:   26.140285\n",
      "Epoch: 477 [30100/50000 (60%)]  \tLoss:   94.546715\trec:   69.521294\tkl:   25.025425\n",
      "Epoch: 477 [40100/50000 (80%)]  \tLoss:   89.285255\trec:   63.950493\tkl:   25.334764\n",
      "====> Epoch: 477 Average train loss: 93.8392\n",
      "====> Validation set loss: 96.2129\n",
      "====> Validation set kl: 25.9217\n",
      "Epoch: 478 [  100/50000 ( 0%)]  \tLoss:   95.496635\trec:   69.314751\tkl:   26.181881\n",
      "Epoch: 478 [10100/50000 (20%)]  \tLoss:   92.563484\trec:   67.879730\tkl:   24.683754\n",
      "Epoch: 478 [20100/50000 (40%)]  \tLoss:   90.998619\trec:   65.708458\tkl:   25.290163\n",
      "Epoch: 478 [30100/50000 (60%)]  \tLoss:   96.621216\trec:   70.655167\tkl:   25.966055\n",
      "Epoch: 478 [40100/50000 (80%)]  \tLoss:   91.477905\trec:   65.095428\tkl:   26.382475\n",
      "====> Epoch: 478 Average train loss: 94.0539\n",
      "====> Validation set loss: 96.3985\n",
      "====> Validation set kl: 25.5177\n",
      "Epoch: 479 [  100/50000 ( 0%)]  \tLoss:   92.007614\trec:   65.938026\tkl:   26.069592\n",
      "Epoch: 479 [10100/50000 (20%)]  \tLoss:   94.880112\trec:   69.420509\tkl:   25.459604\n",
      "Epoch: 479 [20100/50000 (40%)]  \tLoss:   95.992714\trec:   69.128975\tkl:   26.863743\n",
      "Epoch: 479 [30100/50000 (60%)]  \tLoss:   93.457130\trec:   68.580643\tkl:   24.876484\n",
      "Epoch: 479 [40100/50000 (80%)]  \tLoss:   88.040977\trec:   63.503681\tkl:   24.537294\n",
      "====> Epoch: 479 Average train loss: 93.7282\n",
      "====> Validation set loss: 96.8858\n",
      "====> Validation set kl: 25.3925\n",
      "Epoch: 480 [  100/50000 ( 0%)]  \tLoss:   92.187408\trec:   66.843361\tkl:   25.344049\n",
      "Epoch: 480 [10100/50000 (20%)]  \tLoss:   90.356987\trec:   65.163918\tkl:   25.193081\n",
      "Epoch: 480 [20100/50000 (40%)]  \tLoss:   96.106384\trec:   69.881851\tkl:   26.224533\n",
      "Epoch: 480 [30100/50000 (60%)]  \tLoss:   96.539726\trec:   71.560417\tkl:   24.979305\n",
      "Epoch: 480 [40100/50000 (80%)]  \tLoss:   92.929428\trec:   67.347244\tkl:   25.582191\n",
      "====> Epoch: 480 Average train loss: 94.0432\n",
      "====> Validation set loss: 96.2064\n",
      "====> Validation set kl: 25.8196\n",
      "Epoch: 481 [  100/50000 ( 0%)]  \tLoss:   90.324570\trec:   65.078789\tkl:   25.245771\n",
      "Epoch: 481 [10100/50000 (20%)]  \tLoss:   96.479523\trec:   70.380066\tkl:   26.099453\n",
      "Epoch: 481 [20100/50000 (40%)]  \tLoss:   91.179306\trec:   66.241219\tkl:   24.938086\n",
      "Epoch: 481 [30100/50000 (60%)]  \tLoss:   94.236374\trec:   67.855438\tkl:   26.380934\n",
      "Epoch: 481 [40100/50000 (80%)]  \tLoss:   93.382599\trec:   68.209091\tkl:   25.173506\n",
      "====> Epoch: 481 Average train loss: 93.8605\n",
      "====> Validation set loss: 96.4156\n",
      "====> Validation set kl: 25.7016\n",
      "Epoch: 482 [  100/50000 ( 0%)]  \tLoss:   94.880508\trec:   69.253281\tkl:   25.627226\n",
      "Epoch: 482 [10100/50000 (20%)]  \tLoss:   98.835876\trec:   72.346069\tkl:   26.489809\n",
      "Epoch: 482 [20100/50000 (40%)]  \tLoss:   98.190460\trec:   71.047707\tkl:   27.142756\n",
      "Epoch: 482 [30100/50000 (60%)]  \tLoss:   93.937508\trec:   68.619781\tkl:   25.317726\n",
      "Epoch: 482 [40100/50000 (80%)]  \tLoss:   89.571014\trec:   64.453171\tkl:   25.117851\n",
      "====> Epoch: 482 Average train loss: 93.9191\n",
      "====> Validation set loss: 96.5016\n",
      "====> Validation set kl: 25.8165\n",
      "Epoch: 483 [  100/50000 ( 0%)]  \tLoss:   94.061325\trec:   68.073029\tkl:   25.988304\n",
      "Epoch: 483 [10100/50000 (20%)]  \tLoss:   93.967400\trec:   68.038895\tkl:   25.928501\n",
      "Epoch: 483 [20100/50000 (40%)]  \tLoss:   90.285995\trec:   64.949631\tkl:   25.336372\n",
      "Epoch: 483 [30100/50000 (60%)]  \tLoss:   92.967384\trec:   68.030739\tkl:   24.936640\n",
      "Epoch: 483 [40100/50000 (80%)]  \tLoss:   92.422623\trec:   66.205559\tkl:   26.217064\n",
      "====> Epoch: 483 Average train loss: 93.9405\n",
      "====> Validation set loss: 96.2994\n",
      "====> Validation set kl: 25.6540\n",
      "Epoch: 484 [  100/50000 ( 0%)]  \tLoss:   93.824081\trec:   68.194946\tkl:   25.629133\n",
      "Epoch: 484 [10100/50000 (20%)]  \tLoss:   93.095879\trec:   68.053162\tkl:   25.042709\n",
      "Epoch: 484 [20100/50000 (40%)]  \tLoss:   93.228516\trec:   68.586105\tkl:   24.642406\n",
      "Epoch: 484 [30100/50000 (60%)]  \tLoss:   94.594513\trec:   68.616386\tkl:   25.978125\n",
      "Epoch: 484 [40100/50000 (80%)]  \tLoss:   94.870277\trec:   68.829033\tkl:   26.041250\n",
      "====> Epoch: 484 Average train loss: 93.7364\n",
      "====> Validation set loss: 96.0715\n",
      "====> Validation set kl: 25.5350\n",
      "Epoch: 485 [  100/50000 ( 0%)]  \tLoss:   91.066795\trec:   65.525711\tkl:   25.541086\n",
      "Epoch: 485 [10100/50000 (20%)]  \tLoss:   93.629723\trec:   68.113312\tkl:   25.516420\n",
      "Epoch: 485 [20100/50000 (40%)]  \tLoss:   92.509552\trec:   67.003319\tkl:   25.506235\n",
      "Epoch: 485 [30100/50000 (60%)]  \tLoss:   93.496391\trec:   67.275108\tkl:   26.221287\n",
      "Epoch: 485 [40100/50000 (80%)]  \tLoss:   96.477432\trec:   70.140877\tkl:   26.336550\n",
      "====> Epoch: 485 Average train loss: 94.0804\n",
      "====> Validation set loss: 96.3394\n",
      "====> Validation set kl: 25.7338\n",
      "Epoch: 486 [  100/50000 ( 0%)]  \tLoss:   87.843727\trec:   62.863472\tkl:   24.980261\n",
      "Epoch: 486 [10100/50000 (20%)]  \tLoss:   93.774803\trec:   68.497917\tkl:   25.276890\n",
      "Epoch: 486 [20100/50000 (40%)]  \tLoss:   93.037216\trec:   67.906708\tkl:   25.130507\n",
      "Epoch: 486 [30100/50000 (60%)]  \tLoss:   93.457603\trec:   68.088089\tkl:   25.369516\n",
      "Epoch: 486 [40100/50000 (80%)]  \tLoss:   90.847809\trec:   66.155418\tkl:   24.692396\n",
      "====> Epoch: 486 Average train loss: 94.3462\n",
      "====> Validation set loss: 96.3180\n",
      "====> Validation set kl: 25.6416\n",
      "Epoch: 487 [  100/50000 ( 0%)]  \tLoss:   93.433472\trec:   68.050690\tkl:   25.382788\n",
      "Epoch: 487 [10100/50000 (20%)]  \tLoss:   94.249939\trec:   67.072266\tkl:   27.177677\n",
      "Epoch: 487 [20100/50000 (40%)]  \tLoss:   97.216408\trec:   71.944527\tkl:   25.271872\n",
      "Epoch: 487 [30100/50000 (60%)]  \tLoss:   89.597076\trec:   64.584839\tkl:   25.012238\n",
      "Epoch: 487 [40100/50000 (80%)]  \tLoss:   92.683281\trec:   66.657478\tkl:   26.025795\n",
      "====> Epoch: 487 Average train loss: 93.7006\n",
      "====> Validation set loss: 96.5293\n",
      "====> Validation set kl: 25.8522\n",
      "Epoch: 488 [  100/50000 ( 0%)]  \tLoss:   91.529579\trec:   67.115547\tkl:   24.414030\n",
      "Epoch: 488 [10100/50000 (20%)]  \tLoss:   91.897285\trec:   66.415764\tkl:   25.481524\n",
      "Epoch: 488 [20100/50000 (40%)]  \tLoss:   91.909981\trec:   66.811569\tkl:   25.098410\n",
      "Epoch: 488 [30100/50000 (60%)]  \tLoss:   93.372910\trec:   68.262039\tkl:   25.110874\n",
      "Epoch: 488 [40100/50000 (80%)]  \tLoss:   94.966522\trec:   68.633286\tkl:   26.333237\n",
      "====> Epoch: 488 Average train loss: 94.0948\n",
      "====> Validation set loss: 96.1671\n",
      "====> Validation set kl: 25.3845\n",
      "Epoch: 489 [  100/50000 ( 0%)]  \tLoss:   92.620270\trec:   67.867149\tkl:   24.753115\n",
      "Epoch: 489 [10100/50000 (20%)]  \tLoss:   99.837097\trec:   73.166855\tkl:   26.670246\n",
      "Epoch: 489 [20100/50000 (40%)]  \tLoss:   96.007401\trec:   69.278419\tkl:   26.728987\n",
      "Epoch: 489 [30100/50000 (60%)]  \tLoss:   94.105164\trec:   69.194946\tkl:   24.910217\n",
      "Epoch: 489 [40100/50000 (80%)]  \tLoss:   93.968971\trec:   67.614540\tkl:   26.354433\n",
      "====> Epoch: 489 Average train loss: 94.5436\n",
      "====> Validation set loss: 96.8619\n",
      "====> Validation set kl: 25.9512\n",
      "Epoch: 490 [  100/50000 ( 0%)]  \tLoss:   92.916512\trec:   67.383514\tkl:   25.532999\n",
      "Epoch: 490 [10100/50000 (20%)]  \tLoss:   97.533180\trec:   70.256943\tkl:   27.276236\n",
      "Epoch: 490 [20100/50000 (40%)]  \tLoss:   94.678925\trec:   68.311111\tkl:   26.367809\n",
      "Epoch: 490 [30100/50000 (60%)]  \tLoss:   92.857399\trec:   67.629013\tkl:   25.228386\n",
      "Epoch: 490 [40100/50000 (80%)]  \tLoss:   94.802406\trec:   69.137352\tkl:   25.665060\n",
      "====> Epoch: 490 Average train loss: 94.0889\n",
      "====> Validation set loss: 96.3065\n",
      "====> Validation set kl: 25.3165\n",
      "Epoch: 491 [  100/50000 ( 0%)]  \tLoss:   94.435699\trec:   69.180412\tkl:   25.255297\n",
      "Epoch: 491 [10100/50000 (20%)]  \tLoss:   94.411385\trec:   68.662918\tkl:   25.748470\n",
      "Epoch: 491 [20100/50000 (40%)]  \tLoss:   96.712502\trec:   70.966286\tkl:   25.746220\n",
      "Epoch: 491 [30100/50000 (60%)]  \tLoss:   93.134018\trec:   67.592987\tkl:   25.541035\n",
      "Epoch: 491 [40100/50000 (80%)]  \tLoss:   91.685158\trec:   64.342865\tkl:   27.342293\n",
      "====> Epoch: 491 Average train loss: 93.8782\n",
      "====> Validation set loss: 96.8462\n",
      "====> Validation set kl: 26.3477\n",
      "Epoch: 492 [  100/50000 ( 0%)]  \tLoss:   93.892128\trec:   67.392868\tkl:   26.499260\n",
      "Epoch: 492 [10100/50000 (20%)]  \tLoss:   93.745308\trec:   68.145065\tkl:   25.600239\n",
      "Epoch: 492 [20100/50000 (40%)]  \tLoss:   94.470848\trec:   68.419106\tkl:   26.051743\n",
      "Epoch: 492 [30100/50000 (60%)]  \tLoss:   96.220993\trec:   70.465187\tkl:   25.755800\n",
      "Epoch: 492 [40100/50000 (80%)]  \tLoss:   90.431129\trec:   64.616226\tkl:   25.814909\n",
      "====> Epoch: 492 Average train loss: 94.0106\n",
      "====> Validation set loss: 96.3924\n",
      "====> Validation set kl: 26.0064\n",
      "Epoch: 493 [  100/50000 ( 0%)]  \tLoss:   92.750992\trec:   66.169930\tkl:   26.581068\n",
      "Epoch: 493 [10100/50000 (20%)]  \tLoss:   90.814278\trec:   65.310509\tkl:   25.503771\n",
      "Epoch: 493 [20100/50000 (40%)]  \tLoss:   93.035484\trec:   67.542801\tkl:   25.492685\n",
      "Epoch: 493 [30100/50000 (60%)]  \tLoss:   95.559273\trec:   69.362534\tkl:   26.196745\n",
      "Epoch: 493 [40100/50000 (80%)]  \tLoss:   92.753311\trec:   67.528252\tkl:   25.225061\n",
      "====> Epoch: 493 Average train loss: 93.8133\n",
      "====> Validation set loss: 97.0697\n",
      "====> Validation set kl: 25.3513\n",
      "Epoch: 494 [  100/50000 ( 0%)]  \tLoss:   95.948837\trec:   69.886520\tkl:   26.062313\n",
      "Epoch: 494 [10100/50000 (20%)]  \tLoss:   91.141762\trec:   65.896561\tkl:   25.245203\n",
      "Epoch: 494 [20100/50000 (40%)]  \tLoss:   91.821121\trec:   65.869606\tkl:   25.951515\n",
      "Epoch: 494 [30100/50000 (60%)]  \tLoss:   92.187607\trec:   66.894699\tkl:   25.292910\n",
      "Epoch: 494 [40100/50000 (80%)]  \tLoss:   94.027412\trec:   68.456863\tkl:   25.570543\n",
      "====> Epoch: 494 Average train loss: 93.9265\n",
      "====> Validation set loss: 96.1463\n",
      "====> Validation set kl: 25.3931\n",
      "Epoch: 495 [  100/50000 ( 0%)]  \tLoss:   95.491814\trec:   70.184052\tkl:   25.307758\n",
      "Epoch: 495 [10100/50000 (20%)]  \tLoss:   93.891502\trec:   67.811470\tkl:   26.080038\n",
      "Epoch: 495 [20100/50000 (40%)]  \tLoss:   95.521873\trec:   70.009140\tkl:   25.512730\n",
      "Epoch: 495 [30100/50000 (60%)]  \tLoss:   90.878670\trec:   66.100159\tkl:   24.778507\n",
      "Epoch: 495 [40100/50000 (80%)]  \tLoss:   94.898201\trec:   69.416649\tkl:   25.481550\n",
      "====> Epoch: 495 Average train loss: 93.5972\n",
      "====> Validation set loss: 95.8832\n",
      "====> Validation set kl: 25.3359\n",
      "Epoch: 496 [  100/50000 ( 0%)]  \tLoss:   93.262909\trec:   68.260818\tkl:   25.002092\n",
      "Epoch: 496 [10100/50000 (20%)]  \tLoss:   93.799454\trec:   68.302147\tkl:   25.497309\n",
      "Epoch: 496 [20100/50000 (40%)]  \tLoss:   89.950592\trec:   65.351959\tkl:   24.598629\n",
      "Epoch: 496 [30100/50000 (60%)]  \tLoss:   92.943047\trec:   67.032959\tkl:   25.910089\n",
      "Epoch: 496 [40100/50000 (80%)]  \tLoss:   88.604744\trec:   63.424938\tkl:   25.179810\n",
      "====> Epoch: 496 Average train loss: 93.7366\n",
      "====> Validation set loss: 96.7779\n",
      "====> Validation set kl: 25.8297\n",
      "Epoch: 497 [  100/50000 ( 0%)]  \tLoss:   95.389511\trec:   70.293877\tkl:   25.095629\n",
      "Epoch: 497 [10100/50000 (20%)]  \tLoss:   91.566437\trec:   64.343178\tkl:   27.223259\n",
      "Epoch: 497 [20100/50000 (40%)]  \tLoss:   91.107750\trec:   65.484901\tkl:   25.622849\n",
      "Epoch: 497 [30100/50000 (60%)]  \tLoss:   92.488083\trec:   66.360123\tkl:   26.127966\n",
      "Epoch: 497 [40100/50000 (80%)]  \tLoss:   93.365913\trec:   67.527733\tkl:   25.838182\n",
      "====> Epoch: 497 Average train loss: 94.0762\n",
      "====> Validation set loss: 96.1893\n",
      "====> Validation set kl: 25.5397\n",
      "Epoch: 498 [  100/50000 ( 0%)]  \tLoss:   95.914558\trec:   70.788712\tkl:   25.125849\n",
      "Epoch: 498 [10100/50000 (20%)]  \tLoss:   91.174301\trec:   65.901138\tkl:   25.273170\n",
      "Epoch: 498 [20100/50000 (40%)]  \tLoss:   94.436470\trec:   68.860481\tkl:   25.575989\n",
      "Epoch: 498 [30100/50000 (60%)]  \tLoss:   94.092987\trec:   68.462517\tkl:   25.630468\n",
      "Epoch: 498 [40100/50000 (80%)]  \tLoss:   90.763504\trec:   66.153267\tkl:   24.610233\n",
      "====> Epoch: 498 Average train loss: 93.9233\n",
      "====> Validation set loss: 96.2715\n",
      "====> Validation set kl: 25.5990\n",
      "Epoch: 499 [  100/50000 ( 0%)]  \tLoss:   95.001747\trec:   69.430099\tkl:   25.571650\n",
      "Epoch: 499 [10100/50000 (20%)]  \tLoss:   92.520309\trec:   66.339943\tkl:   26.180368\n",
      "Epoch: 499 [20100/50000 (40%)]  \tLoss:   90.950050\trec:   66.816170\tkl:   24.133875\n",
      "Epoch: 499 [30100/50000 (60%)]  \tLoss:   92.978584\trec:   67.984756\tkl:   24.993826\n",
      "Epoch: 499 [40100/50000 (80%)]  \tLoss:   93.079239\trec:   67.892769\tkl:   25.186459\n",
      "====> Epoch: 499 Average train loss: 93.7941\n",
      "====> Validation set loss: 96.3957\n",
      "====> Validation set kl: 26.0211\n",
      "Epoch: 500 [  100/50000 ( 0%)]  \tLoss:   91.829018\trec:   66.280296\tkl:   25.548719\n",
      "Epoch: 500 [10100/50000 (20%)]  \tLoss:   94.901649\trec:   69.322227\tkl:   25.579426\n",
      "Epoch: 500 [20100/50000 (40%)]  \tLoss:   92.860962\trec:   67.335243\tkl:   25.525721\n",
      "Epoch: 500 [30100/50000 (60%)]  \tLoss:   92.359764\trec:   64.883156\tkl:   27.476603\n",
      "Epoch: 500 [40100/50000 (80%)]  \tLoss:   93.545486\trec:   67.231758\tkl:   26.313725\n",
      "====> Epoch: 500 Average train loss: 94.0595\n",
      "====> Validation set loss: 96.4604\n",
      "====> Validation set kl: 25.7567\n",
      "Epoch: 501 [  100/50000 ( 0%)]  \tLoss:   87.639458\trec:   63.190586\tkl:   24.448879\n",
      "Epoch: 501 [10100/50000 (20%)]  \tLoss:   93.105202\trec:   67.045624\tkl:   26.059580\n",
      "Epoch: 501 [20100/50000 (40%)]  \tLoss:   92.408829\trec:   67.166916\tkl:   25.241911\n",
      "Epoch: 501 [30100/50000 (60%)]  \tLoss:   94.780586\trec:   69.157150\tkl:   25.623442\n",
      "Epoch: 501 [40100/50000 (80%)]  \tLoss:   93.339668\trec:   67.060440\tkl:   26.279228\n",
      "====> Epoch: 501 Average train loss: 93.8994\n",
      "====> Validation set loss: 96.2353\n",
      "====> Validation set kl: 25.6677\n",
      "Epoch: 502 [  100/50000 ( 0%)]  \tLoss:   97.023628\trec:   71.112671\tkl:   25.910961\n",
      "Epoch: 502 [10100/50000 (20%)]  \tLoss:   94.175194\trec:   68.082321\tkl:   26.092869\n",
      "Epoch: 502 [20100/50000 (40%)]  \tLoss:   91.941658\trec:   66.938072\tkl:   25.003593\n",
      "Epoch: 502 [30100/50000 (60%)]  \tLoss:   97.181854\trec:   70.713005\tkl:   26.468849\n",
      "Epoch: 502 [40100/50000 (80%)]  \tLoss:   93.150963\trec:   69.017143\tkl:   24.133823\n",
      "====> Epoch: 502 Average train loss: 93.7051\n",
      "====> Validation set loss: 96.1295\n",
      "====> Validation set kl: 25.9172\n",
      "Epoch: 503 [  100/50000 ( 0%)]  \tLoss:   96.664291\trec:   70.566521\tkl:   26.097773\n",
      "Epoch: 503 [10100/50000 (20%)]  \tLoss:   90.768593\trec:   66.003532\tkl:   24.765062\n",
      "Epoch: 503 [20100/50000 (40%)]  \tLoss:   96.700378\trec:   70.789993\tkl:   25.910387\n",
      "Epoch: 503 [30100/50000 (60%)]  \tLoss:   94.763153\trec:   68.018517\tkl:   26.744640\n",
      "Epoch: 503 [40100/50000 (80%)]  \tLoss:   90.189392\trec:   64.500572\tkl:   25.688820\n",
      "====> Epoch: 503 Average train loss: 94.0782\n",
      "====> Validation set loss: 97.5354\n",
      "====> Validation set kl: 26.2801\n",
      "Epoch: 504 [  100/50000 ( 0%)]  \tLoss:   95.139702\trec:   68.953903\tkl:   26.185810\n",
      "Epoch: 504 [10100/50000 (20%)]  \tLoss:   94.373619\trec:   68.158218\tkl:   26.215397\n",
      "Epoch: 504 [20100/50000 (40%)]  \tLoss:   95.445877\trec:   69.277023\tkl:   26.168852\n",
      "Epoch: 504 [30100/50000 (60%)]  \tLoss:   94.358849\trec:   67.532410\tkl:   26.826439\n",
      "Epoch: 504 [40100/50000 (80%)]  \tLoss:   95.983536\trec:   70.404678\tkl:   25.578854\n",
      "====> Epoch: 504 Average train loss: 93.7878\n",
      "====> Validation set loss: 96.7862\n",
      "====> Validation set kl: 26.3216\n",
      "Epoch: 505 [  100/50000 ( 0%)]  \tLoss:   95.034561\trec:   68.178223\tkl:   26.856337\n",
      "Epoch: 505 [10100/50000 (20%)]  \tLoss:   96.117989\trec:   70.065407\tkl:   26.052588\n",
      "Epoch: 505 [20100/50000 (40%)]  \tLoss:   94.110573\trec:   67.354683\tkl:   26.755886\n",
      "Epoch: 505 [30100/50000 (60%)]  \tLoss:   96.494019\trec:   70.098488\tkl:   26.395531\n",
      "Epoch: 505 [40100/50000 (80%)]  \tLoss:   97.156013\trec:   72.495880\tkl:   24.660141\n",
      "====> Epoch: 505 Average train loss: 93.8345\n",
      "====> Validation set loss: 96.0592\n",
      "====> Validation set kl: 25.7432\n",
      "Epoch: 506 [  100/50000 ( 0%)]  \tLoss:   96.871758\trec:   70.909286\tkl:   25.962471\n",
      "Epoch: 506 [10100/50000 (20%)]  \tLoss:   93.409042\trec:   67.823151\tkl:   25.585888\n",
      "Epoch: 506 [20100/50000 (40%)]  \tLoss:   95.653008\trec:   70.303360\tkl:   25.349646\n",
      "Epoch: 506 [30100/50000 (60%)]  \tLoss:   92.730789\trec:   67.161812\tkl:   25.568972\n",
      "Epoch: 506 [40100/50000 (80%)]  \tLoss:   87.943863\trec:   64.531761\tkl:   23.412104\n",
      "====> Epoch: 506 Average train loss: 93.5308\n",
      "====> Validation set loss: 96.3496\n",
      "====> Validation set kl: 25.6477\n",
      "Epoch: 507 [  100/50000 ( 0%)]  \tLoss:   93.381447\trec:   66.989357\tkl:   26.392084\n",
      "Epoch: 507 [10100/50000 (20%)]  \tLoss:   96.834625\trec:   70.761299\tkl:   26.073320\n",
      "Epoch: 507 [20100/50000 (40%)]  \tLoss:   97.208160\trec:   71.497986\tkl:   25.710175\n",
      "Epoch: 507 [30100/50000 (60%)]  \tLoss:   93.509773\trec:   68.259033\tkl:   25.250740\n",
      "Epoch: 507 [40100/50000 (80%)]  \tLoss:   90.081505\trec:   64.776268\tkl:   25.305239\n",
      "====> Epoch: 507 Average train loss: 93.6175\n",
      "====> Validation set loss: 96.0314\n",
      "====> Validation set kl: 25.5291\n",
      "Epoch: 508 [  100/50000 ( 0%)]  \tLoss:   92.921959\trec:   67.847466\tkl:   25.074492\n",
      "Epoch: 508 [10100/50000 (20%)]  \tLoss:   95.850288\trec:   69.160820\tkl:   26.689474\n",
      "Epoch: 508 [20100/50000 (40%)]  \tLoss:   93.855347\trec:   67.975304\tkl:   25.880049\n",
      "Epoch: 508 [30100/50000 (60%)]  \tLoss:   94.571579\trec:   68.450432\tkl:   26.121141\n",
      "Epoch: 508 [40100/50000 (80%)]  \tLoss:   94.542618\trec:   69.104546\tkl:   25.438068\n",
      "====> Epoch: 508 Average train loss: 93.6662\n",
      "====> Validation set loss: 96.4942\n",
      "====> Validation set kl: 25.1474\n",
      "Epoch: 509 [  100/50000 ( 0%)]  \tLoss:   95.364342\trec:   70.416191\tkl:   24.948156\n",
      "Epoch: 509 [10100/50000 (20%)]  \tLoss:   92.810455\trec:   66.720535\tkl:   26.089922\n",
      "Epoch: 509 [20100/50000 (40%)]  \tLoss:   96.243591\trec:   69.694000\tkl:   26.549587\n",
      "Epoch: 509 [30100/50000 (60%)]  \tLoss:   91.768501\trec:   67.650932\tkl:   24.117567\n",
      "Epoch: 509 [40100/50000 (80%)]  \tLoss:   94.329605\trec:   67.515671\tkl:   26.813934\n",
      "====> Epoch: 509 Average train loss: 93.4221\n",
      "====> Validation set loss: 96.2956\n",
      "====> Validation set kl: 25.7880\n",
      "Epoch: 510 [  100/50000 ( 0%)]  \tLoss:   96.788895\trec:   70.426682\tkl:   26.362219\n",
      "Epoch: 510 [10100/50000 (20%)]  \tLoss:   91.314819\trec:   66.071327\tkl:   25.243494\n",
      "Epoch: 510 [20100/50000 (40%)]  \tLoss:   95.183815\trec:   67.663445\tkl:   27.520370\n",
      "Epoch: 510 [30100/50000 (60%)]  \tLoss:   97.244331\trec:   71.567810\tkl:   25.676527\n",
      "Epoch: 510 [40100/50000 (80%)]  \tLoss:   92.932350\trec:   67.486397\tkl:   25.445953\n",
      "====> Epoch: 510 Average train loss: 94.1273\n",
      "====> Validation set loss: 97.7651\n",
      "====> Validation set kl: 27.5612\n",
      "Epoch: 511 [  100/50000 ( 0%)]  \tLoss:   93.485977\trec:   66.363701\tkl:   27.122272\n",
      "Epoch: 511 [10100/50000 (20%)]  \tLoss:   92.912109\trec:   66.585884\tkl:   26.326223\n",
      "Epoch: 511 [20100/50000 (40%)]  \tLoss:   96.723862\trec:   70.925919\tkl:   25.797951\n",
      "Epoch: 511 [30100/50000 (60%)]  \tLoss:   93.690125\trec:   69.194305\tkl:   24.495817\n",
      "Epoch: 511 [40100/50000 (80%)]  \tLoss:   96.641640\trec:   70.207878\tkl:   26.433756\n",
      "====> Epoch: 511 Average train loss: 93.9317\n",
      "====> Validation set loss: 96.2725\n",
      "====> Validation set kl: 25.5643\n",
      "Epoch: 512 [  100/50000 ( 0%)]  \tLoss:   94.123436\trec:   67.797531\tkl:   26.325911\n",
      "Epoch: 512 [10100/50000 (20%)]  \tLoss:   92.578720\trec:   67.286140\tkl:   25.292576\n",
      "Epoch: 512 [20100/50000 (40%)]  \tLoss:   94.720352\trec:   69.438065\tkl:   25.282290\n",
      "Epoch: 512 [30100/50000 (60%)]  \tLoss:   90.743614\trec:   66.782402\tkl:   23.961212\n",
      "Epoch: 512 [40100/50000 (80%)]  \tLoss:   97.499306\trec:   70.803787\tkl:   26.695518\n",
      "====> Epoch: 512 Average train loss: 93.4900\n",
      "====> Validation set loss: 96.1586\n",
      "====> Validation set kl: 25.4742\n",
      "Epoch: 513 [  100/50000 ( 0%)]  \tLoss:   97.135376\trec:   70.300308\tkl:   26.835068\n",
      "Epoch: 513 [10100/50000 (20%)]  \tLoss:   93.216347\trec:   68.115562\tkl:   25.100790\n",
      "Epoch: 513 [20100/50000 (40%)]  \tLoss:   91.419609\trec:   66.237930\tkl:   25.181677\n",
      "Epoch: 513 [30100/50000 (60%)]  \tLoss:   96.184731\trec:   69.930885\tkl:   26.253847\n",
      "Epoch: 513 [40100/50000 (80%)]  \tLoss:   90.576782\trec:   65.404678\tkl:   25.172110\n",
      "====> Epoch: 513 Average train loss: 93.7144\n",
      "====> Validation set loss: 96.5112\n",
      "====> Validation set kl: 25.6100\n",
      "Epoch: 514 [  100/50000 ( 0%)]  \tLoss:   93.893318\trec:   67.465958\tkl:   26.427362\n",
      "Epoch: 514 [10100/50000 (20%)]  \tLoss:   91.327950\trec:   66.371651\tkl:   24.956293\n",
      "Epoch: 514 [20100/50000 (40%)]  \tLoss:   92.161987\trec:   67.498466\tkl:   24.663515\n",
      "Epoch: 514 [30100/50000 (60%)]  \tLoss:   97.700409\trec:   70.050774\tkl:   27.649633\n",
      "Epoch: 514 [40100/50000 (80%)]  \tLoss:   93.921326\trec:   67.983223\tkl:   25.938101\n",
      "====> Epoch: 514 Average train loss: 94.2346\n",
      "====> Validation set loss: 96.5032\n",
      "====> Validation set kl: 25.2439\n",
      "Epoch: 515 [  100/50000 ( 0%)]  \tLoss:   91.189041\trec:   66.834373\tkl:   24.354668\n",
      "Epoch: 515 [10100/50000 (20%)]  \tLoss:   91.731010\trec:   66.963860\tkl:   24.767160\n",
      "Epoch: 515 [20100/50000 (40%)]  \tLoss:   95.982498\trec:   69.652519\tkl:   26.329977\n",
      "Epoch: 515 [30100/50000 (60%)]  \tLoss:   92.650017\trec:   67.368515\tkl:   25.281498\n",
      "Epoch: 515 [40100/50000 (80%)]  \tLoss:   94.533562\trec:   69.352341\tkl:   25.181227\n",
      "====> Epoch: 515 Average train loss: 94.1579\n",
      "====> Validation set loss: 96.4796\n",
      "====> Validation set kl: 25.6839\n",
      "Epoch: 516 [  100/50000 ( 0%)]  \tLoss:   92.860321\trec:   66.376183\tkl:   26.484140\n",
      "Epoch: 516 [10100/50000 (20%)]  \tLoss:   92.796532\trec:   67.120094\tkl:   25.676435\n",
      "Epoch: 516 [20100/50000 (40%)]  \tLoss:   94.124313\trec:   69.116821\tkl:   25.007490\n",
      "Epoch: 516 [30100/50000 (60%)]  \tLoss:   96.028671\trec:   69.529343\tkl:   26.499329\n",
      "Epoch: 516 [40100/50000 (80%)]  \tLoss:   91.030846\trec:   66.610703\tkl:   24.420145\n",
      "====> Epoch: 516 Average train loss: 93.9140\n",
      "====> Validation set loss: 96.2886\n",
      "====> Validation set kl: 25.7642\n",
      "Epoch: 517 [  100/50000 ( 0%)]  \tLoss:   92.675659\trec:   66.928848\tkl:   25.746820\n",
      "Epoch: 517 [10100/50000 (20%)]  \tLoss:   92.212608\trec:   66.153503\tkl:   26.059103\n",
      "Epoch: 517 [20100/50000 (40%)]  \tLoss:   93.564316\trec:   68.565933\tkl:   24.998379\n",
      "Epoch: 517 [30100/50000 (60%)]  \tLoss:   89.343613\trec:   64.393059\tkl:   24.950558\n",
      "Epoch: 517 [40100/50000 (80%)]  \tLoss:   95.181412\trec:   69.581902\tkl:   25.599516\n",
      "====> Epoch: 517 Average train loss: 93.3388\n",
      "====> Validation set loss: 96.3147\n",
      "====> Validation set kl: 25.5575\n",
      "Epoch: 518 [  100/50000 ( 0%)]  \tLoss:   94.565811\trec:   68.656364\tkl:   25.909443\n",
      "Epoch: 518 [10100/50000 (20%)]  \tLoss:   91.420448\trec:   65.850914\tkl:   25.569536\n",
      "Epoch: 518 [20100/50000 (40%)]  \tLoss:   91.218483\trec:   66.176300\tkl:   25.042185\n",
      "Epoch: 518 [30100/50000 (60%)]  \tLoss:   88.261108\trec:   63.773113\tkl:   24.488001\n",
      "Epoch: 518 [40100/50000 (80%)]  \tLoss:   91.439400\trec:   66.016632\tkl:   25.422770\n",
      "====> Epoch: 518 Average train loss: 93.5056\n",
      "====> Validation set loss: 96.3169\n",
      "====> Validation set kl: 25.4784\n",
      "Epoch: 519 [  100/50000 ( 0%)]  \tLoss:   94.297523\trec:   68.261543\tkl:   26.035984\n",
      "Epoch: 519 [10100/50000 (20%)]  \tLoss:   89.172951\trec:   64.843552\tkl:   24.329391\n",
      "Epoch: 519 [20100/50000 (40%)]  \tLoss:   89.720650\trec:   65.017372\tkl:   24.703278\n",
      "Epoch: 519 [30100/50000 (60%)]  \tLoss:   94.052399\trec:   67.997017\tkl:   26.055386\n",
      "Epoch: 519 [40100/50000 (80%)]  \tLoss:   93.084877\trec:   68.230705\tkl:   24.854183\n",
      "====> Epoch: 519 Average train loss: 93.4790\n",
      "====> Validation set loss: 98.0345\n",
      "====> Validation set kl: 24.4305\n",
      "Epoch: 520 [  100/50000 ( 0%)]  \tLoss:   93.709160\trec:   70.355446\tkl:   23.353716\n",
      "Epoch: 520 [10100/50000 (20%)]  \tLoss:   94.646729\trec:   69.054047\tkl:   25.592682\n",
      "Epoch: 520 [20100/50000 (40%)]  \tLoss:   91.044922\trec:   66.444519\tkl:   24.600397\n",
      "Epoch: 520 [30100/50000 (60%)]  \tLoss:   94.911942\trec:   68.872910\tkl:   26.039036\n",
      "Epoch: 520 [40100/50000 (80%)]  \tLoss:   92.261589\trec:   66.978958\tkl:   25.282631\n",
      "====> Epoch: 520 Average train loss: 93.8602\n",
      "====> Validation set loss: 96.4572\n",
      "====> Validation set kl: 26.1039\n",
      "Epoch: 521 [  100/50000 ( 0%)]  \tLoss:   92.232964\trec:   65.750847\tkl:   26.482113\n",
      "Epoch: 521 [10100/50000 (20%)]  \tLoss:   90.176796\trec:   64.738823\tkl:   25.437971\n",
      "Epoch: 521 [20100/50000 (40%)]  \tLoss:   94.782822\trec:   68.395599\tkl:   26.387224\n",
      "Epoch: 521 [30100/50000 (60%)]  \tLoss:   91.583054\trec:   65.283096\tkl:   26.299963\n",
      "Epoch: 521 [40100/50000 (80%)]  \tLoss:   97.474625\trec:   70.954727\tkl:   26.519897\n",
      "====> Epoch: 521 Average train loss: 93.4642\n",
      "====> Validation set loss: 96.3622\n",
      "====> Validation set kl: 25.5758\n",
      "Epoch: 522 [  100/50000 ( 0%)]  \tLoss:   90.198959\trec:   65.748772\tkl:   24.450188\n",
      "Epoch: 522 [10100/50000 (20%)]  \tLoss:   97.699654\trec:   70.927803\tkl:   26.771858\n",
      "Epoch: 522 [20100/50000 (40%)]  \tLoss:   89.993828\trec:   64.631592\tkl:   25.362234\n",
      "Epoch: 522 [30100/50000 (60%)]  \tLoss:   98.441139\trec:   72.129608\tkl:   26.311533\n",
      "Epoch: 522 [40100/50000 (80%)]  \tLoss:   91.035904\trec:   65.375145\tkl:   25.660763\n",
      "====> Epoch: 522 Average train loss: 93.4548\n",
      "====> Validation set loss: 95.8710\n",
      "====> Validation set kl: 25.4037\n",
      "Epoch: 523 [  100/50000 ( 0%)]  \tLoss:   91.796837\trec:   66.802483\tkl:   24.994358\n",
      "Epoch: 523 [10100/50000 (20%)]  \tLoss:   92.485748\trec:   66.571632\tkl:   25.914124\n",
      "Epoch: 523 [20100/50000 (40%)]  \tLoss:   95.227577\trec:   69.381340\tkl:   25.846243\n",
      "Epoch: 523 [30100/50000 (60%)]  \tLoss:   92.091309\trec:   67.073784\tkl:   25.017525\n",
      "Epoch: 523 [40100/50000 (80%)]  \tLoss:   97.752808\trec:   71.521965\tkl:   26.230846\n",
      "====> Epoch: 523 Average train loss: 93.3407\n",
      "====> Validation set loss: 95.8818\n",
      "====> Validation set kl: 25.5459\n",
      "Epoch: 524 [  100/50000 ( 0%)]  \tLoss:   93.597046\trec:   67.710388\tkl:   25.886662\n",
      "Epoch: 524 [10100/50000 (20%)]  \tLoss:   97.947769\trec:   71.132324\tkl:   26.815443\n",
      "Epoch: 524 [20100/50000 (40%)]  \tLoss:   93.957207\trec:   68.279053\tkl:   25.678152\n",
      "Epoch: 524 [30100/50000 (60%)]  \tLoss:   94.209351\trec:   68.568123\tkl:   25.641230\n",
      "Epoch: 524 [40100/50000 (80%)]  \tLoss:   92.776649\trec:   67.560837\tkl:   25.215816\n",
      "====> Epoch: 524 Average train loss: 93.4658\n",
      "====> Validation set loss: 96.0692\n",
      "====> Validation set kl: 25.8360\n",
      "Epoch: 525 [  100/50000 ( 0%)]  \tLoss:   94.147476\trec:   67.595421\tkl:   26.552050\n",
      "Epoch: 525 [10100/50000 (20%)]  \tLoss:   95.206795\trec:   69.515434\tkl:   25.691359\n",
      "Epoch: 525 [20100/50000 (40%)]  \tLoss:   92.614845\trec:   67.147034\tkl:   25.467812\n",
      "Epoch: 525 [30100/50000 (60%)]  \tLoss:   92.078400\trec:   67.314903\tkl:   24.763485\n",
      "Epoch: 525 [40100/50000 (80%)]  \tLoss:   91.000153\trec:   65.640572\tkl:   25.359583\n",
      "====> Epoch: 525 Average train loss: 93.6083\n",
      "====> Validation set loss: 96.0403\n",
      "====> Validation set kl: 25.5134\n",
      "Epoch: 526 [  100/50000 ( 0%)]  \tLoss:   91.982887\trec:   66.802925\tkl:   25.179970\n",
      "Epoch: 526 [10100/50000 (20%)]  \tLoss:   95.725395\trec:   69.186501\tkl:   26.538900\n",
      "Epoch: 526 [20100/50000 (40%)]  \tLoss:   91.947449\trec:   66.511612\tkl:   25.435837\n",
      "Epoch: 526 [30100/50000 (60%)]  \tLoss:   94.073692\trec:   69.683037\tkl:   24.390654\n",
      "Epoch: 526 [40100/50000 (80%)]  \tLoss:   96.261703\trec:   70.340141\tkl:   25.921560\n",
      "====> Epoch: 526 Average train loss: 93.5833\n",
      "====> Validation set loss: 96.0563\n",
      "====> Validation set kl: 25.2692\n",
      "Epoch: 527 [  100/50000 ( 0%)]  \tLoss:   90.450615\trec:   66.289635\tkl:   24.160984\n",
      "Epoch: 527 [10100/50000 (20%)]  \tLoss:   90.671188\trec:   65.093956\tkl:   25.577238\n",
      "Epoch: 527 [20100/50000 (40%)]  \tLoss:   90.402534\trec:   65.596931\tkl:   24.805605\n",
      "Epoch: 527 [30100/50000 (60%)]  \tLoss:   97.820366\trec:   71.647507\tkl:   26.172859\n",
      "Epoch: 527 [40100/50000 (80%)]  \tLoss:   95.176979\trec:   70.039146\tkl:   25.137831\n",
      "====> Epoch: 527 Average train loss: 93.5121\n",
      "====> Validation set loss: 97.2793\n",
      "====> Validation set kl: 25.3621\n",
      "Epoch: 528 [  100/50000 ( 0%)]  \tLoss:   93.683540\trec:   68.209839\tkl:   25.473698\n",
      "Epoch: 528 [10100/50000 (20%)]  \tLoss:   91.519989\trec:   65.611847\tkl:   25.908142\n",
      "Epoch: 528 [20100/50000 (40%)]  \tLoss:   96.757614\trec:   71.025688\tkl:   25.731928\n",
      "Epoch: 528 [30100/50000 (60%)]  \tLoss:   91.388748\trec:   65.716705\tkl:   25.672043\n",
      "Epoch: 528 [40100/50000 (80%)]  \tLoss:   91.471054\trec:   65.396080\tkl:   26.074968\n",
      "====> Epoch: 528 Average train loss: 93.5172\n",
      "====> Validation set loss: 96.1788\n",
      "====> Validation set kl: 25.1260\n",
      "Epoch: 529 [  100/50000 ( 0%)]  \tLoss:   92.742996\trec:   66.849602\tkl:   25.893394\n",
      "Epoch: 529 [10100/50000 (20%)]  \tLoss:   94.127411\trec:   69.203957\tkl:   24.923456\n",
      "Epoch: 529 [20100/50000 (40%)]  \tLoss:   97.205353\trec:   71.555901\tkl:   25.649448\n",
      "Epoch: 529 [30100/50000 (60%)]  \tLoss:   92.168976\trec:   66.873604\tkl:   25.295374\n",
      "Epoch: 529 [40100/50000 (80%)]  \tLoss:   90.528999\trec:   66.060959\tkl:   24.468042\n",
      "====> Epoch: 529 Average train loss: 93.7778\n",
      "====> Validation set loss: 96.2378\n",
      "====> Validation set kl: 25.7434\n",
      "Epoch: 530 [  100/50000 ( 0%)]  \tLoss:   91.658981\trec:   65.394821\tkl:   26.264160\n",
      "Epoch: 530 [10100/50000 (20%)]  \tLoss:   94.858788\trec:   69.017822\tkl:   25.840969\n",
      "Epoch: 530 [20100/50000 (40%)]  \tLoss:   97.152481\trec:   70.680138\tkl:   26.472347\n",
      "Epoch: 530 [30100/50000 (60%)]  \tLoss:   93.751007\trec:   68.494148\tkl:   25.256853\n",
      "Epoch: 530 [40100/50000 (80%)]  \tLoss:   95.451714\trec:   70.079369\tkl:   25.372343\n",
      "====> Epoch: 530 Average train loss: 93.5227\n",
      "====> Validation set loss: 96.0357\n",
      "====> Validation set kl: 25.5051\n",
      "Epoch: 531 [  100/50000 ( 0%)]  \tLoss:   94.642250\trec:   69.040527\tkl:   25.601730\n",
      "Epoch: 531 [10100/50000 (20%)]  \tLoss:   95.582985\trec:   70.255577\tkl:   25.327406\n",
      "Epoch: 531 [20100/50000 (40%)]  \tLoss:   91.205307\trec:   66.614906\tkl:   24.590410\n",
      "Epoch: 531 [30100/50000 (60%)]  \tLoss:   96.283066\trec:   69.352905\tkl:   26.930155\n",
      "Epoch: 531 [40100/50000 (80%)]  \tLoss:   92.310654\trec:   67.296242\tkl:   25.014416\n",
      "====> Epoch: 531 Average train loss: 93.7916\n",
      "====> Validation set loss: 97.8434\n",
      "====> Validation set kl: 27.4389\n",
      "Epoch: 532 [  100/50000 ( 0%)]  \tLoss:   91.304649\trec:   64.643677\tkl:   26.660969\n",
      "Epoch: 532 [10100/50000 (20%)]  \tLoss:   93.686707\trec:   67.696053\tkl:   25.990654\n",
      "Epoch: 532 [20100/50000 (40%)]  \tLoss:   93.231720\trec:   67.549240\tkl:   25.682472\n",
      "Epoch: 532 [30100/50000 (60%)]  \tLoss:   91.710762\trec:   66.471748\tkl:   25.239016\n",
      "Epoch: 532 [40100/50000 (80%)]  \tLoss:   87.923378\trec:   62.681244\tkl:   25.242130\n",
      "====> Epoch: 532 Average train loss: 93.8126\n",
      "====> Validation set loss: 97.9149\n",
      "====> Validation set kl: 26.1970\n",
      "Epoch: 533 [  100/50000 ( 0%)]  \tLoss:   96.207108\trec:   70.204727\tkl:   26.002380\n",
      "Epoch: 533 [10100/50000 (20%)]  \tLoss:   96.355568\trec:   69.828682\tkl:   26.526890\n",
      "Epoch: 533 [20100/50000 (40%)]  \tLoss:   95.341179\trec:   69.036972\tkl:   26.304209\n",
      "Epoch: 533 [30100/50000 (60%)]  \tLoss:   93.740822\trec:   67.993965\tkl:   25.746857\n",
      "Epoch: 533 [40100/50000 (80%)]  \tLoss:   95.583954\trec:   69.923470\tkl:   25.660480\n",
      "====> Epoch: 533 Average train loss: 93.8187\n",
      "====> Validation set loss: 96.2378\n",
      "====> Validation set kl: 25.1257\n",
      "Epoch: 534 [  100/50000 ( 0%)]  \tLoss:   89.258179\trec:   64.706474\tkl:   24.551714\n",
      "Epoch: 534 [10100/50000 (20%)]  \tLoss:   93.000389\trec:   68.022896\tkl:   24.977488\n",
      "Epoch: 534 [20100/50000 (40%)]  \tLoss:   93.691658\trec:   67.710617\tkl:   25.981049\n",
      "Epoch: 534 [30100/50000 (60%)]  \tLoss:   93.192177\trec:   67.305557\tkl:   25.886616\n",
      "Epoch: 534 [40100/50000 (80%)]  \tLoss:   96.315834\trec:   69.076881\tkl:   27.238960\n",
      "====> Epoch: 534 Average train loss: 94.0111\n",
      "====> Validation set loss: 97.8004\n",
      "====> Validation set kl: 27.1494\n",
      "Epoch: 535 [  100/50000 ( 0%)]  \tLoss:   94.168785\trec:   66.826378\tkl:   27.342409\n",
      "Epoch: 535 [10100/50000 (20%)]  \tLoss:   96.446014\trec:   69.997986\tkl:   26.448027\n",
      "Epoch: 535 [20100/50000 (40%)]  \tLoss:   91.259727\trec:   65.709641\tkl:   25.550093\n",
      "Epoch: 535 [30100/50000 (60%)]  \tLoss:   94.777878\trec:   68.333427\tkl:   26.444450\n",
      "Epoch: 535 [40100/50000 (80%)]  \tLoss:   91.110886\trec:   66.476700\tkl:   24.634184\n",
      "====> Epoch: 535 Average train loss: 93.8104\n",
      "====> Validation set loss: 96.7279\n",
      "====> Validation set kl: 25.7600\n",
      "Epoch: 536 [  100/50000 ( 0%)]  \tLoss:   89.975349\trec:   63.900463\tkl:   26.074886\n",
      "Epoch: 536 [10100/50000 (20%)]  \tLoss:   92.391212\trec:   67.617462\tkl:   24.773745\n",
      "Epoch: 536 [20100/50000 (40%)]  \tLoss:   90.536812\trec:   64.610497\tkl:   25.926317\n",
      "Epoch: 536 [30100/50000 (60%)]  \tLoss:   94.700203\trec:   69.013748\tkl:   25.686453\n",
      "Epoch: 536 [40100/50000 (80%)]  \tLoss:   95.333748\trec:   69.596832\tkl:   25.736908\n",
      "====> Epoch: 536 Average train loss: 93.5422\n",
      "====> Validation set loss: 97.3200\n",
      "====> Validation set kl: 27.5750\n",
      "Epoch: 537 [  100/50000 ( 0%)]  \tLoss:   93.505371\trec:   65.822319\tkl:   27.683056\n",
      "Epoch: 537 [10100/50000 (20%)]  \tLoss:   94.297554\trec:   68.467468\tkl:   25.830090\n",
      "Epoch: 537 [20100/50000 (40%)]  \tLoss:   99.501381\trec:   72.503510\tkl:   26.997877\n",
      "Epoch: 537 [30100/50000 (60%)]  \tLoss:   94.902206\trec:   69.319611\tkl:   25.582602\n",
      "Epoch: 537 [40100/50000 (80%)]  \tLoss:   92.986427\trec:   67.473869\tkl:   25.512564\n",
      "====> Epoch: 537 Average train loss: 93.5615\n",
      "====> Validation set loss: 96.4914\n",
      "====> Validation set kl: 25.3497\n",
      "Epoch: 538 [  100/50000 ( 0%)]  \tLoss:   90.815056\trec:   65.856453\tkl:   24.958603\n",
      "Epoch: 538 [10100/50000 (20%)]  \tLoss:   96.936348\trec:   69.340103\tkl:   27.596249\n",
      "Epoch: 538 [20100/50000 (40%)]  \tLoss:   90.862892\trec:   64.991913\tkl:   25.870981\n",
      "Epoch: 538 [30100/50000 (60%)]  \tLoss:   94.803436\trec:   67.992241\tkl:   26.811201\n",
      "Epoch: 538 [40100/50000 (80%)]  \tLoss:   94.131447\trec:   67.840889\tkl:   26.290550\n",
      "====> Epoch: 538 Average train loss: 93.7389\n",
      "====> Validation set loss: 96.1237\n",
      "====> Validation set kl: 25.6660\n",
      "Epoch: 539 [  100/50000 ( 0%)]  \tLoss:   91.162109\trec:   65.573418\tkl:   25.588684\n",
      "Epoch: 539 [10100/50000 (20%)]  \tLoss:   90.564049\trec:   66.046638\tkl:   24.517408\n",
      "Epoch: 539 [20100/50000 (40%)]  \tLoss:   93.496704\trec:   68.378853\tkl:   25.117851\n",
      "Epoch: 539 [30100/50000 (60%)]  \tLoss:   94.852867\trec:   69.219360\tkl:   25.633511\n",
      "Epoch: 539 [40100/50000 (80%)]  \tLoss:   94.209839\trec:   68.948814\tkl:   25.261030\n",
      "====> Epoch: 539 Average train loss: 93.5823\n",
      "====> Validation set loss: 96.0880\n",
      "====> Validation set kl: 25.4709\n",
      "Epoch: 540 [  100/50000 ( 0%)]  \tLoss:   93.334892\trec:   68.119347\tkl:   25.215542\n",
      "Epoch: 540 [10100/50000 (20%)]  \tLoss:   96.346542\trec:   71.584106\tkl:   24.762430\n",
      "Epoch: 540 [20100/50000 (40%)]  \tLoss:   91.417107\trec:   66.971535\tkl:   24.445570\n",
      "Epoch: 540 [30100/50000 (60%)]  \tLoss:   92.922829\trec:   66.757164\tkl:   26.165667\n",
      "Epoch: 540 [40100/50000 (80%)]  \tLoss:   90.993568\trec:   65.443100\tkl:   25.550470\n",
      "====> Epoch: 540 Average train loss: 93.2485\n",
      "====> Validation set loss: 96.2383\n",
      "====> Validation set kl: 26.0208\n",
      "Epoch: 541 [  100/50000 ( 0%)]  \tLoss:   93.723602\trec:   68.022049\tkl:   25.701550\n",
      "Epoch: 541 [10100/50000 (20%)]  \tLoss:   91.234863\trec:   66.454590\tkl:   24.780268\n",
      "Epoch: 541 [20100/50000 (40%)]  \tLoss:   99.312759\trec:   72.519135\tkl:   26.793625\n",
      "Epoch: 541 [30100/50000 (60%)]  \tLoss:   96.571960\trec:   70.786949\tkl:   25.785009\n",
      "Epoch: 541 [40100/50000 (80%)]  \tLoss:   96.294899\trec:   70.732742\tkl:   25.562155\n",
      "====> Epoch: 541 Average train loss: 93.8488\n",
      "====> Validation set loss: 97.1443\n",
      "====> Validation set kl: 26.2233\n",
      "Epoch: 542 [  100/50000 ( 0%)]  \tLoss:   96.041954\trec:   69.493179\tkl:   26.548769\n",
      "Epoch: 542 [10100/50000 (20%)]  \tLoss:   94.555649\trec:   68.038773\tkl:   26.516880\n",
      "Epoch: 542 [20100/50000 (40%)]  \tLoss:   93.737495\trec:   67.209801\tkl:   26.527700\n",
      "Epoch: 542 [30100/50000 (60%)]  \tLoss:   95.948112\trec:   70.286415\tkl:   25.661699\n",
      "Epoch: 542 [40100/50000 (80%)]  \tLoss:   94.114197\trec:   68.357193\tkl:   25.757006\n",
      "====> Epoch: 542 Average train loss: 93.9318\n",
      "====> Validation set loss: 96.2322\n",
      "====> Validation set kl: 25.9447\n",
      "Epoch: 543 [  100/50000 ( 0%)]  \tLoss:   96.003563\trec:   69.861565\tkl:   26.141998\n",
      "Epoch: 543 [10100/50000 (20%)]  \tLoss:   96.978653\trec:   71.257416\tkl:   25.721237\n",
      "Epoch: 543 [20100/50000 (40%)]  \tLoss:   91.006523\trec:   65.345924\tkl:   25.660595\n",
      "Epoch: 543 [30100/50000 (60%)]  \tLoss:   92.706200\trec:   67.126320\tkl:   25.579885\n",
      "Epoch: 543 [40100/50000 (80%)]  \tLoss:   93.181847\trec:   68.082123\tkl:   25.099724\n",
      "====> Epoch: 543 Average train loss: 93.6268\n",
      "====> Validation set loss: 95.9473\n",
      "====> Validation set kl: 25.2933\n",
      "Epoch: 544 [  100/50000 ( 0%)]  \tLoss:   92.291656\trec:   67.641411\tkl:   24.650246\n",
      "Epoch: 544 [10100/50000 (20%)]  \tLoss:   92.484940\trec:   67.203819\tkl:   25.281124\n",
      "Epoch: 544 [20100/50000 (40%)]  \tLoss:   89.593941\trec:   64.399620\tkl:   25.194323\n",
      "Epoch: 544 [30100/50000 (60%)]  \tLoss:   92.814323\trec:   66.586983\tkl:   26.227341\n",
      "Epoch: 544 [40100/50000 (80%)]  \tLoss:   95.705681\trec:   70.413254\tkl:   25.292431\n",
      "====> Epoch: 544 Average train loss: 93.5667\n",
      "====> Validation set loss: 97.0600\n",
      "====> Validation set kl: 25.5812\n",
      "Epoch: 545 [  100/50000 ( 0%)]  \tLoss:   94.907242\trec:   68.932144\tkl:   25.975105\n",
      "Epoch: 545 [10100/50000 (20%)]  \tLoss:   91.957870\trec:   66.474640\tkl:   25.483229\n",
      "Epoch: 545 [20100/50000 (40%)]  \tLoss:   94.975105\trec:   68.030701\tkl:   26.944401\n",
      "Epoch: 545 [30100/50000 (60%)]  \tLoss:   93.910011\trec:   69.536270\tkl:   24.373743\n",
      "Epoch: 545 [40100/50000 (80%)]  \tLoss:   92.077599\trec:   66.454765\tkl:   25.622829\n",
      "====> Epoch: 545 Average train loss: 93.8012\n",
      "====> Validation set loss: 96.1639\n",
      "====> Validation set kl: 25.8339\n",
      "Epoch: 546 [  100/50000 ( 0%)]  \tLoss:   96.944565\trec:   70.637512\tkl:   26.307055\n",
      "Epoch: 546 [10100/50000 (20%)]  \tLoss:   92.631958\trec:   67.318108\tkl:   25.313860\n",
      "Epoch: 546 [20100/50000 (40%)]  \tLoss:   96.116928\trec:   69.719772\tkl:   26.397160\n",
      "Epoch: 546 [30100/50000 (60%)]  \tLoss:   89.198418\trec:   64.060616\tkl:   25.137804\n",
      "Epoch: 546 [40100/50000 (80%)]  \tLoss:   95.319176\trec:   67.842300\tkl:   27.476879\n",
      "====> Epoch: 546 Average train loss: 93.6360\n",
      "====> Validation set loss: 96.5901\n",
      "====> Validation set kl: 26.4555\n",
      "Epoch: 547 [  100/50000 ( 0%)]  \tLoss:   93.211227\trec:   66.684937\tkl:   26.526291\n",
      "Epoch: 547 [10100/50000 (20%)]  \tLoss:   97.167107\trec:   70.134239\tkl:   27.032866\n",
      "Epoch: 547 [20100/50000 (40%)]  \tLoss:   92.781769\trec:   66.282066\tkl:   26.499695\n",
      "Epoch: 547 [30100/50000 (60%)]  \tLoss:   94.650986\trec:   69.507393\tkl:   25.143591\n",
      "Epoch: 547 [40100/50000 (80%)]  \tLoss:  101.142754\trec:   74.442780\tkl:   26.699972\n",
      "====> Epoch: 547 Average train loss: 94.0033\n",
      "====> Validation set loss: 97.0507\n",
      "====> Validation set kl: 25.4150\n",
      "Epoch: 548 [  100/50000 ( 0%)]  \tLoss:   92.137657\trec:   67.174736\tkl:   24.962917\n",
      "Epoch: 548 [10100/50000 (20%)]  \tLoss:   93.758614\trec:   67.998528\tkl:   25.760082\n",
      "Epoch: 548 [20100/50000 (40%)]  \tLoss:   94.157593\trec:   68.572311\tkl:   25.585278\n",
      "Epoch: 548 [30100/50000 (60%)]  \tLoss:   91.598083\trec:   66.486816\tkl:   25.111271\n",
      "Epoch: 548 [40100/50000 (80%)]  \tLoss:   93.474060\trec:   68.023232\tkl:   25.450827\n",
      "====> Epoch: 548 Average train loss: 93.7204\n",
      "====> Validation set loss: 97.0914\n",
      "====> Validation set kl: 25.0953\n",
      "Epoch: 549 [  100/50000 ( 0%)]  \tLoss:   93.608261\trec:   69.194283\tkl:   24.413979\n",
      "Epoch: 549 [10100/50000 (20%)]  \tLoss:   91.623451\trec:   65.608269\tkl:   26.015184\n",
      "Epoch: 549 [20100/50000 (40%)]  \tLoss:   96.597565\trec:   70.456543\tkl:   26.141026\n",
      "Epoch: 549 [30100/50000 (60%)]  \tLoss:   93.544411\trec:   67.904533\tkl:   25.639872\n",
      "Epoch: 549 [40100/50000 (80%)]  \tLoss:   94.976608\trec:   69.373848\tkl:   25.602766\n",
      "====> Epoch: 549 Average train loss: 93.5576\n",
      "====> Validation set loss: 96.0379\n",
      "====> Validation set kl: 25.7229\n",
      "Epoch: 550 [  100/50000 ( 0%)]  \tLoss:   91.387222\trec:   65.333931\tkl:   26.053295\n",
      "Epoch: 550 [10100/50000 (20%)]  \tLoss:   94.709839\trec:   68.469170\tkl:   26.240665\n",
      "Epoch: 550 [20100/50000 (40%)]  \tLoss:   93.142441\trec:   68.395378\tkl:   24.747057\n",
      "Epoch: 550 [30100/50000 (60%)]  \tLoss:   93.237221\trec:   67.646027\tkl:   25.591200\n",
      "Epoch: 550 [40100/50000 (80%)]  \tLoss:   94.235565\trec:   68.615387\tkl:   25.620180\n",
      "====> Epoch: 550 Average train loss: 93.7707\n",
      "====> Validation set loss: 96.9874\n",
      "====> Validation set kl: 26.0948\n",
      "Epoch: 551 [  100/50000 ( 0%)]  \tLoss:   91.222900\trec:   65.648430\tkl:   25.574472\n",
      "Epoch: 551 [10100/50000 (20%)]  \tLoss:   91.206612\trec:   65.167618\tkl:   26.038992\n",
      "Epoch: 551 [20100/50000 (40%)]  \tLoss:   94.960793\trec:   68.964821\tkl:   25.995968\n",
      "Epoch: 551 [30100/50000 (60%)]  \tLoss:   96.389587\trec:   70.780907\tkl:   25.608683\n",
      "Epoch: 551 [40100/50000 (80%)]  \tLoss:   99.068855\trec:   71.299126\tkl:   27.769726\n",
      "====> Epoch: 551 Average train loss: 93.8166\n",
      "====> Validation set loss: 96.6296\n",
      "====> Validation set kl: 25.8965\n",
      "Epoch: 552 [  100/50000 ( 0%)]  \tLoss:   94.765053\trec:   69.033669\tkl:   25.731384\n",
      "Epoch: 552 [10100/50000 (20%)]  \tLoss:   97.012886\trec:   70.040848\tkl:   26.972042\n",
      "Epoch: 552 [20100/50000 (40%)]  \tLoss:   95.825272\trec:   69.436523\tkl:   26.388754\n",
      "Epoch: 552 [30100/50000 (60%)]  \tLoss:   94.179703\trec:   68.182404\tkl:   25.997305\n",
      "Epoch: 552 [40100/50000 (80%)]  \tLoss:   96.873825\trec:   70.328888\tkl:   26.544933\n",
      "====> Epoch: 552 Average train loss: 93.7335\n",
      "====> Validation set loss: 96.1585\n",
      "====> Validation set kl: 25.4462\n",
      "Epoch: 553 [  100/50000 ( 0%)]  \tLoss:   90.071648\trec:   64.987129\tkl:   25.084513\n",
      "Epoch: 553 [10100/50000 (20%)]  \tLoss:   95.666786\trec:   69.221161\tkl:   26.445625\n",
      "Epoch: 553 [20100/50000 (40%)]  \tLoss:   96.979393\trec:   71.511383\tkl:   25.468002\n",
      "Epoch: 553 [30100/50000 (60%)]  \tLoss:   89.822586\trec:   64.104019\tkl:   25.718561\n",
      "Epoch: 553 [40100/50000 (80%)]  \tLoss:   93.325844\trec:   68.119431\tkl:   25.206413\n",
      "====> Epoch: 553 Average train loss: 93.5601\n",
      "====> Validation set loss: 96.7915\n",
      "====> Validation set kl: 25.4200\n",
      "Epoch: 554 [  100/50000 ( 0%)]  \tLoss:   94.580933\trec:   68.681282\tkl:   25.899651\n",
      "Epoch: 554 [10100/50000 (20%)]  \tLoss:   93.469910\trec:   67.630341\tkl:   25.839567\n",
      "Epoch: 554 [20100/50000 (40%)]  \tLoss:   90.125778\trec:   64.836502\tkl:   25.289272\n",
      "Epoch: 554 [30100/50000 (60%)]  \tLoss:   94.719147\trec:   69.000381\tkl:   25.718769\n",
      "Epoch: 554 [40100/50000 (80%)]  \tLoss:   94.004539\trec:   68.682381\tkl:   25.322157\n",
      "====> Epoch: 554 Average train loss: 93.4962\n",
      "====> Validation set loss: 95.8811\n",
      "====> Validation set kl: 25.6441\n",
      "Epoch: 555 [  100/50000 ( 0%)]  \tLoss:   90.961388\trec:   65.200195\tkl:   25.761196\n",
      "Epoch: 555 [10100/50000 (20%)]  \tLoss:   94.904152\trec:   69.226135\tkl:   25.678013\n",
      "Epoch: 555 [20100/50000 (40%)]  \tLoss:   92.201134\trec:   66.712456\tkl:   25.488682\n",
      "Epoch: 555 [30100/50000 (60%)]  \tLoss:   93.134666\trec:   67.438477\tkl:   25.696190\n",
      "Epoch: 555 [40100/50000 (80%)]  \tLoss:   92.281677\trec:   66.515968\tkl:   25.765713\n",
      "====> Epoch: 555 Average train loss: 93.3549\n",
      "====> Validation set loss: 95.9850\n",
      "====> Validation set kl: 25.2336\n",
      "Epoch: 556 [  100/50000 ( 0%)]  \tLoss:   93.675682\trec:   68.611069\tkl:   25.064615\n",
      "Epoch: 556 [10100/50000 (20%)]  \tLoss:   93.500381\trec:   67.809006\tkl:   25.691380\n",
      "Epoch: 556 [20100/50000 (40%)]  \tLoss:   94.397018\trec:   69.906883\tkl:   24.490133\n",
      "Epoch: 556 [30100/50000 (60%)]  \tLoss:   93.008690\trec:   67.666695\tkl:   25.341991\n",
      "Epoch: 556 [40100/50000 (80%)]  \tLoss:   97.949715\trec:   71.306221\tkl:   26.643496\n",
      "====> Epoch: 556 Average train loss: 93.4369\n",
      "====> Validation set loss: 95.7423\n",
      "====> Validation set kl: 25.6373\n",
      "Epoch: 557 [  100/50000 ( 0%)]  \tLoss:   95.489685\trec:   69.924889\tkl:   25.564795\n",
      "Epoch: 557 [10100/50000 (20%)]  \tLoss:   94.745522\trec:   68.154114\tkl:   26.591415\n",
      "Epoch: 557 [20100/50000 (40%)]  \tLoss:   94.055885\trec:   67.749855\tkl:   26.306038\n",
      "Epoch: 557 [30100/50000 (60%)]  \tLoss:   97.059410\trec:   71.316154\tkl:   25.743256\n",
      "Epoch: 557 [40100/50000 (80%)]  \tLoss:   91.883553\trec:   66.587730\tkl:   25.295828\n",
      "====> Epoch: 557 Average train loss: 93.4990\n",
      "====> Validation set loss: 96.2452\n",
      "====> Validation set kl: 25.1433\n",
      "Epoch: 558 [  100/50000 ( 0%)]  \tLoss:   91.994728\trec:   66.421822\tkl:   25.572895\n",
      "Epoch: 558 [10100/50000 (20%)]  \tLoss:   92.233803\trec:   67.657570\tkl:   24.576242\n",
      "Epoch: 558 [20100/50000 (40%)]  \tLoss:   90.950562\trec:   65.094620\tkl:   25.855947\n",
      "Epoch: 558 [30100/50000 (60%)]  \tLoss:   89.800880\trec:   64.961357\tkl:   24.839521\n",
      "Epoch: 558 [40100/50000 (80%)]  \tLoss:   91.521423\trec:   66.479576\tkl:   25.041847\n",
      "====> Epoch: 558 Average train loss: 93.4286\n",
      "====> Validation set loss: 96.2134\n",
      "====> Validation set kl: 25.6968\n",
      "Epoch: 559 [  100/50000 ( 0%)]  \tLoss:   96.741020\trec:   70.769829\tkl:   25.971193\n",
      "Epoch: 559 [10100/50000 (20%)]  \tLoss:   95.295036\trec:   69.471672\tkl:   25.823374\n",
      "Epoch: 559 [20100/50000 (40%)]  \tLoss:   96.509003\trec:   70.541748\tkl:   25.967255\n",
      "Epoch: 559 [30100/50000 (60%)]  \tLoss:   99.677940\trec:   73.119293\tkl:   26.558641\n",
      "Epoch: 559 [40100/50000 (80%)]  \tLoss:   93.538963\trec:   68.386375\tkl:   25.152592\n",
      "====> Epoch: 559 Average train loss: 93.3275\n",
      "====> Validation set loss: 95.8179\n",
      "====> Validation set kl: 25.5543\n",
      "Epoch: 560 [  100/50000 ( 0%)]  \tLoss:   94.857735\trec:   68.767937\tkl:   26.089792\n",
      "Epoch: 560 [10100/50000 (20%)]  \tLoss:   92.543556\trec:   66.629257\tkl:   25.914291\n",
      "Epoch: 560 [20100/50000 (40%)]  \tLoss:   93.432114\trec:   68.014091\tkl:   25.418024\n",
      "Epoch: 560 [30100/50000 (60%)]  \tLoss:   97.697517\trec:   71.761650\tkl:   25.935863\n",
      "Epoch: 560 [40100/50000 (80%)]  \tLoss:   92.854225\trec:   66.636856\tkl:   26.217369\n",
      "====> Epoch: 560 Average train loss: 92.9904\n",
      "====> Validation set loss: 95.9564\n",
      "====> Validation set kl: 25.7782\n",
      "Epoch: 561 [  100/50000 ( 0%)]  \tLoss:   91.632713\trec:   66.944000\tkl:   24.688711\n",
      "Epoch: 561 [10100/50000 (20%)]  \tLoss:   98.545250\trec:   72.877853\tkl:   25.667393\n",
      "Epoch: 561 [20100/50000 (40%)]  \tLoss:   97.015884\trec:   70.904236\tkl:   26.111652\n",
      "Epoch: 561 [30100/50000 (60%)]  \tLoss:   90.319946\trec:   64.999123\tkl:   25.320826\n",
      "Epoch: 561 [40100/50000 (80%)]  \tLoss:   87.536446\trec:   63.166973\tkl:   24.369474\n",
      "====> Epoch: 561 Average train loss: 93.3157\n",
      "====> Validation set loss: 96.7418\n",
      "====> Validation set kl: 25.5400\n",
      "Epoch: 562 [  100/50000 ( 0%)]  \tLoss:   93.244858\trec:   68.186676\tkl:   25.058180\n",
      "Epoch: 562 [10100/50000 (20%)]  \tLoss:   96.441200\trec:   70.834106\tkl:   25.607090\n",
      "Epoch: 562 [20100/50000 (40%)]  \tLoss:   90.979210\trec:   65.142838\tkl:   25.836367\n",
      "Epoch: 562 [30100/50000 (60%)]  \tLoss:   90.546318\trec:   65.470924\tkl:   25.075392\n",
      "Epoch: 562 [40100/50000 (80%)]  \tLoss:   99.611511\trec:   72.062088\tkl:   27.549421\n",
      "====> Epoch: 562 Average train loss: 93.1957\n",
      "====> Validation set loss: 95.7678\n",
      "====> Validation set kl: 25.4083\n",
      "Epoch: 563 [  100/50000 ( 0%)]  \tLoss:   90.458153\trec:   64.908066\tkl:   25.550087\n",
      "Epoch: 563 [10100/50000 (20%)]  \tLoss:   95.263573\trec:   68.306694\tkl:   26.956869\n",
      "Epoch: 563 [20100/50000 (40%)]  \tLoss:   91.861656\trec:   66.377930\tkl:   25.483723\n",
      "Epoch: 563 [30100/50000 (60%)]  \tLoss:   89.571739\trec:   64.300690\tkl:   25.271049\n",
      "Epoch: 563 [40100/50000 (80%)]  \tLoss:   90.884323\trec:   65.784142\tkl:   25.100187\n",
      "====> Epoch: 563 Average train loss: 93.2606\n",
      "====> Validation set loss: 95.9186\n",
      "====> Validation set kl: 26.1151\n",
      "Epoch: 564 [  100/50000 ( 0%)]  \tLoss:   90.691811\trec:   65.895950\tkl:   24.795858\n",
      "Epoch: 564 [10100/50000 (20%)]  \tLoss:  100.271889\trec:   72.866409\tkl:   27.405485\n",
      "Epoch: 564 [20100/50000 (40%)]  \tLoss:   94.623718\trec:   67.115372\tkl:   27.508352\n",
      "Epoch: 564 [30100/50000 (60%)]  \tLoss:   98.184227\trec:   71.897354\tkl:   26.286867\n",
      "Epoch: 564 [40100/50000 (80%)]  \tLoss:   93.237030\trec:   68.003868\tkl:   25.233162\n",
      "====> Epoch: 564 Average train loss: 93.2709\n",
      "====> Validation set loss: 95.8650\n",
      "====> Validation set kl: 25.3584\n",
      "Epoch: 565 [  100/50000 ( 0%)]  \tLoss:   91.793358\trec:   67.168365\tkl:   24.624990\n",
      "Epoch: 565 [10100/50000 (20%)]  \tLoss:   92.831459\trec:   66.890099\tkl:   25.941362\n",
      "Epoch: 565 [20100/50000 (40%)]  \tLoss:   92.782616\trec:   66.964043\tkl:   25.818571\n",
      "Epoch: 565 [30100/50000 (60%)]  \tLoss:   91.013145\trec:   65.614326\tkl:   25.398821\n",
      "Epoch: 565 [40100/50000 (80%)]  \tLoss:   94.844971\trec:   68.894630\tkl:   25.950338\n",
      "====> Epoch: 565 Average train loss: 93.1958\n",
      "====> Validation set loss: 96.0088\n",
      "====> Validation set kl: 25.3004\n",
      "Epoch: 566 [  100/50000 ( 0%)]  \tLoss:   89.933533\trec:   64.801247\tkl:   25.132290\n",
      "Epoch: 566 [10100/50000 (20%)]  \tLoss:   94.285561\trec:   68.421379\tkl:   25.864183\n",
      "Epoch: 566 [20100/50000 (40%)]  \tLoss:   93.870461\trec:   67.878281\tkl:   25.992178\n",
      "Epoch: 566 [30100/50000 (60%)]  \tLoss:   96.990776\trec:   70.246521\tkl:   26.744263\n",
      "Epoch: 566 [40100/50000 (80%)]  \tLoss:   90.249413\trec:   65.027214\tkl:   25.222197\n",
      "====> Epoch: 566 Average train loss: 93.3000\n",
      "====> Validation set loss: 95.7880\n",
      "====> Validation set kl: 25.5345\n",
      "Epoch: 567 [  100/50000 ( 0%)]  \tLoss:   94.198952\trec:   68.854385\tkl:   25.344570\n",
      "Epoch: 567 [10100/50000 (20%)]  \tLoss:   97.162605\trec:   71.090965\tkl:   26.071642\n",
      "Epoch: 567 [20100/50000 (40%)]  \tLoss:   95.815239\trec:   70.052849\tkl:   25.762394\n",
      "Epoch: 567 [30100/50000 (60%)]  \tLoss:   94.141006\trec:   69.228760\tkl:   24.912243\n",
      "Epoch: 567 [40100/50000 (80%)]  \tLoss:   94.651794\trec:   69.779587\tkl:   24.872210\n",
      "====> Epoch: 567 Average train loss: 93.1437\n",
      "====> Validation set loss: 95.9112\n",
      "====> Validation set kl: 25.8659\n",
      "Epoch: 568 [  100/50000 ( 0%)]  \tLoss:   92.663963\trec:   66.801910\tkl:   25.862053\n",
      "Epoch: 568 [10100/50000 (20%)]  \tLoss:   92.197617\trec:   66.480278\tkl:   25.717333\n",
      "Epoch: 568 [20100/50000 (40%)]  \tLoss:   90.053513\trec:   64.681694\tkl:   25.371826\n",
      "Epoch: 568 [30100/50000 (60%)]  \tLoss:   92.940918\trec:   67.489746\tkl:   25.451166\n",
      "Epoch: 568 [40100/50000 (80%)]  \tLoss:   97.403595\trec:   71.711739\tkl:   25.691854\n",
      "====> Epoch: 568 Average train loss: 93.0381\n",
      "====> Validation set loss: 96.2607\n",
      "====> Validation set kl: 25.5005\n",
      "Epoch: 569 [  100/50000 ( 0%)]  \tLoss:   87.647301\trec:   63.741005\tkl:   23.906303\n",
      "Epoch: 569 [10100/50000 (20%)]  \tLoss:   91.268448\trec:   65.873848\tkl:   25.394600\n",
      "Epoch: 569 [20100/50000 (40%)]  \tLoss:   92.779457\trec:   66.894646\tkl:   25.884821\n",
      "Epoch: 569 [30100/50000 (60%)]  \tLoss:   97.244118\trec:   71.141335\tkl:   26.102777\n",
      "Epoch: 569 [40100/50000 (80%)]  \tLoss:   91.544579\trec:   66.128189\tkl:   25.416384\n",
      "====> Epoch: 569 Average train loss: 93.6310\n",
      "====> Validation set loss: 96.1760\n",
      "====> Validation set kl: 25.8547\n",
      "Epoch: 570 [  100/50000 ( 0%)]  \tLoss:   95.012955\trec:   69.187950\tkl:   25.825008\n",
      "Epoch: 570 [10100/50000 (20%)]  \tLoss:   92.330093\trec:   66.627159\tkl:   25.702936\n",
      "Epoch: 570 [20100/50000 (40%)]  \tLoss:   87.227158\trec:   62.454845\tkl:   24.772312\n",
      "Epoch: 570 [30100/50000 (60%)]  \tLoss:   93.674637\trec:   68.260178\tkl:   25.414459\n",
      "Epoch: 570 [40100/50000 (80%)]  \tLoss:   94.795486\trec:   67.965874\tkl:   26.829603\n",
      "====> Epoch: 570 Average train loss: 93.5659\n",
      "====> Validation set loss: 97.4311\n",
      "====> Validation set kl: 25.1689\n",
      "Epoch: 571 [  100/50000 ( 0%)]  \tLoss:   91.362221\trec:   66.478394\tkl:   24.883825\n",
      "Epoch: 571 [10100/50000 (20%)]  \tLoss:   90.130478\trec:   65.308922\tkl:   24.821550\n",
      "Epoch: 571 [20100/50000 (40%)]  \tLoss:   96.897255\trec:   70.451378\tkl:   26.445881\n",
      "Epoch: 571 [30100/50000 (60%)]  \tLoss:   96.201637\trec:   69.550728\tkl:   26.650908\n",
      "Epoch: 571 [40100/50000 (80%)]  \tLoss:   99.183769\trec:   72.156242\tkl:   27.027523\n",
      "====> Epoch: 571 Average train loss: 93.5611\n",
      "====> Validation set loss: 96.0026\n",
      "====> Validation set kl: 25.9095\n",
      "Epoch: 572 [  100/50000 ( 0%)]  \tLoss:   91.892250\trec:   66.834000\tkl:   25.058254\n",
      "Epoch: 572 [10100/50000 (20%)]  \tLoss:   92.990776\trec:   67.737625\tkl:   25.253153\n",
      "Epoch: 572 [20100/50000 (40%)]  \tLoss:   92.913857\trec:   68.175064\tkl:   24.738789\n",
      "Epoch: 572 [30100/50000 (60%)]  \tLoss:   92.393913\trec:   67.041046\tkl:   25.352871\n",
      "Epoch: 572 [40100/50000 (80%)]  \tLoss:   98.436005\trec:   71.615059\tkl:   26.820944\n",
      "====> Epoch: 572 Average train loss: 93.5767\n",
      "====> Validation set loss: 96.8983\n",
      "====> Validation set kl: 25.3219\n",
      "Epoch: 573 [  100/50000 ( 0%)]  \tLoss:   96.732033\trec:   71.268227\tkl:   25.463799\n",
      "Epoch: 573 [10100/50000 (20%)]  \tLoss:   93.762207\trec:   68.589439\tkl:   25.172766\n",
      "Epoch: 573 [20100/50000 (40%)]  \tLoss:   92.232536\trec:   67.164597\tkl:   25.067949\n",
      "Epoch: 573 [30100/50000 (60%)]  \tLoss:   94.467812\trec:   68.584526\tkl:   25.883278\n",
      "Epoch: 573 [40100/50000 (80%)]  \tLoss:   93.186050\trec:   68.067047\tkl:   25.119005\n",
      "====> Epoch: 573 Average train loss: 93.5747\n",
      "====> Validation set loss: 96.1313\n",
      "====> Validation set kl: 25.8700\n",
      "Epoch: 574 [  100/50000 ( 0%)]  \tLoss:   94.074181\trec:   67.583145\tkl:   26.491039\n",
      "Epoch: 574 [10100/50000 (20%)]  \tLoss:   93.999985\trec:   67.657196\tkl:   26.342793\n",
      "Epoch: 574 [20100/50000 (40%)]  \tLoss:   96.370659\trec:   69.411003\tkl:   26.959663\n",
      "Epoch: 574 [30100/50000 (60%)]  \tLoss:   94.233231\trec:   68.855240\tkl:   25.377991\n",
      "Epoch: 574 [40100/50000 (80%)]  \tLoss:   96.560898\trec:   70.006905\tkl:   26.553993\n",
      "====> Epoch: 574 Average train loss: 94.0059\n",
      "====> Validation set loss: 97.3017\n",
      "====> Validation set kl: 26.0129\n",
      "Epoch: 575 [  100/50000 ( 0%)]  \tLoss:   96.980034\trec:   69.986465\tkl:   26.993576\n",
      "Epoch: 575 [10100/50000 (20%)]  \tLoss:   93.628403\trec:   67.650017\tkl:   25.978390\n",
      "Epoch: 575 [20100/50000 (40%)]  \tLoss:   93.716461\trec:   67.840187\tkl:   25.876276\n",
      "Epoch: 575 [30100/50000 (60%)]  \tLoss:   89.522758\trec:   64.619873\tkl:   24.902884\n",
      "Epoch: 575 [40100/50000 (80%)]  \tLoss:   94.657768\trec:   68.153320\tkl:   26.504448\n",
      "====> Epoch: 575 Average train loss: 93.9409\n",
      "====> Validation set loss: 96.7977\n",
      "====> Validation set kl: 25.5302\n",
      "Epoch: 576 [  100/50000 ( 0%)]  \tLoss:   94.133087\trec:   68.611229\tkl:   25.521851\n",
      "Epoch: 576 [10100/50000 (20%)]  \tLoss:   90.650177\trec:   65.555275\tkl:   25.094900\n",
      "Epoch: 576 [20100/50000 (40%)]  \tLoss:   96.129951\trec:   70.966599\tkl:   25.163349\n",
      "Epoch: 576 [30100/50000 (60%)]  \tLoss:   90.587830\trec:   64.436989\tkl:   26.150845\n",
      "Epoch: 576 [40100/50000 (80%)]  \tLoss:   94.406265\trec:   68.779472\tkl:   25.626787\n",
      "====> Epoch: 576 Average train loss: 93.7373\n",
      "====> Validation set loss: 96.6430\n",
      "====> Validation set kl: 26.1135\n",
      "Epoch: 577 [  100/50000 ( 0%)]  \tLoss:   99.774529\trec:   72.282829\tkl:   27.491701\n",
      "Epoch: 577 [10100/50000 (20%)]  \tLoss:   92.471092\trec:   66.401085\tkl:   26.070015\n",
      "Epoch: 577 [20100/50000 (40%)]  \tLoss:   94.721931\trec:   68.278755\tkl:   26.443178\n",
      "Epoch: 577 [30100/50000 (60%)]  \tLoss:   93.251968\trec:   67.406662\tkl:   25.845303\n",
      "Epoch: 577 [40100/50000 (80%)]  \tLoss:   99.664688\trec:   73.267967\tkl:   26.396723\n",
      "====> Epoch: 577 Average train loss: 93.5637\n",
      "====> Validation set loss: 97.6018\n",
      "====> Validation set kl: 26.3033\n",
      "Epoch: 578 [  100/50000 ( 0%)]  \tLoss:   92.902740\trec:   66.525299\tkl:   26.377441\n",
      "Epoch: 578 [10100/50000 (20%)]  \tLoss:   97.764381\trec:   71.241402\tkl:   26.522976\n",
      "Epoch: 578 [20100/50000 (40%)]  \tLoss:   93.119957\trec:   67.937408\tkl:   25.182550\n",
      "Epoch: 578 [30100/50000 (60%)]  \tLoss:   95.653824\trec:   70.351730\tkl:   25.302099\n",
      "Epoch: 578 [40100/50000 (80%)]  \tLoss:   95.213387\trec:   68.916641\tkl:   26.296743\n",
      "====> Epoch: 578 Average train loss: 93.7576\n",
      "====> Validation set loss: 96.2275\n",
      "====> Validation set kl: 25.8482\n",
      "Epoch: 579 [  100/50000 ( 0%)]  \tLoss:   97.538086\trec:   70.622566\tkl:   26.915522\n",
      "Epoch: 579 [10100/50000 (20%)]  \tLoss:   97.221382\trec:   70.775803\tkl:   26.445585\n",
      "Epoch: 579 [20100/50000 (40%)]  \tLoss:   93.281181\trec:   67.902977\tkl:   25.378204\n",
      "Epoch: 579 [30100/50000 (60%)]  \tLoss:   98.811684\trec:   71.632301\tkl:   27.179386\n",
      "Epoch: 579 [40100/50000 (80%)]  \tLoss:   92.314667\trec:   66.844803\tkl:   25.469866\n",
      "====> Epoch: 579 Average train loss: 93.3121\n",
      "====> Validation set loss: 95.9602\n",
      "====> Validation set kl: 25.4713\n",
      "Epoch: 580 [  100/50000 ( 0%)]  \tLoss:   95.796371\trec:   69.467682\tkl:   26.328693\n",
      "Epoch: 580 [10100/50000 (20%)]  \tLoss:   92.737534\trec:   66.874588\tkl:   25.862944\n",
      "Epoch: 580 [20100/50000 (40%)]  \tLoss:   92.250664\trec:   67.910034\tkl:   24.340633\n",
      "Epoch: 580 [30100/50000 (60%)]  \tLoss:   92.986885\trec:   67.130211\tkl:   25.856672\n",
      "Epoch: 580 [40100/50000 (80%)]  \tLoss:   94.418976\trec:   68.873169\tkl:   25.545803\n",
      "====> Epoch: 580 Average train loss: 93.4691\n",
      "====> Validation set loss: 97.2266\n",
      "====> Validation set kl: 26.9755\n",
      "Epoch: 581 [  100/50000 ( 0%)]  \tLoss:   96.604637\trec:   68.759026\tkl:   27.845617\n",
      "Epoch: 581 [10100/50000 (20%)]  \tLoss:   91.196114\trec:   65.646477\tkl:   25.549635\n",
      "Epoch: 581 [20100/50000 (40%)]  \tLoss:   89.818764\trec:   64.297661\tkl:   25.521109\n",
      "Epoch: 581 [30100/50000 (60%)]  \tLoss:   96.263687\trec:   69.851730\tkl:   26.411963\n",
      "Epoch: 581 [40100/50000 (80%)]  \tLoss:   91.788979\trec:   66.186691\tkl:   25.602295\n",
      "====> Epoch: 581 Average train loss: 93.7155\n",
      "====> Validation set loss: 96.3578\n",
      "====> Validation set kl: 25.6423\n",
      "Epoch: 582 [  100/50000 ( 0%)]  \tLoss:   87.368042\trec:   62.320667\tkl:   25.047382\n",
      "Epoch: 582 [10100/50000 (20%)]  \tLoss:   94.923203\trec:   69.623970\tkl:   25.299234\n",
      "Epoch: 582 [20100/50000 (40%)]  \tLoss:   89.591881\trec:   65.048050\tkl:   24.543835\n",
      "Epoch: 582 [30100/50000 (60%)]  \tLoss:   91.476006\trec:   65.362846\tkl:   26.113159\n",
      "Epoch: 582 [40100/50000 (80%)]  \tLoss:   90.865486\trec:   65.226044\tkl:   25.639439\n",
      "====> Epoch: 582 Average train loss: 93.8346\n",
      "====> Validation set loss: 96.7700\n",
      "====> Validation set kl: 25.5707\n",
      "Epoch: 583 [  100/50000 ( 0%)]  \tLoss:   93.577332\trec:   67.690727\tkl:   25.886608\n",
      "Epoch: 583 [10100/50000 (20%)]  \tLoss:   91.260117\trec:   65.208229\tkl:   26.051889\n",
      "Epoch: 583 [20100/50000 (40%)]  \tLoss:   95.423615\trec:   69.516045\tkl:   25.907568\n",
      "Epoch: 583 [30100/50000 (60%)]  \tLoss:   89.144539\trec:   63.789307\tkl:   25.355234\n",
      "Epoch: 583 [40100/50000 (80%)]  \tLoss:   91.111748\trec:   65.854683\tkl:   25.257063\n",
      "====> Epoch: 583 Average train loss: 93.7743\n",
      "====> Validation set loss: 97.2020\n",
      "====> Validation set kl: 25.6436\n",
      "Epoch: 584 [  100/50000 ( 0%)]  \tLoss:   90.505211\trec:   65.352448\tkl:   25.152763\n",
      "Epoch: 584 [10100/50000 (20%)]  \tLoss:   92.774551\trec:   66.530991\tkl:   26.243553\n",
      "Epoch: 584 [20100/50000 (40%)]  \tLoss:   95.635262\trec:   69.529449\tkl:   26.105814\n",
      "Epoch: 584 [30100/50000 (60%)]  \tLoss:   95.727768\trec:   69.517097\tkl:   26.210672\n",
      "Epoch: 584 [40100/50000 (80%)]  \tLoss:   91.065346\trec:   66.635185\tkl:   24.430166\n",
      "====> Epoch: 584 Average train loss: 93.9197\n",
      "====> Validation set loss: 96.4506\n",
      "====> Validation set kl: 25.7965\n",
      "Epoch: 585 [  100/50000 ( 0%)]  \tLoss:   93.495781\trec:   67.103302\tkl:   26.392483\n",
      "Epoch: 585 [10100/50000 (20%)]  \tLoss:   95.594910\trec:   68.946793\tkl:   26.648117\n",
      "Epoch: 585 [20100/50000 (40%)]  \tLoss:   94.182564\trec:   69.833214\tkl:   24.349348\n",
      "Epoch: 585 [30100/50000 (60%)]  \tLoss:   95.381470\trec:   69.873398\tkl:   25.508076\n",
      "Epoch: 585 [40100/50000 (80%)]  \tLoss:   90.665215\trec:   65.289352\tkl:   25.375854\n",
      "====> Epoch: 585 Average train loss: 93.8572\n",
      "====> Validation set loss: 96.0648\n",
      "====> Validation set kl: 25.4473\n",
      "Epoch: 586 [  100/50000 ( 0%)]  \tLoss:   89.917137\trec:   65.660431\tkl:   24.256708\n",
      "Epoch: 586 [10100/50000 (20%)]  \tLoss:   93.139076\trec:   67.644981\tkl:   25.494101\n",
      "Epoch: 586 [20100/50000 (40%)]  \tLoss:   96.365280\trec:   70.034721\tkl:   26.330559\n",
      "Epoch: 586 [30100/50000 (60%)]  \tLoss:   91.385353\trec:   66.059471\tkl:   25.325878\n",
      "Epoch: 586 [40100/50000 (80%)]  \tLoss:   96.918488\trec:   70.374405\tkl:   26.544077\n",
      "====> Epoch: 586 Average train loss: 93.6596\n",
      "====> Validation set loss: 96.1563\n",
      "====> Validation set kl: 26.0303\n",
      "Epoch: 587 [  100/50000 ( 0%)]  \tLoss:   96.787148\trec:   70.704010\tkl:   26.083138\n",
      "Epoch: 587 [10100/50000 (20%)]  \tLoss:   97.612350\trec:   70.884537\tkl:   26.727810\n",
      "Epoch: 587 [20100/50000 (40%)]  \tLoss:   92.052109\trec:   66.667046\tkl:   25.385057\n",
      "Epoch: 587 [30100/50000 (60%)]  \tLoss:   88.074196\trec:   64.006958\tkl:   24.067238\n",
      "Epoch: 587 [40100/50000 (80%)]  \tLoss:   97.363571\trec:   70.331436\tkl:   27.032143\n",
      "====> Epoch: 587 Average train loss: 93.7593\n",
      "====> Validation set loss: 96.9869\n",
      "====> Validation set kl: 25.9033\n",
      "Epoch: 588 [  100/50000 ( 0%)]  \tLoss:   95.780663\trec:   69.883194\tkl:   25.897465\n",
      "Epoch: 588 [10100/50000 (20%)]  \tLoss:   99.507378\trec:   72.325127\tkl:   27.182255\n",
      "Epoch: 588 [20100/50000 (40%)]  \tLoss:   92.960831\trec:   66.920624\tkl:   26.040201\n",
      "Epoch: 588 [30100/50000 (60%)]  \tLoss:   93.302750\trec:   68.563889\tkl:   24.738861\n",
      "Epoch: 588 [40100/50000 (80%)]  \tLoss:   92.411079\trec:   66.745605\tkl:   25.665474\n",
      "====> Epoch: 588 Average train loss: 93.9527\n",
      "====> Validation set loss: 96.5183\n",
      "====> Validation set kl: 25.9490\n",
      "Epoch: 589 [  100/50000 ( 0%)]  \tLoss:   89.174599\trec:   64.546837\tkl:   24.627760\n",
      "Epoch: 589 [10100/50000 (20%)]  \tLoss:   92.167198\trec:   66.460175\tkl:   25.707024\n",
      "Epoch: 589 [20100/50000 (40%)]  \tLoss:   92.940475\trec:   68.109459\tkl:   24.831018\n",
      "Epoch: 589 [30100/50000 (60%)]  \tLoss:   91.012741\trec:   65.522789\tkl:   25.489954\n",
      "Epoch: 589 [40100/50000 (80%)]  \tLoss:   92.768494\trec:   67.182793\tkl:   25.585701\n",
      "====> Epoch: 589 Average train loss: 93.4669\n",
      "====> Validation set loss: 96.2983\n",
      "====> Validation set kl: 25.3734\n",
      "Epoch: 590 [  100/50000 ( 0%)]  \tLoss:   95.850937\trec:   70.001740\tkl:   25.849199\n",
      "Epoch: 590 [10100/50000 (20%)]  \tLoss:   94.348259\trec:   68.646362\tkl:   25.701899\n",
      "Epoch: 590 [20100/50000 (40%)]  \tLoss:   95.001343\trec:   68.986137\tkl:   26.015211\n",
      "Epoch: 590 [30100/50000 (60%)]  \tLoss:   97.640839\trec:   70.756592\tkl:   26.884247\n",
      "Epoch: 590 [40100/50000 (80%)]  \tLoss:   95.326401\trec:   68.478310\tkl:   26.848093\n",
      "====> Epoch: 590 Average train loss: 93.8043\n",
      "====> Validation set loss: 96.4227\n",
      "====> Validation set kl: 25.7486\n",
      "Epoch: 591 [  100/50000 ( 0%)]  \tLoss:   92.680878\trec:   66.916985\tkl:   25.763899\n",
      "Epoch: 591 [10100/50000 (20%)]  \tLoss:   98.094421\trec:   71.191444\tkl:   26.902975\n",
      "Epoch: 591 [20100/50000 (40%)]  \tLoss:   91.959236\trec:   66.505402\tkl:   25.453842\n",
      "Epoch: 591 [30100/50000 (60%)]  \tLoss:   92.051559\trec:   66.039841\tkl:   26.011715\n",
      "Epoch: 591 [40100/50000 (80%)]  \tLoss:   95.069695\trec:   68.698631\tkl:   26.371067\n",
      "====> Epoch: 591 Average train loss: 93.3191\n",
      "====> Validation set loss: 96.0154\n",
      "====> Validation set kl: 25.5311\n",
      "Epoch: 592 [  100/50000 ( 0%)]  \tLoss:   90.874664\trec:   65.265877\tkl:   25.608786\n",
      "Epoch: 592 [10100/50000 (20%)]  \tLoss:   90.847343\trec:   65.081459\tkl:   25.765881\n",
      "Epoch: 592 [20100/50000 (40%)]  \tLoss:   94.573135\trec:   69.029076\tkl:   25.544060\n",
      "Epoch: 592 [30100/50000 (60%)]  \tLoss:   91.690895\trec:   65.871803\tkl:   25.819086\n",
      "Epoch: 592 [40100/50000 (80%)]  \tLoss:   88.887703\trec:   63.865044\tkl:   25.022657\n",
      "====> Epoch: 592 Average train loss: 93.3238\n",
      "====> Validation set loss: 98.0510\n",
      "====> Validation set kl: 26.5396\n",
      "Epoch: 593 [  100/50000 ( 0%)]  \tLoss:   93.161362\trec:   67.221359\tkl:   25.940002\n",
      "Epoch: 593 [10100/50000 (20%)]  \tLoss:   93.564072\trec:   67.984200\tkl:   25.579870\n",
      "Epoch: 593 [20100/50000 (40%)]  \tLoss:   89.471550\trec:   64.349579\tkl:   25.121973\n",
      "Epoch: 593 [30100/50000 (60%)]  \tLoss:   89.295967\trec:   64.203896\tkl:   25.092068\n",
      "Epoch: 593 [40100/50000 (80%)]  \tLoss:   98.004646\trec:   70.903976\tkl:   27.100670\n",
      "====> Epoch: 593 Average train loss: 93.7475\n",
      "====> Validation set loss: 96.6014\n",
      "====> Validation set kl: 26.5208\n",
      "Epoch: 594 [  100/50000 ( 0%)]  \tLoss:   92.872833\trec:   66.705566\tkl:   26.167265\n",
      "Epoch: 594 [10100/50000 (20%)]  \tLoss:   97.981071\trec:   72.044136\tkl:   25.936937\n",
      "Epoch: 594 [20100/50000 (40%)]  \tLoss:   97.407265\trec:   71.551659\tkl:   25.855604\n",
      "Epoch: 594 [30100/50000 (60%)]  \tLoss:   91.209213\trec:   66.181900\tkl:   25.027311\n",
      "Epoch: 594 [40100/50000 (80%)]  \tLoss:   89.782120\trec:   63.617641\tkl:   26.164474\n",
      "====> Epoch: 594 Average train loss: 93.7706\n",
      "====> Validation set loss: 96.3647\n",
      "====> Validation set kl: 25.5684\n",
      "Epoch: 595 [  100/50000 ( 0%)]  \tLoss:   92.468025\trec:   66.521606\tkl:   25.946423\n",
      "Epoch: 595 [10100/50000 (20%)]  \tLoss:   90.943764\trec:   65.689087\tkl:   25.254679\n",
      "Epoch: 595 [20100/50000 (40%)]  \tLoss:   95.231270\trec:   69.100998\tkl:   26.130272\n",
      "Epoch: 595 [30100/50000 (60%)]  \tLoss:   93.611603\trec:   67.390991\tkl:   26.220610\n",
      "Epoch: 595 [40100/50000 (80%)]  \tLoss:   90.127052\trec:   65.007301\tkl:   25.119745\n",
      "====> Epoch: 595 Average train loss: 93.2635\n",
      "====> Validation set loss: 95.8994\n",
      "====> Validation set kl: 25.4273\n",
      "Epoch: 596 [  100/50000 ( 0%)]  \tLoss:   89.850174\trec:   64.721329\tkl:   25.128845\n",
      "Epoch: 596 [10100/50000 (20%)]  \tLoss:   93.417686\trec:   67.258553\tkl:   26.159130\n",
      "Epoch: 596 [20100/50000 (40%)]  \tLoss:   93.604362\trec:   68.221695\tkl:   25.382673\n",
      "Epoch: 596 [30100/50000 (60%)]  \tLoss:   99.845718\trec:   73.276474\tkl:   26.569248\n",
      "Epoch: 596 [40100/50000 (80%)]  \tLoss:   92.032310\trec:   66.434227\tkl:   25.598085\n",
      "====> Epoch: 596 Average train loss: 93.1911\n",
      "====> Validation set loss: 95.7164\n",
      "====> Validation set kl: 25.4698\n",
      "Epoch: 597 [  100/50000 ( 0%)]  \tLoss:   87.722038\trec:   62.992630\tkl:   24.729408\n",
      "Epoch: 597 [10100/50000 (20%)]  \tLoss:   92.002251\trec:   66.803520\tkl:   25.198730\n",
      "Epoch: 597 [20100/50000 (40%)]  \tLoss:   92.528114\trec:   67.116875\tkl:   25.411238\n",
      "Epoch: 597 [30100/50000 (60%)]  \tLoss:   89.822418\trec:   64.595421\tkl:   25.226997\n",
      "Epoch: 597 [40100/50000 (80%)]  \tLoss:   94.704605\trec:   68.736237\tkl:   25.968378\n",
      "====> Epoch: 597 Average train loss: 93.2244\n",
      "====> Validation set loss: 95.8325\n",
      "====> Validation set kl: 25.4937\n",
      "Epoch: 598 [  100/50000 ( 0%)]  \tLoss:   94.625069\trec:   69.077042\tkl:   25.548029\n",
      "Epoch: 598 [10100/50000 (20%)]  \tLoss:   92.309952\trec:   66.881630\tkl:   25.428322\n",
      "Epoch: 598 [20100/50000 (40%)]  \tLoss:   91.844185\trec:   66.629761\tkl:   25.214430\n",
      "Epoch: 598 [30100/50000 (60%)]  \tLoss:   98.563004\trec:   71.869438\tkl:   26.693565\n",
      "Epoch: 598 [40100/50000 (80%)]  \tLoss:   95.127861\trec:   68.824242\tkl:   26.303616\n",
      "====> Epoch: 598 Average train loss: 93.0614\n",
      "====> Validation set loss: 97.0283\n",
      "====> Validation set kl: 25.9232\n",
      "Epoch: 599 [  100/50000 ( 0%)]  \tLoss:   91.496735\trec:   65.686745\tkl:   25.809998\n",
      "Epoch: 599 [10100/50000 (20%)]  \tLoss:   89.756279\trec:   64.425041\tkl:   25.331242\n",
      "Epoch: 599 [20100/50000 (40%)]  \tLoss:   97.354721\trec:   72.051445\tkl:   25.303280\n",
      "Epoch: 599 [30100/50000 (60%)]  \tLoss:   91.884872\trec:   66.294289\tkl:   25.590586\n",
      "Epoch: 599 [40100/50000 (80%)]  \tLoss:   96.987617\trec:   70.272003\tkl:   26.715614\n",
      "====> Epoch: 599 Average train loss: 93.2711\n",
      "====> Validation set loss: 96.6393\n",
      "====> Validation set kl: 25.4138\n",
      "Epoch: 600 [  100/50000 ( 0%)]  \tLoss:   96.015991\trec:   70.703476\tkl:   25.312517\n",
      "Epoch: 600 [10100/50000 (20%)]  \tLoss:   92.488037\trec:   66.649529\tkl:   25.838503\n",
      "Epoch: 600 [20100/50000 (40%)]  \tLoss:   92.481010\trec:   66.695518\tkl:   25.785496\n",
      "Epoch: 600 [30100/50000 (60%)]  \tLoss:   91.757744\trec:   66.967285\tkl:   24.790457\n",
      "Epoch: 600 [40100/50000 (80%)]  \tLoss:   92.443192\trec:   66.976921\tkl:   25.466269\n",
      "====> Epoch: 600 Average train loss: 93.1971\n",
      "====> Validation set loss: 95.9998\n",
      "====> Validation set kl: 25.5115\n",
      "Epoch: 601 [  100/50000 ( 0%)]  \tLoss:   94.345741\trec:   68.548317\tkl:   25.797421\n",
      "Epoch: 601 [10100/50000 (20%)]  \tLoss:   91.734230\trec:   65.389763\tkl:   26.344465\n",
      "Epoch: 601 [20100/50000 (40%)]  \tLoss:   94.003555\trec:   68.387390\tkl:   25.616161\n",
      "Epoch: 601 [30100/50000 (60%)]  \tLoss:   91.325020\trec:   66.188484\tkl:   25.136530\n",
      "Epoch: 601 [40100/50000 (80%)]  \tLoss:   98.869240\trec:   71.871269\tkl:   26.997965\n",
      "====> Epoch: 601 Average train loss: 93.5916\n",
      "====> Validation set loss: 96.1650\n",
      "====> Validation set kl: 25.8813\n",
      "Epoch: 602 [  100/50000 ( 0%)]  \tLoss:   93.906601\trec:   66.998886\tkl:   26.907715\n",
      "Epoch: 602 [10100/50000 (20%)]  \tLoss:   96.095749\trec:   68.943748\tkl:   27.152004\n",
      "Epoch: 602 [20100/50000 (40%)]  \tLoss:   97.030098\trec:   70.158806\tkl:   26.871294\n",
      "Epoch: 602 [30100/50000 (60%)]  \tLoss:   90.391945\trec:   64.715157\tkl:   25.676785\n",
      "Epoch: 602 [40100/50000 (80%)]  \tLoss:   88.208328\trec:   63.722645\tkl:   24.485685\n",
      "====> Epoch: 602 Average train loss: 93.4137\n",
      "====> Validation set loss: 96.0606\n",
      "====> Validation set kl: 25.1182\n",
      "Epoch: 603 [  100/50000 ( 0%)]  \tLoss:   90.090836\trec:   65.490303\tkl:   24.600534\n",
      "Epoch: 603 [10100/50000 (20%)]  \tLoss:   95.546539\trec:   69.024040\tkl:   26.522505\n",
      "Epoch: 603 [20100/50000 (40%)]  \tLoss:   96.788223\trec:   69.727821\tkl:   27.060398\n",
      "Epoch: 603 [30100/50000 (60%)]  \tLoss:   98.900185\trec:   72.152443\tkl:   26.747746\n",
      "Epoch: 603 [40100/50000 (80%)]  \tLoss:   95.892616\trec:   69.390564\tkl:   26.502050\n",
      "====> Epoch: 603 Average train loss: 94.0062\n",
      "====> Validation set loss: 96.1525\n",
      "====> Validation set kl: 25.9966\n",
      "Epoch: 604 [  100/50000 ( 0%)]  \tLoss:   88.406441\trec:   63.248379\tkl:   25.158068\n",
      "Epoch: 604 [10100/50000 (20%)]  \tLoss:   96.008507\trec:   70.048393\tkl:   25.960104\n",
      "Epoch: 604 [20100/50000 (40%)]  \tLoss:   93.256874\trec:   68.245506\tkl:   25.011366\n",
      "Epoch: 604 [30100/50000 (60%)]  \tLoss:   91.552330\trec:   66.556885\tkl:   24.995449\n",
      "Epoch: 604 [40100/50000 (80%)]  \tLoss:   90.987419\trec:   66.425308\tkl:   24.562109\n",
      "====> Epoch: 604 Average train loss: 93.2291\n",
      "====> Validation set loss: 96.9991\n",
      "====> Validation set kl: 25.8416\n",
      "Epoch: 605 [  100/50000 ( 0%)]  \tLoss:   94.280838\trec:   69.419289\tkl:   24.861549\n",
      "Epoch: 605 [10100/50000 (20%)]  \tLoss:   93.236450\trec:   67.423546\tkl:   25.812902\n",
      "Epoch: 605 [20100/50000 (40%)]  \tLoss:   91.710526\trec:   66.359138\tkl:   25.351383\n",
      "Epoch: 605 [30100/50000 (60%)]  \tLoss:   90.195290\trec:   64.704887\tkl:   25.490400\n",
      "Epoch: 605 [40100/50000 (80%)]  \tLoss:   96.093971\trec:   70.914726\tkl:   25.179243\n",
      "====> Epoch: 605 Average train loss: 93.2224\n",
      "====> Validation set loss: 96.4385\n",
      "====> Validation set kl: 25.3966\n",
      "Epoch: 606 [  100/50000 ( 0%)]  \tLoss:   91.932533\trec:   67.235184\tkl:   24.697353\n",
      "Epoch: 606 [10100/50000 (20%)]  \tLoss:   92.734444\trec:   68.286720\tkl:   24.447723\n",
      "Epoch: 606 [20100/50000 (40%)]  \tLoss:   95.360405\trec:   67.996651\tkl:   27.363749\n",
      "Epoch: 606 [30100/50000 (60%)]  \tLoss:   94.391464\trec:   69.052643\tkl:   25.338814\n",
      "Epoch: 606 [40100/50000 (80%)]  \tLoss:   91.417419\trec:   66.009277\tkl:   25.408138\n",
      "====> Epoch: 606 Average train loss: 93.3554\n",
      "====> Validation set loss: 95.8960\n",
      "====> Validation set kl: 25.5788\n",
      "Epoch: 607 [  100/50000 ( 0%)]  \tLoss:   93.548080\trec:   67.112373\tkl:   26.435707\n",
      "Epoch: 607 [10100/50000 (20%)]  \tLoss:   93.020096\trec:   67.529892\tkl:   25.490208\n",
      "Epoch: 607 [20100/50000 (40%)]  \tLoss:   97.300201\trec:   70.157478\tkl:   27.142721\n",
      "Epoch: 607 [30100/50000 (60%)]  \tLoss:   92.923096\trec:   67.628075\tkl:   25.295017\n",
      "Epoch: 607 [40100/50000 (80%)]  \tLoss:   92.415535\trec:   67.382072\tkl:   25.033464\n",
      "====> Epoch: 607 Average train loss: 93.2283\n",
      "====> Validation set loss: 96.1203\n",
      "====> Validation set kl: 25.6567\n",
      "Epoch: 608 [  100/50000 ( 0%)]  \tLoss:   91.615425\trec:   65.915092\tkl:   25.700336\n",
      "Epoch: 608 [10100/50000 (20%)]  \tLoss:   93.832680\trec:   68.338341\tkl:   25.494343\n",
      "Epoch: 608 [20100/50000 (40%)]  \tLoss:   98.472282\trec:   72.273872\tkl:   26.198408\n",
      "Epoch: 608 [30100/50000 (60%)]  \tLoss:   93.082138\trec:   67.244576\tkl:   25.837555\n",
      "Epoch: 608 [40100/50000 (80%)]  \tLoss:   89.759064\trec:   64.486748\tkl:   25.272316\n",
      "====> Epoch: 608 Average train loss: 93.3195\n",
      "====> Validation set loss: 96.0022\n",
      "====> Validation set kl: 25.8049\n",
      "Epoch: 609 [  100/50000 ( 0%)]  \tLoss:   94.023758\trec:   67.547852\tkl:   26.475908\n",
      "Epoch: 609 [10100/50000 (20%)]  \tLoss:   96.808189\trec:   70.486084\tkl:   26.322111\n",
      "Epoch: 609 [20100/50000 (40%)]  \tLoss:   91.096642\trec:   66.143204\tkl:   24.953442\n",
      "Epoch: 609 [30100/50000 (60%)]  \tLoss:   93.361275\trec:   67.830177\tkl:   25.531097\n",
      "Epoch: 609 [40100/50000 (80%)]  \tLoss:   92.839157\trec:   66.769508\tkl:   26.069651\n",
      "====> Epoch: 609 Average train loss: 93.2898\n",
      "====> Validation set loss: 95.7573\n",
      "====> Validation set kl: 25.7101\n",
      "Epoch: 610 [  100/50000 ( 0%)]  \tLoss:   90.850113\trec:   65.907990\tkl:   24.942122\n",
      "Epoch: 610 [10100/50000 (20%)]  \tLoss:   93.489258\trec:   67.223297\tkl:   26.265961\n",
      "Epoch: 610 [20100/50000 (40%)]  \tLoss:   92.948318\trec:   67.430382\tkl:   25.517937\n",
      "Epoch: 610 [30100/50000 (60%)]  \tLoss:   93.036804\trec:   67.790649\tkl:   25.246155\n",
      "Epoch: 610 [40100/50000 (80%)]  \tLoss:   92.276382\trec:   67.297188\tkl:   24.979189\n",
      "====> Epoch: 610 Average train loss: 93.1555\n",
      "====> Validation set loss: 95.9934\n",
      "====> Validation set kl: 25.6335\n",
      "Epoch: 611 [  100/50000 ( 0%)]  \tLoss:   95.539429\trec:   69.872055\tkl:   25.667381\n",
      "Epoch: 611 [10100/50000 (20%)]  \tLoss:   94.537872\trec:   68.368500\tkl:   26.169374\n",
      "Epoch: 611 [20100/50000 (40%)]  \tLoss:   91.699707\trec:   66.259171\tkl:   25.440538\n",
      "Epoch: 611 [30100/50000 (60%)]  \tLoss:   91.694191\trec:   67.071381\tkl:   24.622810\n",
      "Epoch: 611 [40100/50000 (80%)]  \tLoss:   92.178223\trec:   67.104370\tkl:   25.073843\n",
      "====> Epoch: 611 Average train loss: 93.5296\n",
      "====> Validation set loss: 96.2714\n",
      "====> Validation set kl: 25.4520\n",
      "Epoch: 612 [  100/50000 ( 0%)]  \tLoss:   91.216423\trec:   66.341850\tkl:   24.874569\n",
      "Epoch: 612 [10100/50000 (20%)]  \tLoss:   93.263229\trec:   66.842583\tkl:   26.420647\n",
      "Epoch: 612 [20100/50000 (40%)]  \tLoss:   91.994072\trec:   66.779015\tkl:   25.215061\n",
      "Epoch: 612 [30100/50000 (60%)]  \tLoss:   90.786407\trec:   65.582535\tkl:   25.203861\n",
      "Epoch: 612 [40100/50000 (80%)]  \tLoss:   94.977829\trec:   69.200035\tkl:   25.777794\n",
      "====> Epoch: 612 Average train loss: 93.2208\n",
      "====> Validation set loss: 95.8648\n",
      "====> Validation set kl: 25.6649\n",
      "Epoch: 613 [  100/50000 ( 0%)]  \tLoss:   90.051758\trec:   65.162315\tkl:   24.889441\n",
      "Epoch: 613 [10100/50000 (20%)]  \tLoss:   92.854538\trec:   67.032288\tkl:   25.822250\n",
      "Epoch: 613 [20100/50000 (40%)]  \tLoss:   96.942673\trec:   70.489594\tkl:   26.453075\n",
      "Epoch: 613 [30100/50000 (60%)]  \tLoss:   90.465981\trec:   65.463104\tkl:   25.002878\n",
      "Epoch: 613 [40100/50000 (80%)]  \tLoss:   92.764336\trec:   66.275909\tkl:   26.488426\n",
      "====> Epoch: 613 Average train loss: 93.1966\n",
      "====> Validation set loss: 96.8621\n",
      "====> Validation set kl: 25.5783\n",
      "Epoch: 614 [  100/50000 ( 0%)]  \tLoss:   95.768600\trec:   70.187065\tkl:   25.581535\n",
      "Epoch: 614 [10100/50000 (20%)]  \tLoss:   96.671783\trec:   71.586433\tkl:   25.085348\n",
      "Epoch: 614 [20100/50000 (40%)]  \tLoss:   95.379059\trec:   69.029747\tkl:   26.349318\n",
      "Epoch: 614 [30100/50000 (60%)]  \tLoss:   94.130157\trec:   69.151413\tkl:   24.978743\n",
      "Epoch: 614 [40100/50000 (80%)]  \tLoss:   94.670044\trec:   68.466141\tkl:   26.203909\n",
      "====> Epoch: 614 Average train loss: 93.4194\n",
      "====> Validation set loss: 96.1293\n",
      "====> Validation set kl: 26.0320\n",
      "Epoch: 615 [  100/50000 ( 0%)]  \tLoss:   90.741364\trec:   64.492615\tkl:   26.248755\n",
      "Epoch: 615 [10100/50000 (20%)]  \tLoss:   92.254066\trec:   65.253586\tkl:   27.000488\n",
      "Epoch: 615 [20100/50000 (40%)]  \tLoss:   92.148827\trec:   66.350006\tkl:   25.798820\n",
      "Epoch: 615 [30100/50000 (60%)]  \tLoss:   88.180542\trec:   62.882595\tkl:   25.297949\n",
      "Epoch: 615 [40100/50000 (80%)]  \tLoss:   93.533066\trec:   68.471046\tkl:   25.062021\n",
      "====> Epoch: 615 Average train loss: 93.2936\n",
      "====> Validation set loss: 96.3898\n",
      "====> Validation set kl: 25.3961\n",
      "Epoch: 616 [  100/50000 ( 0%)]  \tLoss:   93.384552\trec:   68.098228\tkl:   25.286324\n",
      "Epoch: 616 [10100/50000 (20%)]  \tLoss:   91.312233\trec:   66.259575\tkl:   25.052660\n",
      "Epoch: 616 [20100/50000 (40%)]  \tLoss:   91.044151\trec:   65.553406\tkl:   25.490740\n",
      "Epoch: 616 [30100/50000 (60%)]  \tLoss:   90.923347\trec:   65.575890\tkl:   25.347456\n",
      "Epoch: 616 [40100/50000 (80%)]  \tLoss:   96.537300\trec:   70.285927\tkl:   26.251379\n",
      "====> Epoch: 616 Average train loss: 93.1237\n",
      "====> Validation set loss: 96.2173\n",
      "====> Validation set kl: 25.7872\n",
      "Epoch: 617 [  100/50000 ( 0%)]  \tLoss:   92.861267\trec:   67.051689\tkl:   25.809584\n",
      "Epoch: 617 [10100/50000 (20%)]  \tLoss:   93.289513\trec:   68.076210\tkl:   25.213305\n",
      "Epoch: 617 [20100/50000 (40%)]  \tLoss:   94.990089\trec:   68.416252\tkl:   26.573835\n",
      "Epoch: 617 [30100/50000 (60%)]  \tLoss:   92.610229\trec:   66.894333\tkl:   25.715899\n",
      "Epoch: 617 [40100/50000 (80%)]  \tLoss:   91.700104\trec:   66.117485\tkl:   25.582617\n",
      "====> Epoch: 617 Average train loss: 93.1765\n",
      "====> Validation set loss: 95.7803\n",
      "====> Validation set kl: 25.4760\n",
      "Epoch: 618 [  100/50000 ( 0%)]  \tLoss:   91.508484\trec:   66.306900\tkl:   25.201578\n",
      "Epoch: 618 [10100/50000 (20%)]  \tLoss:   91.368614\trec:   64.362061\tkl:   27.006550\n",
      "Epoch: 618 [20100/50000 (40%)]  \tLoss:   93.312477\trec:   68.412117\tkl:   24.900358\n",
      "Epoch: 618 [30100/50000 (60%)]  \tLoss:   97.764900\trec:   71.432350\tkl:   26.332550\n",
      "Epoch: 618 [40100/50000 (80%)]  \tLoss:   94.861069\trec:   68.115044\tkl:   26.746027\n",
      "====> Epoch: 618 Average train loss: 93.1069\n",
      "====> Validation set loss: 96.9857\n",
      "====> Validation set kl: 25.5382\n",
      "Epoch: 619 [  100/50000 ( 0%)]  \tLoss:   97.235123\trec:   71.627365\tkl:   25.607763\n",
      "Epoch: 619 [10100/50000 (20%)]  \tLoss:   96.753632\trec:   69.999779\tkl:   26.753855\n",
      "Epoch: 619 [20100/50000 (40%)]  \tLoss:   89.055786\trec:   64.084435\tkl:   24.971354\n",
      "Epoch: 619 [30100/50000 (60%)]  \tLoss:   91.143082\trec:   65.825623\tkl:   25.317455\n",
      "Epoch: 619 [40100/50000 (80%)]  \tLoss:   93.547813\trec:   68.773506\tkl:   24.774303\n",
      "====> Epoch: 619 Average train loss: 93.0714\n",
      "====> Validation set loss: 95.7635\n",
      "====> Validation set kl: 25.6373\n",
      "Epoch: 620 [  100/50000 ( 0%)]  \tLoss:   88.547760\trec:   63.295170\tkl:   25.252594\n",
      "Epoch: 620 [10100/50000 (20%)]  \tLoss:   95.039841\trec:   68.580887\tkl:   26.458958\n",
      "Epoch: 620 [20100/50000 (40%)]  \tLoss:   91.596794\trec:   66.291145\tkl:   25.305647\n",
      "Epoch: 620 [30100/50000 (60%)]  \tLoss:   88.070137\trec:   63.557812\tkl:   24.512321\n",
      "Epoch: 620 [40100/50000 (80%)]  \tLoss:   90.085815\trec:   65.267097\tkl:   24.818718\n",
      "====> Epoch: 620 Average train loss: 93.0549\n",
      "====> Validation set loss: 95.5583\n",
      "====> Validation set kl: 25.6366\n",
      "Epoch: 621 [  100/50000 ( 0%)]  \tLoss:   95.143456\trec:   69.786476\tkl:   25.356979\n",
      "Epoch: 621 [10100/50000 (20%)]  \tLoss:   91.946541\trec:   65.791801\tkl:   26.154737\n",
      "Epoch: 621 [20100/50000 (40%)]  \tLoss:   91.548355\trec:   66.405022\tkl:   25.143335\n",
      "Epoch: 621 [30100/50000 (60%)]  \tLoss:   95.243423\trec:   69.408234\tkl:   25.835196\n",
      "Epoch: 621 [40100/50000 (80%)]  \tLoss:  100.477730\trec:   73.051338\tkl:   27.426392\n",
      "====> Epoch: 621 Average train loss: 93.0107\n",
      "====> Validation set loss: 96.4925\n",
      "====> Validation set kl: 24.8771\n",
      "Epoch: 622 [  100/50000 ( 0%)]  \tLoss:   94.208687\trec:   69.484825\tkl:   24.723864\n",
      "Epoch: 622 [10100/50000 (20%)]  \tLoss:   95.812401\trec:   70.195602\tkl:   25.616796\n",
      "Epoch: 622 [20100/50000 (40%)]  \tLoss:   92.742241\trec:   65.724991\tkl:   27.017258\n",
      "Epoch: 622 [30100/50000 (60%)]  \tLoss:   92.933777\trec:   67.554924\tkl:   25.378855\n",
      "Epoch: 622 [40100/50000 (80%)]  \tLoss:   95.083984\trec:   69.098267\tkl:   25.985714\n",
      "====> Epoch: 622 Average train loss: 93.0715\n",
      "====> Validation set loss: 95.9371\n",
      "====> Validation set kl: 25.3552\n",
      "Epoch: 623 [  100/50000 ( 0%)]  \tLoss:   94.012466\trec:   68.386757\tkl:   25.625711\n",
      "Epoch: 623 [10100/50000 (20%)]  \tLoss:   90.990685\trec:   65.772270\tkl:   25.218409\n",
      "Epoch: 623 [20100/50000 (40%)]  \tLoss:   91.283386\trec:   66.388893\tkl:   24.894489\n",
      "Epoch: 623 [30100/50000 (60%)]  \tLoss:   95.405457\trec:   69.231331\tkl:   26.174128\n",
      "Epoch: 623 [40100/50000 (80%)]  \tLoss:   94.214294\trec:   69.004128\tkl:   25.210169\n",
      "====> Epoch: 623 Average train loss: 92.7579\n",
      "====> Validation set loss: 95.5356\n",
      "====> Validation set kl: 25.4570\n",
      "Epoch: 624 [  100/50000 ( 0%)]  \tLoss:   88.753334\trec:   63.940186\tkl:   24.813156\n",
      "Epoch: 624 [10100/50000 (20%)]  \tLoss:   91.737976\trec:   66.652122\tkl:   25.085852\n",
      "Epoch: 624 [20100/50000 (40%)]  \tLoss:   95.524605\trec:   68.586220\tkl:   26.938385\n",
      "Epoch: 624 [30100/50000 (60%)]  \tLoss:   90.342186\trec:   64.813683\tkl:   25.528505\n",
      "Epoch: 624 [40100/50000 (80%)]  \tLoss:   86.038750\trec:   61.525867\tkl:   24.512880\n",
      "====> Epoch: 624 Average train loss: 92.7753\n",
      "====> Validation set loss: 95.5814\n",
      "====> Validation set kl: 25.3649\n",
      "Epoch: 625 [  100/50000 ( 0%)]  \tLoss:   90.208374\trec:   65.095085\tkl:   25.113293\n",
      "Epoch: 625 [10100/50000 (20%)]  \tLoss:   96.661598\trec:   69.822990\tkl:   26.838615\n",
      "Epoch: 625 [20100/50000 (40%)]  \tLoss:   93.644920\trec:   67.850624\tkl:   25.794302\n",
      "Epoch: 625 [30100/50000 (60%)]  \tLoss:   93.030235\trec:   66.955566\tkl:   26.074673\n",
      "Epoch: 625 [40100/50000 (80%)]  \tLoss:   95.821022\trec:   70.664398\tkl:   25.156628\n",
      "====> Epoch: 625 Average train loss: 92.8208\n",
      "====> Validation set loss: 96.0253\n",
      "====> Validation set kl: 25.6848\n",
      "Epoch: 626 [  100/50000 ( 0%)]  \tLoss:   90.685799\trec:   64.819649\tkl:   25.866146\n",
      "Epoch: 626 [10100/50000 (20%)]  \tLoss:   88.879471\trec:   63.522442\tkl:   25.357025\n",
      "Epoch: 626 [20100/50000 (40%)]  \tLoss:   93.464211\trec:   68.136421\tkl:   25.327780\n",
      "Epoch: 626 [30100/50000 (60%)]  \tLoss:   89.830017\trec:   64.941689\tkl:   24.888325\n",
      "Epoch: 626 [40100/50000 (80%)]  \tLoss:   95.216499\trec:   68.995438\tkl:   26.221060\n",
      "====> Epoch: 626 Average train loss: 92.9307\n",
      "====> Validation set loss: 96.1378\n",
      "====> Validation set kl: 25.2497\n",
      "Epoch: 627 [  100/50000 ( 0%)]  \tLoss:   92.057007\trec:   66.381104\tkl:   25.675913\n",
      "Epoch: 627 [10100/50000 (20%)]  \tLoss:   95.100075\trec:   68.495789\tkl:   26.604286\n",
      "Epoch: 627 [20100/50000 (40%)]  \tLoss:   92.995308\trec:   67.520897\tkl:   25.474409\n",
      "Epoch: 627 [30100/50000 (60%)]  \tLoss:   91.407478\trec:   66.572853\tkl:   24.834621\n",
      "Epoch: 627 [40100/50000 (80%)]  \tLoss:   95.625420\trec:   69.271667\tkl:   26.353750\n",
      "====> Epoch: 627 Average train loss: 93.0412\n",
      "====> Validation set loss: 96.2151\n",
      "====> Validation set kl: 26.1631\n",
      "Epoch: 628 [  100/50000 ( 0%)]  \tLoss:   93.951363\trec:   67.250587\tkl:   26.700775\n",
      "Epoch: 628 [10100/50000 (20%)]  \tLoss:   87.580002\trec:   63.519783\tkl:   24.060211\n",
      "Epoch: 628 [20100/50000 (40%)]  \tLoss:   94.457695\trec:   68.829124\tkl:   25.628571\n",
      "Epoch: 628 [30100/50000 (60%)]  \tLoss:   94.024818\trec:   68.120018\tkl:   25.904799\n",
      "Epoch: 628 [40100/50000 (80%)]  \tLoss:   89.854248\trec:   65.310196\tkl:   24.544050\n",
      "====> Epoch: 628 Average train loss: 93.1465\n",
      "====> Validation set loss: 96.0537\n",
      "====> Validation set kl: 25.8714\n",
      "Epoch: 629 [  100/50000 ( 0%)]  \tLoss:   93.603523\trec:   67.193138\tkl:   26.410385\n",
      "Epoch: 629 [10100/50000 (20%)]  \tLoss:   90.924957\trec:   66.735596\tkl:   24.189369\n",
      "Epoch: 629 [20100/50000 (40%)]  \tLoss:   89.628281\trec:   64.646461\tkl:   24.981821\n",
      "Epoch: 629 [30100/50000 (60%)]  \tLoss:   93.620216\trec:   69.048065\tkl:   24.572144\n",
      "Epoch: 629 [40100/50000 (80%)]  \tLoss:   88.263748\trec:   62.446781\tkl:   25.816973\n",
      "====> Epoch: 629 Average train loss: 92.9249\n",
      "====> Validation set loss: 95.9996\n",
      "====> Validation set kl: 25.7529\n",
      "Epoch: 630 [  100/50000 ( 0%)]  \tLoss:   89.355347\trec:   64.271118\tkl:   25.084229\n",
      "Epoch: 630 [10100/50000 (20%)]  \tLoss:   97.730095\trec:   70.660629\tkl:   27.069471\n",
      "Epoch: 630 [20100/50000 (40%)]  \tLoss:   91.628769\trec:   65.698860\tkl:   25.929905\n",
      "Epoch: 630 [30100/50000 (60%)]  \tLoss:   92.739578\trec:   66.912712\tkl:   25.826870\n",
      "Epoch: 630 [40100/50000 (80%)]  \tLoss:   92.405640\trec:   66.668983\tkl:   25.736654\n",
      "====> Epoch: 630 Average train loss: 93.2558\n",
      "====> Validation set loss: 95.7966\n",
      "====> Validation set kl: 25.5776\n",
      "Epoch: 631 [  100/50000 ( 0%)]  \tLoss:   93.822540\trec:   68.497894\tkl:   25.324642\n",
      "Epoch: 631 [10100/50000 (20%)]  \tLoss:   94.264801\trec:   68.546265\tkl:   25.718536\n",
      "Epoch: 631 [20100/50000 (40%)]  \tLoss:   92.214516\trec:   66.510193\tkl:   25.704329\n",
      "Epoch: 631 [30100/50000 (60%)]  \tLoss:   95.193573\trec:   69.763115\tkl:   25.430449\n",
      "Epoch: 631 [40100/50000 (80%)]  \tLoss:   93.004547\trec:   67.800186\tkl:   25.204370\n",
      "====> Epoch: 631 Average train loss: 93.0052\n",
      "====> Validation set loss: 96.8038\n",
      "====> Validation set kl: 25.0214\n",
      "Epoch: 632 [  100/50000 ( 0%)]  \tLoss:   95.447090\trec:   70.071480\tkl:   25.375612\n",
      "Epoch: 632 [10100/50000 (20%)]  \tLoss:   91.308052\trec:   65.672195\tkl:   25.635857\n",
      "Epoch: 632 [20100/50000 (40%)]  \tLoss:   95.285904\trec:   69.335327\tkl:   25.950583\n",
      "Epoch: 632 [30100/50000 (60%)]  \tLoss:   93.693779\trec:   67.642090\tkl:   26.051687\n",
      "Epoch: 632 [40100/50000 (80%)]  \tLoss:   94.859253\trec:   69.310631\tkl:   25.548622\n",
      "====> Epoch: 632 Average train loss: 93.4991\n",
      "====> Validation set loss: 96.9604\n",
      "====> Validation set kl: 26.2505\n",
      "Epoch: 633 [  100/50000 ( 0%)]  \tLoss:   95.331024\trec:   69.562515\tkl:   25.768507\n",
      "Epoch: 633 [10100/50000 (20%)]  \tLoss:   92.137589\trec:   65.858391\tkl:   26.279198\n",
      "Epoch: 633 [20100/50000 (40%)]  \tLoss:   90.939140\trec:   65.460411\tkl:   25.478735\n",
      "Epoch: 633 [30100/50000 (60%)]  \tLoss:   93.314316\trec:   67.807053\tkl:   25.507259\n",
      "Epoch: 633 [40100/50000 (80%)]  \tLoss:   95.604546\trec:   69.754326\tkl:   25.850224\n",
      "====> Epoch: 633 Average train loss: 93.6891\n",
      "====> Validation set loss: 96.6022\n",
      "====> Validation set kl: 25.5946\n",
      "Epoch: 634 [  100/50000 ( 0%)]  \tLoss:   93.549263\trec:   67.888916\tkl:   25.660353\n",
      "Epoch: 634 [10100/50000 (20%)]  \tLoss:   96.383087\trec:   70.753288\tkl:   25.629801\n",
      "Epoch: 634 [20100/50000 (40%)]  \tLoss:   95.350975\trec:   69.218750\tkl:   26.132215\n",
      "Epoch: 634 [30100/50000 (60%)]  \tLoss:   90.393173\trec:   64.809418\tkl:   25.583752\n",
      "Epoch: 634 [40100/50000 (80%)]  \tLoss:   95.372528\trec:   69.280914\tkl:   26.091614\n",
      "====> Epoch: 634 Average train loss: 93.3301\n",
      "====> Validation set loss: 96.4807\n",
      "====> Validation set kl: 26.0724\n",
      "Epoch: 635 [  100/50000 ( 0%)]  \tLoss:   91.968201\trec:   66.305260\tkl:   25.662941\n",
      "Epoch: 635 [10100/50000 (20%)]  \tLoss:   93.249344\trec:   66.900024\tkl:   26.349316\n",
      "Epoch: 635 [20100/50000 (40%)]  \tLoss:   97.060310\trec:   70.820213\tkl:   26.240095\n",
      "Epoch: 635 [30100/50000 (60%)]  \tLoss:   95.337074\trec:   69.403755\tkl:   25.933323\n",
      "Epoch: 635 [40100/50000 (80%)]  \tLoss:   96.491753\trec:   70.500908\tkl:   25.990845\n",
      "====> Epoch: 635 Average train loss: 93.4019\n",
      "====> Validation set loss: 96.2670\n",
      "====> Validation set kl: 25.3613\n",
      "Epoch: 636 [  100/50000 ( 0%)]  \tLoss:   95.296776\trec:   69.898682\tkl:   25.398096\n",
      "Epoch: 636 [10100/50000 (20%)]  \tLoss:   90.801094\trec:   64.725731\tkl:   26.075365\n",
      "Epoch: 636 [20100/50000 (40%)]  \tLoss:   92.716423\trec:   66.289757\tkl:   26.426664\n",
      "Epoch: 636 [30100/50000 (60%)]  \tLoss:   88.062401\trec:   63.279987\tkl:   24.782410\n",
      "Epoch: 636 [40100/50000 (80%)]  \tLoss:   97.182381\trec:   70.849365\tkl:   26.333012\n",
      "====> Epoch: 636 Average train loss: 93.0246\n",
      "====> Validation set loss: 95.9247\n",
      "====> Validation set kl: 25.3662\n",
      "Epoch: 637 [  100/50000 ( 0%)]  \tLoss:   97.422722\trec:   70.806641\tkl:   26.616081\n",
      "Epoch: 637 [10100/50000 (20%)]  \tLoss:   96.730873\trec:   71.409546\tkl:   25.321329\n",
      "Epoch: 637 [20100/50000 (40%)]  \tLoss:   93.898544\trec:   68.171722\tkl:   25.726824\n",
      "Epoch: 637 [30100/50000 (60%)]  \tLoss:   93.577568\trec:   67.172585\tkl:   26.404985\n",
      "Epoch: 637 [40100/50000 (80%)]  \tLoss:   94.988441\trec:   69.355568\tkl:   25.632874\n",
      "====> Epoch: 637 Average train loss: 93.2860\n",
      "====> Validation set loss: 96.0770\n",
      "====> Validation set kl: 25.8690\n",
      "Epoch: 638 [  100/50000 ( 0%)]  \tLoss:   94.437294\trec:   68.849937\tkl:   25.587351\n",
      "Epoch: 638 [10100/50000 (20%)]  \tLoss:   94.136650\trec:   67.554863\tkl:   26.581787\n",
      "Epoch: 638 [20100/50000 (40%)]  \tLoss:   90.568474\trec:   65.536552\tkl:   25.031929\n",
      "Epoch: 638 [30100/50000 (60%)]  \tLoss:   90.985092\trec:   66.284615\tkl:   24.700474\n",
      "Epoch: 638 [40100/50000 (80%)]  \tLoss:   90.099197\trec:   65.034073\tkl:   25.065121\n",
      "====> Epoch: 638 Average train loss: 93.3603\n",
      "====> Validation set loss: 96.5463\n",
      "====> Validation set kl: 25.9347\n",
      "Epoch: 639 [  100/50000 ( 0%)]  \tLoss:   94.752655\trec:   68.941269\tkl:   25.811386\n",
      "Epoch: 639 [10100/50000 (20%)]  \tLoss:   94.234627\trec:   68.644691\tkl:   25.589939\n",
      "Epoch: 639 [20100/50000 (40%)]  \tLoss:   92.209480\trec:   66.416695\tkl:   25.792788\n",
      "Epoch: 639 [30100/50000 (60%)]  \tLoss:   92.377090\trec:   67.133224\tkl:   25.243862\n",
      "Epoch: 639 [40100/50000 (80%)]  \tLoss:   90.837349\trec:   65.813332\tkl:   25.024021\n",
      "====> Epoch: 639 Average train loss: 93.1018\n",
      "====> Validation set loss: 96.1398\n",
      "====> Validation set kl: 25.8021\n",
      "Epoch: 640 [  100/50000 ( 0%)]  \tLoss:   91.130943\trec:   64.971298\tkl:   26.159645\n",
      "Epoch: 640 [10100/50000 (20%)]  \tLoss:   92.701347\trec:   67.691376\tkl:   25.009975\n",
      "Epoch: 640 [20100/50000 (40%)]  \tLoss:   93.837784\trec:   68.385712\tkl:   25.452066\n",
      "Epoch: 640 [30100/50000 (60%)]  \tLoss:   94.729034\trec:   69.076096\tkl:   25.652937\n",
      "Epoch: 640 [40100/50000 (80%)]  \tLoss:   93.695213\trec:   68.016922\tkl:   25.678286\n",
      "====> Epoch: 640 Average train loss: 93.2654\n",
      "====> Validation set loss: 95.9839\n",
      "====> Validation set kl: 25.7469\n",
      "Epoch: 641 [  100/50000 ( 0%)]  \tLoss:   91.247940\trec:   65.583572\tkl:   25.664364\n",
      "Epoch: 641 [10100/50000 (20%)]  \tLoss:   93.051880\trec:   66.980255\tkl:   26.071629\n",
      "Epoch: 641 [20100/50000 (40%)]  \tLoss:   93.603813\trec:   68.276192\tkl:   25.327629\n",
      "Epoch: 641 [30100/50000 (60%)]  \tLoss:   91.758202\trec:   65.931381\tkl:   25.826822\n",
      "Epoch: 641 [40100/50000 (80%)]  \tLoss:   90.789352\trec:   65.070190\tkl:   25.719154\n",
      "====> Epoch: 641 Average train loss: 93.3249\n",
      "====> Validation set loss: 95.9693\n",
      "====> Validation set kl: 25.5450\n",
      "Epoch: 642 [  100/50000 ( 0%)]  \tLoss:   94.695320\trec:   69.202049\tkl:   25.493271\n",
      "Epoch: 642 [10100/50000 (20%)]  \tLoss:   91.346924\trec:   65.941246\tkl:   25.405674\n",
      "Epoch: 642 [20100/50000 (40%)]  \tLoss:   95.027809\trec:   68.324005\tkl:   26.703808\n",
      "Epoch: 642 [30100/50000 (60%)]  \tLoss:   95.648651\trec:   70.015640\tkl:   25.633003\n",
      "Epoch: 642 [40100/50000 (80%)]  \tLoss:   95.966499\trec:   69.208221\tkl:   26.758280\n",
      "====> Epoch: 642 Average train loss: 93.3653\n",
      "====> Validation set loss: 96.4899\n",
      "====> Validation set kl: 25.9331\n",
      "Epoch: 643 [  100/50000 ( 0%)]  \tLoss:   95.429024\trec:   68.972664\tkl:   26.456352\n",
      "Epoch: 643 [10100/50000 (20%)]  \tLoss:   92.602905\trec:   66.957275\tkl:   25.645630\n",
      "Epoch: 643 [20100/50000 (40%)]  \tLoss:   95.300293\trec:   69.375023\tkl:   25.925262\n",
      "Epoch: 643 [30100/50000 (60%)]  \tLoss:   94.887764\trec:   68.672569\tkl:   26.215195\n",
      "Epoch: 643 [40100/50000 (80%)]  \tLoss:   92.930504\trec:   67.640831\tkl:   25.289673\n",
      "====> Epoch: 643 Average train loss: 93.1177\n",
      "====> Validation set loss: 96.6182\n",
      "====> Validation set kl: 25.3229\n",
      "Epoch: 644 [  100/50000 ( 0%)]  \tLoss:   90.550346\trec:   65.784294\tkl:   24.766060\n",
      "Epoch: 644 [10100/50000 (20%)]  \tLoss:   95.080910\trec:   68.592331\tkl:   26.488577\n",
      "Epoch: 644 [20100/50000 (40%)]  \tLoss:   90.214516\trec:   65.036934\tkl:   25.177589\n",
      "Epoch: 644 [30100/50000 (60%)]  \tLoss:   92.274277\trec:   65.614471\tkl:   26.659809\n",
      "Epoch: 644 [40100/50000 (80%)]  \tLoss:   90.972916\trec:   66.109657\tkl:   24.863264\n",
      "====> Epoch: 644 Average train loss: 93.3075\n",
      "====> Validation set loss: 95.7379\n",
      "====> Validation set kl: 25.6682\n",
      "Epoch: 645 [  100/50000 ( 0%)]  \tLoss:   94.237144\trec:   68.040794\tkl:   26.196350\n",
      "Epoch: 645 [10100/50000 (20%)]  \tLoss:   90.606110\trec:   66.046349\tkl:   24.559757\n",
      "Epoch: 645 [20100/50000 (40%)]  \tLoss:   95.457603\trec:   69.865669\tkl:   25.591930\n",
      "Epoch: 645 [30100/50000 (60%)]  \tLoss:   92.571510\trec:   66.469231\tkl:   26.102287\n",
      "Epoch: 645 [40100/50000 (80%)]  \tLoss:   93.499611\trec:   68.549217\tkl:   24.950390\n",
      "====> Epoch: 645 Average train loss: 93.1730\n",
      "====> Validation set loss: 95.8317\n",
      "====> Validation set kl: 25.6321\n",
      "Epoch: 646 [  100/50000 ( 0%)]  \tLoss:   96.453659\trec:   69.968674\tkl:   26.484987\n",
      "Epoch: 646 [10100/50000 (20%)]  \tLoss:   92.806465\trec:   66.671043\tkl:   26.135414\n",
      "Epoch: 646 [20100/50000 (40%)]  \tLoss:   94.339371\trec:   68.973679\tkl:   25.365692\n",
      "Epoch: 646 [30100/50000 (60%)]  \tLoss:   95.408569\trec:   70.173660\tkl:   25.234909\n",
      "Epoch: 646 [40100/50000 (80%)]  \tLoss:   90.726929\trec:   65.811691\tkl:   24.915241\n",
      "====> Epoch: 646 Average train loss: 92.9657\n",
      "====> Validation set loss: 96.0136\n",
      "====> Validation set kl: 25.9427\n",
      "Epoch: 647 [  100/50000 ( 0%)]  \tLoss:   98.417091\trec:   71.999962\tkl:   26.417131\n",
      "Epoch: 647 [10100/50000 (20%)]  \tLoss:   93.462929\trec:   67.141006\tkl:   26.321928\n",
      "Epoch: 647 [20100/50000 (40%)]  \tLoss:   97.723740\trec:   70.558815\tkl:   27.164928\n",
      "Epoch: 647 [30100/50000 (60%)]  \tLoss:   92.702118\trec:   67.877426\tkl:   24.824694\n",
      "Epoch: 647 [40100/50000 (80%)]  \tLoss:   87.281387\trec:   63.583954\tkl:   23.697432\n",
      "====> Epoch: 647 Average train loss: 93.4408\n",
      "====> Validation set loss: 96.3692\n",
      "====> Validation set kl: 25.6676\n",
      "Epoch: 648 [  100/50000 ( 0%)]  \tLoss:   89.693130\trec:   64.397942\tkl:   25.295193\n",
      "Epoch: 648 [10100/50000 (20%)]  \tLoss:   96.529510\trec:   69.201988\tkl:   27.327518\n",
      "Epoch: 648 [20100/50000 (40%)]  \tLoss:   91.768387\trec:   66.396431\tkl:   25.371950\n",
      "Epoch: 648 [30100/50000 (60%)]  \tLoss:   91.013603\trec:   66.004669\tkl:   25.008936\n",
      "Epoch: 648 [40100/50000 (80%)]  \tLoss:   98.292839\trec:   73.034523\tkl:   25.258318\n",
      "====> Epoch: 648 Average train loss: 93.4873\n",
      "====> Validation set loss: 97.4201\n",
      "====> Validation set kl: 26.5893\n",
      "Epoch: 649 [  100/50000 ( 0%)]  \tLoss:   91.950592\trec:   65.494102\tkl:   26.456491\n",
      "Epoch: 649 [10100/50000 (20%)]  \tLoss:   92.540627\trec:   67.556992\tkl:   24.983633\n",
      "Epoch: 649 [20100/50000 (40%)]  \tLoss:   88.456490\trec:   63.636269\tkl:   24.820225\n",
      "Epoch: 649 [30100/50000 (60%)]  \tLoss:   94.147179\trec:   68.277672\tkl:   25.869503\n",
      "Epoch: 649 [40100/50000 (80%)]  \tLoss:   92.429306\trec:   66.387680\tkl:   26.041628\n",
      "====> Epoch: 649 Average train loss: 93.3571\n",
      "====> Validation set loss: 96.1792\n",
      "====> Validation set kl: 25.6144\n",
      "Epoch: 650 [  100/50000 ( 0%)]  \tLoss:   92.124352\trec:   66.942802\tkl:   25.181547\n",
      "Epoch: 650 [10100/50000 (20%)]  \tLoss:   92.817039\trec:   67.211876\tkl:   25.605165\n",
      "Epoch: 650 [20100/50000 (40%)]  \tLoss:   93.794174\trec:   67.416656\tkl:   26.377514\n",
      "Epoch: 650 [30100/50000 (60%)]  \tLoss:   93.721825\trec:   67.487373\tkl:   26.234453\n",
      "Epoch: 650 [40100/50000 (80%)]  \tLoss:   93.364372\trec:   68.352402\tkl:   25.011972\n",
      "====> Epoch: 650 Average train loss: 93.2368\n",
      "====> Validation set loss: 95.9590\n",
      "====> Validation set kl: 25.2759\n",
      "Epoch: 651 [  100/50000 ( 0%)]  \tLoss:   91.405899\trec:   66.914665\tkl:   24.491240\n",
      "Epoch: 651 [10100/50000 (20%)]  \tLoss:   93.489685\trec:   67.696533\tkl:   25.793154\n",
      "Epoch: 651 [20100/50000 (40%)]  \tLoss:   88.548149\trec:   64.056641\tkl:   24.491512\n",
      "Epoch: 651 [30100/50000 (60%)]  \tLoss:   94.566284\trec:   68.991226\tkl:   25.575058\n",
      "Epoch: 651 [40100/50000 (80%)]  \tLoss:   91.902802\trec:   66.872101\tkl:   25.030704\n",
      "====> Epoch: 651 Average train loss: 93.2238\n",
      "====> Validation set loss: 95.9053\n",
      "====> Validation set kl: 25.3881\n",
      "Epoch: 652 [  100/50000 ( 0%)]  \tLoss:   90.183090\trec:   65.206085\tkl:   24.977003\n",
      "Epoch: 652 [10100/50000 (20%)]  \tLoss:   94.474907\trec:   69.773849\tkl:   24.701063\n",
      "Epoch: 652 [20100/50000 (40%)]  \tLoss:   92.193542\trec:   66.928543\tkl:   25.265001\n",
      "Epoch: 652 [30100/50000 (60%)]  \tLoss:   91.190613\trec:   65.693008\tkl:   25.497610\n",
      "Epoch: 652 [40100/50000 (80%)]  \tLoss:   98.164696\trec:   71.723007\tkl:   26.441690\n",
      "====> Epoch: 652 Average train loss: 93.2454\n",
      "====> Validation set loss: 96.3179\n",
      "====> Validation set kl: 25.4549\n",
      "Epoch: 653 [  100/50000 ( 0%)]  \tLoss:   94.492241\trec:   68.930717\tkl:   25.561531\n",
      "Epoch: 653 [10100/50000 (20%)]  \tLoss:   93.966553\trec:   67.930344\tkl:   26.036213\n",
      "Epoch: 653 [20100/50000 (40%)]  \tLoss:   95.429108\trec:   69.526642\tkl:   25.902473\n",
      "Epoch: 653 [30100/50000 (60%)]  \tLoss:   87.583969\trec:   62.877129\tkl:   24.706844\n",
      "Epoch: 653 [40100/50000 (80%)]  \tLoss:   89.415962\trec:   64.167412\tkl:   25.248556\n",
      "====> Epoch: 653 Average train loss: 93.0227\n",
      "====> Validation set loss: 95.9977\n",
      "====> Validation set kl: 25.6752\n",
      "Epoch: 654 [  100/50000 ( 0%)]  \tLoss:   92.981834\trec:   67.592392\tkl:   25.389441\n",
      "Epoch: 654 [10100/50000 (20%)]  \tLoss:   89.713242\trec:   64.270248\tkl:   25.442997\n",
      "Epoch: 654 [20100/50000 (40%)]  \tLoss:   93.676620\trec:   68.394348\tkl:   25.282267\n",
      "Epoch: 654 [30100/50000 (60%)]  \tLoss:   92.475700\trec:   65.891685\tkl:   26.584023\n",
      "Epoch: 654 [40100/50000 (80%)]  \tLoss:   94.266914\trec:   67.777756\tkl:   26.489157\n",
      "====> Epoch: 654 Average train loss: 93.1513\n",
      "====> Validation set loss: 95.9774\n",
      "====> Validation set kl: 25.5923\n",
      "Epoch: 655 [  100/50000 ( 0%)]  \tLoss:   96.330093\trec:   70.180519\tkl:   26.149572\n",
      "Epoch: 655 [10100/50000 (20%)]  \tLoss:   91.491127\trec:   66.650528\tkl:   24.840611\n",
      "Epoch: 655 [20100/50000 (40%)]  \tLoss:   93.587448\trec:   68.208603\tkl:   25.378849\n",
      "Epoch: 655 [30100/50000 (60%)]  \tLoss:   97.812057\trec:   70.920006\tkl:   26.892048\n",
      "Epoch: 655 [40100/50000 (80%)]  \tLoss:   92.449852\trec:   67.077919\tkl:   25.371931\n",
      "====> Epoch: 655 Average train loss: 93.1475\n",
      "====> Validation set loss: 96.3584\n",
      "====> Validation set kl: 25.9797\n",
      "Epoch: 656 [  100/50000 ( 0%)]  \tLoss:   91.171333\trec:   65.739220\tkl:   25.432116\n",
      "Epoch: 656 [10100/50000 (20%)]  \tLoss:   88.113770\trec:   63.539097\tkl:   24.574673\n",
      "Epoch: 656 [20100/50000 (40%)]  \tLoss:   94.105057\trec:   68.845757\tkl:   25.259298\n",
      "Epoch: 656 [30100/50000 (60%)]  \tLoss:   95.775848\trec:   70.087677\tkl:   25.688175\n",
      "Epoch: 656 [40100/50000 (80%)]  \tLoss:   98.458412\trec:   71.609840\tkl:   26.848579\n",
      "====> Epoch: 656 Average train loss: 93.2958\n",
      "====> Validation set loss: 96.5187\n",
      "====> Validation set kl: 25.6221\n",
      "Epoch: 657 [  100/50000 ( 0%)]  \tLoss:   96.113541\trec:   69.811287\tkl:   26.302254\n",
      "Epoch: 657 [10100/50000 (20%)]  \tLoss:   95.255287\trec:   69.060844\tkl:   26.194452\n",
      "Epoch: 657 [20100/50000 (40%)]  \tLoss:   99.807251\trec:   72.483429\tkl:   27.323828\n",
      "Epoch: 657 [30100/50000 (60%)]  \tLoss:   95.821892\trec:   70.229195\tkl:   25.592697\n",
      "Epoch: 657 [40100/50000 (80%)]  \tLoss:   93.502380\trec:   67.609627\tkl:   25.892756\n",
      "====> Epoch: 657 Average train loss: 93.2943\n",
      "====> Validation set loss: 96.5447\n",
      "====> Validation set kl: 25.5632\n",
      "Epoch: 658 [  100/50000 ( 0%)]  \tLoss:   92.028915\trec:   66.502769\tkl:   25.526148\n",
      "Epoch: 658 [10100/50000 (20%)]  \tLoss:   96.435654\trec:   70.002480\tkl:   26.433170\n",
      "Epoch: 658 [20100/50000 (40%)]  \tLoss:   92.301010\trec:   66.410324\tkl:   25.890696\n",
      "Epoch: 658 [30100/50000 (60%)]  \tLoss:   92.036171\trec:   66.013062\tkl:   26.023115\n",
      "Epoch: 658 [40100/50000 (80%)]  \tLoss:   95.359589\trec:   69.285568\tkl:   26.074020\n",
      "====> Epoch: 658 Average train loss: 93.2575\n",
      "====> Validation set loss: 95.7480\n",
      "====> Validation set kl: 25.5607\n",
      "Epoch: 659 [  100/50000 ( 0%)]  \tLoss:   91.832642\trec:   66.691826\tkl:   25.140816\n",
      "Epoch: 659 [10100/50000 (20%)]  \tLoss:   93.824081\trec:   67.601990\tkl:   26.222094\n",
      "Epoch: 659 [20100/50000 (40%)]  \tLoss:   93.070465\trec:   68.647827\tkl:   24.422638\n",
      "Epoch: 659 [30100/50000 (60%)]  \tLoss:   94.225739\trec:   67.699417\tkl:   26.526318\n",
      "Epoch: 659 [40100/50000 (80%)]  \tLoss:   92.841881\trec:   67.881493\tkl:   24.960392\n",
      "====> Epoch: 659 Average train loss: 93.3656\n",
      "====> Validation set loss: 96.2986\n",
      "====> Validation set kl: 25.6523\n",
      "Epoch: 660 [  100/50000 ( 0%)]  \tLoss:   91.928078\trec:   66.632675\tkl:   25.295403\n",
      "Epoch: 660 [10100/50000 (20%)]  \tLoss:   95.194824\trec:   69.496590\tkl:   25.698233\n",
      "Epoch: 660 [20100/50000 (40%)]  \tLoss:   98.173592\trec:   71.083946\tkl:   27.089647\n",
      "Epoch: 660 [30100/50000 (60%)]  \tLoss:   89.510796\trec:   65.588623\tkl:   23.922182\n",
      "Epoch: 660 [40100/50000 (80%)]  \tLoss:   92.195763\trec:   66.330421\tkl:   25.865335\n",
      "====> Epoch: 660 Average train loss: 93.2742\n",
      "====> Validation set loss: 96.0204\n",
      "====> Validation set kl: 25.8850\n",
      "Epoch: 661 [  100/50000 ( 0%)]  \tLoss:   92.297859\trec:   65.821793\tkl:   26.476067\n",
      "Epoch: 661 [10100/50000 (20%)]  \tLoss:   90.412437\trec:   65.532623\tkl:   24.879808\n",
      "Epoch: 661 [20100/50000 (40%)]  \tLoss:   94.088707\trec:   68.667198\tkl:   25.421518\n",
      "Epoch: 661 [30100/50000 (60%)]  \tLoss:   96.557800\trec:   70.235901\tkl:   26.321896\n",
      "Epoch: 661 [40100/50000 (80%)]  \tLoss:   92.021706\trec:   65.913857\tkl:   26.107851\n",
      "====> Epoch: 661 Average train loss: 93.1519\n",
      "====> Validation set loss: 97.1829\n",
      "====> Validation set kl: 26.6598\n",
      "Epoch: 662 [  100/50000 ( 0%)]  \tLoss:   93.633087\trec:   66.434204\tkl:   27.198879\n",
      "Epoch: 662 [10100/50000 (20%)]  \tLoss:   92.919151\trec:   66.555153\tkl:   26.363993\n",
      "Epoch: 662 [20100/50000 (40%)]  \tLoss:   92.180313\trec:   66.217125\tkl:   25.963188\n",
      "Epoch: 662 [30100/50000 (60%)]  \tLoss:   90.923965\trec:   65.722359\tkl:   25.201599\n",
      "Epoch: 662 [40100/50000 (80%)]  \tLoss:   93.771118\trec:   67.957809\tkl:   25.813313\n",
      "====> Epoch: 662 Average train loss: 93.1464\n",
      "====> Validation set loss: 96.2454\n",
      "====> Validation set kl: 25.6194\n",
      "Epoch: 663 [  100/50000 ( 0%)]  \tLoss:   94.284531\trec:   68.035355\tkl:   26.249172\n",
      "Epoch: 663 [10100/50000 (20%)]  \tLoss:   89.428902\trec:   64.802086\tkl:   24.626810\n",
      "Epoch: 663 [20100/50000 (40%)]  \tLoss:   90.766228\trec:   64.735596\tkl:   26.030632\n",
      "Epoch: 663 [30100/50000 (60%)]  \tLoss:   96.008156\trec:   69.311646\tkl:   26.696501\n",
      "Epoch: 663 [40100/50000 (80%)]  \tLoss:   95.584991\trec:   69.135002\tkl:   26.449989\n",
      "====> Epoch: 663 Average train loss: 93.2862\n",
      "====> Validation set loss: 96.2234\n",
      "====> Validation set kl: 25.4648\n",
      "Epoch: 664 [  100/50000 ( 0%)]  \tLoss:   92.026886\trec:   67.230995\tkl:   24.795885\n",
      "Epoch: 664 [10100/50000 (20%)]  \tLoss:   92.265953\trec:   67.081841\tkl:   25.184109\n",
      "Epoch: 664 [20100/50000 (40%)]  \tLoss:   94.801483\trec:   68.801628\tkl:   25.999853\n",
      "Epoch: 664 [30100/50000 (60%)]  \tLoss:   96.950371\trec:   70.740288\tkl:   26.210085\n",
      "Epoch: 664 [40100/50000 (80%)]  \tLoss:   95.752274\trec:   68.921455\tkl:   26.830820\n",
      "====> Epoch: 664 Average train loss: 93.3893\n",
      "====> Validation set loss: 96.1622\n",
      "====> Validation set kl: 25.7148\n",
      "Epoch: 665 [  100/50000 ( 0%)]  \tLoss:   96.655945\trec:   70.219429\tkl:   26.436520\n",
      "Epoch: 665 [10100/50000 (20%)]  \tLoss:   92.500259\trec:   66.195992\tkl:   26.304272\n",
      "Epoch: 665 [20100/50000 (40%)]  \tLoss:   91.819122\trec:   66.675064\tkl:   25.144053\n",
      "Epoch: 665 [30100/50000 (60%)]  \tLoss:   93.714188\trec:   67.244560\tkl:   26.469631\n",
      "Epoch: 665 [40100/50000 (80%)]  \tLoss:   92.056610\trec:   65.982483\tkl:   26.074123\n",
      "====> Epoch: 665 Average train loss: 93.0025\n",
      "====> Validation set loss: 95.7059\n",
      "====> Validation set kl: 25.5450\n",
      "Epoch: 666 [  100/50000 ( 0%)]  \tLoss:   94.273712\trec:   68.856216\tkl:   25.417492\n",
      "Epoch: 666 [10100/50000 (20%)]  \tLoss:   92.569550\trec:   65.769562\tkl:   26.799988\n",
      "Epoch: 666 [20100/50000 (40%)]  \tLoss:   91.441872\trec:   66.088127\tkl:   25.353739\n",
      "Epoch: 666 [30100/50000 (60%)]  \tLoss:   95.136520\trec:   67.879097\tkl:   27.257421\n",
      "Epoch: 666 [40100/50000 (80%)]  \tLoss:   91.485313\trec:   65.024094\tkl:   26.461220\n",
      "====> Epoch: 666 Average train loss: 93.1142\n",
      "====> Validation set loss: 95.7766\n",
      "====> Validation set kl: 25.5397\n",
      "Epoch: 667 [  100/50000 ( 0%)]  \tLoss:   93.242432\trec:   67.347366\tkl:   25.895065\n",
      "Epoch: 667 [10100/50000 (20%)]  \tLoss:   90.942520\trec:   65.795097\tkl:   25.147419\n",
      "Epoch: 667 [20100/50000 (40%)]  \tLoss:   95.043045\trec:   68.157799\tkl:   26.885246\n",
      "Epoch: 667 [30100/50000 (60%)]  \tLoss:   91.256714\trec:   66.099243\tkl:   25.157471\n",
      "Epoch: 667 [40100/50000 (80%)]  \tLoss:   96.006447\trec:   69.252197\tkl:   26.754242\n",
      "====> Epoch: 667 Average train loss: 92.8671\n",
      "====> Validation set loss: 95.5370\n",
      "====> Validation set kl: 25.6614\n",
      "Epoch: 668 [  100/50000 ( 0%)]  \tLoss:   93.426582\trec:   67.134171\tkl:   26.292416\n",
      "Epoch: 668 [10100/50000 (20%)]  \tLoss:   92.120049\trec:   65.814827\tkl:   26.305216\n",
      "Epoch: 668 [20100/50000 (40%)]  \tLoss:   97.099083\trec:   70.389458\tkl:   26.709618\n",
      "Epoch: 668 [30100/50000 (60%)]  \tLoss:   92.594337\trec:   67.674835\tkl:   24.919495\n",
      "Epoch: 668 [40100/50000 (80%)]  \tLoss:   94.844589\trec:   68.637527\tkl:   26.207058\n",
      "====> Epoch: 668 Average train loss: 92.8380\n",
      "====> Validation set loss: 96.1662\n",
      "====> Validation set kl: 25.7546\n",
      "Epoch: 669 [  100/50000 ( 0%)]  \tLoss:   94.889687\trec:   68.828430\tkl:   26.061258\n",
      "Epoch: 669 [10100/50000 (20%)]  \tLoss:   92.572090\trec:   66.855278\tkl:   25.716801\n",
      "Epoch: 669 [20100/50000 (40%)]  \tLoss:   92.386345\trec:   66.325165\tkl:   26.061186\n",
      "Epoch: 669 [30100/50000 (60%)]  \tLoss:   91.728127\trec:   66.745544\tkl:   24.982580\n",
      "Epoch: 669 [40100/50000 (80%)]  \tLoss:   90.381889\trec:   65.226372\tkl:   25.155512\n",
      "====> Epoch: 669 Average train loss: 92.9537\n",
      "====> Validation set loss: 96.0501\n",
      "====> Validation set kl: 25.8481\n",
      "Epoch: 670 [  100/50000 ( 0%)]  \tLoss:   95.990623\trec:   68.903931\tkl:   27.086693\n",
      "Epoch: 670 [10100/50000 (20%)]  \tLoss:   96.368591\trec:   70.312508\tkl:   26.056082\n",
      "Epoch: 670 [20100/50000 (40%)]  \tLoss:   93.034874\trec:   67.248856\tkl:   25.786015\n",
      "Epoch: 670 [30100/50000 (60%)]  \tLoss:   95.892967\trec:   69.287811\tkl:   26.605150\n",
      "Epoch: 670 [40100/50000 (80%)]  \tLoss:   98.096657\trec:   72.125732\tkl:   25.970930\n",
      "====> Epoch: 670 Average train loss: 93.0508\n",
      "====> Validation set loss: 96.0058\n",
      "====> Validation set kl: 25.3256\n",
      "Epoch: 671 [  100/50000 ( 0%)]  \tLoss:   90.490845\trec:   65.553070\tkl:   24.937780\n",
      "Epoch: 671 [10100/50000 (20%)]  \tLoss:   87.639099\trec:   62.321472\tkl:   25.317629\n",
      "Epoch: 671 [20100/50000 (40%)]  \tLoss:   90.828712\trec:   65.481941\tkl:   25.346762\n",
      "Epoch: 671 [30100/50000 (60%)]  \tLoss:   91.235291\trec:   65.152649\tkl:   26.082644\n",
      "Epoch: 671 [40100/50000 (80%)]  \tLoss:   92.231560\trec:   66.654686\tkl:   25.576870\n",
      "====> Epoch: 671 Average train loss: 92.6974\n",
      "====> Validation set loss: 95.4481\n",
      "====> Validation set kl: 25.4998\n",
      "Epoch: 672 [  100/50000 ( 0%)]  \tLoss:   94.481636\trec:   69.307106\tkl:   25.174528\n",
      "Epoch: 672 [10100/50000 (20%)]  \tLoss:   91.393799\trec:   66.421631\tkl:   24.972164\n",
      "Epoch: 672 [20100/50000 (40%)]  \tLoss:   86.514259\trec:   62.176640\tkl:   24.337612\n",
      "Epoch: 672 [30100/50000 (60%)]  \tLoss:   94.915405\trec:   69.125153\tkl:   25.790253\n",
      "Epoch: 672 [40100/50000 (80%)]  \tLoss:   87.948341\trec:   62.663631\tkl:   25.284702\n",
      "====> Epoch: 672 Average train loss: 92.8073\n",
      "====> Validation set loss: 95.8529\n",
      "====> Validation set kl: 25.2692\n",
      "Epoch: 673 [  100/50000 ( 0%)]  \tLoss:   95.739296\trec:   68.959938\tkl:   26.779354\n",
      "Epoch: 673 [10100/50000 (20%)]  \tLoss:   95.491852\trec:   69.317924\tkl:   26.173925\n",
      "Epoch: 673 [20100/50000 (40%)]  \tLoss:   93.745407\trec:   68.004761\tkl:   25.740648\n",
      "Epoch: 673 [30100/50000 (60%)]  \tLoss:   96.286850\trec:   69.773102\tkl:   26.513752\n",
      "Epoch: 673 [40100/50000 (80%)]  \tLoss:   91.252731\trec:   66.395233\tkl:   24.857500\n",
      "====> Epoch: 673 Average train loss: 92.7418\n",
      "====> Validation set loss: 96.1324\n",
      "====> Validation set kl: 24.7014\n",
      "Epoch: 674 [  100/50000 ( 0%)]  \tLoss:   92.444313\trec:   68.348831\tkl:   24.095488\n",
      "Epoch: 674 [10100/50000 (20%)]  \tLoss:   97.477478\trec:   71.041794\tkl:   26.435686\n",
      "Epoch: 674 [20100/50000 (40%)]  \tLoss:   94.601547\trec:   68.491661\tkl:   26.109892\n",
      "Epoch: 674 [30100/50000 (60%)]  \tLoss:   94.009766\trec:   68.652504\tkl:   25.357258\n",
      "Epoch: 674 [40100/50000 (80%)]  \tLoss:   90.549255\trec:   65.106476\tkl:   25.442778\n",
      "====> Epoch: 674 Average train loss: 92.9508\n",
      "====> Validation set loss: 95.9044\n",
      "====> Validation set kl: 25.8553\n",
      "Epoch: 675 [  100/50000 ( 0%)]  \tLoss:   96.897469\trec:   69.959747\tkl:   26.937723\n",
      "Epoch: 675 [10100/50000 (20%)]  \tLoss:   94.649376\trec:   68.452606\tkl:   26.196768\n",
      "Epoch: 675 [20100/50000 (40%)]  \tLoss:   94.595047\trec:   68.551109\tkl:   26.043941\n",
      "Epoch: 675 [30100/50000 (60%)]  \tLoss:   91.617104\trec:   66.633957\tkl:   24.983154\n",
      "Epoch: 675 [40100/50000 (80%)]  \tLoss:   92.970421\trec:   67.780777\tkl:   25.189640\n",
      "====> Epoch: 675 Average train loss: 92.5479\n",
      "====> Validation set loss: 95.4850\n",
      "====> Validation set kl: 25.2682\n",
      "Epoch: 676 [  100/50000 ( 0%)]  \tLoss:   93.881485\trec:   68.956573\tkl:   24.924917\n",
      "Epoch: 676 [10100/50000 (20%)]  \tLoss:   90.967049\trec:   66.316910\tkl:   24.650133\n",
      "Epoch: 676 [20100/50000 (40%)]  \tLoss:   92.971886\trec:   66.790138\tkl:   26.181746\n",
      "Epoch: 676 [30100/50000 (60%)]  \tLoss:   91.097626\trec:   65.246765\tkl:   25.850857\n",
      "Epoch: 676 [40100/50000 (80%)]  \tLoss:   93.434334\trec:   67.385841\tkl:   26.048496\n",
      "====> Epoch: 676 Average train loss: 93.0242\n",
      "====> Validation set loss: 95.5929\n",
      "====> Validation set kl: 25.6862\n",
      "Epoch: 677 [  100/50000 ( 0%)]  \tLoss:   92.040581\trec:   67.089424\tkl:   24.951157\n",
      "Epoch: 677 [10100/50000 (20%)]  \tLoss:   92.377457\trec:   66.991768\tkl:   25.385693\n",
      "Epoch: 677 [20100/50000 (40%)]  \tLoss:   96.154266\trec:   70.298027\tkl:   25.856239\n",
      "Epoch: 677 [30100/50000 (60%)]  \tLoss:   95.543106\trec:   68.986900\tkl:   26.556206\n",
      "Epoch: 677 [40100/50000 (80%)]  \tLoss:   94.265739\trec:   68.428513\tkl:   25.837233\n",
      "====> Epoch: 677 Average train loss: 92.8723\n",
      "====> Validation set loss: 96.0599\n",
      "====> Validation set kl: 25.4576\n",
      "Epoch: 678 [  100/50000 ( 0%)]  \tLoss:   92.663231\trec:   67.722755\tkl:   24.940475\n",
      "Epoch: 678 [10100/50000 (20%)]  \tLoss:   93.604591\trec:   67.374802\tkl:   26.229782\n",
      "Epoch: 678 [20100/50000 (40%)]  \tLoss:   95.817398\trec:   69.979729\tkl:   25.837677\n",
      "Epoch: 678 [30100/50000 (60%)]  \tLoss:   90.880905\trec:   64.711945\tkl:   26.168962\n",
      "Epoch: 678 [40100/50000 (80%)]  \tLoss:   93.197853\trec:   67.569473\tkl:   25.628374\n",
      "====> Epoch: 678 Average train loss: 93.1024\n",
      "====> Validation set loss: 97.1635\n",
      "====> Validation set kl: 25.3346\n",
      "Epoch: 679 [  100/50000 ( 0%)]  \tLoss:   97.097221\trec:   71.238152\tkl:   25.859066\n",
      "Epoch: 679 [10100/50000 (20%)]  \tLoss:   91.286781\trec:   66.512108\tkl:   24.774675\n",
      "Epoch: 679 [20100/50000 (40%)]  \tLoss:   96.662704\trec:   69.799484\tkl:   26.863220\n",
      "Epoch: 679 [30100/50000 (60%)]  \tLoss:   95.862831\trec:   69.192848\tkl:   26.669983\n",
      "Epoch: 679 [40100/50000 (80%)]  \tLoss:   95.006248\trec:   68.075607\tkl:   26.930643\n",
      "====> Epoch: 679 Average train loss: 93.3207\n",
      "====> Validation set loss: 96.2629\n",
      "====> Validation set kl: 25.5092\n",
      "Epoch: 680 [  100/50000 ( 0%)]  \tLoss:   94.295425\trec:   68.059395\tkl:   26.236031\n",
      "Epoch: 680 [10100/50000 (20%)]  \tLoss:   96.332870\trec:   69.685112\tkl:   26.647753\n",
      "Epoch: 680 [20100/50000 (40%)]  \tLoss:   93.435173\trec:   68.271431\tkl:   25.163734\n",
      "Epoch: 680 [30100/50000 (60%)]  \tLoss:   91.743515\trec:   65.721970\tkl:   26.021540\n",
      "Epoch: 680 [40100/50000 (80%)]  \tLoss:   90.160545\trec:   64.213676\tkl:   25.946861\n",
      "====> Epoch: 680 Average train loss: 93.1445\n",
      "====> Validation set loss: 96.4270\n",
      "====> Validation set kl: 25.3605\n",
      "Epoch: 681 [  100/50000 ( 0%)]  \tLoss:   95.125587\trec:   70.364624\tkl:   24.760956\n",
      "Epoch: 681 [10100/50000 (20%)]  \tLoss:   92.627579\trec:   66.275124\tkl:   26.352448\n",
      "Epoch: 681 [20100/50000 (40%)]  \tLoss:   95.595665\trec:   69.205246\tkl:   26.390421\n",
      "Epoch: 681 [30100/50000 (60%)]  \tLoss:   93.188301\trec:   68.139175\tkl:   25.049118\n",
      "Epoch: 681 [40100/50000 (80%)]  \tLoss:   87.599785\trec:   62.124706\tkl:   25.475075\n",
      "====> Epoch: 681 Average train loss: 93.0655\n",
      "====> Validation set loss: 95.6544\n",
      "====> Validation set kl: 25.5419\n",
      "Epoch: 682 [  100/50000 ( 0%)]  \tLoss:   91.710808\trec:   67.171555\tkl:   24.539259\n",
      "Epoch: 682 [10100/50000 (20%)]  \tLoss:   91.429626\trec:   65.438316\tkl:   25.991306\n",
      "Epoch: 682 [20100/50000 (40%)]  \tLoss:   87.923553\trec:   63.174984\tkl:   24.748571\n",
      "Epoch: 682 [30100/50000 (60%)]  \tLoss:   92.775993\trec:   67.492790\tkl:   25.283203\n",
      "Epoch: 682 [40100/50000 (80%)]  \tLoss:   91.800484\trec:   66.794411\tkl:   25.006077\n",
      "====> Epoch: 682 Average train loss: 92.6439\n",
      "====> Validation set loss: 95.6317\n",
      "====> Validation set kl: 25.5184\n",
      "Epoch: 683 [  100/50000 ( 0%)]  \tLoss:   89.801842\trec:   65.024353\tkl:   24.777487\n",
      "Epoch: 683 [10100/50000 (20%)]  \tLoss:   93.152931\trec:   67.199150\tkl:   25.953779\n",
      "Epoch: 683 [20100/50000 (40%)]  \tLoss:   90.544762\trec:   64.609650\tkl:   25.935112\n",
      "Epoch: 683 [30100/50000 (60%)]  \tLoss:   95.393143\trec:   68.703850\tkl:   26.689293\n",
      "Epoch: 683 [40100/50000 (80%)]  \tLoss:   94.216911\trec:   68.516624\tkl:   25.700285\n",
      "====> Epoch: 683 Average train loss: 92.8644\n",
      "====> Validation set loss: 95.7932\n",
      "====> Validation set kl: 25.7679\n",
      "Epoch: 684 [  100/50000 ( 0%)]  \tLoss:   94.102959\trec:   67.934845\tkl:   26.168118\n",
      "Epoch: 684 [10100/50000 (20%)]  \tLoss:   96.341049\trec:   70.128532\tkl:   26.212528\n",
      "Epoch: 684 [20100/50000 (40%)]  \tLoss:   90.617714\trec:   64.742920\tkl:   25.874794\n",
      "Epoch: 684 [30100/50000 (60%)]  \tLoss:   94.222244\trec:   67.602196\tkl:   26.620052\n",
      "Epoch: 684 [40100/50000 (80%)]  \tLoss:   92.778618\trec:   66.493767\tkl:   26.284855\n",
      "====> Epoch: 684 Average train loss: 92.7713\n",
      "====> Validation set loss: 95.6460\n",
      "====> Validation set kl: 25.5798\n",
      "Epoch: 685 [  100/50000 ( 0%)]  \tLoss:   99.115982\trec:   73.034874\tkl:   26.081108\n",
      "Epoch: 685 [10100/50000 (20%)]  \tLoss:   92.889656\trec:   67.477516\tkl:   25.412136\n",
      "Epoch: 685 [20100/50000 (40%)]  \tLoss:   95.808159\trec:   69.583305\tkl:   26.224855\n",
      "Epoch: 685 [30100/50000 (60%)]  \tLoss:   91.750069\trec:   65.721466\tkl:   26.028605\n",
      "Epoch: 685 [40100/50000 (80%)]  \tLoss:   95.075546\trec:   68.401558\tkl:   26.673979\n",
      "====> Epoch: 685 Average train loss: 92.6986\n",
      "====> Validation set loss: 95.7278\n",
      "====> Validation set kl: 25.6633\n",
      "Epoch: 686 [  100/50000 ( 0%)]  \tLoss:   90.368317\trec:   64.157326\tkl:   26.210999\n",
      "Epoch: 686 [10100/50000 (20%)]  \tLoss:   93.825035\trec:   67.116768\tkl:   26.708269\n",
      "Epoch: 686 [20100/50000 (40%)]  \tLoss:   97.404884\trec:   70.548737\tkl:   26.856136\n",
      "Epoch: 686 [30100/50000 (60%)]  \tLoss:   92.911896\trec:   66.429230\tkl:   26.482666\n",
      "Epoch: 686 [40100/50000 (80%)]  \tLoss:   94.573326\trec:   68.232056\tkl:   26.341267\n",
      "====> Epoch: 686 Average train loss: 92.8900\n",
      "====> Validation set loss: 95.9262\n",
      "====> Validation set kl: 26.0021\n",
      "Epoch: 687 [  100/50000 ( 0%)]  \tLoss:   94.128105\trec:   67.421570\tkl:   26.706528\n",
      "Epoch: 687 [10100/50000 (20%)]  \tLoss:   87.540283\trec:   63.648758\tkl:   23.891525\n",
      "Epoch: 687 [20100/50000 (40%)]  \tLoss:   96.035934\trec:   70.268120\tkl:   25.767818\n",
      "Epoch: 687 [30100/50000 (60%)]  \tLoss:   96.819305\trec:   70.389412\tkl:   26.429890\n",
      "Epoch: 687 [40100/50000 (80%)]  \tLoss:   91.172028\trec:   66.382790\tkl:   24.789232\n",
      "====> Epoch: 687 Average train loss: 93.5322\n",
      "====> Validation set loss: 96.2632\n",
      "====> Validation set kl: 25.8792\n",
      "Epoch: 688 [  100/50000 ( 0%)]  \tLoss:   96.226578\trec:   69.501686\tkl:   26.724899\n",
      "Epoch: 688 [10100/50000 (20%)]  \tLoss:   92.841736\trec:   66.286835\tkl:   26.554899\n",
      "Epoch: 688 [20100/50000 (40%)]  \tLoss:   98.725342\trec:   71.476212\tkl:   27.249128\n",
      "Epoch: 688 [30100/50000 (60%)]  \tLoss:   92.110565\trec:   67.075249\tkl:   25.035316\n",
      "Epoch: 688 [40100/50000 (80%)]  \tLoss:   90.670776\trec:   65.433937\tkl:   25.236841\n",
      "====> Epoch: 688 Average train loss: 93.2093\n",
      "====> Validation set loss: 96.1698\n",
      "====> Validation set kl: 25.9997\n",
      "Epoch: 689 [  100/50000 ( 0%)]  \tLoss:   91.699112\trec:   65.373810\tkl:   26.325304\n",
      "Epoch: 689 [10100/50000 (20%)]  \tLoss:   91.004921\trec:   65.633881\tkl:   25.371037\n",
      "Epoch: 689 [20100/50000 (40%)]  \tLoss:   92.320625\trec:   66.648384\tkl:   25.672237\n",
      "Epoch: 689 [30100/50000 (60%)]  \tLoss:   88.993446\trec:   63.816414\tkl:   25.177029\n",
      "Epoch: 689 [40100/50000 (80%)]  \tLoss:   96.343155\trec:   69.626701\tkl:   26.716452\n",
      "====> Epoch: 689 Average train loss: 92.9385\n",
      "====> Validation set loss: 95.8204\n",
      "====> Validation set kl: 25.7340\n",
      "Epoch: 690 [  100/50000 ( 0%)]  \tLoss:   89.724442\trec:   64.622749\tkl:   25.101686\n",
      "Epoch: 690 [10100/50000 (20%)]  \tLoss:   90.781685\trec:   66.165085\tkl:   24.616598\n",
      "Epoch: 690 [20100/50000 (40%)]  \tLoss:   92.453369\trec:   66.496254\tkl:   25.957113\n",
      "Epoch: 690 [30100/50000 (60%)]  \tLoss:   90.653999\trec:   65.497368\tkl:   25.156635\n",
      "Epoch: 690 [40100/50000 (80%)]  \tLoss:   94.922966\trec:   68.595390\tkl:   26.327576\n",
      "====> Epoch: 690 Average train loss: 93.0654\n",
      "====> Validation set loss: 95.8998\n",
      "====> Validation set kl: 25.5038\n",
      "Epoch: 691 [  100/50000 ( 0%)]  \tLoss:   91.375732\trec:   64.925713\tkl:   26.450020\n",
      "Epoch: 691 [10100/50000 (20%)]  \tLoss:   91.892685\trec:   66.361862\tkl:   25.530823\n",
      "Epoch: 691 [20100/50000 (40%)]  \tLoss:   96.526581\trec:   71.717422\tkl:   24.809162\n",
      "Epoch: 691 [30100/50000 (60%)]  \tLoss:   92.303444\trec:   66.541534\tkl:   25.761915\n",
      "Epoch: 691 [40100/50000 (80%)]  \tLoss:   94.235214\trec:   67.897751\tkl:   26.337458\n",
      "====> Epoch: 691 Average train loss: 92.9803\n",
      "====> Validation set loss: 96.2264\n",
      "====> Validation set kl: 25.1511\n",
      "Epoch: 692 [  100/50000 ( 0%)]  \tLoss:   95.821686\trec:   70.638367\tkl:   25.183323\n",
      "Epoch: 692 [10100/50000 (20%)]  \tLoss:   90.618080\trec:   65.124046\tkl:   25.494032\n",
      "Epoch: 692 [20100/50000 (40%)]  \tLoss:   94.434181\trec:   67.934006\tkl:   26.500177\n",
      "Epoch: 692 [30100/50000 (60%)]  \tLoss:   93.695496\trec:   68.748306\tkl:   24.947195\n",
      "Epoch: 692 [40100/50000 (80%)]  \tLoss:   95.190239\trec:   69.187874\tkl:   26.002371\n",
      "====> Epoch: 692 Average train loss: 92.6000\n",
      "====> Validation set loss: 96.2793\n",
      "====> Validation set kl: 25.7661\n",
      "Epoch: 693 [  100/50000 ( 0%)]  \tLoss:   92.665443\trec:   67.323097\tkl:   25.342348\n",
      "Epoch: 693 [10100/50000 (20%)]  \tLoss:   93.290253\trec:   66.626328\tkl:   26.663925\n",
      "Epoch: 693 [20100/50000 (40%)]  \tLoss:   89.910606\trec:   63.745155\tkl:   26.165443\n",
      "Epoch: 693 [30100/50000 (60%)]  \tLoss:   92.363800\trec:   67.210274\tkl:   25.153528\n",
      "Epoch: 693 [40100/50000 (80%)]  \tLoss:   94.322731\trec:   68.346550\tkl:   25.976183\n",
      "====> Epoch: 693 Average train loss: 92.9689\n",
      "====> Validation set loss: 96.0979\n",
      "====> Validation set kl: 25.7556\n",
      "Epoch: 694 [  100/50000 ( 0%)]  \tLoss:   94.613380\trec:   67.907974\tkl:   26.705400\n",
      "Epoch: 694 [10100/50000 (20%)]  \tLoss:   94.471886\trec:   69.014961\tkl:   25.456917\n",
      "Epoch: 694 [20100/50000 (40%)]  \tLoss:   92.755386\trec:   67.793381\tkl:   24.962006\n",
      "Epoch: 694 [30100/50000 (60%)]  \tLoss:   96.725174\trec:   70.207726\tkl:   26.517456\n",
      "Epoch: 694 [40100/50000 (80%)]  \tLoss:   94.744041\trec:   68.588287\tkl:   26.155752\n",
      "====> Epoch: 694 Average train loss: 93.2087\n",
      "====> Validation set loss: 95.7399\n",
      "====> Validation set kl: 25.6953\n",
      "Epoch: 695 [  100/50000 ( 0%)]  \tLoss:   88.028053\trec:   62.744602\tkl:   25.283449\n",
      "Epoch: 695 [10100/50000 (20%)]  \tLoss:   90.826080\trec:   64.569000\tkl:   26.257076\n",
      "Epoch: 695 [20100/50000 (40%)]  \tLoss:   90.591484\trec:   65.663185\tkl:   24.928301\n",
      "Epoch: 695 [30100/50000 (60%)]  \tLoss:   95.626305\trec:   69.617722\tkl:   26.008589\n",
      "Epoch: 695 [40100/50000 (80%)]  \tLoss:   93.153961\trec:   66.324814\tkl:   26.829145\n",
      "====> Epoch: 695 Average train loss: 93.3420\n",
      "====> Validation set loss: 97.7425\n",
      "====> Validation set kl: 25.9340\n",
      "Epoch: 696 [  100/50000 ( 0%)]  \tLoss:   94.899200\trec:   68.325470\tkl:   26.573730\n",
      "Epoch: 696 [10100/50000 (20%)]  \tLoss:   91.017319\trec:   66.323494\tkl:   24.693825\n",
      "Epoch: 696 [20100/50000 (40%)]  \tLoss:   91.939705\trec:   66.027710\tkl:   25.911989\n",
      "Epoch: 696 [30100/50000 (60%)]  \tLoss:   94.197105\trec:   68.091850\tkl:   26.105261\n",
      "Epoch: 696 [40100/50000 (80%)]  \tLoss:   91.771454\trec:   65.786621\tkl:   25.984831\n",
      "====> Epoch: 696 Average train loss: 93.5995\n",
      "====> Validation set loss: 96.2502\n",
      "====> Validation set kl: 25.9949\n",
      "Epoch: 697 [  100/50000 ( 0%)]  \tLoss:   94.147011\trec:   68.397537\tkl:   25.749472\n",
      "Epoch: 697 [10100/50000 (20%)]  \tLoss:   93.641823\trec:   67.754456\tkl:   25.887367\n",
      "Epoch: 697 [20100/50000 (40%)]  \tLoss:   91.648842\trec:   65.321144\tkl:   26.327705\n",
      "Epoch: 697 [30100/50000 (60%)]  \tLoss:   92.988892\trec:   66.661270\tkl:   26.327623\n",
      "Epoch: 697 [40100/50000 (80%)]  \tLoss:   94.155899\trec:   68.009918\tkl:   26.145981\n",
      "====> Epoch: 697 Average train loss: 93.6668\n",
      "====> Validation set loss: 97.7255\n",
      "====> Validation set kl: 26.4316\n",
      "Epoch: 698 [  100/50000 ( 0%)]  \tLoss:   97.824059\trec:   70.789963\tkl:   27.034096\n",
      "Epoch: 698 [10100/50000 (20%)]  \tLoss:   89.616989\trec:   64.921471\tkl:   24.695520\n",
      "Epoch: 698 [20100/50000 (40%)]  \tLoss:   88.091476\trec:   62.282940\tkl:   25.808537\n",
      "Epoch: 698 [30100/50000 (60%)]  \tLoss:   95.741035\trec:   69.995041\tkl:   25.745991\n",
      "Epoch: 698 [40100/50000 (80%)]  \tLoss:   96.842651\trec:   70.142570\tkl:   26.700081\n",
      "====> Epoch: 698 Average train loss: 93.4839\n",
      "====> Validation set loss: 96.4574\n",
      "====> Validation set kl: 26.1621\n",
      "Epoch: 699 [  100/50000 ( 0%)]  \tLoss:   91.574860\trec:   65.327461\tkl:   26.247406\n",
      "Epoch: 699 [10100/50000 (20%)]  \tLoss:   96.696510\trec:   69.262985\tkl:   27.433525\n",
      "Epoch: 699 [20100/50000 (40%)]  \tLoss:   89.409897\trec:   64.282234\tkl:   25.127663\n",
      "Epoch: 699 [30100/50000 (60%)]  \tLoss:   93.256523\trec:   68.150238\tkl:   25.106289\n",
      "Epoch: 699 [40100/50000 (80%)]  \tLoss:   91.502144\trec:   65.876648\tkl:   25.625500\n",
      "====> Epoch: 699 Average train loss: 93.5782\n",
      "====> Validation set loss: 96.5132\n",
      "====> Validation set kl: 26.0122\n",
      "Epoch: 700 [  100/50000 ( 0%)]  \tLoss:   96.918373\trec:   70.242226\tkl:   26.676149\n",
      "Epoch: 700 [10100/50000 (20%)]  \tLoss:   93.572479\trec:   66.851364\tkl:   26.721117\n",
      "Epoch: 700 [20100/50000 (40%)]  \tLoss:   91.759071\trec:   66.755928\tkl:   25.003143\n",
      "Epoch: 700 [30100/50000 (60%)]  \tLoss:   95.345268\trec:   69.569191\tkl:   25.776079\n",
      "Epoch: 700 [40100/50000 (80%)]  \tLoss:   92.818985\trec:   67.959572\tkl:   24.859419\n",
      "====> Epoch: 700 Average train loss: 93.1127\n",
      "====> Validation set loss: 96.4278\n",
      "====> Validation set kl: 25.5515\n",
      "Epoch: 701 [  100/50000 ( 0%)]  \tLoss:   89.643829\trec:   64.637535\tkl:   25.006283\n",
      "Epoch: 701 [10100/50000 (20%)]  \tLoss:   90.375053\trec:   65.635735\tkl:   24.739321\n",
      "Epoch: 701 [20100/50000 (40%)]  \tLoss:   91.087074\trec:   65.158836\tkl:   25.928244\n",
      "Epoch: 701 [30100/50000 (60%)]  \tLoss:   91.190460\trec:   65.458809\tkl:   25.731651\n",
      "Epoch: 701 [40100/50000 (80%)]  \tLoss:   88.834892\trec:   64.316063\tkl:   24.518827\n",
      "====> Epoch: 701 Average train loss: 93.3538\n",
      "====> Validation set loss: 96.6591\n",
      "====> Validation set kl: 26.3811\n",
      "Epoch: 702 [  100/50000 ( 0%)]  \tLoss:   92.664566\trec:   65.760490\tkl:   26.904087\n",
      "Epoch: 702 [10100/50000 (20%)]  \tLoss:   88.531807\trec:   64.450981\tkl:   24.080822\n",
      "Epoch: 702 [20100/50000 (40%)]  \tLoss:   93.944374\trec:   67.413780\tkl:   26.530596\n",
      "Epoch: 702 [30100/50000 (60%)]  \tLoss:   93.068298\trec:   68.371353\tkl:   24.696938\n",
      "Epoch: 702 [40100/50000 (80%)]  \tLoss:   93.176498\trec:   67.792145\tkl:   25.384365\n",
      "====> Epoch: 702 Average train loss: 93.0731\n",
      "====> Validation set loss: 96.6199\n",
      "====> Validation set kl: 25.9019\n",
      "Epoch: 703 [  100/50000 ( 0%)]  \tLoss:   96.914940\trec:   70.830605\tkl:   26.084341\n",
      "Epoch: 703 [10100/50000 (20%)]  \tLoss:   92.363068\trec:   66.504829\tkl:   25.858238\n",
      "Epoch: 703 [20100/50000 (40%)]  \tLoss:   95.169693\trec:   68.009010\tkl:   27.160685\n",
      "Epoch: 703 [30100/50000 (60%)]  \tLoss:   94.886093\trec:   69.084747\tkl:   25.801342\n",
      "Epoch: 703 [40100/50000 (80%)]  \tLoss:   93.566628\trec:   67.466133\tkl:   26.100498\n",
      "====> Epoch: 703 Average train loss: 93.5998\n",
      "====> Validation set loss: 96.1351\n",
      "====> Validation set kl: 25.1281\n",
      "Epoch: 704 [  100/50000 ( 0%)]  \tLoss:   93.297050\trec:   67.485100\tkl:   25.811947\n",
      "Epoch: 704 [10100/50000 (20%)]  \tLoss:   90.047867\trec:   64.920502\tkl:   25.127363\n",
      "Epoch: 704 [20100/50000 (40%)]  \tLoss:   88.963318\trec:   64.089355\tkl:   24.873964\n",
      "Epoch: 704 [30100/50000 (60%)]  \tLoss:   86.895523\trec:   62.029465\tkl:   24.866062\n",
      "Epoch: 704 [40100/50000 (80%)]  \tLoss:   93.488853\trec:   67.405327\tkl:   26.083527\n",
      "====> Epoch: 704 Average train loss: 92.8196\n",
      "====> Validation set loss: 95.6557\n",
      "====> Validation set kl: 25.7258\n",
      "Epoch: 705 [  100/50000 ( 0%)]  \tLoss:   94.145378\trec:   67.907539\tkl:   26.237839\n",
      "Epoch: 705 [10100/50000 (20%)]  \tLoss:   93.242729\trec:   67.755051\tkl:   25.487684\n",
      "Epoch: 705 [20100/50000 (40%)]  \tLoss:   89.100143\trec:   63.315685\tkl:   25.784458\n",
      "Epoch: 705 [30100/50000 (60%)]  \tLoss:   94.953453\trec:   68.416840\tkl:   26.536615\n",
      "Epoch: 705 [40100/50000 (80%)]  \tLoss:   94.205643\trec:   68.188499\tkl:   26.017147\n",
      "====> Epoch: 705 Average train loss: 92.9936\n",
      "====> Validation set loss: 97.1175\n",
      "====> Validation set kl: 25.5926\n",
      "Epoch: 706 [  100/50000 ( 0%)]  \tLoss:   95.284317\trec:   69.604576\tkl:   25.679743\n",
      "Epoch: 706 [10100/50000 (20%)]  \tLoss:   96.204132\trec:   69.745796\tkl:   26.458332\n",
      "Epoch: 706 [20100/50000 (40%)]  \tLoss:   96.296928\trec:   69.793037\tkl:   26.503891\n",
      "Epoch: 706 [30100/50000 (60%)]  \tLoss:   95.537598\trec:   69.700104\tkl:   25.837494\n",
      "Epoch: 706 [40100/50000 (80%)]  \tLoss:   92.879570\trec:   67.602226\tkl:   25.277346\n",
      "====> Epoch: 706 Average train loss: 93.3799\n",
      "====> Validation set loss: 96.2279\n",
      "====> Validation set kl: 25.7418\n",
      "Epoch: 707 [  100/50000 ( 0%)]  \tLoss:   91.146782\trec:   65.946754\tkl:   25.200029\n",
      "Epoch: 707 [10100/50000 (20%)]  \tLoss:   92.289703\trec:   67.337234\tkl:   24.952471\n",
      "Epoch: 707 [20100/50000 (40%)]  \tLoss:   94.110001\trec:   67.932617\tkl:   26.177372\n",
      "Epoch: 707 [30100/50000 (60%)]  \tLoss:   91.729073\trec:   65.993126\tkl:   25.735939\n",
      "Epoch: 707 [40100/50000 (80%)]  \tLoss:   97.851715\trec:   70.880112\tkl:   26.971605\n",
      "====> Epoch: 707 Average train loss: 93.4303\n",
      "====> Validation set loss: 96.7470\n",
      "====> Validation set kl: 26.0650\n",
      "Epoch: 708 [  100/50000 ( 0%)]  \tLoss:   91.692436\trec:   65.190948\tkl:   26.501495\n",
      "Epoch: 708 [10100/50000 (20%)]  \tLoss:   95.195190\trec:   68.975845\tkl:   26.219351\n",
      "Epoch: 708 [20100/50000 (40%)]  \tLoss:   93.839333\trec:   68.202477\tkl:   25.636854\n",
      "Epoch: 708 [30100/50000 (60%)]  \tLoss:   89.697754\trec:   64.701469\tkl:   24.996277\n",
      "Epoch: 708 [40100/50000 (80%)]  \tLoss:   89.114220\trec:   64.082733\tkl:   25.031488\n",
      "====> Epoch: 708 Average train loss: 93.2098\n",
      "====> Validation set loss: 96.0424\n",
      "====> Validation set kl: 26.0848\n",
      "Epoch: 709 [  100/50000 ( 0%)]  \tLoss:   93.822067\trec:   67.627525\tkl:   26.194542\n",
      "Epoch: 709 [10100/50000 (20%)]  \tLoss:   95.161514\trec:   69.022751\tkl:   26.138760\n",
      "Epoch: 709 [20100/50000 (40%)]  \tLoss:   90.494644\trec:   65.592072\tkl:   24.902582\n",
      "Epoch: 709 [30100/50000 (60%)]  \tLoss:   97.402725\trec:   70.367531\tkl:   27.035194\n",
      "Epoch: 709 [40100/50000 (80%)]  \tLoss:   94.999870\trec:   70.061432\tkl:   24.938440\n",
      "====> Epoch: 709 Average train loss: 93.5199\n",
      "====> Validation set loss: 96.8565\n",
      "====> Validation set kl: 25.8372\n",
      "Epoch: 710 [  100/50000 ( 0%)]  \tLoss:   90.619240\trec:   65.249123\tkl:   25.370113\n",
      "Epoch: 710 [10100/50000 (20%)]  \tLoss:   96.465889\trec:   70.473137\tkl:   25.992748\n",
      "Epoch: 710 [20100/50000 (40%)]  \tLoss:   92.464996\trec:   66.557388\tkl:   25.907610\n",
      "Epoch: 710 [30100/50000 (60%)]  \tLoss:   93.846931\trec:   68.358223\tkl:   25.488716\n",
      "Epoch: 710 [40100/50000 (80%)]  \tLoss:   98.479599\trec:   71.287735\tkl:   27.191862\n",
      "====> Epoch: 710 Average train loss: 93.3080\n",
      "====> Validation set loss: 96.4149\n",
      "====> Validation set kl: 26.0508\n",
      "Epoch: 711 [  100/50000 ( 0%)]  \tLoss:   92.852036\trec:   66.380104\tkl:   26.471933\n",
      "Epoch: 711 [10100/50000 (20%)]  \tLoss:   92.532692\trec:   67.294312\tkl:   25.238373\n",
      "Epoch: 711 [20100/50000 (40%)]  \tLoss:   93.956718\trec:   68.396278\tkl:   25.560434\n",
      "Epoch: 711 [30100/50000 (60%)]  \tLoss:   90.420731\trec:   65.095024\tkl:   25.325701\n",
      "Epoch: 711 [40100/50000 (80%)]  \tLoss:   93.322739\trec:   67.582146\tkl:   25.740601\n",
      "====> Epoch: 711 Average train loss: 92.9516\n",
      "====> Validation set loss: 96.2536\n",
      "====> Validation set kl: 25.8079\n",
      "Epoch: 712 [  100/50000 ( 0%)]  \tLoss:   94.818153\trec:   68.678589\tkl:   26.139563\n",
      "Epoch: 712 [10100/50000 (20%)]  \tLoss:   91.964630\trec:   66.241806\tkl:   25.722818\n",
      "Epoch: 712 [20100/50000 (40%)]  \tLoss:   96.527992\trec:   69.748535\tkl:   26.779459\n",
      "Epoch: 712 [30100/50000 (60%)]  \tLoss:   93.398857\trec:   67.345123\tkl:   26.053730\n",
      "Epoch: 712 [40100/50000 (80%)]  \tLoss:   92.761658\trec:   67.953224\tkl:   24.808437\n",
      "====> Epoch: 712 Average train loss: 93.0031\n",
      "====> Validation set loss: 96.1813\n",
      "====> Validation set kl: 25.2795\n",
      "Epoch: 713 [  100/50000 ( 0%)]  \tLoss:   87.982849\trec:   64.047150\tkl:   23.935699\n",
      "Epoch: 713 [10100/50000 (20%)]  \tLoss:   92.544220\trec:   67.230370\tkl:   25.313847\n",
      "Epoch: 713 [20100/50000 (40%)]  \tLoss:   97.798874\trec:   71.474236\tkl:   26.324642\n",
      "Epoch: 713 [30100/50000 (60%)]  \tLoss:   93.780136\trec:   67.352577\tkl:   26.427559\n",
      "Epoch: 713 [40100/50000 (80%)]  \tLoss:   96.973190\trec:   70.383514\tkl:   26.589674\n",
      "====> Epoch: 713 Average train loss: 93.1743\n",
      "====> Validation set loss: 96.0743\n",
      "====> Validation set kl: 25.7707\n",
      "Epoch: 714 [  100/50000 ( 0%)]  \tLoss:   92.400024\trec:   67.277199\tkl:   25.122829\n",
      "Epoch: 714 [10100/50000 (20%)]  \tLoss:   95.415199\trec:   69.190582\tkl:   26.224621\n",
      "Epoch: 714 [20100/50000 (40%)]  \tLoss:   94.602646\trec:   68.347504\tkl:   26.255140\n",
      "Epoch: 714 [30100/50000 (60%)]  \tLoss:   93.654839\trec:   67.009781\tkl:   26.645063\n",
      "Epoch: 714 [40100/50000 (80%)]  \tLoss:   88.623413\trec:   63.478115\tkl:   25.145300\n",
      "====> Epoch: 714 Average train loss: 93.5028\n",
      "====> Validation set loss: 96.6306\n",
      "====> Validation set kl: 25.8440\n",
      "Epoch: 715 [  100/50000 ( 0%)]  \tLoss:   93.657753\trec:   68.851891\tkl:   24.805864\n",
      "Epoch: 715 [10100/50000 (20%)]  \tLoss:   96.933403\trec:   71.771172\tkl:   25.162235\n",
      "Epoch: 715 [20100/50000 (40%)]  \tLoss:   92.296738\trec:   65.874489\tkl:   26.422251\n",
      "Epoch: 715 [30100/50000 (60%)]  \tLoss:   94.709366\trec:   67.816490\tkl:   26.892874\n",
      "Epoch: 715 [40100/50000 (80%)]  \tLoss:   94.249741\trec:   68.039124\tkl:   26.210617\n",
      "====> Epoch: 715 Average train loss: 93.0106\n",
      "====> Validation set loss: 96.3278\n",
      "====> Validation set kl: 26.0092\n",
      "Epoch: 716 [  100/50000 ( 0%)]  \tLoss:   88.085739\trec:   63.113106\tkl:   24.972631\n",
      "Epoch: 716 [10100/50000 (20%)]  \tLoss:   95.674606\trec:   69.716782\tkl:   25.957829\n",
      "Epoch: 716 [20100/50000 (40%)]  \tLoss:   92.501099\trec:   67.230232\tkl:   25.270866\n",
      "Epoch: 716 [30100/50000 (60%)]  \tLoss:   95.198143\trec:   69.068916\tkl:   26.129225\n",
      "Epoch: 716 [40100/50000 (80%)]  \tLoss:   91.995155\trec:   65.498344\tkl:   26.496803\n",
      "====> Epoch: 716 Average train loss: 93.1045\n",
      "====> Validation set loss: 96.5570\n",
      "====> Validation set kl: 26.3739\n",
      "Epoch: 717 [  100/50000 ( 0%)]  \tLoss:   91.432045\trec:   64.767776\tkl:   26.664267\n",
      "Epoch: 717 [10100/50000 (20%)]  \tLoss:   92.007828\trec:   66.030937\tkl:   25.976887\n",
      "Epoch: 717 [20100/50000 (40%)]  \tLoss:   93.006111\trec:   66.684914\tkl:   26.321203\n",
      "Epoch: 717 [30100/50000 (60%)]  \tLoss:   94.059395\trec:   68.389435\tkl:   25.669960\n",
      "Epoch: 717 [40100/50000 (80%)]  \tLoss:   91.394844\trec:   65.766190\tkl:   25.628653\n",
      "====> Epoch: 717 Average train loss: 93.3892\n",
      "====> Validation set loss: 96.5488\n",
      "====> Validation set kl: 26.0721\n",
      "Epoch: 718 [  100/50000 ( 0%)]  \tLoss:   89.918007\trec:   64.710518\tkl:   25.207485\n",
      "Epoch: 718 [10100/50000 (20%)]  \tLoss:   96.031403\trec:   69.915352\tkl:   26.116049\n",
      "Epoch: 718 [20100/50000 (40%)]  \tLoss:   95.995956\trec:   69.558754\tkl:   26.437202\n",
      "Epoch: 718 [30100/50000 (60%)]  \tLoss:   93.351074\trec:   67.566666\tkl:   25.784403\n",
      "Epoch: 718 [40100/50000 (80%)]  \tLoss:   96.294998\trec:   69.054008\tkl:   27.240986\n",
      "====> Epoch: 718 Average train loss: 93.1589\n",
      "====> Validation set loss: 96.0506\n",
      "====> Validation set kl: 25.8063\n",
      "Epoch: 719 [  100/50000 ( 0%)]  \tLoss:   92.213432\trec:   66.383858\tkl:   25.829569\n",
      "Epoch: 719 [10100/50000 (20%)]  \tLoss:   95.796196\trec:   69.107124\tkl:   26.689072\n",
      "Epoch: 719 [20100/50000 (40%)]  \tLoss:   94.414566\trec:   69.096886\tkl:   25.317690\n",
      "Epoch: 719 [30100/50000 (60%)]  \tLoss:   94.115456\trec:   67.762154\tkl:   26.353302\n",
      "Epoch: 719 [40100/50000 (80%)]  \tLoss:   97.259331\trec:   69.499054\tkl:   27.760283\n",
      "====> Epoch: 719 Average train loss: 92.8750\n",
      "====> Validation set loss: 95.8668\n",
      "====> Validation set kl: 26.1224\n",
      "Epoch: 720 [  100/50000 ( 0%)]  \tLoss:   95.160210\trec:   68.238174\tkl:   26.922045\n",
      "Epoch: 720 [10100/50000 (20%)]  \tLoss:   90.066383\trec:   65.299118\tkl:   24.767269\n",
      "Epoch: 720 [20100/50000 (40%)]  \tLoss:   93.346100\trec:   67.915276\tkl:   25.430830\n",
      "Epoch: 720 [30100/50000 (60%)]  \tLoss:   95.406914\trec:   69.885841\tkl:   25.521076\n",
      "Epoch: 720 [40100/50000 (80%)]  \tLoss:   94.872147\trec:   69.611954\tkl:   25.260197\n",
      "====> Epoch: 720 Average train loss: 92.8008\n",
      "====> Validation set loss: 96.0171\n",
      "====> Validation set kl: 25.6312\n",
      "Epoch: 721 [  100/50000 ( 0%)]  \tLoss:   93.150063\trec:   67.167046\tkl:   25.983017\n",
      "Epoch: 721 [10100/50000 (20%)]  \tLoss:   92.384193\trec:   66.086769\tkl:   26.297434\n",
      "Epoch: 721 [20100/50000 (40%)]  \tLoss:   91.946518\trec:   66.578415\tkl:   25.368105\n",
      "Epoch: 721 [30100/50000 (60%)]  \tLoss:   93.985977\trec:   67.490158\tkl:   26.495810\n",
      "Epoch: 721 [40100/50000 (80%)]  \tLoss:   95.236572\trec:   68.723442\tkl:   26.513130\n",
      "====> Epoch: 721 Average train loss: 93.0958\n",
      "====> Validation set loss: 96.7313\n",
      "====> Validation set kl: 25.9668\n",
      "Epoch: 722 [  100/50000 ( 0%)]  \tLoss:   93.666267\trec:   66.652733\tkl:   27.013536\n",
      "Epoch: 722 [10100/50000 (20%)]  \tLoss:   94.078018\trec:   68.556122\tkl:   25.521891\n",
      "Epoch: 722 [20100/50000 (40%)]  \tLoss:   92.543747\trec:   67.629791\tkl:   24.913960\n",
      "Epoch: 722 [30100/50000 (60%)]  \tLoss:   94.724800\trec:   68.087921\tkl:   26.636881\n",
      "Epoch: 722 [40100/50000 (80%)]  \tLoss:   91.322342\trec:   65.894600\tkl:   25.427738\n",
      "====> Epoch: 722 Average train loss: 92.9971\n",
      "====> Validation set loss: 95.7428\n",
      "====> Validation set kl: 25.7122\n",
      "Epoch: 723 [  100/50000 ( 0%)]  \tLoss:   95.755402\trec:   69.721901\tkl:   26.033501\n",
      "Epoch: 723 [10100/50000 (20%)]  \tLoss:   95.563248\trec:   70.030693\tkl:   25.532557\n",
      "Epoch: 723 [20100/50000 (40%)]  \tLoss:   96.331276\trec:   69.245865\tkl:   27.085411\n",
      "Epoch: 723 [30100/50000 (60%)]  \tLoss:   89.715317\trec:   64.812584\tkl:   24.902731\n",
      "Epoch: 723 [40100/50000 (80%)]  \tLoss:   94.587128\trec:   68.904846\tkl:   25.682281\n",
      "====> Epoch: 723 Average train loss: 92.9108\n",
      "====> Validation set loss: 95.9541\n",
      "====> Validation set kl: 25.9587\n",
      "Epoch: 724 [  100/50000 ( 0%)]  \tLoss:   90.098396\trec:   64.767555\tkl:   25.330835\n",
      "Epoch: 724 [10100/50000 (20%)]  \tLoss:   90.099098\trec:   64.168716\tkl:   25.930389\n",
      "Epoch: 724 [20100/50000 (40%)]  \tLoss:   98.447594\trec:   71.025002\tkl:   27.422590\n",
      "Epoch: 724 [30100/50000 (60%)]  \tLoss:   90.940506\trec:   65.366089\tkl:   25.574411\n",
      "Epoch: 724 [40100/50000 (80%)]  \tLoss:   91.894562\trec:   66.881516\tkl:   25.013044\n",
      "====> Epoch: 724 Average train loss: 92.9655\n",
      "====> Validation set loss: 95.8687\n",
      "====> Validation set kl: 25.6461\n",
      "Epoch: 725 [  100/50000 ( 0%)]  \tLoss:   89.954277\trec:   64.460732\tkl:   25.493549\n",
      "Epoch: 725 [10100/50000 (20%)]  \tLoss:   89.023483\trec:   62.957005\tkl:   26.066477\n",
      "Epoch: 725 [20100/50000 (40%)]  \tLoss:   87.120949\trec:   61.654034\tkl:   25.466915\n",
      "Epoch: 725 [30100/50000 (60%)]  \tLoss:   88.761650\trec:   62.891201\tkl:   25.870451\n",
      "Epoch: 725 [40100/50000 (80%)]  \tLoss:   91.829445\trec:   66.913368\tkl:   24.916073\n",
      "====> Epoch: 725 Average train loss: 92.8218\n",
      "====> Validation set loss: 96.2971\n",
      "====> Validation set kl: 25.4505\n",
      "Epoch: 726 [  100/50000 ( 0%)]  \tLoss:   90.269897\trec:   64.617119\tkl:   25.652788\n",
      "Epoch: 726 [10100/50000 (20%)]  \tLoss:   87.539062\trec:   62.926769\tkl:   24.612289\n",
      "Epoch: 726 [20100/50000 (40%)]  \tLoss:   94.973396\trec:   68.402824\tkl:   26.570566\n",
      "Epoch: 726 [30100/50000 (60%)]  \tLoss:   95.079666\trec:   68.241287\tkl:   26.838383\n",
      "Epoch: 726 [40100/50000 (80%)]  \tLoss:   96.030975\trec:   69.777130\tkl:   26.253853\n",
      "====> Epoch: 726 Average train loss: 93.2298\n",
      "====> Validation set loss: 95.8808\n",
      "====> Validation set kl: 25.9783\n",
      "Epoch: 727 [  100/50000 ( 0%)]  \tLoss:   90.996284\trec:   64.721695\tkl:   26.274593\n",
      "Epoch: 727 [10100/50000 (20%)]  \tLoss:   92.521667\trec:   66.895477\tkl:   25.626194\n",
      "Epoch: 727 [20100/50000 (40%)]  \tLoss:   92.106697\trec:   65.956337\tkl:   26.150358\n",
      "Epoch: 727 [30100/50000 (60%)]  \tLoss:   89.536499\trec:   64.094582\tkl:   25.441919\n",
      "Epoch: 727 [40100/50000 (80%)]  \tLoss:   95.364258\trec:   68.849586\tkl:   26.514668\n",
      "====> Epoch: 727 Average train loss: 92.9605\n",
      "====> Validation set loss: 96.3676\n",
      "====> Validation set kl: 26.2249\n",
      "Epoch: 728 [  100/50000 ( 0%)]  \tLoss:   92.969528\trec:   66.252800\tkl:   26.716732\n",
      "Epoch: 728 [10100/50000 (20%)]  \tLoss:   90.268532\trec:   63.777401\tkl:   26.491127\n",
      "Epoch: 728 [20100/50000 (40%)]  \tLoss:   92.962234\trec:   67.398720\tkl:   25.563515\n",
      "Epoch: 728 [30100/50000 (60%)]  \tLoss:   91.844429\trec:   67.113701\tkl:   24.730722\n",
      "Epoch: 728 [40100/50000 (80%)]  \tLoss:   91.454254\trec:   65.852242\tkl:   25.602016\n",
      "====> Epoch: 728 Average train loss: 92.9774\n",
      "====> Validation set loss: 96.0116\n",
      "====> Validation set kl: 26.0391\n",
      "Epoch: 729 [  100/50000 ( 0%)]  \tLoss:   90.984047\trec:   65.553230\tkl:   25.430820\n",
      "Epoch: 729 [10100/50000 (20%)]  \tLoss:   90.039413\trec:   64.568123\tkl:   25.471283\n",
      "Epoch: 729 [20100/50000 (40%)]  \tLoss:   94.101425\trec:   68.552963\tkl:   25.548458\n",
      "Epoch: 729 [30100/50000 (60%)]  \tLoss:   91.228394\trec:   65.595360\tkl:   25.633034\n",
      "Epoch: 729 [40100/50000 (80%)]  \tLoss:   94.230339\trec:   68.624092\tkl:   25.606245\n",
      "====> Epoch: 729 Average train loss: 92.9649\n",
      "====> Validation set loss: 96.0227\n",
      "====> Validation set kl: 25.8028\n",
      "Epoch: 730 [  100/50000 ( 0%)]  \tLoss:   95.798965\trec:   68.928520\tkl:   26.870440\n",
      "Epoch: 730 [10100/50000 (20%)]  \tLoss:   90.514977\trec:   65.994728\tkl:   24.520243\n",
      "Epoch: 730 [20100/50000 (40%)]  \tLoss:   94.134727\trec:   67.516670\tkl:   26.618053\n",
      "Epoch: 730 [30100/50000 (60%)]  \tLoss:   93.045723\trec:   67.470840\tkl:   25.574877\n",
      "Epoch: 730 [40100/50000 (80%)]  \tLoss:   91.452202\trec:   65.360390\tkl:   26.091818\n",
      "====> Epoch: 730 Average train loss: 92.9044\n",
      "====> Validation set loss: 95.7251\n",
      "====> Validation set kl: 25.7618\n",
      "Epoch: 731 [  100/50000 ( 0%)]  \tLoss:   87.305038\trec:   61.842487\tkl:   25.462551\n",
      "Epoch: 731 [10100/50000 (20%)]  \tLoss:   91.626991\trec:   66.835495\tkl:   24.791496\n",
      "Epoch: 731 [20100/50000 (40%)]  \tLoss:   93.022354\trec:   66.736794\tkl:   26.285557\n",
      "Epoch: 731 [30100/50000 (60%)]  \tLoss:   89.460724\trec:   64.953384\tkl:   24.507334\n",
      "Epoch: 731 [40100/50000 (80%)]  \tLoss:   92.660820\trec:   66.752838\tkl:   25.907982\n",
      "====> Epoch: 731 Average train loss: 92.8321\n",
      "====> Validation set loss: 95.9954\n",
      "====> Validation set kl: 25.6863\n",
      "Epoch: 732 [  100/50000 ( 0%)]  \tLoss:   91.586090\trec:   66.219315\tkl:   25.366774\n",
      "Epoch: 732 [10100/50000 (20%)]  \tLoss:   93.116920\trec:   67.077171\tkl:   26.039753\n",
      "Epoch: 732 [20100/50000 (40%)]  \tLoss:   94.225319\trec:   68.475372\tkl:   25.749950\n",
      "Epoch: 732 [30100/50000 (60%)]  \tLoss:   93.229767\trec:   66.188171\tkl:   27.041595\n",
      "Epoch: 732 [40100/50000 (80%)]  \tLoss:   94.123344\trec:   68.023651\tkl:   26.099699\n",
      "====> Epoch: 732 Average train loss: 92.7217\n",
      "====> Validation set loss: 96.1495\n",
      "====> Validation set kl: 25.7663\n",
      "Epoch: 733 [  100/50000 ( 0%)]  \tLoss:   92.683670\trec:   66.500809\tkl:   26.182863\n",
      "Epoch: 733 [10100/50000 (20%)]  \tLoss:   92.947701\trec:   66.948761\tkl:   25.998947\n",
      "Epoch: 733 [20100/50000 (40%)]  \tLoss:   90.935684\trec:   65.152206\tkl:   25.783482\n",
      "Epoch: 733 [30100/50000 (60%)]  \tLoss:   92.387482\trec:   66.594238\tkl:   25.793236\n",
      "Epoch: 733 [40100/50000 (80%)]  \tLoss:   94.097946\trec:   68.521362\tkl:   25.576584\n",
      "====> Epoch: 733 Average train loss: 92.6315\n",
      "====> Validation set loss: 95.9831\n",
      "====> Validation set kl: 25.9460\n",
      "Epoch: 734 [  100/50000 ( 0%)]  \tLoss:   95.433815\trec:   69.340576\tkl:   26.093239\n",
      "Epoch: 734 [10100/50000 (20%)]  \tLoss:   92.882759\trec:   67.271416\tkl:   25.611347\n",
      "Epoch: 734 [20100/50000 (40%)]  \tLoss:   93.659752\trec:   68.758247\tkl:   24.901510\n",
      "Epoch: 734 [30100/50000 (60%)]  \tLoss:   92.758408\trec:   67.416939\tkl:   25.341461\n",
      "Epoch: 734 [40100/50000 (80%)]  \tLoss:   93.308456\trec:   67.119087\tkl:   26.189362\n",
      "====> Epoch: 734 Average train loss: 92.7765\n",
      "====> Validation set loss: 96.3981\n",
      "====> Validation set kl: 26.2782\n",
      "Epoch: 735 [  100/50000 ( 0%)]  \tLoss:   95.925697\trec:   69.264786\tkl:   26.660910\n",
      "Epoch: 735 [10100/50000 (20%)]  \tLoss:   94.728729\trec:   68.698776\tkl:   26.029955\n",
      "Epoch: 735 [20100/50000 (40%)]  \tLoss:   94.580467\trec:   68.840668\tkl:   25.739794\n",
      "Epoch: 735 [30100/50000 (60%)]  \tLoss:   93.634430\trec:   67.960075\tkl:   25.674356\n",
      "Epoch: 735 [40100/50000 (80%)]  \tLoss:   94.935463\trec:   68.598907\tkl:   26.336559\n",
      "====> Epoch: 735 Average train loss: 92.7720\n",
      "====> Validation set loss: 95.6865\n",
      "====> Validation set kl: 25.8389\n",
      "Epoch: 736 [  100/50000 ( 0%)]  \tLoss:   90.613785\trec:   65.262177\tkl:   25.351616\n",
      "Epoch: 736 [10100/50000 (20%)]  \tLoss:   93.761147\trec:   68.175827\tkl:   25.585327\n",
      "Epoch: 736 [20100/50000 (40%)]  \tLoss:   90.759033\trec:   65.591209\tkl:   25.167822\n",
      "Epoch: 736 [30100/50000 (60%)]  \tLoss:   94.830894\trec:   68.077827\tkl:   26.753061\n",
      "Epoch: 736 [40100/50000 (80%)]  \tLoss:   91.812553\trec:   67.800461\tkl:   24.012098\n",
      "====> Epoch: 736 Average train loss: 92.7003\n",
      "====> Validation set loss: 96.2597\n",
      "====> Validation set kl: 25.2383\n",
      "Epoch: 737 [  100/50000 ( 0%)]  \tLoss:   92.352081\trec:   67.199745\tkl:   25.152330\n",
      "Epoch: 737 [10100/50000 (20%)]  \tLoss:   90.245880\trec:   64.215347\tkl:   26.030527\n",
      "Epoch: 737 [20100/50000 (40%)]  \tLoss:   92.601608\trec:   66.338463\tkl:   26.263145\n",
      "Epoch: 737 [30100/50000 (60%)]  \tLoss:   97.162514\trec:   71.124435\tkl:   26.038090\n",
      "Epoch: 737 [40100/50000 (80%)]  \tLoss:   92.919449\trec:   67.301567\tkl:   25.617878\n",
      "====> Epoch: 737 Average train loss: 93.2441\n",
      "====> Validation set loss: 96.7309\n",
      "====> Validation set kl: 26.7252\n",
      "Epoch: 738 [  100/50000 ( 0%)]  \tLoss:   94.556763\trec:   68.034630\tkl:   26.522135\n",
      "Epoch: 738 [10100/50000 (20%)]  \tLoss:   87.606064\trec:   62.662762\tkl:   24.943300\n",
      "Epoch: 738 [20100/50000 (40%)]  \tLoss:   91.871346\trec:   66.150314\tkl:   25.721029\n",
      "Epoch: 738 [30100/50000 (60%)]  \tLoss:   89.055214\trec:   64.390762\tkl:   24.664457\n",
      "Epoch: 738 [40100/50000 (80%)]  \tLoss:   95.369492\trec:   68.709724\tkl:   26.659763\n",
      "====> Epoch: 738 Average train loss: 92.9400\n",
      "====> Validation set loss: 96.5502\n",
      "====> Validation set kl: 25.8644\n",
      "Epoch: 739 [  100/50000 ( 0%)]  \tLoss:   97.730904\trec:   70.612068\tkl:   27.118839\n",
      "Epoch: 739 [10100/50000 (20%)]  \tLoss:   90.612450\trec:   65.680054\tkl:   24.932392\n",
      "Epoch: 739 [20100/50000 (40%)]  \tLoss:   94.793076\trec:   68.260605\tkl:   26.532473\n",
      "Epoch: 739 [30100/50000 (60%)]  \tLoss:   92.776909\trec:   68.424973\tkl:   24.351940\n",
      "Epoch: 739 [40100/50000 (80%)]  \tLoss:   93.822174\trec:   67.471748\tkl:   26.350431\n",
      "====> Epoch: 739 Average train loss: 93.1064\n",
      "====> Validation set loss: 96.0355\n",
      "====> Validation set kl: 25.8959\n",
      "Epoch: 740 [  100/50000 ( 0%)]  \tLoss:   93.151840\trec:   66.690147\tkl:   26.461697\n",
      "Epoch: 740 [10100/50000 (20%)]  \tLoss:   89.296524\trec:   63.704990\tkl:   25.591532\n",
      "Epoch: 740 [20100/50000 (40%)]  \tLoss:   96.036598\trec:   68.887016\tkl:   27.149576\n",
      "Epoch: 740 [30100/50000 (60%)]  \tLoss:   91.370644\trec:   65.207680\tkl:   26.162968\n",
      "Epoch: 740 [40100/50000 (80%)]  \tLoss:   89.495575\trec:   64.455780\tkl:   25.039795\n",
      "====> Epoch: 740 Average train loss: 93.1671\n",
      "====> Validation set loss: 95.9104\n",
      "====> Validation set kl: 25.6832\n",
      "Epoch: 741 [  100/50000 ( 0%)]  \tLoss:   90.418266\trec:   65.846092\tkl:   24.572180\n",
      "Epoch: 741 [10100/50000 (20%)]  \tLoss:   93.409187\trec:   67.011154\tkl:   26.398033\n",
      "Epoch: 741 [20100/50000 (40%)]  \tLoss:   93.549210\trec:   67.546783\tkl:   26.002419\n",
      "Epoch: 741 [30100/50000 (60%)]  \tLoss:   91.038765\trec:   65.637344\tkl:   25.401423\n",
      "Epoch: 741 [40100/50000 (80%)]  \tLoss:   95.116615\trec:   67.606705\tkl:   27.509918\n",
      "====> Epoch: 741 Average train loss: 92.9632\n",
      "====> Validation set loss: 96.2145\n",
      "====> Validation set kl: 26.2607\n",
      "Epoch: 742 [  100/50000 ( 0%)]  \tLoss:   95.202324\trec:   67.903679\tkl:   27.298645\n",
      "Epoch: 742 [10100/50000 (20%)]  \tLoss:   96.336502\trec:   68.822868\tkl:   27.513632\n",
      "Epoch: 742 [20100/50000 (40%)]  \tLoss:   94.525200\trec:   69.089287\tkl:   25.435915\n",
      "Epoch: 742 [30100/50000 (60%)]  \tLoss:   92.803688\trec:   66.787743\tkl:   26.015944\n",
      "Epoch: 742 [40100/50000 (80%)]  \tLoss:   90.322578\trec:   64.998962\tkl:   25.323618\n",
      "====> Epoch: 742 Average train loss: 93.0440\n",
      "====> Validation set loss: 96.2918\n",
      "====> Validation set kl: 26.1248\n",
      "Epoch: 743 [  100/50000 ( 0%)]  \tLoss:   88.820152\trec:   63.035976\tkl:   25.784176\n",
      "Epoch: 743 [10100/50000 (20%)]  \tLoss:   93.125381\trec:   66.924553\tkl:   26.200827\n",
      "Epoch: 743 [20100/50000 (40%)]  \tLoss:   92.288223\trec:   67.424248\tkl:   24.863970\n",
      "Epoch: 743 [30100/50000 (60%)]  \tLoss:   94.586983\trec:   68.419083\tkl:   26.167902\n",
      "Epoch: 743 [40100/50000 (80%)]  \tLoss:   94.357613\trec:   68.173965\tkl:   26.183651\n",
      "====> Epoch: 743 Average train loss: 92.7216\n",
      "====> Validation set loss: 95.6561\n",
      "====> Validation set kl: 25.8071\n",
      "Epoch: 744 [  100/50000 ( 0%)]  \tLoss:   92.014503\trec:   66.403679\tkl:   25.610817\n",
      "Epoch: 744 [10100/50000 (20%)]  \tLoss:   96.377480\trec:   69.992722\tkl:   26.384760\n",
      "Epoch: 744 [20100/50000 (40%)]  \tLoss:   95.713135\trec:   69.048134\tkl:   26.664999\n",
      "Epoch: 744 [30100/50000 (60%)]  \tLoss:   90.074196\trec:   65.448341\tkl:   24.625856\n",
      "Epoch: 744 [40100/50000 (80%)]  \tLoss:   92.521400\trec:   66.716919\tkl:   25.804478\n",
      "====> Epoch: 744 Average train loss: 92.5862\n",
      "====> Validation set loss: 95.8063\n",
      "====> Validation set kl: 25.8255\n",
      "Epoch: 745 [  100/50000 ( 0%)]  \tLoss:   96.703331\trec:   70.292374\tkl:   26.410954\n",
      "Epoch: 745 [10100/50000 (20%)]  \tLoss:   92.001427\trec:   66.854683\tkl:   25.146738\n",
      "Epoch: 745 [20100/50000 (40%)]  \tLoss:   93.227867\trec:   67.023766\tkl:   26.204109\n",
      "Epoch: 745 [30100/50000 (60%)]  \tLoss:   91.647964\trec:   66.452751\tkl:   25.195221\n",
      "Epoch: 745 [40100/50000 (80%)]  \tLoss:   86.884727\trec:   63.184971\tkl:   23.699759\n",
      "====> Epoch: 745 Average train loss: 92.8519\n",
      "====> Validation set loss: 97.0220\n",
      "====> Validation set kl: 25.7755\n",
      "Epoch: 746 [  100/50000 ( 0%)]  \tLoss:   91.672241\trec:   66.262970\tkl:   25.409279\n",
      "Epoch: 746 [10100/50000 (20%)]  \tLoss:   93.206108\trec:   67.224289\tkl:   25.981821\n",
      "Epoch: 746 [20100/50000 (40%)]  \tLoss:   93.202583\trec:   65.527863\tkl:   27.674728\n",
      "Epoch: 746 [30100/50000 (60%)]  \tLoss:   90.736954\trec:   64.521423\tkl:   26.215525\n",
      "Epoch: 746 [40100/50000 (80%)]  \tLoss:   89.645592\trec:   64.271019\tkl:   25.374573\n",
      "====> Epoch: 746 Average train loss: 93.3173\n",
      "====> Validation set loss: 96.6957\n",
      "====> Validation set kl: 25.6934\n",
      "Epoch: 747 [  100/50000 ( 0%)]  \tLoss:   91.536133\trec:   66.117783\tkl:   25.418346\n",
      "Epoch: 747 [10100/50000 (20%)]  \tLoss:   90.830391\trec:   65.655029\tkl:   25.175356\n",
      "Epoch: 747 [20100/50000 (40%)]  \tLoss:   91.806915\trec:   66.409332\tkl:   25.397585\n",
      "Epoch: 747 [30100/50000 (60%)]  \tLoss:   91.586678\trec:   66.175644\tkl:   25.411030\n",
      "Epoch: 747 [40100/50000 (80%)]  \tLoss:   90.453262\trec:   64.487877\tkl:   25.965384\n",
      "====> Epoch: 747 Average train loss: 93.0049\n",
      "====> Validation set loss: 95.9374\n",
      "====> Validation set kl: 25.9293\n",
      "Epoch: 748 [  100/50000 ( 0%)]  \tLoss:   97.030273\trec:   70.573708\tkl:   26.456572\n",
      "Epoch: 748 [10100/50000 (20%)]  \tLoss:   96.712799\trec:   71.117409\tkl:   25.595392\n",
      "Epoch: 748 [20100/50000 (40%)]  \tLoss:   94.539528\trec:   67.984612\tkl:   26.554926\n",
      "Epoch: 748 [30100/50000 (60%)]  \tLoss:   92.155655\trec:   66.343536\tkl:   25.812120\n",
      "Epoch: 748 [40100/50000 (80%)]  \tLoss:   94.309509\trec:   68.541512\tkl:   25.767992\n",
      "====> Epoch: 748 Average train loss: 93.7051\n",
      "====> Validation set loss: 96.3952\n",
      "====> Validation set kl: 26.1204\n",
      "Epoch: 749 [  100/50000 ( 0%)]  \tLoss:   91.381157\trec:   65.959251\tkl:   25.421911\n",
      "Epoch: 749 [10100/50000 (20%)]  \tLoss:   98.543716\trec:   71.768044\tkl:   26.775673\n",
      "Epoch: 749 [20100/50000 (40%)]  \tLoss:   96.055389\trec:   70.086075\tkl:   25.969309\n",
      "Epoch: 749 [30100/50000 (60%)]  \tLoss:   93.481392\trec:   67.124657\tkl:   26.356735\n",
      "Epoch: 749 [40100/50000 (80%)]  \tLoss:   94.770401\trec:   68.467514\tkl:   26.302885\n",
      "====> Epoch: 749 Average train loss: 93.0614\n",
      "====> Validation set loss: 96.3504\n",
      "====> Validation set kl: 26.0432\n",
      "Epoch: 750 [  100/50000 ( 0%)]  \tLoss:   91.421249\trec:   66.107018\tkl:   25.314226\n",
      "Epoch: 750 [10100/50000 (20%)]  \tLoss:   93.103500\trec:   67.461113\tkl:   25.642391\n",
      "Epoch: 750 [20100/50000 (40%)]  \tLoss:   90.535805\trec:   66.030289\tkl:   24.505524\n",
      "Epoch: 750 [30100/50000 (60%)]  \tLoss:   92.113701\trec:   66.541183\tkl:   25.572517\n",
      "Epoch: 750 [40100/50000 (80%)]  \tLoss:   93.757393\trec:   68.109077\tkl:   25.648315\n",
      "====> Epoch: 750 Average train loss: 93.3840\n",
      "====> Validation set loss: 96.8243\n",
      "====> Validation set kl: 25.8643\n",
      "Epoch: 751 [  100/50000 ( 0%)]  \tLoss:   90.914825\trec:   66.482277\tkl:   24.432547\n",
      "Epoch: 751 [10100/50000 (20%)]  \tLoss:   93.899086\trec:   68.403770\tkl:   25.495321\n",
      "Epoch: 751 [20100/50000 (40%)]  \tLoss:   91.629311\trec:   66.186378\tkl:   25.442934\n",
      "Epoch: 751 [30100/50000 (60%)]  \tLoss:   92.776817\trec:   67.354630\tkl:   25.422180\n",
      "Epoch: 751 [40100/50000 (80%)]  \tLoss:   90.546951\trec:   65.565765\tkl:   24.981184\n",
      "====> Epoch: 751 Average train loss: 93.0312\n",
      "====> Validation set loss: 95.9129\n",
      "====> Validation set kl: 25.7291\n",
      "Epoch: 752 [  100/50000 ( 0%)]  \tLoss:   90.733368\trec:   65.952087\tkl:   24.781279\n",
      "Epoch: 752 [10100/50000 (20%)]  \tLoss:   90.504982\trec:   64.894829\tkl:   25.610144\n",
      "Epoch: 752 [20100/50000 (40%)]  \tLoss:   92.330513\trec:   66.587997\tkl:   25.742516\n",
      "Epoch: 752 [30100/50000 (60%)]  \tLoss:   95.875854\trec:   69.640076\tkl:   26.235775\n",
      "Epoch: 752 [40100/50000 (80%)]  \tLoss:   92.978668\trec:   66.226685\tkl:   26.751987\n",
      "====> Epoch: 752 Average train loss: 93.0267\n",
      "====> Validation set loss: 96.2122\n",
      "====> Validation set kl: 26.1411\n",
      "Epoch: 753 [  100/50000 ( 0%)]  \tLoss:   92.143608\trec:   66.311943\tkl:   25.831673\n",
      "Epoch: 753 [10100/50000 (20%)]  \tLoss:   95.248863\trec:   68.956734\tkl:   26.292128\n",
      "Epoch: 753 [20100/50000 (40%)]  \tLoss:   92.703346\trec:   66.496521\tkl:   26.206823\n",
      "Epoch: 753 [30100/50000 (60%)]  \tLoss:   91.380066\trec:   65.884621\tkl:   25.495441\n",
      "Epoch: 753 [40100/50000 (80%)]  \tLoss:   94.763283\trec:   68.423195\tkl:   26.340084\n",
      "====> Epoch: 753 Average train loss: 92.8052\n",
      "====> Validation set loss: 96.1309\n",
      "====> Validation set kl: 25.5409\n",
      "Epoch: 754 [  100/50000 ( 0%)]  \tLoss:   94.093178\trec:   68.283424\tkl:   25.809761\n",
      "Epoch: 754 [10100/50000 (20%)]  \tLoss:   90.392525\trec:   65.122047\tkl:   25.270475\n",
      "Epoch: 754 [20100/50000 (40%)]  \tLoss:   94.970428\trec:   68.918884\tkl:   26.051542\n",
      "Epoch: 754 [30100/50000 (60%)]  \tLoss:   92.305923\trec:   66.469055\tkl:   25.836872\n",
      "Epoch: 754 [40100/50000 (80%)]  \tLoss:   94.072517\trec:   68.126755\tkl:   25.945757\n",
      "====> Epoch: 754 Average train loss: 92.7296\n",
      "====> Validation set loss: 95.8485\n",
      "====> Validation set kl: 25.6843\n",
      "Epoch: 755 [  100/50000 ( 0%)]  \tLoss:   92.862793\trec:   67.502716\tkl:   25.360073\n",
      "Epoch: 755 [10100/50000 (20%)]  \tLoss:   93.132195\trec:   66.652496\tkl:   26.479698\n",
      "Epoch: 755 [20100/50000 (40%)]  \tLoss:   93.252792\trec:   67.545898\tkl:   25.706884\n",
      "Epoch: 755 [30100/50000 (60%)]  \tLoss:   96.196930\trec:   69.094627\tkl:   27.102299\n",
      "Epoch: 755 [40100/50000 (80%)]  \tLoss:   93.539253\trec:   67.341301\tkl:   26.197954\n",
      "====> Epoch: 755 Average train loss: 93.0649\n",
      "====> Validation set loss: 95.7885\n",
      "====> Validation set kl: 25.7092\n",
      "Epoch: 756 [  100/50000 ( 0%)]  \tLoss:   89.408028\trec:   63.958118\tkl:   25.449903\n",
      "Epoch: 756 [10100/50000 (20%)]  \tLoss:   92.613640\trec:   66.560127\tkl:   26.053507\n",
      "Epoch: 756 [20100/50000 (40%)]  \tLoss:   89.495163\trec:   64.377098\tkl:   25.118069\n",
      "Epoch: 756 [30100/50000 (60%)]  \tLoss:   93.754936\trec:   68.417709\tkl:   25.337236\n",
      "Epoch: 756 [40100/50000 (80%)]  \tLoss:   92.505898\trec:   67.322235\tkl:   25.183659\n",
      "====> Epoch: 756 Average train loss: 92.7297\n",
      "====> Validation set loss: 96.2598\n",
      "====> Validation set kl: 25.8481\n",
      "Epoch: 757 [  100/50000 ( 0%)]  \tLoss:   88.962204\trec:   63.097832\tkl:   25.864370\n",
      "Epoch: 757 [10100/50000 (20%)]  \tLoss:   94.274704\trec:   69.141571\tkl:   25.133137\n",
      "Epoch: 757 [20100/50000 (40%)]  \tLoss:   97.009254\trec:   70.425270\tkl:   26.583981\n",
      "Epoch: 757 [30100/50000 (60%)]  \tLoss:   95.472244\trec:   68.845634\tkl:   26.626614\n",
      "Epoch: 757 [40100/50000 (80%)]  \tLoss:   91.225319\trec:   65.496803\tkl:   25.728519\n",
      "====> Epoch: 757 Average train loss: 92.6010\n",
      "====> Validation set loss: 95.7910\n",
      "====> Validation set kl: 25.7046\n",
      "Epoch: 758 [  100/50000 ( 0%)]  \tLoss:   88.137695\trec:   63.223492\tkl:   24.914202\n",
      "Epoch: 758 [10100/50000 (20%)]  \tLoss:   89.133904\trec:   63.717598\tkl:   25.416304\n",
      "Epoch: 758 [20100/50000 (40%)]  \tLoss:   96.831650\trec:   70.027412\tkl:   26.804234\n",
      "Epoch: 758 [30100/50000 (60%)]  \tLoss:   96.826393\trec:   70.446358\tkl:   26.380035\n",
      "Epoch: 758 [40100/50000 (80%)]  \tLoss:   92.094589\trec:   65.761360\tkl:   26.333221\n",
      "====> Epoch: 758 Average train loss: 92.7654\n",
      "====> Validation set loss: 96.0171\n",
      "====> Validation set kl: 25.5442\n",
      "Epoch: 759 [  100/50000 ( 0%)]  \tLoss:   95.801132\trec:   70.026016\tkl:   25.775120\n",
      "Epoch: 759 [10100/50000 (20%)]  \tLoss:   95.240311\trec:   69.880997\tkl:   25.359306\n",
      "Epoch: 759 [20100/50000 (40%)]  \tLoss:   95.550194\trec:   69.884422\tkl:   25.665766\n",
      "Epoch: 759 [30100/50000 (60%)]  \tLoss:   94.710739\trec:   68.845322\tkl:   25.865419\n",
      "Epoch: 759 [40100/50000 (80%)]  \tLoss:   94.478813\trec:   67.839996\tkl:   26.638821\n",
      "====> Epoch: 759 Average train loss: 92.9525\n",
      "====> Validation set loss: 95.7964\n",
      "====> Validation set kl: 25.5927\n",
      "Epoch: 760 [  100/50000 ( 0%)]  \tLoss:   93.810036\trec:   68.084572\tkl:   25.725468\n",
      "Epoch: 760 [10100/50000 (20%)]  \tLoss:   88.104126\trec:   63.140839\tkl:   24.963293\n",
      "Epoch: 760 [20100/50000 (40%)]  \tLoss:   91.147163\trec:   65.095695\tkl:   26.051474\n",
      "Epoch: 760 [30100/50000 (60%)]  \tLoss:   96.020546\trec:   68.982224\tkl:   27.038321\n",
      "Epoch: 760 [40100/50000 (80%)]  \tLoss:   89.953003\trec:   63.368755\tkl:   26.584255\n",
      "====> Epoch: 760 Average train loss: 92.6283\n",
      "====> Validation set loss: 95.8337\n",
      "====> Validation set kl: 25.7615\n",
      "Epoch: 761 [  100/50000 ( 0%)]  \tLoss:   93.472710\trec:   67.468277\tkl:   26.004442\n",
      "Epoch: 761 [10100/50000 (20%)]  \tLoss:   93.726501\trec:   68.153282\tkl:   25.573227\n",
      "Epoch: 761 [20100/50000 (40%)]  \tLoss:   91.899658\trec:   65.522530\tkl:   26.377127\n",
      "Epoch: 761 [30100/50000 (60%)]  \tLoss:   98.800522\trec:   71.775749\tkl:   27.024771\n",
      "Epoch: 761 [40100/50000 (80%)]  \tLoss:   90.748360\trec:   64.956055\tkl:   25.792297\n",
      "====> Epoch: 761 Average train loss: 92.6569\n",
      "====> Validation set loss: 95.8264\n",
      "====> Validation set kl: 26.1788\n",
      "Epoch: 762 [  100/50000 ( 0%)]  \tLoss:   92.273369\trec:   66.657005\tkl:   25.616362\n",
      "Epoch: 762 [10100/50000 (20%)]  \tLoss:   90.915352\trec:   65.865082\tkl:   25.050270\n",
      "Epoch: 762 [20100/50000 (40%)]  \tLoss:   89.928864\trec:   63.206455\tkl:   26.722414\n",
      "Epoch: 762 [30100/50000 (60%)]  \tLoss:   93.291611\trec:   66.151817\tkl:   27.139791\n",
      "Epoch: 762 [40100/50000 (80%)]  \tLoss:   89.114174\trec:   65.145203\tkl:   23.968979\n",
      "====> Epoch: 762 Average train loss: 92.5919\n",
      "====> Validation set loss: 96.0290\n",
      "====> Validation set kl: 25.4588\n",
      "Epoch: 763 [  100/50000 ( 0%)]  \tLoss:   92.826225\trec:   67.803787\tkl:   25.022444\n",
      "Epoch: 763 [10100/50000 (20%)]  \tLoss:   94.625053\trec:   68.236145\tkl:   26.388908\n",
      "Epoch: 763 [20100/50000 (40%)]  \tLoss:   96.142380\trec:   69.396629\tkl:   26.745747\n",
      "Epoch: 763 [30100/50000 (60%)]  \tLoss:   94.206841\trec:   68.135376\tkl:   26.071468\n",
      "Epoch: 763 [40100/50000 (80%)]  \tLoss:   95.147911\trec:   69.523094\tkl:   25.624809\n",
      "====> Epoch: 763 Average train loss: 92.8847\n",
      "====> Validation set loss: 96.4285\n",
      "====> Validation set kl: 25.6717\n",
      "Epoch: 764 [  100/50000 ( 0%)]  \tLoss:   92.452271\trec:   66.977196\tkl:   25.475075\n",
      "Epoch: 764 [10100/50000 (20%)]  \tLoss:   92.318474\trec:   66.640518\tkl:   25.677961\n",
      "Epoch: 764 [20100/50000 (40%)]  \tLoss:   96.017029\trec:   69.993523\tkl:   26.023506\n",
      "Epoch: 764 [30100/50000 (60%)]  \tLoss:   92.343048\trec:   66.835060\tkl:   25.507986\n",
      "Epoch: 764 [40100/50000 (80%)]  \tLoss:   87.467186\trec:   61.498215\tkl:   25.968969\n",
      "====> Epoch: 764 Average train loss: 92.5382\n",
      "====> Validation set loss: 96.1726\n",
      "====> Validation set kl: 24.9590\n",
      "Epoch: 765 [  100/50000 ( 0%)]  \tLoss:   93.308365\trec:   68.082916\tkl:   25.225447\n",
      "Epoch: 765 [10100/50000 (20%)]  \tLoss:   95.870995\trec:   69.341530\tkl:   26.529467\n",
      "Epoch: 765 [20100/50000 (40%)]  \tLoss:   91.422905\trec:   65.380714\tkl:   26.042192\n",
      "Epoch: 765 [30100/50000 (60%)]  \tLoss:   90.418915\trec:   65.207237\tkl:   25.211679\n",
      "Epoch: 765 [40100/50000 (80%)]  \tLoss:   93.226700\trec:   66.304596\tkl:   26.922094\n",
      "====> Epoch: 765 Average train loss: 93.0030\n",
      "====> Validation set loss: 96.1510\n",
      "====> Validation set kl: 25.8543\n",
      "Epoch: 766 [  100/50000 ( 0%)]  \tLoss:   90.548927\trec:   65.721581\tkl:   24.827343\n",
      "Epoch: 766 [10100/50000 (20%)]  \tLoss:   90.679298\trec:   64.869629\tkl:   25.809658\n",
      "Epoch: 766 [20100/50000 (40%)]  \tLoss:   93.044853\trec:   66.652496\tkl:   26.392351\n",
      "Epoch: 766 [30100/50000 (60%)]  \tLoss:   95.482872\trec:   69.373741\tkl:   26.109131\n",
      "Epoch: 766 [40100/50000 (80%)]  \tLoss:   93.156883\trec:   67.480156\tkl:   25.676731\n",
      "====> Epoch: 766 Average train loss: 93.4774\n",
      "====> Validation set loss: 96.2129\n",
      "====> Validation set kl: 26.2532\n",
      "Epoch: 767 [  100/50000 ( 0%)]  \tLoss:   94.960030\trec:   67.824539\tkl:   27.135485\n",
      "Epoch: 767 [10100/50000 (20%)]  \tLoss:   93.261284\trec:   67.719650\tkl:   25.541636\n",
      "Epoch: 767 [20100/50000 (40%)]  \tLoss:   90.112251\trec:   64.399422\tkl:   25.712833\n",
      "Epoch: 767 [30100/50000 (60%)]  \tLoss:   94.935509\trec:   68.907028\tkl:   26.028479\n",
      "Epoch: 767 [40100/50000 (80%)]  \tLoss:   91.038902\trec:   65.091934\tkl:   25.946970\n",
      "====> Epoch: 767 Average train loss: 92.9601\n",
      "====> Validation set loss: 95.8613\n",
      "====> Validation set kl: 25.8833\n",
      "Epoch: 768 [  100/50000 ( 0%)]  \tLoss:   94.128944\trec:   67.787849\tkl:   26.341091\n",
      "Epoch: 768 [10100/50000 (20%)]  \tLoss:   92.786446\trec:   68.000954\tkl:   24.785486\n",
      "Epoch: 768 [20100/50000 (40%)]  \tLoss:   90.943260\trec:   65.550949\tkl:   25.392309\n",
      "Epoch: 768 [30100/50000 (60%)]  \tLoss:   90.720840\trec:   65.458771\tkl:   25.262070\n",
      "Epoch: 768 [40100/50000 (80%)]  \tLoss:   93.345497\trec:   67.325798\tkl:   26.019693\n",
      "====> Epoch: 768 Average train loss: 93.2028\n",
      "====> Validation set loss: 95.9137\n",
      "====> Validation set kl: 25.6561\n",
      "Epoch: 769 [  100/50000 ( 0%)]  \tLoss:   88.324875\trec:   63.271397\tkl:   25.053474\n",
      "Epoch: 769 [10100/50000 (20%)]  \tLoss:   96.540245\trec:   69.836906\tkl:   26.703337\n",
      "Epoch: 769 [20100/50000 (40%)]  \tLoss:   95.681335\trec:   69.369835\tkl:   26.311506\n",
      "Epoch: 769 [30100/50000 (60%)]  \tLoss:   92.280731\trec:   65.616302\tkl:   26.664431\n",
      "Epoch: 769 [40100/50000 (80%)]  \tLoss:   91.314507\trec:   66.056839\tkl:   25.257675\n",
      "====> Epoch: 769 Average train loss: 93.1357\n",
      "====> Validation set loss: 95.8690\n",
      "====> Validation set kl: 26.1119\n",
      "Epoch: 770 [  100/50000 ( 0%)]  \tLoss:   95.517967\trec:   69.341270\tkl:   26.176701\n",
      "Epoch: 770 [10100/50000 (20%)]  \tLoss:   92.700333\trec:   67.158775\tkl:   25.541552\n",
      "Epoch: 770 [20100/50000 (40%)]  \tLoss:   91.473160\trec:   65.925446\tkl:   25.547722\n",
      "Epoch: 770 [30100/50000 (60%)]  \tLoss:   96.412804\trec:   69.174667\tkl:   27.238132\n",
      "Epoch: 770 [40100/50000 (80%)]  \tLoss:   89.465279\trec:   64.791267\tkl:   24.674013\n",
      "====> Epoch: 770 Average train loss: 92.5410\n",
      "====> Validation set loss: 95.8301\n",
      "====> Validation set kl: 25.6982\n",
      "Epoch: 771 [  100/50000 ( 0%)]  \tLoss:   90.049232\trec:   64.837364\tkl:   25.211874\n",
      "Epoch: 771 [10100/50000 (20%)]  \tLoss:   89.668999\trec:   65.112411\tkl:   24.556597\n",
      "Epoch: 771 [20100/50000 (40%)]  \tLoss:   93.675194\trec:   67.557724\tkl:   26.117468\n",
      "Epoch: 771 [30100/50000 (60%)]  \tLoss:   90.348770\trec:   65.606865\tkl:   24.741903\n",
      "Epoch: 771 [40100/50000 (80%)]  \tLoss:   93.172989\trec:   68.641037\tkl:   24.531948\n",
      "====> Epoch: 771 Average train loss: 92.6620\n",
      "====> Validation set loss: 95.7703\n",
      "====> Validation set kl: 25.7239\n",
      "Epoch: 772 [  100/50000 ( 0%)]  \tLoss:   91.130020\trec:   65.409958\tkl:   25.720060\n",
      "Epoch: 772 [10100/50000 (20%)]  \tLoss:   99.306763\trec:   71.287956\tkl:   28.018808\n",
      "Epoch: 772 [20100/50000 (40%)]  \tLoss:   92.653893\trec:   67.316261\tkl:   25.337631\n",
      "Epoch: 772 [30100/50000 (60%)]  \tLoss:   87.598923\trec:   63.914173\tkl:   23.684746\n",
      "Epoch: 772 [40100/50000 (80%)]  \tLoss:   90.598686\trec:   65.193092\tkl:   25.405603\n",
      "====> Epoch: 772 Average train loss: 92.9646\n",
      "====> Validation set loss: 96.6306\n",
      "====> Validation set kl: 24.7379\n",
      "Epoch: 773 [  100/50000 ( 0%)]  \tLoss:   93.501404\trec:   69.333252\tkl:   24.168148\n",
      "Epoch: 773 [10100/50000 (20%)]  \tLoss:   91.391457\trec:   65.840981\tkl:   25.550476\n",
      "Epoch: 773 [20100/50000 (40%)]  \tLoss:   91.477570\trec:   66.414574\tkl:   25.062992\n",
      "Epoch: 773 [30100/50000 (60%)]  \tLoss:   95.741463\trec:   68.970444\tkl:   26.771025\n",
      "Epoch: 773 [40100/50000 (80%)]  \tLoss:   91.980324\trec:   65.938164\tkl:   26.042160\n",
      "====> Epoch: 773 Average train loss: 93.0307\n",
      "====> Validation set loss: 96.1333\n",
      "====> Validation set kl: 26.1156\n",
      "Epoch: 774 [  100/50000 ( 0%)]  \tLoss:   89.182594\trec:   63.109688\tkl:   26.072912\n",
      "Epoch: 774 [10100/50000 (20%)]  \tLoss:   92.948700\trec:   67.730804\tkl:   25.217892\n",
      "Epoch: 774 [20100/50000 (40%)]  \tLoss:   91.802940\trec:   66.531387\tkl:   25.271547\n",
      "Epoch: 774 [30100/50000 (60%)]  \tLoss:   93.349434\trec:   66.725189\tkl:   26.624243\n",
      "Epoch: 774 [40100/50000 (80%)]  \tLoss:   90.491982\trec:   64.576942\tkl:   25.915035\n",
      "====> Epoch: 774 Average train loss: 92.8128\n",
      "====> Validation set loss: 96.0418\n",
      "====> Validation set kl: 25.8704\n",
      "Epoch: 775 [  100/50000 ( 0%)]  \tLoss:   90.880524\trec:   65.555573\tkl:   25.324947\n",
      "Epoch: 775 [10100/50000 (20%)]  \tLoss:   93.112335\trec:   66.250000\tkl:   26.862335\n",
      "Epoch: 775 [20100/50000 (40%)]  \tLoss:   94.554581\trec:   68.740288\tkl:   25.814289\n",
      "Epoch: 775 [30100/50000 (60%)]  \tLoss:   93.609901\trec:   67.398552\tkl:   26.211344\n",
      "Epoch: 775 [40100/50000 (80%)]  \tLoss:   95.196396\trec:   67.810814\tkl:   27.385578\n",
      "====> Epoch: 775 Average train loss: 92.9940\n",
      "====> Validation set loss: 95.5829\n",
      "====> Validation set kl: 25.7681\n",
      "Epoch: 776 [  100/50000 ( 0%)]  \tLoss:   92.189705\trec:   66.801720\tkl:   25.387993\n",
      "Epoch: 776 [10100/50000 (20%)]  \tLoss:   94.010971\trec:   67.274216\tkl:   26.736757\n",
      "Epoch: 776 [20100/50000 (40%)]  \tLoss:   91.288826\trec:   65.460564\tkl:   25.828264\n",
      "Epoch: 776 [30100/50000 (60%)]  \tLoss:   89.757607\trec:   64.258949\tkl:   25.498653\n",
      "Epoch: 776 [40100/50000 (80%)]  \tLoss:   95.293991\trec:   68.562012\tkl:   26.731979\n",
      "====> Epoch: 776 Average train loss: 92.6713\n",
      "====> Validation set loss: 96.0543\n",
      "====> Validation set kl: 26.0138\n",
      "Epoch: 777 [  100/50000 ( 0%)]  \tLoss:   94.202950\trec:   67.920830\tkl:   26.282124\n",
      "Epoch: 777 [10100/50000 (20%)]  \tLoss:   93.023964\trec:   67.296776\tkl:   25.727186\n",
      "Epoch: 777 [20100/50000 (40%)]  \tLoss:   95.670151\trec:   69.153290\tkl:   26.516865\n",
      "Epoch: 777 [30100/50000 (60%)]  \tLoss:   95.973053\trec:   69.388725\tkl:   26.584330\n",
      "Epoch: 777 [40100/50000 (80%)]  \tLoss:   97.284081\trec:   71.021080\tkl:   26.263002\n",
      "====> Epoch: 777 Average train loss: 92.7693\n",
      "====> Validation set loss: 95.9795\n",
      "====> Validation set kl: 26.0155\n",
      "Epoch: 778 [  100/50000 ( 0%)]  \tLoss:   93.840622\trec:   67.827995\tkl:   26.012627\n",
      "Epoch: 778 [10100/50000 (20%)]  \tLoss:   91.632675\trec:   65.437302\tkl:   26.195374\n",
      "Epoch: 778 [20100/50000 (40%)]  \tLoss:   93.248695\trec:   66.513863\tkl:   26.734837\n",
      "Epoch: 778 [30100/50000 (60%)]  \tLoss:   96.410622\trec:   68.463028\tkl:   27.947601\n",
      "Epoch: 778 [40100/50000 (80%)]  \tLoss:   93.887222\trec:   67.754845\tkl:   26.132380\n",
      "====> Epoch: 778 Average train loss: 92.9969\n",
      "====> Validation set loss: 95.8278\n",
      "====> Validation set kl: 25.9751\n",
      "Epoch: 779 [  100/50000 ( 0%)]  \tLoss:   89.882126\trec:   64.545380\tkl:   25.336750\n",
      "Epoch: 779 [10100/50000 (20%)]  \tLoss:   93.497139\trec:   67.061012\tkl:   26.436123\n",
      "Epoch: 779 [20100/50000 (40%)]  \tLoss:   90.384903\trec:   65.418808\tkl:   24.966099\n",
      "Epoch: 779 [30100/50000 (60%)]  \tLoss:   94.568810\trec:   67.502174\tkl:   27.066633\n",
      "Epoch: 779 [40100/50000 (80%)]  \tLoss:   90.403397\trec:   64.102539\tkl:   26.300859\n",
      "====> Epoch: 779 Average train loss: 92.6962\n",
      "====> Validation set loss: 95.7334\n",
      "====> Validation set kl: 25.8690\n",
      "Epoch: 780 [  100/50000 ( 0%)]  \tLoss:   90.927544\trec:   64.723785\tkl:   26.203756\n",
      "Epoch: 780 [10100/50000 (20%)]  \tLoss:   96.738144\trec:   71.575859\tkl:   25.162289\n",
      "Epoch: 780 [20100/50000 (40%)]  \tLoss:   86.071365\trec:   61.455822\tkl:   24.615547\n",
      "Epoch: 780 [30100/50000 (60%)]  \tLoss:   91.351402\trec:   65.493637\tkl:   25.857767\n",
      "Epoch: 780 [40100/50000 (80%)]  \tLoss:   94.074707\trec:   67.413261\tkl:   26.661440\n",
      "====> Epoch: 780 Average train loss: 92.8617\n",
      "====> Validation set loss: 96.0058\n",
      "====> Validation set kl: 25.5969\n",
      "Epoch: 781 [  100/50000 ( 0%)]  \tLoss:   92.376343\trec:   67.879356\tkl:   24.496988\n",
      "Epoch: 781 [10100/50000 (20%)]  \tLoss:   96.995338\trec:   70.576645\tkl:   26.418695\n",
      "Epoch: 781 [20100/50000 (40%)]  \tLoss:   88.166710\trec:   64.050270\tkl:   24.116432\n",
      "Epoch: 781 [30100/50000 (60%)]  \tLoss:   92.358017\trec:   66.250069\tkl:   26.107948\n",
      "Epoch: 781 [40100/50000 (80%)]  \tLoss:   96.959351\trec:   69.573364\tkl:   27.385994\n",
      "====> Epoch: 781 Average train loss: 92.6069\n",
      "====> Validation set loss: 96.3196\n",
      "====> Validation set kl: 25.2635\n",
      "Epoch: 782 [  100/50000 ( 0%)]  \tLoss:   88.583183\trec:   64.560745\tkl:   24.022436\n",
      "Epoch: 782 [10100/50000 (20%)]  \tLoss:   94.863678\trec:   69.185631\tkl:   25.678047\n",
      "Epoch: 782 [20100/50000 (40%)]  \tLoss:   91.968590\trec:   66.947601\tkl:   25.020992\n",
      "Epoch: 782 [30100/50000 (60%)]  \tLoss:   94.585724\trec:   68.419579\tkl:   26.166147\n",
      "Epoch: 782 [40100/50000 (80%)]  \tLoss:   97.060951\trec:   71.787102\tkl:   25.273857\n",
      "====> Epoch: 782 Average train loss: 93.1722\n",
      "====> Validation set loss: 96.4706\n",
      "====> Validation set kl: 25.6502\n",
      "Epoch: 783 [  100/50000 ( 0%)]  \tLoss:   93.752411\trec:   68.030106\tkl:   25.722307\n",
      "Epoch: 783 [10100/50000 (20%)]  \tLoss:   88.220985\trec:   62.441776\tkl:   25.779205\n",
      "Epoch: 783 [20100/50000 (40%)]  \tLoss:   95.111092\trec:   68.755959\tkl:   26.355137\n",
      "Epoch: 783 [30100/50000 (60%)]  \tLoss:   89.979431\trec:   63.502956\tkl:   26.476475\n",
      "Epoch: 783 [40100/50000 (80%)]  \tLoss:   92.565781\trec:   65.920158\tkl:   26.645620\n",
      "====> Epoch: 783 Average train loss: 92.6863\n",
      "====> Validation set loss: 95.7344\n",
      "====> Validation set kl: 25.5634\n",
      "Epoch: 784 [  100/50000 ( 0%)]  \tLoss:   88.848984\trec:   64.176460\tkl:   24.672516\n",
      "Epoch: 784 [10100/50000 (20%)]  \tLoss:   95.821182\trec:   70.055939\tkl:   25.765242\n",
      "Epoch: 784 [20100/50000 (40%)]  \tLoss:   89.648125\trec:   63.921562\tkl:   25.726564\n",
      "Epoch: 784 [30100/50000 (60%)]  \tLoss:   92.927246\trec:   67.393379\tkl:   25.533857\n",
      "Epoch: 784 [40100/50000 (80%)]  \tLoss:   92.430191\trec:   66.256439\tkl:   26.173750\n",
      "====> Epoch: 784 Average train loss: 93.1295\n",
      "====> Validation set loss: 97.3320\n",
      "====> Validation set kl: 26.0266\n",
      "Epoch: 785 [  100/50000 ( 0%)]  \tLoss:   93.681557\trec:   66.959206\tkl:   26.722347\n",
      "Epoch: 785 [10100/50000 (20%)]  \tLoss:   96.874619\trec:   69.439522\tkl:   27.435099\n",
      "Epoch: 785 [20100/50000 (40%)]  \tLoss:   90.277618\trec:   64.441376\tkl:   25.836237\n",
      "Epoch: 785 [30100/50000 (60%)]  \tLoss:   94.512245\trec:   67.946503\tkl:   26.565742\n",
      "Epoch: 785 [40100/50000 (80%)]  \tLoss:   95.822495\trec:   69.262352\tkl:   26.560143\n",
      "====> Epoch: 785 Average train loss: 93.9036\n",
      "====> Validation set loss: 96.0715\n",
      "====> Validation set kl: 25.9893\n",
      "Epoch: 786 [  100/50000 ( 0%)]  \tLoss:   90.937576\trec:   65.518501\tkl:   25.419069\n",
      "Epoch: 786 [10100/50000 (20%)]  \tLoss:   94.969879\trec:   68.391243\tkl:   26.578640\n",
      "Epoch: 786 [20100/50000 (40%)]  \tLoss:   92.249939\trec:   66.909882\tkl:   25.340054\n",
      "Epoch: 786 [30100/50000 (60%)]  \tLoss:   92.616501\trec:   66.117073\tkl:   26.499428\n",
      "Epoch: 786 [40100/50000 (80%)]  \tLoss:   88.949097\trec:   63.791821\tkl:   25.157284\n",
      "====> Epoch: 786 Average train loss: 93.0189\n",
      "====> Validation set loss: 95.9831\n",
      "====> Validation set kl: 25.8688\n",
      "Epoch: 787 [  100/50000 ( 0%)]  \tLoss:   93.736130\trec:   68.747063\tkl:   24.989069\n",
      "Epoch: 787 [10100/50000 (20%)]  \tLoss:   93.931786\trec:   67.061821\tkl:   26.869963\n",
      "Epoch: 787 [20100/50000 (40%)]  \tLoss:   93.664047\trec:   67.496941\tkl:   26.167110\n",
      "Epoch: 787 [30100/50000 (60%)]  \tLoss:   92.972984\trec:   67.295403\tkl:   25.677586\n",
      "Epoch: 787 [40100/50000 (80%)]  \tLoss:   93.982079\trec:   66.965889\tkl:   27.016193\n",
      "====> Epoch: 787 Average train loss: 92.6918\n",
      "====> Validation set loss: 96.6778\n",
      "====> Validation set kl: 25.5792\n",
      "Epoch: 788 [  100/50000 ( 0%)]  \tLoss:   89.230606\trec:   64.065208\tkl:   25.165390\n",
      "Epoch: 788 [10100/50000 (20%)]  \tLoss:   96.244537\trec:   68.920464\tkl:   27.324074\n",
      "Epoch: 788 [20100/50000 (40%)]  \tLoss:   94.923164\trec:   67.448273\tkl:   27.474890\n",
      "Epoch: 788 [30100/50000 (60%)]  \tLoss:   92.362831\trec:   66.686378\tkl:   25.676449\n",
      "Epoch: 788 [40100/50000 (80%)]  \tLoss:   95.152985\trec:   68.025047\tkl:   27.127934\n",
      "====> Epoch: 788 Average train loss: 93.4481\n",
      "====> Validation set loss: 96.2844\n",
      "====> Validation set kl: 26.0199\n",
      "Epoch: 789 [  100/50000 ( 0%)]  \tLoss:   94.784912\trec:   69.074089\tkl:   25.710819\n",
      "Epoch: 789 [10100/50000 (20%)]  \tLoss:   87.465561\trec:   62.305813\tkl:   25.159750\n",
      "Epoch: 789 [20100/50000 (40%)]  \tLoss:   86.533005\trec:   61.867840\tkl:   24.665161\n",
      "Epoch: 789 [30100/50000 (60%)]  \tLoss:   95.853485\trec:   69.708061\tkl:   26.145420\n",
      "Epoch: 789 [40100/50000 (80%)]  \tLoss:   91.208847\trec:   65.289444\tkl:   25.919409\n",
      "====> Epoch: 789 Average train loss: 92.8085\n",
      "====> Validation set loss: 95.8559\n",
      "====> Validation set kl: 26.0636\n",
      "Epoch: 790 [  100/50000 ( 0%)]  \tLoss:   91.839157\trec:   65.205887\tkl:   26.633276\n",
      "Epoch: 790 [10100/50000 (20%)]  \tLoss:   95.318062\trec:   68.945503\tkl:   26.372566\n",
      "Epoch: 790 [20100/50000 (40%)]  \tLoss:   93.591530\trec:   66.507912\tkl:   27.083620\n",
      "Epoch: 790 [30100/50000 (60%)]  \tLoss:   92.416794\trec:   66.189903\tkl:   26.226894\n",
      "Epoch: 790 [40100/50000 (80%)]  \tLoss:   93.399223\trec:   67.461014\tkl:   25.938211\n",
      "====> Epoch: 790 Average train loss: 92.5430\n",
      "====> Validation set loss: 95.7590\n",
      "====> Validation set kl: 25.7298\n",
      "Epoch: 791 [  100/50000 ( 0%)]  \tLoss:   90.775810\trec:   64.980034\tkl:   25.795771\n",
      "Epoch: 791 [10100/50000 (20%)]  \tLoss:   91.378563\trec:   65.888947\tkl:   25.489620\n",
      "Epoch: 791 [20100/50000 (40%)]  \tLoss:   92.445587\trec:   66.395020\tkl:   26.050564\n",
      "Epoch: 791 [30100/50000 (60%)]  \tLoss:   95.115204\trec:   68.664764\tkl:   26.450436\n",
      "Epoch: 791 [40100/50000 (80%)]  \tLoss:   93.677948\trec:   67.988052\tkl:   25.689899\n",
      "====> Epoch: 791 Average train loss: 92.6986\n",
      "====> Validation set loss: 95.7636\n",
      "====> Validation set kl: 25.6739\n",
      "Epoch: 792 [  100/50000 ( 0%)]  \tLoss:   89.214439\trec:   64.177017\tkl:   25.037428\n",
      "Epoch: 792 [10100/50000 (20%)]  \tLoss:   90.562134\trec:   65.076782\tkl:   25.485352\n",
      "Epoch: 792 [20100/50000 (40%)]  \tLoss:   92.491890\trec:   67.348419\tkl:   25.143471\n",
      "Epoch: 792 [30100/50000 (60%)]  \tLoss:   89.107994\trec:   63.428925\tkl:   25.679071\n",
      "Epoch: 792 [40100/50000 (80%)]  \tLoss:   87.647446\trec:   62.888397\tkl:   24.759054\n",
      "====> Epoch: 792 Average train loss: 92.7961\n",
      "====> Validation set loss: 96.4469\n",
      "====> Validation set kl: 26.0783\n",
      "Epoch: 793 [  100/50000 ( 0%)]  \tLoss:   93.032059\trec:   66.164032\tkl:   26.868029\n",
      "Epoch: 793 [10100/50000 (20%)]  \tLoss:   90.677399\trec:   64.428055\tkl:   26.249348\n",
      "Epoch: 793 [20100/50000 (40%)]  \tLoss:   93.692764\trec:   67.918892\tkl:   25.773867\n",
      "Epoch: 793 [30100/50000 (60%)]  \tLoss:   95.284492\trec:   68.934532\tkl:   26.349966\n",
      "Epoch: 793 [40100/50000 (80%)]  \tLoss:   88.075943\trec:   62.520790\tkl:   25.555159\n",
      "====> Epoch: 793 Average train loss: 92.6762\n",
      "====> Validation set loss: 95.7738\n",
      "====> Validation set kl: 25.8008\n",
      "Epoch: 794 [  100/50000 ( 0%)]  \tLoss:   95.211388\trec:   69.084595\tkl:   26.126789\n",
      "Epoch: 794 [10100/50000 (20%)]  \tLoss:   91.440681\trec:   65.594543\tkl:   25.846138\n",
      "Epoch: 794 [20100/50000 (40%)]  \tLoss:   93.227760\trec:   65.724556\tkl:   27.503204\n",
      "Epoch: 794 [30100/50000 (60%)]  \tLoss:   91.060997\trec:   65.571564\tkl:   25.489435\n",
      "Epoch: 794 [40100/50000 (80%)]  \tLoss:   90.369194\trec:   65.050217\tkl:   25.318977\n",
      "====> Epoch: 794 Average train loss: 92.5759\n",
      "====> Validation set loss: 95.7708\n",
      "====> Validation set kl: 25.9260\n",
      "Epoch: 795 [  100/50000 ( 0%)]  \tLoss:   93.767441\trec:   67.356613\tkl:   26.410835\n",
      "Epoch: 795 [10100/50000 (20%)]  \tLoss:   93.322617\trec:   68.624092\tkl:   24.698524\n",
      "Epoch: 795 [20100/50000 (40%)]  \tLoss:   95.743935\trec:   68.733818\tkl:   27.010109\n",
      "Epoch: 795 [30100/50000 (60%)]  \tLoss:   94.962128\trec:   68.923828\tkl:   26.038305\n",
      "Epoch: 795 [40100/50000 (80%)]  \tLoss:   92.607712\trec:   66.526115\tkl:   26.081596\n",
      "====> Epoch: 795 Average train loss: 92.4469\n",
      "====> Validation set loss: 95.8313\n",
      "====> Validation set kl: 25.9029\n",
      "Epoch: 796 [  100/50000 ( 0%)]  \tLoss:   94.309830\trec:   67.950241\tkl:   26.359589\n",
      "Epoch: 796 [10100/50000 (20%)]  \tLoss:   94.063927\trec:   67.812286\tkl:   26.251635\n",
      "Epoch: 796 [20100/50000 (40%)]  \tLoss:   91.669708\trec:   65.915558\tkl:   25.754150\n",
      "Epoch: 796 [30100/50000 (60%)]  \tLoss:   90.597328\trec:   64.789230\tkl:   25.808105\n",
      "Epoch: 796 [40100/50000 (80%)]  \tLoss:   91.459381\trec:   66.466576\tkl:   24.992805\n",
      "====> Epoch: 796 Average train loss: 92.5277\n",
      "====> Validation set loss: 96.2878\n",
      "====> Validation set kl: 26.0139\n",
      "Epoch: 797 [  100/50000 ( 0%)]  \tLoss:   95.017120\trec:   67.979454\tkl:   27.037663\n",
      "Epoch: 797 [10100/50000 (20%)]  \tLoss:   95.797394\trec:   69.556099\tkl:   26.241291\n",
      "Epoch: 797 [20100/50000 (40%)]  \tLoss:   89.888237\trec:   63.444271\tkl:   26.443964\n",
      "Epoch: 797 [30100/50000 (60%)]  \tLoss:   97.492661\trec:   70.239769\tkl:   27.252895\n",
      "Epoch: 797 [40100/50000 (80%)]  \tLoss:   87.883614\trec:   63.760330\tkl:   24.123278\n",
      "====> Epoch: 797 Average train loss: 92.8629\n",
      "====> Validation set loss: 95.6890\n",
      "====> Validation set kl: 25.7591\n",
      "Epoch: 798 [  100/50000 ( 0%)]  \tLoss:   94.041191\trec:   68.471275\tkl:   25.569912\n",
      "Epoch: 798 [10100/50000 (20%)]  \tLoss:   94.711700\trec:   67.970856\tkl:   26.740841\n",
      "Epoch: 798 [20100/50000 (40%)]  \tLoss:   91.693321\trec:   66.169380\tkl:   25.523935\n",
      "Epoch: 798 [30100/50000 (60%)]  \tLoss:   93.830444\trec:   68.295113\tkl:   25.535326\n",
      "Epoch: 798 [40100/50000 (80%)]  \tLoss:   93.326050\trec:   66.714523\tkl:   26.611530\n",
      "====> Epoch: 798 Average train loss: 92.7005\n",
      "====> Validation set loss: 96.2217\n",
      "====> Validation set kl: 25.9253\n",
      "Epoch: 799 [  100/50000 ( 0%)]  \tLoss:   95.322914\trec:   70.025909\tkl:   25.297009\n",
      "Epoch: 799 [10100/50000 (20%)]  \tLoss:   96.101326\trec:   69.547012\tkl:   26.554317\n",
      "Epoch: 799 [20100/50000 (40%)]  \tLoss:   93.026909\trec:   66.611794\tkl:   26.415117\n",
      "Epoch: 799 [30100/50000 (60%)]  \tLoss:   91.540962\trec:   65.229294\tkl:   26.311674\n",
      "Epoch: 799 [40100/50000 (80%)]  \tLoss:   88.487267\trec:   63.302361\tkl:   25.184902\n",
      "====> Epoch: 799 Average train loss: 92.6874\n",
      "====> Validation set loss: 96.0428\n",
      "====> Validation set kl: 25.7740\n",
      "Epoch: 800 [  100/50000 ( 0%)]  \tLoss:   92.159706\trec:   65.801323\tkl:   26.358374\n",
      "Epoch: 800 [10100/50000 (20%)]  \tLoss:   93.900993\trec:   69.068504\tkl:   24.832493\n",
      "Epoch: 800 [20100/50000 (40%)]  \tLoss:   94.234169\trec:   68.557312\tkl:   25.676855\n",
      "Epoch: 800 [30100/50000 (60%)]  \tLoss:   92.763885\trec:   67.522530\tkl:   25.241354\n",
      "Epoch: 800 [40100/50000 (80%)]  \tLoss:   88.944267\trec:   63.736389\tkl:   25.207876\n",
      "====> Epoch: 800 Average train loss: 92.7216\n",
      "====> Validation set loss: 95.7383\n",
      "====> Validation set kl: 25.5870\n",
      "Epoch: 801 [  100/50000 ( 0%)]  \tLoss:   90.289902\trec:   65.091927\tkl:   25.197975\n",
      "Epoch: 801 [10100/50000 (20%)]  \tLoss:   89.690567\trec:   64.891182\tkl:   24.799387\n",
      "Epoch: 801 [20100/50000 (40%)]  \tLoss:   90.049591\trec:   64.782837\tkl:   25.266743\n",
      "Epoch: 801 [30100/50000 (60%)]  \tLoss:   93.643700\trec:   66.716797\tkl:   26.926901\n",
      "Epoch: 801 [40100/50000 (80%)]  \tLoss:   90.405914\trec:   65.122398\tkl:   25.283520\n",
      "====> Epoch: 801 Average train loss: 92.7405\n",
      "====> Validation set loss: 96.9314\n",
      "====> Validation set kl: 26.4373\n",
      "Epoch: 802 [  100/50000 ( 0%)]  \tLoss:   94.389023\trec:   68.214043\tkl:   26.174984\n",
      "Epoch: 802 [10100/50000 (20%)]  \tLoss:   93.113571\trec:   67.855637\tkl:   25.257944\n",
      "Epoch: 802 [20100/50000 (40%)]  \tLoss:   95.411118\trec:   68.799713\tkl:   26.611408\n",
      "Epoch: 802 [30100/50000 (60%)]  \tLoss:   95.884453\trec:   70.855766\tkl:   25.028677\n",
      "Epoch: 802 [40100/50000 (80%)]  \tLoss:   92.729744\trec:   67.150726\tkl:   25.579023\n",
      "====> Epoch: 802 Average train loss: 92.8248\n",
      "====> Validation set loss: 96.1855\n",
      "====> Validation set kl: 26.4413\n",
      "Epoch: 803 [  100/50000 ( 0%)]  \tLoss:   90.460426\trec:   64.043213\tkl:   26.417215\n",
      "Epoch: 803 [10100/50000 (20%)]  \tLoss:   90.501915\trec:   65.022682\tkl:   25.479223\n",
      "Epoch: 803 [20100/50000 (40%)]  \tLoss:   90.113976\trec:   64.700798\tkl:   25.413176\n",
      "Epoch: 803 [30100/50000 (60%)]  \tLoss:   90.517548\trec:   64.558556\tkl:   25.958988\n",
      "Epoch: 803 [40100/50000 (80%)]  \tLoss:   91.875015\trec:   66.623459\tkl:   25.251564\n",
      "====> Epoch: 803 Average train loss: 92.7187\n",
      "====> Validation set loss: 96.8631\n",
      "====> Validation set kl: 26.3236\n",
      "Epoch: 804 [  100/50000 ( 0%)]  \tLoss:   94.968719\trec:   68.078316\tkl:   26.890398\n",
      "Epoch: 804 [10100/50000 (20%)]  \tLoss:   93.919487\trec:   67.646919\tkl:   26.272570\n",
      "Epoch: 804 [20100/50000 (40%)]  \tLoss:   89.422218\trec:   64.150055\tkl:   25.272158\n",
      "Epoch: 804 [30100/50000 (60%)]  \tLoss:   93.736191\trec:   67.299072\tkl:   26.437111\n",
      "Epoch: 804 [40100/50000 (80%)]  \tLoss:   91.097794\trec:   65.708549\tkl:   25.389240\n",
      "====> Epoch: 804 Average train loss: 92.9979\n",
      "====> Validation set loss: 96.1609\n",
      "====> Validation set kl: 25.8105\n",
      "Epoch: 805 [  100/50000 ( 0%)]  \tLoss:   92.858299\trec:   66.992569\tkl:   25.865730\n",
      "Epoch: 805 [10100/50000 (20%)]  \tLoss:   95.479965\trec:   69.727730\tkl:   25.752237\n",
      "Epoch: 805 [20100/50000 (40%)]  \tLoss:   93.804520\trec:   67.252296\tkl:   26.552225\n",
      "Epoch: 805 [30100/50000 (60%)]  \tLoss:   94.176186\trec:   68.038437\tkl:   26.137749\n",
      "Epoch: 805 [40100/50000 (80%)]  \tLoss:   92.013405\trec:   66.120552\tkl:   25.892849\n",
      "====> Epoch: 805 Average train loss: 93.1828\n",
      "====> Validation set loss: 96.5483\n",
      "====> Validation set kl: 25.7866\n",
      "Epoch: 806 [  100/50000 ( 0%)]  \tLoss:   92.222343\trec:   66.252609\tkl:   25.969738\n",
      "Epoch: 806 [10100/50000 (20%)]  \tLoss:   97.179253\trec:   70.435036\tkl:   26.744221\n",
      "Epoch: 806 [20100/50000 (40%)]  \tLoss:   94.130409\trec:   67.619469\tkl:   26.510939\n",
      "Epoch: 806 [30100/50000 (60%)]  \tLoss:   90.677246\trec:   64.698479\tkl:   25.978764\n",
      "Epoch: 806 [40100/50000 (80%)]  \tLoss:   91.703285\trec:   66.742058\tkl:   24.961233\n",
      "====> Epoch: 806 Average train loss: 92.7043\n",
      "====> Validation set loss: 96.3278\n",
      "====> Validation set kl: 25.6020\n",
      "Epoch: 807 [  100/50000 ( 0%)]  \tLoss:   92.928749\trec:   67.665382\tkl:   25.263359\n",
      "Epoch: 807 [10100/50000 (20%)]  \tLoss:   96.508820\trec:   70.441475\tkl:   26.067343\n",
      "Epoch: 807 [20100/50000 (40%)]  \tLoss:   97.566132\trec:   71.041756\tkl:   26.524380\n",
      "Epoch: 807 [30100/50000 (60%)]  \tLoss:   95.234413\trec:   68.016106\tkl:   27.218302\n",
      "Epoch: 807 [40100/50000 (80%)]  \tLoss:   90.083435\trec:   63.707226\tkl:   26.376205\n",
      "====> Epoch: 807 Average train loss: 93.3090\n",
      "====> Validation set loss: 96.4168\n",
      "====> Validation set kl: 26.1651\n",
      "Epoch: 808 [  100/50000 ( 0%)]  \tLoss:   92.363945\trec:   66.067574\tkl:   26.296371\n",
      "Epoch: 808 [10100/50000 (20%)]  \tLoss:   91.197060\trec:   66.002693\tkl:   25.194366\n",
      "Epoch: 808 [20100/50000 (40%)]  \tLoss:   90.020546\trec:   64.419075\tkl:   25.601471\n",
      "Epoch: 808 [30100/50000 (60%)]  \tLoss:   91.717644\trec:   66.239059\tkl:   25.478586\n",
      "Epoch: 808 [40100/50000 (80%)]  \tLoss:   93.781898\trec:   67.171814\tkl:   26.610085\n",
      "====> Epoch: 808 Average train loss: 92.7670\n",
      "====> Validation set loss: 96.1225\n",
      "====> Validation set kl: 25.5966\n",
      "Epoch: 809 [  100/50000 ( 0%)]  \tLoss:   91.008446\trec:   65.280205\tkl:   25.728245\n",
      "Epoch: 809 [10100/50000 (20%)]  \tLoss:   91.855812\trec:   65.257530\tkl:   26.598276\n",
      "Epoch: 809 [20100/50000 (40%)]  \tLoss:   88.835838\trec:   63.310722\tkl:   25.525116\n",
      "Epoch: 809 [30100/50000 (60%)]  \tLoss:   94.178848\trec:   68.346535\tkl:   25.832310\n",
      "Epoch: 809 [40100/50000 (80%)]  \tLoss:   90.706017\trec:   65.943062\tkl:   24.762951\n",
      "====> Epoch: 809 Average train loss: 92.7532\n",
      "====> Validation set loss: 95.9332\n",
      "====> Validation set kl: 25.4177\n",
      "Epoch: 810 [  100/50000 ( 0%)]  \tLoss:   91.648163\trec:   66.246025\tkl:   25.402134\n",
      "Epoch: 810 [10100/50000 (20%)]  \tLoss:   91.057198\trec:   65.804214\tkl:   25.252981\n",
      "Epoch: 810 [20100/50000 (40%)]  \tLoss:   93.150917\trec:   67.752525\tkl:   25.398394\n",
      "Epoch: 810 [30100/50000 (60%)]  \tLoss:   90.988953\trec:   64.824272\tkl:   26.164675\n",
      "Epoch: 810 [40100/50000 (80%)]  \tLoss:   91.710625\trec:   65.853233\tkl:   25.857389\n",
      "====> Epoch: 810 Average train loss: 92.7703\n",
      "====> Validation set loss: 95.7593\n",
      "====> Validation set kl: 25.7859\n",
      "Epoch: 811 [  100/50000 ( 0%)]  \tLoss:   87.034653\trec:   61.586899\tkl:   25.447758\n",
      "Epoch: 811 [10100/50000 (20%)]  \tLoss:   91.788551\trec:   67.073723\tkl:   24.714830\n",
      "Epoch: 811 [20100/50000 (40%)]  \tLoss:   93.033432\trec:   67.484062\tkl:   25.549364\n",
      "Epoch: 811 [30100/50000 (60%)]  \tLoss:   89.270294\trec:   64.303032\tkl:   24.967258\n",
      "Epoch: 811 [40100/50000 (80%)]  \tLoss:   93.447594\trec:   67.805962\tkl:   25.641640\n",
      "====> Epoch: 811 Average train loss: 92.8652\n",
      "====> Validation set loss: 95.8105\n",
      "====> Validation set kl: 25.4695\n",
      "Epoch: 812 [  100/50000 ( 0%)]  \tLoss:   94.646362\trec:   68.579521\tkl:   26.066843\n",
      "Epoch: 812 [10100/50000 (20%)]  \tLoss:   89.115952\trec:   63.785507\tkl:   25.330454\n",
      "Epoch: 812 [20100/50000 (40%)]  \tLoss:   92.722565\trec:   67.092300\tkl:   25.630270\n",
      "Epoch: 812 [30100/50000 (60%)]  \tLoss:   92.017342\trec:   65.645264\tkl:   26.372074\n",
      "Epoch: 812 [40100/50000 (80%)]  \tLoss:   92.907364\trec:   67.318771\tkl:   25.588591\n",
      "====> Epoch: 812 Average train loss: 93.2946\n",
      "====> Validation set loss: 96.1699\n",
      "====> Validation set kl: 25.9517\n",
      "Epoch: 813 [  100/50000 ( 0%)]  \tLoss:   92.765312\trec:   67.074936\tkl:   25.690376\n",
      "Epoch: 813 [10100/50000 (20%)]  \tLoss:   94.055565\trec:   68.295235\tkl:   25.760328\n",
      "Epoch: 813 [20100/50000 (40%)]  \tLoss:   95.807068\trec:   69.120224\tkl:   26.686842\n",
      "Epoch: 813 [30100/50000 (60%)]  \tLoss:   93.082802\trec:   66.774307\tkl:   26.308498\n",
      "Epoch: 813 [40100/50000 (80%)]  \tLoss:   92.356735\trec:   65.556709\tkl:   26.800034\n",
      "====> Epoch: 813 Average train loss: 92.9484\n",
      "====> Validation set loss: 95.9722\n",
      "====> Validation set kl: 26.0384\n",
      "Epoch: 814 [  100/50000 ( 0%)]  \tLoss:   89.791344\trec:   63.145531\tkl:   26.645817\n",
      "Epoch: 814 [10100/50000 (20%)]  \tLoss:   87.967758\trec:   62.987106\tkl:   24.980654\n",
      "Epoch: 814 [20100/50000 (40%)]  \tLoss:   92.958496\trec:   66.810616\tkl:   26.147873\n",
      "Epoch: 814 [30100/50000 (60%)]  \tLoss:   92.802773\trec:   67.235596\tkl:   25.567177\n",
      "Epoch: 814 [40100/50000 (80%)]  \tLoss:   91.140831\trec:   66.057655\tkl:   25.083176\n",
      "====> Epoch: 814 Average train loss: 92.9636\n",
      "====> Validation set loss: 95.9277\n",
      "====> Validation set kl: 25.6329\n",
      "Epoch: 815 [  100/50000 ( 0%)]  \tLoss:   94.486717\trec:   69.565254\tkl:   24.921467\n",
      "Epoch: 815 [10100/50000 (20%)]  \tLoss:   92.351753\trec:   67.569206\tkl:   24.782551\n",
      "Epoch: 815 [20100/50000 (40%)]  \tLoss:   95.356308\trec:   68.859230\tkl:   26.497074\n",
      "Epoch: 815 [30100/50000 (60%)]  \tLoss:   96.101746\trec:   70.254791\tkl:   25.846954\n",
      "Epoch: 815 [40100/50000 (80%)]  \tLoss:   94.358223\trec:   67.471291\tkl:   26.886932\n",
      "====> Epoch: 815 Average train loss: 92.7896\n",
      "====> Validation set loss: 96.0512\n",
      "====> Validation set kl: 25.8102\n",
      "Epoch: 816 [  100/50000 ( 0%)]  \tLoss:   96.507957\trec:   70.505768\tkl:   26.002188\n",
      "Epoch: 816 [10100/50000 (20%)]  \tLoss:   88.135178\trec:   63.810711\tkl:   24.324459\n",
      "Epoch: 816 [20100/50000 (40%)]  \tLoss:   92.302803\trec:   67.152611\tkl:   25.150190\n",
      "Epoch: 816 [30100/50000 (60%)]  \tLoss:   93.826363\trec:   66.595406\tkl:   27.230957\n",
      "Epoch: 816 [40100/50000 (80%)]  \tLoss:   93.580780\trec:   68.443130\tkl:   25.137651\n",
      "====> Epoch: 816 Average train loss: 92.8734\n",
      "====> Validation set loss: 96.3087\n",
      "====> Validation set kl: 25.8501\n",
      "Epoch: 817 [  100/50000 ( 0%)]  \tLoss:   94.042465\trec:   68.315598\tkl:   25.726871\n",
      "Epoch: 817 [10100/50000 (20%)]  \tLoss:   95.468712\trec:   69.562393\tkl:   25.906319\n",
      "Epoch: 817 [20100/50000 (40%)]  \tLoss:   93.531815\trec:   67.824776\tkl:   25.707039\n",
      "Epoch: 817 [30100/50000 (60%)]  \tLoss:   92.246346\trec:   66.034790\tkl:   26.211552\n",
      "Epoch: 817 [40100/50000 (80%)]  \tLoss:   93.434265\trec:   67.262360\tkl:   26.171904\n",
      "====> Epoch: 817 Average train loss: 92.9204\n",
      "====> Validation set loss: 96.3149\n",
      "====> Validation set kl: 25.8559\n",
      "Epoch: 818 [  100/50000 ( 0%)]  \tLoss:   90.070946\trec:   63.897869\tkl:   26.173075\n",
      "Epoch: 818 [10100/50000 (20%)]  \tLoss:   94.110634\trec:   68.386650\tkl:   25.723984\n",
      "Epoch: 818 [20100/50000 (40%)]  \tLoss:   95.087379\trec:   68.895172\tkl:   26.192209\n",
      "Epoch: 818 [30100/50000 (60%)]  \tLoss:   93.960320\trec:   67.134911\tkl:   26.825407\n",
      "Epoch: 818 [40100/50000 (80%)]  \tLoss:   91.201347\trec:   65.140137\tkl:   26.061214\n",
      "====> Epoch: 818 Average train loss: 93.1551\n",
      "====> Validation set loss: 96.0741\n",
      "====> Validation set kl: 25.9084\n",
      "Epoch: 819 [  100/50000 ( 0%)]  \tLoss:   98.445816\trec:   71.392990\tkl:   27.052826\n",
      "Epoch: 819 [10100/50000 (20%)]  \tLoss:   93.121346\trec:   66.767998\tkl:   26.353344\n",
      "Epoch: 819 [20100/50000 (40%)]  \tLoss:   90.717262\trec:   64.968468\tkl:   25.748793\n",
      "Epoch: 819 [30100/50000 (60%)]  \tLoss:   94.383949\trec:   68.217766\tkl:   26.166189\n",
      "Epoch: 819 [40100/50000 (80%)]  \tLoss:   94.956581\trec:   69.283615\tkl:   25.672964\n",
      "====> Epoch: 819 Average train loss: 92.7592\n",
      "====> Validation set loss: 96.1148\n",
      "====> Validation set kl: 25.6335\n",
      "Epoch: 820 [  100/50000 ( 0%)]  \tLoss:   96.436394\trec:   70.209885\tkl:   26.226505\n",
      "Epoch: 820 [10100/50000 (20%)]  \tLoss:   91.490089\trec:   66.192596\tkl:   25.297487\n",
      "Epoch: 820 [20100/50000 (40%)]  \tLoss:   94.024727\trec:   68.239136\tkl:   25.785591\n",
      "Epoch: 820 [30100/50000 (60%)]  \tLoss:   96.516701\trec:   69.981255\tkl:   26.535439\n",
      "Epoch: 820 [40100/50000 (80%)]  \tLoss:   96.004410\trec:   70.196579\tkl:   25.807831\n",
      "====> Epoch: 820 Average train loss: 93.2561\n",
      "====> Validation set loss: 96.0892\n",
      "====> Validation set kl: 25.8155\n",
      "Epoch: 821 [  100/50000 ( 0%)]  \tLoss:   92.558144\trec:   66.755554\tkl:   25.802589\n",
      "Epoch: 821 [10100/50000 (20%)]  \tLoss:   92.445778\trec:   66.048477\tkl:   26.397299\n",
      "Epoch: 821 [20100/50000 (40%)]  \tLoss:   91.694565\trec:   66.712585\tkl:   24.981979\n",
      "Epoch: 821 [30100/50000 (60%)]  \tLoss:   91.798409\trec:   66.056595\tkl:   25.741812\n",
      "Epoch: 821 [40100/50000 (80%)]  \tLoss:   95.348709\trec:   69.469444\tkl:   25.879265\n",
      "====> Epoch: 821 Average train loss: 93.0090\n",
      "====> Validation set loss: 96.4576\n",
      "====> Validation set kl: 25.4829\n",
      "Epoch: 822 [  100/50000 ( 0%)]  \tLoss:   85.444878\trec:   61.747906\tkl:   23.696972\n",
      "Epoch: 822 [10100/50000 (20%)]  \tLoss:   87.682877\trec:   62.492367\tkl:   25.190516\n",
      "Epoch: 822 [20100/50000 (40%)]  \tLoss:   94.085426\trec:   68.010658\tkl:   26.074776\n",
      "Epoch: 822 [30100/50000 (60%)]  \tLoss:   92.600830\trec:   67.272499\tkl:   25.328333\n",
      "Epoch: 822 [40100/50000 (80%)]  \tLoss:   89.761284\trec:   64.249763\tkl:   25.511522\n",
      "====> Epoch: 822 Average train loss: 92.9769\n",
      "====> Validation set loss: 96.4695\n",
      "====> Validation set kl: 25.1438\n",
      "Epoch: 823 [  100/50000 ( 0%)]  \tLoss:   92.335396\trec:   67.389969\tkl:   24.945427\n",
      "Epoch: 823 [10100/50000 (20%)]  \tLoss:   94.995338\trec:   69.236267\tkl:   25.759069\n",
      "Epoch: 823 [20100/50000 (40%)]  \tLoss:   94.258888\trec:   69.582703\tkl:   24.676178\n",
      "Epoch: 823 [30100/50000 (60%)]  \tLoss:   89.696815\trec:   63.616985\tkl:   26.079826\n",
      "Epoch: 823 [40100/50000 (80%)]  \tLoss:   93.424843\trec:   65.921638\tkl:   27.503202\n",
      "====> Epoch: 823 Average train loss: 93.1981\n",
      "====> Validation set loss: 96.2557\n",
      "====> Validation set kl: 25.9614\n",
      "Epoch: 824 [  100/50000 ( 0%)]  \tLoss:   92.076637\trec:   65.933601\tkl:   26.143032\n",
      "Epoch: 824 [10100/50000 (20%)]  \tLoss:   92.632751\trec:   66.393021\tkl:   26.239725\n",
      "Epoch: 824 [20100/50000 (40%)]  \tLoss:   93.567078\trec:   68.085953\tkl:   25.481131\n",
      "Epoch: 824 [30100/50000 (60%)]  \tLoss:   96.452286\trec:   68.941063\tkl:   27.511215\n",
      "Epoch: 824 [40100/50000 (80%)]  \tLoss:   88.791046\trec:   62.801464\tkl:   25.989580\n",
      "====> Epoch: 824 Average train loss: 92.7058\n",
      "====> Validation set loss: 95.7876\n",
      "====> Validation set kl: 25.7026\n",
      "Epoch: 825 [  100/50000 ( 0%)]  \tLoss:   91.139366\trec:   66.050232\tkl:   25.089130\n",
      "Epoch: 825 [10100/50000 (20%)]  \tLoss:   91.183456\trec:   65.458122\tkl:   25.725334\n",
      "Epoch: 825 [20100/50000 (40%)]  \tLoss:   93.472198\trec:   66.994064\tkl:   26.478134\n",
      "Epoch: 825 [30100/50000 (60%)]  \tLoss:   91.926773\trec:   66.459656\tkl:   25.467119\n",
      "Epoch: 825 [40100/50000 (80%)]  \tLoss:   92.275131\trec:   66.092598\tkl:   26.182539\n",
      "====> Epoch: 825 Average train loss: 92.6159\n",
      "====> Validation set loss: 95.9029\n",
      "====> Validation set kl: 25.9498\n",
      "Epoch: 826 [  100/50000 ( 0%)]  \tLoss:   90.043282\trec:   63.825596\tkl:   26.217690\n",
      "Epoch: 826 [10100/50000 (20%)]  \tLoss:   96.789139\trec:   70.651711\tkl:   26.137430\n",
      "Epoch: 826 [20100/50000 (40%)]  \tLoss:   94.373009\trec:   69.504074\tkl:   24.868933\n",
      "Epoch: 826 [30100/50000 (60%)]  \tLoss:   96.748505\trec:   70.628983\tkl:   26.119520\n",
      "Epoch: 826 [40100/50000 (80%)]  \tLoss:   93.106346\trec:   67.132744\tkl:   25.973600\n",
      "====> Epoch: 826 Average train loss: 93.0299\n",
      "====> Validation set loss: 95.9954\n",
      "====> Validation set kl: 25.7903\n",
      "Epoch: 827 [  100/50000 ( 0%)]  \tLoss:   91.772354\trec:   66.179466\tkl:   25.592882\n",
      "Epoch: 827 [10100/50000 (20%)]  \tLoss:   90.235504\trec:   64.322105\tkl:   25.913403\n",
      "Epoch: 827 [20100/50000 (40%)]  \tLoss:   88.532326\trec:   63.191273\tkl:   25.341055\n",
      "Epoch: 827 [30100/50000 (60%)]  \tLoss:   90.878326\trec:   65.148315\tkl:   25.730009\n",
      "Epoch: 827 [40100/50000 (80%)]  \tLoss:   92.178589\trec:   66.393478\tkl:   25.785118\n",
      "====> Epoch: 827 Average train loss: 92.6919\n",
      "====> Validation set loss: 96.3742\n",
      "====> Validation set kl: 26.0327\n",
      "Epoch: 828 [  100/50000 ( 0%)]  \tLoss:   95.110977\trec:   68.518913\tkl:   26.592064\n",
      "Epoch: 828 [10100/50000 (20%)]  \tLoss:   91.852020\trec:   66.379387\tkl:   25.472631\n",
      "Epoch: 828 [20100/50000 (40%)]  \tLoss:   87.400009\trec:   62.075264\tkl:   25.324747\n",
      "Epoch: 828 [30100/50000 (60%)]  \tLoss:   90.747246\trec:   64.691032\tkl:   26.056213\n",
      "Epoch: 828 [40100/50000 (80%)]  \tLoss:   94.598053\trec:   68.631821\tkl:   25.966238\n",
      "====> Epoch: 828 Average train loss: 92.7722\n",
      "====> Validation set loss: 97.4018\n",
      "====> Validation set kl: 26.0767\n",
      "Epoch: 829 [  100/50000 ( 0%)]  \tLoss:   96.054146\trec:   69.911552\tkl:   26.142591\n",
      "Epoch: 829 [10100/50000 (20%)]  \tLoss:   94.097008\trec:   66.299950\tkl:   27.797066\n",
      "Epoch: 829 [20100/50000 (40%)]  \tLoss:   89.907333\trec:   64.657791\tkl:   25.249540\n",
      "Epoch: 829 [30100/50000 (60%)]  \tLoss:   97.566635\trec:   70.586990\tkl:   26.979650\n",
      "Epoch: 829 [40100/50000 (80%)]  \tLoss:   92.543701\trec:   66.382599\tkl:   26.161098\n",
      "====> Epoch: 829 Average train loss: 93.2783\n",
      "====> Validation set loss: 95.9236\n",
      "====> Validation set kl: 25.6658\n",
      "Epoch: 830 [  100/50000 ( 0%)]  \tLoss:   93.564011\trec:   68.186790\tkl:   25.377218\n",
      "Epoch: 830 [10100/50000 (20%)]  \tLoss:   92.199318\trec:   67.061203\tkl:   25.138109\n",
      "Epoch: 830 [20100/50000 (40%)]  \tLoss:   90.344528\trec:   65.399323\tkl:   24.945200\n",
      "Epoch: 830 [30100/50000 (60%)]  \tLoss:   88.762970\trec:   62.991180\tkl:   25.771791\n",
      "Epoch: 830 [40100/50000 (80%)]  \tLoss:   92.100395\trec:   67.020386\tkl:   25.080013\n",
      "====> Epoch: 830 Average train loss: 92.6589\n",
      "====> Validation set loss: 96.4026\n",
      "====> Validation set kl: 25.8956\n",
      "Epoch: 831 [  100/50000 ( 0%)]  \tLoss:   89.256119\trec:   64.174065\tkl:   25.082058\n",
      "Epoch: 831 [10100/50000 (20%)]  \tLoss:   90.744713\trec:   63.805157\tkl:   26.939558\n",
      "Epoch: 831 [20100/50000 (40%)]  \tLoss:   91.060799\trec:   65.420143\tkl:   25.640654\n",
      "Epoch: 831 [30100/50000 (60%)]  \tLoss:   94.390541\trec:   69.000221\tkl:   25.390320\n",
      "Epoch: 831 [40100/50000 (80%)]  \tLoss:   92.255516\trec:   66.513802\tkl:   25.741716\n",
      "====> Epoch: 831 Average train loss: 92.8105\n",
      "====> Validation set loss: 96.9265\n",
      "====> Validation set kl: 26.1844\n",
      "Epoch: 832 [  100/50000 ( 0%)]  \tLoss:   96.452377\trec:   70.812889\tkl:   25.639494\n",
      "Epoch: 832 [10100/50000 (20%)]  \tLoss:   94.132149\trec:   68.589455\tkl:   25.542700\n",
      "Epoch: 832 [20100/50000 (40%)]  \tLoss:   90.542381\trec:   64.475624\tkl:   26.066755\n",
      "Epoch: 832 [30100/50000 (60%)]  \tLoss:   90.459122\trec:   64.675392\tkl:   25.783722\n",
      "Epoch: 832 [40100/50000 (80%)]  \tLoss:   93.044090\trec:   66.027145\tkl:   27.016941\n",
      "====> Epoch: 832 Average train loss: 92.9798\n",
      "====> Validation set loss: 96.1095\n",
      "====> Validation set kl: 26.0674\n",
      "Epoch: 833 [  100/50000 ( 0%)]  \tLoss:   92.207329\trec:   66.484627\tkl:   25.722708\n",
      "Epoch: 833 [10100/50000 (20%)]  \tLoss:   92.713806\trec:   66.930305\tkl:   25.783501\n",
      "Epoch: 833 [20100/50000 (40%)]  \tLoss:   92.228676\trec:   66.908180\tkl:   25.320496\n",
      "Epoch: 833 [30100/50000 (60%)]  \tLoss:   92.645195\trec:   66.542786\tkl:   26.102407\n",
      "Epoch: 833 [40100/50000 (80%)]  \tLoss:   94.397072\trec:   68.674698\tkl:   25.722368\n",
      "====> Epoch: 833 Average train loss: 92.6573\n",
      "====> Validation set loss: 95.8369\n",
      "====> Validation set kl: 25.5178\n",
      "Epoch: 834 [  100/50000 ( 0%)]  \tLoss:   90.366913\trec:   65.432304\tkl:   24.934610\n",
      "Epoch: 834 [10100/50000 (20%)]  \tLoss:   88.635262\trec:   62.988968\tkl:   25.646292\n",
      "Epoch: 834 [20100/50000 (40%)]  \tLoss:   93.971588\trec:   69.001419\tkl:   24.970173\n",
      "Epoch: 834 [30100/50000 (60%)]  \tLoss:   92.599304\trec:   66.572121\tkl:   26.027184\n",
      "Epoch: 834 [40100/50000 (80%)]  \tLoss:   92.119545\trec:   66.716774\tkl:   25.402767\n",
      "====> Epoch: 834 Average train loss: 92.5188\n",
      "====> Validation set loss: 95.8145\n",
      "====> Validation set kl: 25.6471\n",
      "Epoch: 835 [  100/50000 ( 0%)]  \tLoss:   89.560143\trec:   64.837860\tkl:   24.722286\n",
      "Epoch: 835 [10100/50000 (20%)]  \tLoss:   92.954399\trec:   66.557182\tkl:   26.397217\n",
      "Epoch: 835 [20100/50000 (40%)]  \tLoss:   91.589394\trec:   65.880180\tkl:   25.709204\n",
      "Epoch: 835 [30100/50000 (60%)]  \tLoss:   91.974823\trec:   67.176666\tkl:   24.798151\n",
      "Epoch: 835 [40100/50000 (80%)]  \tLoss:   94.559998\trec:   67.701988\tkl:   26.858017\n",
      "====> Epoch: 835 Average train loss: 92.5203\n",
      "====> Validation set loss: 95.6197\n",
      "====> Validation set kl: 26.0019\n",
      "Epoch: 836 [  100/50000 ( 0%)]  \tLoss:   91.940262\trec:   66.205299\tkl:   25.734961\n",
      "Epoch: 836 [10100/50000 (20%)]  \tLoss:   92.632423\trec:   67.042412\tkl:   25.590010\n",
      "Epoch: 836 [20100/50000 (40%)]  \tLoss:   91.369469\trec:   66.738518\tkl:   24.630955\n",
      "Epoch: 836 [30100/50000 (60%)]  \tLoss:   93.404228\trec:   67.671478\tkl:   25.732748\n",
      "Epoch: 836 [40100/50000 (80%)]  \tLoss:   91.682213\trec:   65.297821\tkl:   26.384394\n",
      "====> Epoch: 836 Average train loss: 92.3837\n",
      "====> Validation set loss: 95.5890\n",
      "====> Validation set kl: 25.6656\n",
      "Epoch: 837 [  100/50000 ( 0%)]  \tLoss:   96.226227\trec:   70.140762\tkl:   26.085463\n",
      "Epoch: 837 [10100/50000 (20%)]  \tLoss:   93.489662\trec:   66.933434\tkl:   26.556227\n",
      "Epoch: 837 [20100/50000 (40%)]  \tLoss:   94.851715\trec:   68.122696\tkl:   26.729021\n",
      "Epoch: 837 [30100/50000 (60%)]  \tLoss:   92.894653\trec:   67.207214\tkl:   25.687441\n",
      "Epoch: 837 [40100/50000 (80%)]  \tLoss:   94.254448\trec:   68.418869\tkl:   25.835585\n",
      "====> Epoch: 837 Average train loss: 92.4188\n",
      "====> Validation set loss: 95.5850\n",
      "====> Validation set kl: 25.6390\n",
      "Epoch: 838 [  100/50000 ( 0%)]  \tLoss:   86.582672\trec:   62.144566\tkl:   24.438114\n",
      "Epoch: 838 [10100/50000 (20%)]  \tLoss:   88.793648\trec:   62.709892\tkl:   26.083757\n",
      "Epoch: 838 [20100/50000 (40%)]  \tLoss:   93.917351\trec:   68.338318\tkl:   25.579031\n",
      "Epoch: 838 [30100/50000 (60%)]  \tLoss:   93.196762\trec:   67.079918\tkl:   26.116848\n",
      "Epoch: 838 [40100/50000 (80%)]  \tLoss:   85.179893\trec:   60.411648\tkl:   24.768242\n",
      "====> Epoch: 838 Average train loss: 92.2645\n",
      "====> Validation set loss: 95.8720\n",
      "====> Validation set kl: 25.8235\n",
      "Epoch: 839 [  100/50000 ( 0%)]  \tLoss:   88.932480\trec:   63.324322\tkl:   25.608162\n",
      "Epoch: 839 [10100/50000 (20%)]  \tLoss:   93.237289\trec:   67.235458\tkl:   26.001839\n",
      "Epoch: 839 [20100/50000 (40%)]  \tLoss:   95.740753\trec:   69.531387\tkl:   26.209364\n",
      "Epoch: 839 [30100/50000 (60%)]  \tLoss:   92.583260\trec:   66.644707\tkl:   25.938555\n",
      "Epoch: 839 [40100/50000 (80%)]  \tLoss:   91.507607\trec:   66.252342\tkl:   25.255266\n",
      "====> Epoch: 839 Average train loss: 92.2836\n",
      "====> Validation set loss: 96.1383\n",
      "====> Validation set kl: 25.0299\n",
      "Epoch: 840 [  100/50000 ( 0%)]  \tLoss:   92.938484\trec:   67.592087\tkl:   25.346395\n",
      "Epoch: 840 [10100/50000 (20%)]  \tLoss:   92.207260\trec:   66.325630\tkl:   25.881632\n",
      "Epoch: 840 [20100/50000 (40%)]  \tLoss:   93.471794\trec:   67.660316\tkl:   25.811483\n",
      "Epoch: 840 [30100/50000 (60%)]  \tLoss:   89.268921\trec:   63.465244\tkl:   25.803677\n",
      "Epoch: 840 [40100/50000 (80%)]  \tLoss:   92.218765\trec:   67.043228\tkl:   25.175537\n",
      "====> Epoch: 840 Average train loss: 92.3180\n",
      "====> Validation set loss: 95.4391\n",
      "====> Validation set kl: 25.8719\n",
      "Epoch: 841 [  100/50000 ( 0%)]  \tLoss:   90.051842\trec:   64.987907\tkl:   25.063940\n",
      "Epoch: 841 [10100/50000 (20%)]  \tLoss:   91.873268\trec:   67.067314\tkl:   24.805960\n",
      "Epoch: 841 [20100/50000 (40%)]  \tLoss:   94.173531\trec:   69.317375\tkl:   24.856152\n",
      "Epoch: 841 [30100/50000 (60%)]  \tLoss:   89.067711\trec:   63.884624\tkl:   25.183096\n",
      "Epoch: 841 [40100/50000 (80%)]  \tLoss:   91.356056\trec:   65.047890\tkl:   26.308167\n",
      "====> Epoch: 841 Average train loss: 92.9571\n",
      "====> Validation set loss: 96.0434\n",
      "====> Validation set kl: 25.6078\n",
      "Epoch: 842 [  100/50000 ( 0%)]  \tLoss:   91.913223\trec:   65.833534\tkl:   26.079691\n",
      "Epoch: 842 [10100/50000 (20%)]  \tLoss:   97.001549\trec:   70.932671\tkl:   26.068876\n",
      "Epoch: 842 [20100/50000 (40%)]  \tLoss:   90.825005\trec:   65.456833\tkl:   25.368176\n",
      "Epoch: 842 [30100/50000 (60%)]  \tLoss:   96.448387\trec:   69.103661\tkl:   27.344723\n",
      "Epoch: 842 [40100/50000 (80%)]  \tLoss:   94.049629\trec:   67.358917\tkl:   26.690710\n",
      "====> Epoch: 842 Average train loss: 92.4855\n",
      "====> Validation set loss: 95.8564\n",
      "====> Validation set kl: 25.9085\n",
      "Epoch: 843 [  100/50000 ( 0%)]  \tLoss:   88.575180\trec:   63.299622\tkl:   25.275564\n",
      "Epoch: 843 [10100/50000 (20%)]  \tLoss:   90.248726\trec:   63.957546\tkl:   26.291185\n",
      "Epoch: 843 [20100/50000 (40%)]  \tLoss:   90.351288\trec:   65.119522\tkl:   25.231762\n",
      "Epoch: 843 [30100/50000 (60%)]  \tLoss:   92.152168\trec:   66.297363\tkl:   25.854805\n",
      "Epoch: 843 [40100/50000 (80%)]  \tLoss:   92.653946\trec:   66.202858\tkl:   26.451086\n",
      "====> Epoch: 843 Average train loss: 92.3455\n",
      "====> Validation set loss: 95.8531\n",
      "====> Validation set kl: 25.7661\n",
      "Epoch: 844 [  100/50000 ( 0%)]  \tLoss:   91.309669\trec:   66.735878\tkl:   24.573792\n",
      "Epoch: 844 [10100/50000 (20%)]  \tLoss:   88.312393\trec:   62.303032\tkl:   26.009361\n",
      "Epoch: 844 [20100/50000 (40%)]  \tLoss:   93.909843\trec:   68.329903\tkl:   25.579947\n",
      "Epoch: 844 [30100/50000 (60%)]  \tLoss:   93.657104\trec:   67.487122\tkl:   26.169991\n",
      "Epoch: 844 [40100/50000 (80%)]  \tLoss:   93.377419\trec:   66.921867\tkl:   26.455553\n",
      "====> Epoch: 844 Average train loss: 92.2009\n",
      "====> Validation set loss: 95.4075\n",
      "====> Validation set kl: 25.6054\n",
      "Epoch: 845 [  100/50000 ( 0%)]  \tLoss:   90.588280\trec:   65.363884\tkl:   25.224400\n",
      "Epoch: 845 [10100/50000 (20%)]  \tLoss:   91.962379\trec:   65.997574\tkl:   25.964802\n",
      "Epoch: 845 [20100/50000 (40%)]  \tLoss:   91.657555\trec:   66.263664\tkl:   25.393890\n",
      "Epoch: 845 [30100/50000 (60%)]  \tLoss:   93.104881\trec:   67.356773\tkl:   25.748108\n",
      "Epoch: 845 [40100/50000 (80%)]  \tLoss:   93.505936\trec:   67.257225\tkl:   26.248714\n",
      "====> Epoch: 845 Average train loss: 92.4716\n",
      "====> Validation set loss: 96.2787\n",
      "====> Validation set kl: 26.8329\n",
      "Epoch: 846 [  100/50000 ( 0%)]  \tLoss:   92.234970\trec:   64.862381\tkl:   27.372587\n",
      "Epoch: 846 [10100/50000 (20%)]  \tLoss:   89.668472\trec:   64.184830\tkl:   25.483639\n",
      "Epoch: 846 [20100/50000 (40%)]  \tLoss:   93.539482\trec:   67.243904\tkl:   26.295576\n",
      "Epoch: 846 [30100/50000 (60%)]  \tLoss:   92.049782\trec:   66.297867\tkl:   25.751915\n",
      "Epoch: 846 [40100/50000 (80%)]  \tLoss:   86.520065\trec:   62.121685\tkl:   24.398380\n",
      "====> Epoch: 846 Average train loss: 92.2050\n",
      "====> Validation set loss: 95.5940\n",
      "====> Validation set kl: 25.8665\n",
      "Epoch: 847 [  100/50000 ( 0%)]  \tLoss:   91.928062\trec:   66.063560\tkl:   25.864506\n",
      "Epoch: 847 [10100/50000 (20%)]  \tLoss:   92.743752\trec:   66.463615\tkl:   26.280134\n",
      "Epoch: 847 [20100/50000 (40%)]  \tLoss:   94.412964\trec:   67.807838\tkl:   26.605129\n",
      "Epoch: 847 [30100/50000 (60%)]  \tLoss:   92.374092\trec:   66.238052\tkl:   26.136038\n",
      "Epoch: 847 [40100/50000 (80%)]  \tLoss:   93.220184\trec:   67.571342\tkl:   25.648838\n",
      "====> Epoch: 847 Average train loss: 92.3467\n",
      "====> Validation set loss: 95.4014\n",
      "====> Validation set kl: 25.5606\n",
      "Epoch: 848 [  100/50000 ( 0%)]  \tLoss:   93.950348\trec:   68.170334\tkl:   25.780014\n",
      "Epoch: 848 [10100/50000 (20%)]  \tLoss:   96.617783\trec:   69.634567\tkl:   26.983219\n",
      "Epoch: 848 [20100/50000 (40%)]  \tLoss:   88.618172\trec:   63.790409\tkl:   24.827761\n",
      "Epoch: 848 [30100/50000 (60%)]  \tLoss:   89.080055\trec:   63.822021\tkl:   25.258032\n",
      "Epoch: 848 [40100/50000 (80%)]  \tLoss:   92.590935\trec:   66.452095\tkl:   26.138838\n",
      "====> Epoch: 848 Average train loss: 92.0394\n",
      "====> Validation set loss: 96.4200\n",
      "====> Validation set kl: 26.2852\n",
      "Epoch: 849 [  100/50000 ( 0%)]  \tLoss:   92.284355\trec:   65.056641\tkl:   27.227718\n",
      "Epoch: 849 [10100/50000 (20%)]  \tLoss:   94.934235\trec:   68.704102\tkl:   26.230133\n",
      "Epoch: 849 [20100/50000 (40%)]  \tLoss:   92.397949\trec:   66.887421\tkl:   25.510532\n",
      "Epoch: 849 [30100/50000 (60%)]  \tLoss:   94.487335\trec:   67.845215\tkl:   26.642118\n",
      "Epoch: 849 [40100/50000 (80%)]  \tLoss:   92.464874\trec:   66.209175\tkl:   26.255697\n",
      "====> Epoch: 849 Average train loss: 92.0779\n",
      "====> Validation set loss: 96.8307\n",
      "====> Validation set kl: 26.5246\n",
      "Epoch: 850 [  100/50000 ( 0%)]  \tLoss:   94.433105\trec:   67.761192\tkl:   26.671909\n",
      "Epoch: 850 [10100/50000 (20%)]  \tLoss:   94.503700\trec:   68.147537\tkl:   26.356159\n",
      "Epoch: 850 [20100/50000 (40%)]  \tLoss:   88.801697\trec:   63.923725\tkl:   24.877977\n",
      "Epoch: 850 [30100/50000 (60%)]  \tLoss:   94.044098\trec:   68.096878\tkl:   25.947226\n",
      "Epoch: 850 [40100/50000 (80%)]  \tLoss:   97.019478\trec:   70.516052\tkl:   26.503428\n",
      "====> Epoch: 850 Average train loss: 92.2579\n",
      "====> Validation set loss: 95.3566\n",
      "====> Validation set kl: 25.8134\n",
      "Epoch: 851 [  100/50000 ( 0%)]  \tLoss:   89.039108\trec:   63.801514\tkl:   25.237595\n",
      "Epoch: 851 [10100/50000 (20%)]  \tLoss:   86.255127\trec:   60.726120\tkl:   25.529003\n",
      "Epoch: 851 [20100/50000 (40%)]  \tLoss:   91.243484\trec:   65.435249\tkl:   25.808235\n",
      "Epoch: 851 [30100/50000 (60%)]  \tLoss:   94.859283\trec:   69.302925\tkl:   25.556360\n",
      "Epoch: 851 [40100/50000 (80%)]  \tLoss:   96.049751\trec:   69.846321\tkl:   26.203430\n",
      "====> Epoch: 851 Average train loss: 92.0668\n",
      "====> Validation set loss: 95.4368\n",
      "====> Validation set kl: 25.2103\n",
      "Epoch: 852 [  100/50000 ( 0%)]  \tLoss:   88.255684\trec:   64.259789\tkl:   23.995888\n",
      "Epoch: 852 [10100/50000 (20%)]  \tLoss:   92.169411\trec:   67.014168\tkl:   25.155247\n",
      "Epoch: 852 [20100/50000 (40%)]  \tLoss:   92.201515\trec:   65.604561\tkl:   26.596951\n",
      "Epoch: 852 [30100/50000 (60%)]  \tLoss:   94.716393\trec:   67.809860\tkl:   26.906530\n",
      "Epoch: 852 [40100/50000 (80%)]  \tLoss:   90.393341\trec:   64.576607\tkl:   25.816729\n",
      "====> Epoch: 852 Average train loss: 92.2408\n",
      "====> Validation set loss: 95.3674\n",
      "====> Validation set kl: 25.7755\n",
      "Epoch: 853 [  100/50000 ( 0%)]  \tLoss:   87.846519\trec:   63.202000\tkl:   24.644524\n",
      "Epoch: 853 [10100/50000 (20%)]  \tLoss:   94.966141\trec:   67.701149\tkl:   27.264992\n",
      "Epoch: 853 [20100/50000 (40%)]  \tLoss:   94.189682\trec:   67.911842\tkl:   26.277842\n",
      "Epoch: 853 [30100/50000 (60%)]  \tLoss:   89.575928\trec:   63.944355\tkl:   25.631571\n",
      "Epoch: 853 [40100/50000 (80%)]  \tLoss:   87.498146\trec:   61.507782\tkl:   25.990358\n",
      "====> Epoch: 853 Average train loss: 92.4552\n",
      "====> Validation set loss: 95.6760\n",
      "====> Validation set kl: 25.9099\n",
      "Epoch: 854 [  100/50000 ( 0%)]  \tLoss:   95.765289\trec:   69.453140\tkl:   26.312153\n",
      "Epoch: 854 [10100/50000 (20%)]  \tLoss:   90.001076\trec:   63.936279\tkl:   26.064789\n",
      "Epoch: 854 [20100/50000 (40%)]  \tLoss:   94.635300\trec:   67.654335\tkl:   26.980965\n",
      "Epoch: 854 [30100/50000 (60%)]  \tLoss:   91.707497\trec:   66.046158\tkl:   25.661337\n",
      "Epoch: 854 [40100/50000 (80%)]  \tLoss:   89.082672\trec:   63.271736\tkl:   25.810932\n",
      "====> Epoch: 854 Average train loss: 92.1832\n",
      "====> Validation set loss: 95.5409\n",
      "====> Validation set kl: 26.0794\n",
      "Epoch: 855 [  100/50000 ( 0%)]  \tLoss:   94.898125\trec:   68.135773\tkl:   26.762356\n",
      "Epoch: 855 [10100/50000 (20%)]  \tLoss:   94.303093\trec:   67.852219\tkl:   26.450878\n",
      "Epoch: 855 [20100/50000 (40%)]  \tLoss:   95.475250\trec:   68.886070\tkl:   26.589180\n",
      "Epoch: 855 [30100/50000 (60%)]  \tLoss:   96.880585\trec:   69.428993\tkl:   27.451593\n",
      "Epoch: 855 [40100/50000 (80%)]  \tLoss:   93.547218\trec:   67.332565\tkl:   26.214647\n",
      "====> Epoch: 855 Average train loss: 92.2733\n",
      "====> Validation set loss: 95.8701\n",
      "====> Validation set kl: 26.0114\n",
      "Epoch: 856 [  100/50000 ( 0%)]  \tLoss:   89.940704\trec:   64.244179\tkl:   25.696518\n",
      "Epoch: 856 [10100/50000 (20%)]  \tLoss:   90.285500\trec:   64.995163\tkl:   25.290329\n",
      "Epoch: 856 [20100/50000 (40%)]  \tLoss:   91.906723\trec:   67.632393\tkl:   24.274338\n",
      "Epoch: 856 [30100/50000 (60%)]  \tLoss:   96.876564\trec:   70.219788\tkl:   26.656769\n",
      "Epoch: 856 [40100/50000 (80%)]  \tLoss:   94.946495\trec:   67.784660\tkl:   27.161833\n",
      "====> Epoch: 856 Average train loss: 92.8625\n",
      "====> Validation set loss: 96.8473\n",
      "====> Validation set kl: 26.2116\n",
      "Epoch: 857 [  100/50000 ( 0%)]  \tLoss:   90.083183\trec:   64.464874\tkl:   25.618315\n",
      "Epoch: 857 [10100/50000 (20%)]  \tLoss:   89.215050\trec:   63.130566\tkl:   26.084482\n",
      "Epoch: 857 [20100/50000 (40%)]  \tLoss:   98.267090\trec:   70.943130\tkl:   27.323950\n",
      "Epoch: 857 [30100/50000 (60%)]  \tLoss:   97.146111\trec:   69.968628\tkl:   27.177483\n",
      "Epoch: 857 [40100/50000 (80%)]  \tLoss:   90.430573\trec:   64.345001\tkl:   26.085579\n",
      "====> Epoch: 857 Average train loss: 93.1778\n",
      "====> Validation set loss: 96.3518\n",
      "====> Validation set kl: 26.2839\n",
      "Epoch: 858 [  100/50000 ( 0%)]  \tLoss:   91.778351\trec:   65.994751\tkl:   25.783596\n",
      "Epoch: 858 [10100/50000 (20%)]  \tLoss:   93.368095\trec:   67.646027\tkl:   25.722065\n",
      "Epoch: 858 [20100/50000 (40%)]  \tLoss:   95.461563\trec:   69.613426\tkl:   25.848131\n",
      "Epoch: 858 [30100/50000 (60%)]  \tLoss:   93.866089\trec:   67.782639\tkl:   26.083462\n",
      "Epoch: 858 [40100/50000 (80%)]  \tLoss:   95.319588\trec:   69.866280\tkl:   25.453304\n",
      "====> Epoch: 858 Average train loss: 92.5507\n",
      "====> Validation set loss: 95.9073\n",
      "====> Validation set kl: 26.0896\n",
      "Epoch: 859 [  100/50000 ( 0%)]  \tLoss:   95.394760\trec:   68.399506\tkl:   26.995262\n",
      "Epoch: 859 [10100/50000 (20%)]  \tLoss:   87.301384\trec:   62.463947\tkl:   24.837440\n",
      "Epoch: 859 [20100/50000 (40%)]  \tLoss:   87.272118\trec:   61.465271\tkl:   25.806845\n",
      "Epoch: 859 [30100/50000 (60%)]  \tLoss:   91.424629\trec:   66.019180\tkl:   25.405449\n",
      "Epoch: 859 [40100/50000 (80%)]  \tLoss:   89.137794\trec:   63.693001\tkl:   25.444792\n",
      "====> Epoch: 859 Average train loss: 92.4545\n",
      "====> Validation set loss: 95.9728\n",
      "====> Validation set kl: 26.0203\n",
      "Epoch: 860 [  100/50000 ( 0%)]  \tLoss:   90.176247\trec:   64.998238\tkl:   25.178009\n",
      "Epoch: 860 [10100/50000 (20%)]  \tLoss:   93.342323\trec:   66.692627\tkl:   26.649694\n",
      "Epoch: 860 [20100/50000 (40%)]  \tLoss:   91.516144\trec:   65.920883\tkl:   25.595259\n",
      "Epoch: 860 [30100/50000 (60%)]  \tLoss:   91.544991\trec:   65.473557\tkl:   26.071430\n",
      "Epoch: 860 [40100/50000 (80%)]  \tLoss:   91.478165\trec:   65.945190\tkl:   25.532969\n",
      "====> Epoch: 860 Average train loss: 92.6735\n",
      "====> Validation set loss: 96.2139\n",
      "====> Validation set kl: 25.4976\n",
      "Epoch: 861 [  100/50000 ( 0%)]  \tLoss:   93.402168\trec:   67.776939\tkl:   25.625231\n",
      "Epoch: 861 [10100/50000 (20%)]  \tLoss:   95.770546\trec:   69.494850\tkl:   26.275700\n",
      "Epoch: 861 [20100/50000 (40%)]  \tLoss:   90.593536\trec:   64.861130\tkl:   25.732405\n",
      "Epoch: 861 [30100/50000 (60%)]  \tLoss:   89.324638\trec:   63.314678\tkl:   26.009960\n",
      "Epoch: 861 [40100/50000 (80%)]  \tLoss:   93.551094\trec:   68.121178\tkl:   25.429916\n",
      "====> Epoch: 861 Average train loss: 92.5253\n",
      "====> Validation set loss: 95.7979\n",
      "====> Validation set kl: 25.8011\n",
      "Epoch: 862 [  100/50000 ( 0%)]  \tLoss:   89.744102\trec:   64.244003\tkl:   25.500099\n",
      "Epoch: 862 [10100/50000 (20%)]  \tLoss:   93.861893\trec:   67.696892\tkl:   26.164997\n",
      "Epoch: 862 [20100/50000 (40%)]  \tLoss:   89.300903\trec:   63.798809\tkl:   25.502102\n",
      "Epoch: 862 [30100/50000 (60%)]  \tLoss:   88.853218\trec:   64.186447\tkl:   24.666775\n",
      "Epoch: 862 [40100/50000 (80%)]  \tLoss:   93.399452\trec:   66.468117\tkl:   26.931335\n",
      "====> Epoch: 862 Average train loss: 92.3300\n",
      "====> Validation set loss: 95.7899\n",
      "====> Validation set kl: 25.5659\n",
      "Epoch: 863 [  100/50000 ( 0%)]  \tLoss:   91.067398\trec:   65.029755\tkl:   26.037645\n",
      "Epoch: 863 [10100/50000 (20%)]  \tLoss:   92.272537\trec:   67.455193\tkl:   24.817343\n",
      "Epoch: 863 [20100/50000 (40%)]  \tLoss:   95.392067\trec:   68.915848\tkl:   26.476219\n",
      "Epoch: 863 [30100/50000 (60%)]  \tLoss:   91.221687\trec:   65.278053\tkl:   25.943634\n",
      "Epoch: 863 [40100/50000 (80%)]  \tLoss:   91.940575\trec:   65.901588\tkl:   26.038980\n",
      "====> Epoch: 863 Average train loss: 92.5067\n",
      "====> Validation set loss: 95.5420\n",
      "====> Validation set kl: 25.8316\n",
      "Epoch: 864 [  100/50000 ( 0%)]  \tLoss:   91.374496\trec:   66.383873\tkl:   24.990622\n",
      "Epoch: 864 [10100/50000 (20%)]  \tLoss:   93.240356\trec:   66.947105\tkl:   26.293257\n",
      "Epoch: 864 [20100/50000 (40%)]  \tLoss:   93.341492\trec:   67.405556\tkl:   25.935934\n",
      "Epoch: 864 [30100/50000 (60%)]  \tLoss:   97.816505\trec:   71.791191\tkl:   26.025316\n",
      "Epoch: 864 [40100/50000 (80%)]  \tLoss:   93.479668\trec:   66.711266\tkl:   26.768402\n",
      "====> Epoch: 864 Average train loss: 92.7877\n",
      "====> Validation set loss: 95.8101\n",
      "====> Validation set kl: 25.8745\n",
      "Epoch: 865 [  100/50000 ( 0%)]  \tLoss:   92.987892\trec:   67.319344\tkl:   25.668541\n",
      "Epoch: 865 [10100/50000 (20%)]  \tLoss:   91.808708\trec:   66.117836\tkl:   25.690874\n",
      "Epoch: 865 [20100/50000 (40%)]  \tLoss:   92.352966\trec:   65.177811\tkl:   27.175154\n",
      "Epoch: 865 [30100/50000 (60%)]  \tLoss:   92.216980\trec:   66.372620\tkl:   25.844357\n",
      "Epoch: 865 [40100/50000 (80%)]  \tLoss:   92.784210\trec:   67.257179\tkl:   25.527029\n",
      "====> Epoch: 865 Average train loss: 92.6243\n",
      "====> Validation set loss: 96.3407\n",
      "====> Validation set kl: 26.1966\n",
      "Epoch: 866 [  100/50000 ( 0%)]  \tLoss:   95.071648\trec:   68.702560\tkl:   26.369093\n",
      "Epoch: 866 [10100/50000 (20%)]  \tLoss:   95.778160\trec:   69.336090\tkl:   26.442068\n",
      "Epoch: 866 [20100/50000 (40%)]  \tLoss:   92.623634\trec:   67.081551\tkl:   25.542082\n",
      "Epoch: 866 [30100/50000 (60%)]  \tLoss:   95.079414\trec:   68.210083\tkl:   26.869326\n",
      "Epoch: 866 [40100/50000 (80%)]  \tLoss:   93.834000\trec:   67.897629\tkl:   25.936375\n",
      "====> Epoch: 866 Average train loss: 92.4191\n",
      "====> Validation set loss: 95.7816\n",
      "====> Validation set kl: 25.8989\n",
      "Epoch: 867 [  100/50000 ( 0%)]  \tLoss:   90.684998\trec:   64.261360\tkl:   26.423637\n",
      "Epoch: 867 [10100/50000 (20%)]  \tLoss:   95.154106\trec:   68.155563\tkl:   26.998545\n",
      "Epoch: 867 [20100/50000 (40%)]  \tLoss:   92.052711\trec:   65.928810\tkl:   26.123911\n",
      "Epoch: 867 [30100/50000 (60%)]  \tLoss:   92.521851\trec:   66.922668\tkl:   25.599186\n",
      "Epoch: 867 [40100/50000 (80%)]  \tLoss:   93.039253\trec:   67.738693\tkl:   25.300556\n",
      "====> Epoch: 867 Average train loss: 92.6814\n",
      "====> Validation set loss: 95.9110\n",
      "====> Validation set kl: 26.1389\n",
      "Epoch: 868 [  100/50000 ( 0%)]  \tLoss:   93.186462\trec:   66.184723\tkl:   27.001743\n",
      "Epoch: 868 [10100/50000 (20%)]  \tLoss:   93.138550\trec:   66.957954\tkl:   26.180603\n",
      "Epoch: 868 [20100/50000 (40%)]  \tLoss:   84.451950\trec:   60.311321\tkl:   24.140629\n",
      "Epoch: 868 [30100/50000 (60%)]  \tLoss:   91.663185\trec:   66.493172\tkl:   25.170010\n",
      "Epoch: 868 [40100/50000 (80%)]  \tLoss:   90.398315\trec:   65.045494\tkl:   25.352829\n",
      "====> Epoch: 868 Average train loss: 92.4353\n",
      "====> Validation set loss: 95.4865\n",
      "====> Validation set kl: 25.4433\n",
      "Epoch: 869 [  100/50000 ( 0%)]  \tLoss:   92.316238\trec:   66.050270\tkl:   26.265968\n",
      "Epoch: 869 [10100/50000 (20%)]  \tLoss:   91.354767\trec:   66.049042\tkl:   25.305719\n",
      "Epoch: 869 [20100/50000 (40%)]  \tLoss:   88.585220\trec:   62.152294\tkl:   26.432926\n",
      "Epoch: 869 [30100/50000 (60%)]  \tLoss:   90.561523\trec:   65.671440\tkl:   24.890078\n",
      "Epoch: 869 [40100/50000 (80%)]  \tLoss:   94.001915\trec:   67.889801\tkl:   26.112118\n",
      "====> Epoch: 869 Average train loss: 92.4846\n",
      "====> Validation set loss: 95.6474\n",
      "====> Validation set kl: 25.9350\n",
      "Epoch: 870 [  100/50000 ( 0%)]  \tLoss:   94.566307\trec:   68.539352\tkl:   26.026957\n",
      "Epoch: 870 [10100/50000 (20%)]  \tLoss:   96.095314\trec:   70.058159\tkl:   26.037149\n",
      "Epoch: 870 [20100/50000 (40%)]  \tLoss:   93.282684\trec:   67.662704\tkl:   25.619982\n",
      "Epoch: 870 [30100/50000 (60%)]  \tLoss:   92.969879\trec:   67.456795\tkl:   25.513081\n",
      "Epoch: 870 [40100/50000 (80%)]  \tLoss:   94.833824\trec:   68.744850\tkl:   26.088972\n",
      "====> Epoch: 870 Average train loss: 92.4774\n",
      "====> Validation set loss: 95.7737\n",
      "====> Validation set kl: 25.6568\n",
      "Epoch: 871 [  100/50000 ( 0%)]  \tLoss:   90.139931\trec:   65.079590\tkl:   25.060341\n",
      "Epoch: 871 [10100/50000 (20%)]  \tLoss:   92.314903\trec:   66.833305\tkl:   25.481594\n",
      "Epoch: 871 [20100/50000 (40%)]  \tLoss:   90.985870\trec:   65.801331\tkl:   25.184534\n",
      "Epoch: 871 [30100/50000 (60%)]  \tLoss:   92.142578\trec:   65.437057\tkl:   26.705524\n",
      "Epoch: 871 [40100/50000 (80%)]  \tLoss:   89.652328\trec:   63.838181\tkl:   25.814148\n",
      "====> Epoch: 871 Average train loss: 92.2081\n",
      "====> Validation set loss: 95.4179\n",
      "====> Validation set kl: 25.6486\n",
      "Epoch: 872 [  100/50000 ( 0%)]  \tLoss:   95.728691\trec:   69.353340\tkl:   26.375343\n",
      "Epoch: 872 [10100/50000 (20%)]  \tLoss:   90.458496\trec:   64.676384\tkl:   25.782104\n",
      "Epoch: 872 [20100/50000 (40%)]  \tLoss:   91.863190\trec:   66.340332\tkl:   25.522863\n",
      "Epoch: 872 [30100/50000 (60%)]  \tLoss:   89.540405\trec:   64.279961\tkl:   25.260448\n",
      "Epoch: 872 [40100/50000 (80%)]  \tLoss:   92.522156\trec:   66.650673\tkl:   25.871481\n",
      "====> Epoch: 872 Average train loss: 92.2415\n",
      "====> Validation set loss: 95.6946\n",
      "====> Validation set kl: 25.7322\n",
      "Epoch: 873 [  100/50000 ( 0%)]  \tLoss:   91.349861\trec:   65.298996\tkl:   26.050865\n",
      "Epoch: 873 [10100/50000 (20%)]  \tLoss:   95.092995\trec:   68.893044\tkl:   26.199951\n",
      "Epoch: 873 [20100/50000 (40%)]  \tLoss:   91.799644\trec:   65.670891\tkl:   26.128763\n",
      "Epoch: 873 [30100/50000 (60%)]  \tLoss:   91.593170\trec:   67.665924\tkl:   23.927248\n",
      "Epoch: 873 [40100/50000 (80%)]  \tLoss:   91.803276\trec:   67.312920\tkl:   24.490358\n",
      "====> Epoch: 873 Average train loss: 92.2894\n",
      "====> Validation set loss: 95.2825\n",
      "====> Validation set kl: 25.7531\n",
      "Epoch: 874 [  100/50000 ( 0%)]  \tLoss:   92.464569\trec:   66.808601\tkl:   25.655962\n",
      "Epoch: 874 [10100/50000 (20%)]  \tLoss:   90.473778\trec:   64.959770\tkl:   25.514002\n",
      "Epoch: 874 [20100/50000 (40%)]  \tLoss:   92.810097\trec:   66.649307\tkl:   26.160786\n",
      "Epoch: 874 [30100/50000 (60%)]  \tLoss:   90.897041\trec:   66.102409\tkl:   24.794630\n",
      "Epoch: 874 [40100/50000 (80%)]  \tLoss:   88.912804\trec:   63.490845\tkl:   25.421961\n",
      "====> Epoch: 874 Average train loss: 92.3722\n",
      "====> Validation set loss: 95.7999\n",
      "====> Validation set kl: 25.5784\n",
      "Epoch: 875 [  100/50000 ( 0%)]  \tLoss:   93.115662\trec:   66.863853\tkl:   26.251808\n",
      "Epoch: 875 [10100/50000 (20%)]  \tLoss:   88.686630\trec:   62.662342\tkl:   26.024284\n",
      "Epoch: 875 [20100/50000 (40%)]  \tLoss:   96.166397\trec:   70.476974\tkl:   25.689421\n",
      "Epoch: 875 [30100/50000 (60%)]  \tLoss:   96.987358\trec:   70.417458\tkl:   26.569899\n",
      "Epoch: 875 [40100/50000 (80%)]  \tLoss:   91.406685\trec:   66.629372\tkl:   24.777311\n",
      "====> Epoch: 875 Average train loss: 92.7086\n",
      "====> Validation set loss: 95.7189\n",
      "====> Validation set kl: 25.8132\n",
      "Epoch: 876 [  100/50000 ( 0%)]  \tLoss:   90.584450\trec:   64.105202\tkl:   26.479242\n",
      "Epoch: 876 [10100/50000 (20%)]  \tLoss:   92.870522\trec:   66.753006\tkl:   26.117519\n",
      "Epoch: 876 [20100/50000 (40%)]  \tLoss:   89.857040\trec:   64.332588\tkl:   25.524450\n",
      "Epoch: 876 [30100/50000 (60%)]  \tLoss:   95.226578\trec:   69.013634\tkl:   26.212948\n",
      "Epoch: 876 [40100/50000 (80%)]  \tLoss:   97.618652\trec:   70.776527\tkl:   26.842121\n",
      "====> Epoch: 876 Average train loss: 92.2258\n",
      "====> Validation set loss: 96.1566\n",
      "====> Validation set kl: 25.9669\n",
      "Epoch: 877 [  100/50000 ( 0%)]  \tLoss:   89.966873\trec:   64.230461\tkl:   25.736406\n",
      "Epoch: 877 [10100/50000 (20%)]  \tLoss:   89.632080\trec:   64.770523\tkl:   24.861559\n",
      "Epoch: 877 [20100/50000 (40%)]  \tLoss:   94.749451\trec:   68.872780\tkl:   25.876665\n",
      "Epoch: 877 [30100/50000 (60%)]  \tLoss:   90.925034\trec:   64.822174\tkl:   26.102865\n",
      "Epoch: 877 [40100/50000 (80%)]  \tLoss:   88.600021\trec:   63.706776\tkl:   24.893240\n",
      "====> Epoch: 877 Average train loss: 92.3961\n",
      "====> Validation set loss: 96.0083\n",
      "====> Validation set kl: 25.9371\n",
      "Epoch: 878 [  100/50000 ( 0%)]  \tLoss:   94.413490\trec:   68.242485\tkl:   26.171015\n",
      "Epoch: 878 [10100/50000 (20%)]  \tLoss:   98.031128\trec:   71.198067\tkl:   26.833065\n",
      "Epoch: 878 [20100/50000 (40%)]  \tLoss:   88.780861\trec:   63.520947\tkl:   25.259909\n",
      "Epoch: 878 [30100/50000 (60%)]  \tLoss:   94.707985\trec:   68.416206\tkl:   26.291779\n",
      "Epoch: 878 [40100/50000 (80%)]  \tLoss:   90.111519\trec:   64.961418\tkl:   25.150097\n",
      "====> Epoch: 878 Average train loss: 92.2414\n",
      "====> Validation set loss: 95.7363\n",
      "====> Validation set kl: 25.8782\n",
      "Epoch: 879 [  100/50000 ( 0%)]  \tLoss:   95.654167\trec:   69.520622\tkl:   26.133547\n",
      "Epoch: 879 [10100/50000 (20%)]  \tLoss:   86.788986\trec:   61.829193\tkl:   24.959785\n",
      "Epoch: 879 [20100/50000 (40%)]  \tLoss:   94.290894\trec:   67.633636\tkl:   26.657255\n",
      "Epoch: 879 [30100/50000 (60%)]  \tLoss:   88.620613\trec:   63.088634\tkl:   25.531977\n",
      "Epoch: 879 [40100/50000 (80%)]  \tLoss:   86.628212\trec:   61.078445\tkl:   25.549763\n",
      "====> Epoch: 879 Average train loss: 92.2909\n",
      "====> Validation set loss: 95.8197\n",
      "====> Validation set kl: 25.8728\n",
      "Epoch: 880 [  100/50000 ( 0%)]  \tLoss:   92.617813\trec:   66.384705\tkl:   26.233110\n",
      "Epoch: 880 [10100/50000 (20%)]  \tLoss:   91.395027\trec:   66.472878\tkl:   24.922148\n",
      "Epoch: 880 [20100/50000 (40%)]  \tLoss:   95.713844\trec:   69.843956\tkl:   25.869896\n",
      "Epoch: 880 [30100/50000 (60%)]  \tLoss:   94.862579\trec:   68.907059\tkl:   25.955515\n",
      "Epoch: 880 [40100/50000 (80%)]  \tLoss:   93.345406\trec:   67.662201\tkl:   25.683212\n",
      "====> Epoch: 880 Average train loss: 92.0033\n",
      "====> Validation set loss: 95.7088\n",
      "====> Validation set kl: 25.6569\n",
      "Epoch: 881 [  100/50000 ( 0%)]  \tLoss:   90.885162\trec:   65.274101\tkl:   25.611061\n",
      "Epoch: 881 [10100/50000 (20%)]  \tLoss:   92.536850\trec:   66.501534\tkl:   26.035313\n",
      "Epoch: 881 [20100/50000 (40%)]  \tLoss:   89.365845\trec:   63.534870\tkl:   25.830978\n",
      "Epoch: 881 [30100/50000 (60%)]  \tLoss:   92.433739\trec:   66.564980\tkl:   25.868761\n",
      "Epoch: 881 [40100/50000 (80%)]  \tLoss:   88.930229\trec:   63.830135\tkl:   25.100100\n",
      "====> Epoch: 881 Average train loss: 92.0271\n",
      "====> Validation set loss: 95.5753\n",
      "====> Validation set kl: 25.8448\n",
      "Epoch: 882 [  100/50000 ( 0%)]  \tLoss:   91.051834\trec:   64.598946\tkl:   26.452883\n",
      "Epoch: 882 [10100/50000 (20%)]  \tLoss:   92.487274\trec:   66.899338\tkl:   25.587936\n",
      "Epoch: 882 [20100/50000 (40%)]  \tLoss:   91.856148\trec:   66.119102\tkl:   25.737045\n",
      "Epoch: 882 [30100/50000 (60%)]  \tLoss:   93.167305\trec:   67.188652\tkl:   25.978647\n",
      "Epoch: 882 [40100/50000 (80%)]  \tLoss:   90.486816\trec:   65.503922\tkl:   24.982895\n",
      "====> Epoch: 882 Average train loss: 91.9838\n",
      "====> Validation set loss: 95.5024\n",
      "====> Validation set kl: 26.0080\n",
      "Epoch: 883 [  100/50000 ( 0%)]  \tLoss:   91.442467\trec:   65.519547\tkl:   25.922926\n",
      "Epoch: 883 [10100/50000 (20%)]  \tLoss:   93.299721\trec:   67.670074\tkl:   25.629652\n",
      "Epoch: 883 [20100/50000 (40%)]  \tLoss:   88.941719\trec:   63.633369\tkl:   25.308346\n",
      "Epoch: 883 [30100/50000 (60%)]  \tLoss:   82.833107\trec:   58.685917\tkl:   24.147182\n",
      "Epoch: 883 [40100/50000 (80%)]  \tLoss:   96.145576\trec:   69.205513\tkl:   26.940060\n",
      "====> Epoch: 883 Average train loss: 92.0658\n",
      "====> Validation set loss: 95.7564\n",
      "====> Validation set kl: 26.0928\n",
      "Epoch: 884 [  100/50000 ( 0%)]  \tLoss:   94.287636\trec:   67.096481\tkl:   27.191153\n",
      "Epoch: 884 [10100/50000 (20%)]  \tLoss:   93.624695\trec:   67.165855\tkl:   26.458841\n",
      "Epoch: 884 [20100/50000 (40%)]  \tLoss:   92.907402\trec:   66.280502\tkl:   26.626904\n",
      "Epoch: 884 [30100/50000 (60%)]  \tLoss:   94.484825\trec:   67.153976\tkl:   27.330845\n",
      "Epoch: 884 [40100/50000 (80%)]  \tLoss:   91.202614\trec:   65.565010\tkl:   25.637604\n",
      "====> Epoch: 884 Average train loss: 92.4425\n",
      "====> Validation set loss: 95.7448\n",
      "====> Validation set kl: 26.0939\n",
      "Epoch: 885 [  100/50000 ( 0%)]  \tLoss:   91.165115\trec:   64.754425\tkl:   26.410698\n",
      "Epoch: 885 [10100/50000 (20%)]  \tLoss:   94.785576\trec:   68.632271\tkl:   26.153299\n",
      "Epoch: 885 [20100/50000 (40%)]  \tLoss:   93.898415\trec:   66.579422\tkl:   27.318996\n",
      "Epoch: 885 [30100/50000 (60%)]  \tLoss:   94.516075\trec:   68.814713\tkl:   25.701363\n",
      "Epoch: 885 [40100/50000 (80%)]  \tLoss:   93.765343\trec:   67.963211\tkl:   25.802132\n",
      "====> Epoch: 885 Average train loss: 92.1112\n",
      "====> Validation set loss: 95.5949\n",
      "====> Validation set kl: 26.4205\n",
      "Epoch: 886 [  100/50000 ( 0%)]  \tLoss:   90.216934\trec:   63.680210\tkl:   26.536718\n",
      "Epoch: 886 [10100/50000 (20%)]  \tLoss:   88.998894\trec:   63.471073\tkl:   25.527819\n",
      "Epoch: 886 [20100/50000 (40%)]  \tLoss:   93.515152\trec:   66.950836\tkl:   26.564310\n",
      "Epoch: 886 [30100/50000 (60%)]  \tLoss:   88.492325\trec:   63.574978\tkl:   24.917349\n",
      "Epoch: 886 [40100/50000 (80%)]  \tLoss:   95.151413\trec:   69.814392\tkl:   25.337019\n",
      "====> Epoch: 886 Average train loss: 91.9403\n",
      "====> Validation set loss: 95.4481\n",
      "====> Validation set kl: 25.6513\n",
      "Epoch: 887 [  100/50000 ( 0%)]  \tLoss:   88.462784\trec:   63.066200\tkl:   25.396580\n",
      "Epoch: 887 [10100/50000 (20%)]  \tLoss:   89.744415\trec:   63.816208\tkl:   25.928200\n",
      "Epoch: 887 [20100/50000 (40%)]  \tLoss:   90.241531\trec:   63.651672\tkl:   26.589855\n",
      "Epoch: 887 [30100/50000 (60%)]  \tLoss:   91.175323\trec:   64.307877\tkl:   26.867449\n",
      "Epoch: 887 [40100/50000 (80%)]  \tLoss:   88.456772\trec:   64.073715\tkl:   24.383066\n",
      "====> Epoch: 887 Average train loss: 92.2014\n",
      "====> Validation set loss: 95.4828\n",
      "====> Validation set kl: 25.8291\n",
      "Epoch: 888 [  100/50000 ( 0%)]  \tLoss:   91.890678\trec:   65.905495\tkl:   25.985180\n",
      "Epoch: 888 [10100/50000 (20%)]  \tLoss:   89.639374\trec:   64.705650\tkl:   24.933716\n",
      "Epoch: 888 [20100/50000 (40%)]  \tLoss:   91.479706\trec:   66.185036\tkl:   25.294678\n",
      "Epoch: 888 [30100/50000 (60%)]  \tLoss:   91.857071\trec:   65.647423\tkl:   26.209650\n",
      "Epoch: 888 [40100/50000 (80%)]  \tLoss:   91.119141\trec:   65.579262\tkl:   25.539883\n",
      "====> Epoch: 888 Average train loss: 92.1757\n",
      "====> Validation set loss: 95.8726\n",
      "====> Validation set kl: 26.1307\n",
      "Epoch: 889 [  100/50000 ( 0%)]  \tLoss:   91.561386\trec:   65.706436\tkl:   25.854948\n",
      "Epoch: 889 [10100/50000 (20%)]  \tLoss:   93.893913\trec:   67.614853\tkl:   26.279062\n",
      "Epoch: 889 [20100/50000 (40%)]  \tLoss:   90.774063\trec:   65.495857\tkl:   25.278206\n",
      "Epoch: 889 [30100/50000 (60%)]  \tLoss:   94.687225\trec:   68.356949\tkl:   26.330282\n",
      "Epoch: 889 [40100/50000 (80%)]  \tLoss:   92.828629\trec:   66.500916\tkl:   26.327711\n",
      "====> Epoch: 889 Average train loss: 92.1297\n",
      "====> Validation set loss: 95.6891\n",
      "====> Validation set kl: 25.7255\n",
      "Epoch: 890 [  100/50000 ( 0%)]  \tLoss:   91.218086\trec:   65.796181\tkl:   25.421904\n",
      "Epoch: 890 [10100/50000 (20%)]  \tLoss:   93.783409\trec:   67.043068\tkl:   26.740341\n",
      "Epoch: 890 [20100/50000 (40%)]  \tLoss:   91.686867\trec:   65.014076\tkl:   26.672785\n",
      "Epoch: 890 [30100/50000 (60%)]  \tLoss:   91.072823\trec:   65.015701\tkl:   26.057119\n",
      "Epoch: 890 [40100/50000 (80%)]  \tLoss:   95.722664\trec:   69.436272\tkl:   26.286394\n",
      "====> Epoch: 890 Average train loss: 92.3209\n",
      "====> Validation set loss: 95.5396\n",
      "====> Validation set kl: 25.9213\n",
      "Epoch: 891 [  100/50000 ( 0%)]  \tLoss:   94.700356\trec:   68.775978\tkl:   25.924385\n",
      "Epoch: 891 [10100/50000 (20%)]  \tLoss:   91.957802\trec:   66.253151\tkl:   25.704653\n",
      "Epoch: 891 [20100/50000 (40%)]  \tLoss:   93.268875\trec:   66.530441\tkl:   26.738438\n",
      "Epoch: 891 [30100/50000 (60%)]  \tLoss:   91.036308\trec:   64.651627\tkl:   26.384680\n",
      "Epoch: 891 [40100/50000 (80%)]  \tLoss:   94.426208\trec:   67.596016\tkl:   26.830191\n",
      "====> Epoch: 891 Average train loss: 92.5261\n",
      "====> Validation set loss: 95.7211\n",
      "====> Validation set kl: 25.7310\n",
      "Epoch: 892 [  100/50000 ( 0%)]  \tLoss:   89.772087\trec:   64.162193\tkl:   25.609901\n",
      "Epoch: 892 [10100/50000 (20%)]  \tLoss:   92.448845\trec:   67.478409\tkl:   24.970438\n",
      "Epoch: 892 [20100/50000 (40%)]  \tLoss:   88.965408\trec:   64.203201\tkl:   24.762201\n",
      "Epoch: 892 [30100/50000 (60%)]  \tLoss:   92.119438\trec:   66.239838\tkl:   25.879601\n",
      "Epoch: 892 [40100/50000 (80%)]  \tLoss:   94.151382\trec:   67.858261\tkl:   26.293121\n",
      "====> Epoch: 892 Average train loss: 92.0599\n",
      "====> Validation set loss: 96.0549\n",
      "====> Validation set kl: 25.3401\n",
      "Epoch: 893 [  100/50000 ( 0%)]  \tLoss:   91.797340\trec:   66.048843\tkl:   25.748495\n",
      "Epoch: 893 [10100/50000 (20%)]  \tLoss:   86.996246\trec:   62.450012\tkl:   24.546240\n",
      "Epoch: 893 [20100/50000 (40%)]  \tLoss:   93.444588\trec:   67.349655\tkl:   26.094927\n",
      "Epoch: 893 [30100/50000 (60%)]  \tLoss:   91.313065\trec:   66.016548\tkl:   25.296509\n",
      "Epoch: 893 [40100/50000 (80%)]  \tLoss:   90.817169\trec:   66.078545\tkl:   24.738617\n",
      "====> Epoch: 893 Average train loss: 91.9745\n",
      "====> Validation set loss: 95.6497\n",
      "====> Validation set kl: 25.6695\n",
      "Epoch: 894 [  100/50000 ( 0%)]  \tLoss:   91.151367\trec:   65.009560\tkl:   26.141804\n",
      "Epoch: 894 [10100/50000 (20%)]  \tLoss:   89.456596\trec:   65.021866\tkl:   24.434738\n",
      "Epoch: 894 [20100/50000 (40%)]  \tLoss:   92.764099\trec:   66.021851\tkl:   26.742249\n",
      "Epoch: 894 [30100/50000 (60%)]  \tLoss:   93.477409\trec:   67.054527\tkl:   26.422880\n",
      "Epoch: 894 [40100/50000 (80%)]  \tLoss:   97.457878\trec:   72.444664\tkl:   25.013212\n",
      "====> Epoch: 894 Average train loss: 91.8742\n",
      "====> Validation set loss: 95.6807\n",
      "====> Validation set kl: 25.8729\n",
      "Epoch: 895 [  100/50000 ( 0%)]  \tLoss:   91.929176\trec:   65.687325\tkl:   26.241852\n",
      "Epoch: 895 [10100/50000 (20%)]  \tLoss:   89.681618\trec:   65.361526\tkl:   24.320097\n",
      "Epoch: 895 [20100/50000 (40%)]  \tLoss:   90.892235\trec:   66.230949\tkl:   24.661283\n",
      "Epoch: 895 [30100/50000 (60%)]  \tLoss:   90.498077\trec:   64.742065\tkl:   25.756008\n",
      "Epoch: 895 [40100/50000 (80%)]  \tLoss:   90.291550\trec:   64.758957\tkl:   25.532600\n",
      "====> Epoch: 895 Average train loss: 92.0918\n",
      "====> Validation set loss: 95.7681\n",
      "====> Validation set kl: 25.6234\n",
      "Epoch: 896 [  100/50000 ( 0%)]  \tLoss:   93.771149\trec:   67.644554\tkl:   26.126600\n",
      "Epoch: 896 [10100/50000 (20%)]  \tLoss:   87.844116\trec:   63.156693\tkl:   24.687429\n",
      "Epoch: 896 [20100/50000 (40%)]  \tLoss:   91.606453\trec:   66.347839\tkl:   25.258610\n",
      "Epoch: 896 [30100/50000 (60%)]  \tLoss:   91.129669\trec:   66.080513\tkl:   25.049145\n",
      "Epoch: 896 [40100/50000 (80%)]  \tLoss:   95.117722\trec:   69.334503\tkl:   25.783215\n",
      "====> Epoch: 896 Average train loss: 92.3577\n",
      "====> Validation set loss: 95.8131\n",
      "====> Validation set kl: 25.8224\n",
      "Epoch: 897 [  100/50000 ( 0%)]  \tLoss:   91.874619\trec:   65.642517\tkl:   26.232100\n",
      "Epoch: 897 [10100/50000 (20%)]  \tLoss:   95.025337\trec:   67.757782\tkl:   27.267561\n",
      "Epoch: 897 [20100/50000 (40%)]  \tLoss:   93.707260\trec:   67.298683\tkl:   26.408588\n",
      "Epoch: 897 [30100/50000 (60%)]  \tLoss:   94.435890\trec:   68.004440\tkl:   26.431450\n",
      "Epoch: 897 [40100/50000 (80%)]  \tLoss:   94.009735\trec:   68.112556\tkl:   25.897179\n",
      "====> Epoch: 897 Average train loss: 92.4170\n",
      "====> Validation set loss: 95.6947\n",
      "====> Validation set kl: 25.7627\n",
      "Epoch: 898 [  100/50000 ( 0%)]  \tLoss:   89.260170\trec:   64.069763\tkl:   25.190405\n",
      "Epoch: 898 [10100/50000 (20%)]  \tLoss:   93.896889\trec:   66.934189\tkl:   26.962700\n",
      "Epoch: 898 [20100/50000 (40%)]  \tLoss:   90.198715\trec:   64.958252\tkl:   25.240461\n",
      "Epoch: 898 [30100/50000 (60%)]  \tLoss:   89.627380\trec:   63.538555\tkl:   26.088825\n",
      "Epoch: 898 [40100/50000 (80%)]  \tLoss:   91.694962\trec:   65.375328\tkl:   26.319628\n",
      "====> Epoch: 898 Average train loss: 92.1988\n",
      "====> Validation set loss: 95.5530\n",
      "====> Validation set kl: 25.8157\n",
      "Epoch: 899 [  100/50000 ( 0%)]  \tLoss:   94.379936\trec:   67.327545\tkl:   27.052389\n",
      "Epoch: 899 [10100/50000 (20%)]  \tLoss:   91.948990\trec:   65.636795\tkl:   26.312197\n",
      "Epoch: 899 [20100/50000 (40%)]  \tLoss:   91.254311\trec:   65.712120\tkl:   25.542196\n",
      "Epoch: 899 [30100/50000 (60%)]  \tLoss:   96.977554\trec:   70.120918\tkl:   26.856646\n",
      "Epoch: 899 [40100/50000 (80%)]  \tLoss:   96.535797\trec:   69.035416\tkl:   27.500380\n",
      "====> Epoch: 899 Average train loss: 92.1793\n",
      "====> Validation set loss: 95.4737\n",
      "====> Validation set kl: 25.4072\n",
      "Epoch: 900 [  100/50000 ( 0%)]  \tLoss:   86.741669\trec:   62.759579\tkl:   23.982086\n",
      "Epoch: 900 [10100/50000 (20%)]  \tLoss:   90.901596\trec:   65.093681\tkl:   25.807919\n",
      "Epoch: 900 [20100/50000 (40%)]  \tLoss:   89.388580\trec:   64.515984\tkl:   24.872599\n",
      "Epoch: 900 [30100/50000 (60%)]  \tLoss:   91.223572\trec:   65.459930\tkl:   25.763638\n",
      "Epoch: 900 [40100/50000 (80%)]  \tLoss:   88.552124\trec:   63.478317\tkl:   25.073805\n",
      "====> Epoch: 900 Average train loss: 92.1505\n",
      "====> Validation set loss: 95.9377\n",
      "====> Validation set kl: 25.8836\n",
      "Epoch: 901 [  100/50000 ( 0%)]  \tLoss:   93.045815\trec:   67.868858\tkl:   25.176954\n",
      "Epoch: 901 [10100/50000 (20%)]  \tLoss:   95.444366\trec:   67.723068\tkl:   27.721298\n",
      "Epoch: 901 [20100/50000 (40%)]  \tLoss:   94.556953\trec:   68.399605\tkl:   26.157345\n",
      "Epoch: 901 [30100/50000 (60%)]  \tLoss:   93.616531\trec:   68.004913\tkl:   25.611620\n",
      "Epoch: 901 [40100/50000 (80%)]  \tLoss:   92.941879\trec:   67.164558\tkl:   25.777327\n",
      "====> Epoch: 901 Average train loss: 92.0613\n",
      "====> Validation set loss: 95.7241\n",
      "====> Validation set kl: 26.2315\n",
      "Epoch: 902 [  100/50000 ( 0%)]  \tLoss:   93.961281\trec:   67.331535\tkl:   26.629747\n",
      "Epoch: 902 [10100/50000 (20%)]  \tLoss:   92.713943\trec:   68.711617\tkl:   24.002327\n",
      "Epoch: 902 [20100/50000 (40%)]  \tLoss:   87.994316\trec:   63.708344\tkl:   24.285976\n",
      "Epoch: 902 [30100/50000 (60%)]  \tLoss:   90.998230\trec:   64.696045\tkl:   26.302189\n",
      "Epoch: 902 [40100/50000 (80%)]  \tLoss:   94.913155\trec:   68.029305\tkl:   26.883846\n",
      "====> Epoch: 902 Average train loss: 92.0223\n",
      "====> Validation set loss: 95.9243\n",
      "====> Validation set kl: 26.0653\n",
      "Epoch: 903 [  100/50000 ( 0%)]  \tLoss:   95.284058\trec:   69.037239\tkl:   26.246817\n",
      "Epoch: 903 [10100/50000 (20%)]  \tLoss:   89.538925\trec:   63.969940\tkl:   25.568987\n",
      "Epoch: 903 [20100/50000 (40%)]  \tLoss:   95.057930\trec:   68.261314\tkl:   26.796618\n",
      "Epoch: 903 [30100/50000 (60%)]  \tLoss:   95.594154\trec:   69.270401\tkl:   26.323765\n",
      "Epoch: 903 [40100/50000 (80%)]  \tLoss:   92.066208\trec:   67.266197\tkl:   24.800018\n",
      "====> Epoch: 903 Average train loss: 92.2435\n",
      "====> Validation set loss: 95.4976\n",
      "====> Validation set kl: 25.2945\n",
      "Epoch: 904 [  100/50000 ( 0%)]  \tLoss:   92.800224\trec:   67.529076\tkl:   25.271149\n",
      "Epoch: 904 [10100/50000 (20%)]  \tLoss:   94.034195\trec:   68.157486\tkl:   25.876717\n",
      "Epoch: 904 [20100/50000 (40%)]  \tLoss:   89.174942\trec:   63.774418\tkl:   25.400522\n",
      "Epoch: 904 [30100/50000 (60%)]  \tLoss:   94.679764\trec:   68.345604\tkl:   26.334162\n",
      "Epoch: 904 [40100/50000 (80%)]  \tLoss:   96.175789\trec:   69.315880\tkl:   26.859915\n",
      "====> Epoch: 904 Average train loss: 91.9423\n",
      "====> Validation set loss: 95.6266\n",
      "====> Validation set kl: 25.3281\n",
      "Epoch: 905 [  100/50000 ( 0%)]  \tLoss:   87.232887\trec:   62.865105\tkl:   24.367785\n",
      "Epoch: 905 [10100/50000 (20%)]  \tLoss:   94.085518\trec:   67.086838\tkl:   26.998678\n",
      "Epoch: 905 [20100/50000 (40%)]  \tLoss:   90.727455\trec:   65.332603\tkl:   25.394863\n",
      "Epoch: 905 [30100/50000 (60%)]  \tLoss:   92.903534\trec:   65.900505\tkl:   27.003029\n",
      "Epoch: 905 [40100/50000 (80%)]  \tLoss:   95.809273\trec:   69.583015\tkl:   26.226254\n",
      "====> Epoch: 905 Average train loss: 92.0176\n",
      "====> Validation set loss: 95.2317\n",
      "====> Validation set kl: 25.5193\n",
      "Epoch: 906 [  100/50000 ( 0%)]  \tLoss:   93.130646\trec:   66.589180\tkl:   26.541460\n",
      "Epoch: 906 [10100/50000 (20%)]  \tLoss:   90.464752\trec:   65.319305\tkl:   25.145451\n",
      "Epoch: 906 [20100/50000 (40%)]  \tLoss:   90.844978\trec:   65.900894\tkl:   24.944078\n",
      "Epoch: 906 [30100/50000 (60%)]  \tLoss:   91.281769\trec:   64.794044\tkl:   26.487726\n",
      "Epoch: 906 [40100/50000 (80%)]  \tLoss:   91.972771\trec:   66.080315\tkl:   25.892450\n",
      "====> Epoch: 906 Average train loss: 91.8822\n",
      "====> Validation set loss: 95.6537\n",
      "====> Validation set kl: 25.5250\n",
      "Epoch: 907 [  100/50000 ( 0%)]  \tLoss:   91.092476\trec:   66.210457\tkl:   24.882027\n",
      "Epoch: 907 [10100/50000 (20%)]  \tLoss:   93.052010\trec:   66.467010\tkl:   26.584995\n",
      "Epoch: 907 [20100/50000 (40%)]  \tLoss:   92.255142\trec:   66.260010\tkl:   25.995136\n",
      "Epoch: 907 [30100/50000 (60%)]  \tLoss:   87.911568\trec:   62.734734\tkl:   25.176836\n",
      "Epoch: 907 [40100/50000 (80%)]  \tLoss:   94.842087\trec:   68.011421\tkl:   26.830667\n",
      "====> Epoch: 907 Average train loss: 92.0305\n",
      "====> Validation set loss: 95.6785\n",
      "====> Validation set kl: 25.7639\n",
      "Epoch: 908 [  100/50000 ( 0%)]  \tLoss:   92.000839\trec:   66.735870\tkl:   25.264973\n",
      "Epoch: 908 [10100/50000 (20%)]  \tLoss:   92.921913\trec:   65.466797\tkl:   27.455116\n",
      "Epoch: 908 [20100/50000 (40%)]  \tLoss:   93.878288\trec:   67.891335\tkl:   25.986959\n",
      "Epoch: 908 [30100/50000 (60%)]  \tLoss:   94.488846\trec:   68.480759\tkl:   26.008091\n",
      "Epoch: 908 [40100/50000 (80%)]  \tLoss:   93.040535\trec:   66.397514\tkl:   26.643015\n",
      "====> Epoch: 908 Average train loss: 92.3414\n",
      "====> Validation set loss: 95.4368\n",
      "====> Validation set kl: 25.5181\n",
      "Epoch: 909 [  100/50000 ( 0%)]  \tLoss:   92.011055\trec:   66.418472\tkl:   25.592583\n",
      "Epoch: 909 [10100/50000 (20%)]  \tLoss:   91.061005\trec:   65.955086\tkl:   25.105921\n",
      "Epoch: 909 [20100/50000 (40%)]  \tLoss:   93.412300\trec:   67.187065\tkl:   26.225233\n",
      "Epoch: 909 [30100/50000 (60%)]  \tLoss:   90.303482\trec:   66.501846\tkl:   23.801632\n",
      "Epoch: 909 [40100/50000 (80%)]  \tLoss:   97.670021\trec:   70.946770\tkl:   26.723242\n",
      "====> Epoch: 909 Average train loss: 92.3404\n",
      "====> Validation set loss: 95.8712\n",
      "====> Validation set kl: 25.8182\n",
      "Epoch: 910 [  100/50000 ( 0%)]  \tLoss:   91.455353\trec:   65.845581\tkl:   25.609764\n",
      "Epoch: 910 [10100/50000 (20%)]  \tLoss:   86.096367\trec:   61.575397\tkl:   24.520971\n",
      "Epoch: 910 [20100/50000 (40%)]  \tLoss:   95.683945\trec:   69.051163\tkl:   26.632786\n",
      "Epoch: 910 [30100/50000 (60%)]  \tLoss:   92.352150\trec:   66.917145\tkl:   25.434999\n",
      "Epoch: 910 [40100/50000 (80%)]  \tLoss:   90.667809\trec:   65.710968\tkl:   24.956842\n",
      "====> Epoch: 910 Average train loss: 92.6930\n",
      "====> Validation set loss: 95.8826\n",
      "====> Validation set kl: 25.7933\n",
      "Epoch: 911 [  100/50000 ( 0%)]  \tLoss:   92.269272\trec:   67.218781\tkl:   25.050493\n",
      "Epoch: 911 [10100/50000 (20%)]  \tLoss:   92.018494\trec:   66.641724\tkl:   25.376764\n",
      "Epoch: 911 [20100/50000 (40%)]  \tLoss:   91.496483\trec:   65.102608\tkl:   26.393871\n",
      "Epoch: 911 [30100/50000 (60%)]  \tLoss:   92.901207\trec:   66.848198\tkl:   26.053013\n",
      "Epoch: 911 [40100/50000 (80%)]  \tLoss:   93.320015\trec:   66.788887\tkl:   26.531136\n",
      "====> Epoch: 911 Average train loss: 92.2183\n",
      "====> Validation set loss: 95.6399\n",
      "====> Validation set kl: 25.4764\n",
      "Epoch: 912 [  100/50000 ( 0%)]  \tLoss:   90.604912\trec:   65.154121\tkl:   25.450785\n",
      "Epoch: 912 [10100/50000 (20%)]  \tLoss:   92.354851\trec:   66.612617\tkl:   25.742233\n",
      "Epoch: 912 [20100/50000 (40%)]  \tLoss:   92.208717\trec:   66.246376\tkl:   25.962343\n",
      "Epoch: 912 [30100/50000 (60%)]  \tLoss:   90.973686\trec:   64.929939\tkl:   26.043747\n",
      "Epoch: 912 [40100/50000 (80%)]  \tLoss:   94.219276\trec:   68.243057\tkl:   25.976215\n",
      "====> Epoch: 912 Average train loss: 92.4713\n",
      "====> Validation set loss: 95.7342\n",
      "====> Validation set kl: 25.6209\n",
      "Epoch: 913 [  100/50000 ( 0%)]  \tLoss:   91.113083\trec:   65.556602\tkl:   25.556482\n",
      "Epoch: 913 [10100/50000 (20%)]  \tLoss:   88.744179\trec:   64.230446\tkl:   24.513725\n",
      "Epoch: 913 [20100/50000 (40%)]  \tLoss:   91.005264\trec:   65.463905\tkl:   25.541357\n",
      "Epoch: 913 [30100/50000 (60%)]  \tLoss:   92.312660\trec:   65.324905\tkl:   26.987755\n",
      "Epoch: 913 [40100/50000 (80%)]  \tLoss:   93.808136\trec:   66.903191\tkl:   26.904947\n",
      "====> Epoch: 913 Average train loss: 91.9407\n",
      "====> Validation set loss: 95.4828\n",
      "====> Validation set kl: 25.3278\n",
      "Epoch: 914 [  100/50000 ( 0%)]  \tLoss:   95.197266\trec:   68.879913\tkl:   26.317348\n",
      "Epoch: 914 [10100/50000 (20%)]  \tLoss:   90.188873\trec:   64.675171\tkl:   25.513702\n",
      "Epoch: 914 [20100/50000 (40%)]  \tLoss:   94.207458\trec:   67.393242\tkl:   26.814213\n",
      "Epoch: 914 [30100/50000 (60%)]  \tLoss:   90.070679\trec:   64.127930\tkl:   25.942759\n",
      "Epoch: 914 [40100/50000 (80%)]  \tLoss:   91.677589\trec:   65.578659\tkl:   26.098923\n",
      "====> Epoch: 914 Average train loss: 91.9900\n",
      "====> Validation set loss: 95.7983\n",
      "====> Validation set kl: 26.2725\n",
      "Epoch: 915 [  100/50000 ( 0%)]  \tLoss:   93.598602\trec:   67.334137\tkl:   26.264462\n",
      "Epoch: 915 [10100/50000 (20%)]  \tLoss:   92.126328\trec:   66.687714\tkl:   25.438608\n",
      "Epoch: 915 [20100/50000 (40%)]  \tLoss:   96.800064\trec:   70.161354\tkl:   26.638710\n",
      "Epoch: 915 [30100/50000 (60%)]  \tLoss:   92.912697\trec:   66.995224\tkl:   25.917471\n",
      "Epoch: 915 [40100/50000 (80%)]  \tLoss:   93.363251\trec:   68.114090\tkl:   25.249159\n",
      "====> Epoch: 915 Average train loss: 92.4942\n",
      "====> Validation set loss: 96.3638\n",
      "====> Validation set kl: 25.1796\n",
      "Epoch: 916 [  100/50000 ( 0%)]  \tLoss:   91.337936\trec:   66.038185\tkl:   25.299753\n",
      "Epoch: 916 [10100/50000 (20%)]  \tLoss:   91.008949\trec:   64.449898\tkl:   26.559055\n",
      "Epoch: 916 [20100/50000 (40%)]  \tLoss:   91.551270\trec:   66.342186\tkl:   25.209082\n",
      "Epoch: 916 [30100/50000 (60%)]  \tLoss:   89.546577\trec:   64.377258\tkl:   25.169319\n",
      "Epoch: 916 [40100/50000 (80%)]  \tLoss:   89.255157\trec:   63.307995\tkl:   25.947161\n",
      "====> Epoch: 916 Average train loss: 92.4348\n",
      "====> Validation set loss: 95.6697\n",
      "====> Validation set kl: 26.0152\n",
      "Epoch: 917 [  100/50000 ( 0%)]  \tLoss:   88.835487\trec:   62.896423\tkl:   25.939056\n",
      "Epoch: 917 [10100/50000 (20%)]  \tLoss:   91.699570\trec:   64.842400\tkl:   26.857168\n",
      "Epoch: 917 [20100/50000 (40%)]  \tLoss:   89.299492\trec:   65.051414\tkl:   24.248081\n",
      "Epoch: 917 [30100/50000 (60%)]  \tLoss:   93.846870\trec:   68.806496\tkl:   25.040384\n",
      "Epoch: 917 [40100/50000 (80%)]  \tLoss:   95.930977\trec:   69.143806\tkl:   26.787170\n",
      "====> Epoch: 917 Average train loss: 91.9708\n",
      "====> Validation set loss: 95.2579\n",
      "====> Validation set kl: 25.5093\n",
      "Epoch: 918 [  100/50000 ( 0%)]  \tLoss:   96.127831\trec:   70.396461\tkl:   25.731361\n",
      "Epoch: 918 [10100/50000 (20%)]  \tLoss:   91.092201\trec:   65.521606\tkl:   25.570595\n",
      "Epoch: 918 [20100/50000 (40%)]  \tLoss:   89.517723\trec:   63.995468\tkl:   25.522257\n",
      "Epoch: 918 [30100/50000 (60%)]  \tLoss:   94.223793\trec:   68.465836\tkl:   25.757961\n",
      "Epoch: 918 [40100/50000 (80%)]  \tLoss:   90.021584\trec:   64.889023\tkl:   25.132559\n",
      "====> Epoch: 918 Average train loss: 92.0143\n",
      "====> Validation set loss: 96.6284\n",
      "====> Validation set kl: 25.7073\n",
      "Epoch: 919 [  100/50000 ( 0%)]  \tLoss:   94.633514\trec:   68.267212\tkl:   26.366299\n",
      "Epoch: 919 [10100/50000 (20%)]  \tLoss:   92.362450\trec:   66.950577\tkl:   25.411875\n",
      "Epoch: 919 [20100/50000 (40%)]  \tLoss:   93.320427\trec:   66.620621\tkl:   26.699804\n",
      "Epoch: 919 [30100/50000 (60%)]  \tLoss:   91.560753\trec:   66.210831\tkl:   25.349918\n",
      "Epoch: 919 [40100/50000 (80%)]  \tLoss:   91.905350\trec:   66.053436\tkl:   25.851908\n",
      "====> Epoch: 919 Average train loss: 92.1145\n",
      "====> Validation set loss: 95.5242\n",
      "====> Validation set kl: 25.6015\n",
      "Epoch: 920 [  100/50000 ( 0%)]  \tLoss:   93.958786\trec:   67.378471\tkl:   26.580313\n",
      "Epoch: 920 [10100/50000 (20%)]  \tLoss:   91.137283\trec:   65.035339\tkl:   26.101946\n",
      "Epoch: 920 [20100/50000 (40%)]  \tLoss:   92.366371\trec:   66.866989\tkl:   25.499386\n",
      "Epoch: 920 [30100/50000 (60%)]  \tLoss:   88.341896\trec:   63.571220\tkl:   24.770676\n",
      "Epoch: 920 [40100/50000 (80%)]  \tLoss:   95.636475\trec:   69.140068\tkl:   26.496401\n",
      "====> Epoch: 920 Average train loss: 92.3511\n",
      "====> Validation set loss: 95.5550\n",
      "====> Validation set kl: 25.9039\n",
      "Epoch: 921 [  100/50000 ( 0%)]  \tLoss:   92.139259\trec:   65.893814\tkl:   26.245443\n",
      "Epoch: 921 [10100/50000 (20%)]  \tLoss:   91.190987\trec:   64.219559\tkl:   26.971422\n",
      "Epoch: 921 [20100/50000 (40%)]  \tLoss:   89.421364\trec:   64.535904\tkl:   24.885454\n",
      "Epoch: 921 [30100/50000 (60%)]  \tLoss:   97.185646\trec:   69.966568\tkl:   27.219072\n",
      "Epoch: 921 [40100/50000 (80%)]  \tLoss:   94.404999\trec:   68.213326\tkl:   26.191669\n",
      "====> Epoch: 921 Average train loss: 92.4697\n",
      "====> Validation set loss: 95.6839\n",
      "====> Validation set kl: 26.0436\n",
      "Epoch: 922 [  100/50000 ( 0%)]  \tLoss:   88.456200\trec:   63.420349\tkl:   25.035847\n",
      "Epoch: 922 [10100/50000 (20%)]  \tLoss:   94.076523\trec:   68.734200\tkl:   25.342321\n",
      "Epoch: 922 [20100/50000 (40%)]  \tLoss:   91.786621\trec:   66.203690\tkl:   25.582933\n",
      "Epoch: 922 [30100/50000 (60%)]  \tLoss:   93.628868\trec:   67.881470\tkl:   25.747395\n",
      "Epoch: 922 [40100/50000 (80%)]  \tLoss:   94.349892\trec:   68.241592\tkl:   26.108295\n",
      "====> Epoch: 922 Average train loss: 92.0140\n",
      "====> Validation set loss: 95.5760\n",
      "====> Validation set kl: 26.1035\n",
      "Epoch: 923 [  100/50000 ( 0%)]  \tLoss:   91.452827\trec:   65.672852\tkl:   25.779978\n",
      "Epoch: 923 [10100/50000 (20%)]  \tLoss:   92.738731\trec:   66.855820\tkl:   25.882912\n",
      "Epoch: 923 [20100/50000 (40%)]  \tLoss:   94.047905\trec:   67.991867\tkl:   26.056042\n",
      "Epoch: 923 [30100/50000 (60%)]  \tLoss:   91.711861\trec:   66.367226\tkl:   25.344641\n",
      "Epoch: 923 [40100/50000 (80%)]  \tLoss:   95.277496\trec:   69.443077\tkl:   25.834419\n",
      "====> Epoch: 923 Average train loss: 92.5680\n",
      "====> Validation set loss: 95.9877\n",
      "====> Validation set kl: 25.7161\n",
      "Epoch: 924 [  100/50000 ( 0%)]  \tLoss:   93.477341\trec:   66.968971\tkl:   26.508371\n",
      "Epoch: 924 [10100/50000 (20%)]  \tLoss:   90.218338\trec:   64.247093\tkl:   25.971245\n",
      "Epoch: 924 [20100/50000 (40%)]  \tLoss:   93.966537\trec:   66.862389\tkl:   27.104149\n",
      "Epoch: 924 [30100/50000 (60%)]  \tLoss:   95.184624\trec:   68.689621\tkl:   26.495012\n",
      "Epoch: 924 [40100/50000 (80%)]  \tLoss:   93.080856\trec:   66.865059\tkl:   26.215803\n",
      "====> Epoch: 924 Average train loss: 92.4411\n",
      "====> Validation set loss: 95.6614\n",
      "====> Validation set kl: 25.8523\n",
      "Epoch: 925 [  100/50000 ( 0%)]  \tLoss:   89.596992\trec:   64.007256\tkl:   25.589729\n",
      "Epoch: 925 [10100/50000 (20%)]  \tLoss:   93.188446\trec:   66.900894\tkl:   26.287548\n",
      "Epoch: 925 [20100/50000 (40%)]  \tLoss:   95.064384\trec:   68.652275\tkl:   26.412109\n",
      "Epoch: 925 [30100/50000 (60%)]  \tLoss:   88.703941\trec:   63.815090\tkl:   24.888851\n",
      "Epoch: 925 [40100/50000 (80%)]  \tLoss:   93.498779\trec:   67.931137\tkl:   25.567640\n",
      "====> Epoch: 925 Average train loss: 92.3789\n",
      "====> Validation set loss: 96.0382\n",
      "====> Validation set kl: 25.9871\n",
      "Epoch: 926 [  100/50000 ( 0%)]  \tLoss:   89.458466\trec:   63.724091\tkl:   25.734375\n",
      "Epoch: 926 [10100/50000 (20%)]  \tLoss:   94.043839\trec:   67.767319\tkl:   26.276520\n",
      "Epoch: 926 [20100/50000 (40%)]  \tLoss:   94.778664\trec:   68.013046\tkl:   26.765615\n",
      "Epoch: 926 [30100/50000 (60%)]  \tLoss:   93.323608\trec:   66.805420\tkl:   26.518192\n",
      "Epoch: 926 [40100/50000 (80%)]  \tLoss:   93.412399\trec:   67.145302\tkl:   26.267103\n",
      "====> Epoch: 926 Average train loss: 92.1322\n",
      "====> Validation set loss: 95.6498\n",
      "====> Validation set kl: 25.8808\n",
      "Epoch: 927 [  100/50000 ( 0%)]  \tLoss:   90.955063\trec:   64.963623\tkl:   25.991447\n",
      "Epoch: 927 [10100/50000 (20%)]  \tLoss:   96.565575\trec:   69.940681\tkl:   26.624895\n",
      "Epoch: 927 [20100/50000 (40%)]  \tLoss:   90.746132\trec:   65.889786\tkl:   24.856344\n",
      "Epoch: 927 [30100/50000 (60%)]  \tLoss:   90.792999\trec:   64.884468\tkl:   25.908525\n",
      "Epoch: 927 [40100/50000 (80%)]  \tLoss:   93.678398\trec:   68.098885\tkl:   25.579515\n",
      "====> Epoch: 927 Average train loss: 92.4420\n",
      "====> Validation set loss: 96.2503\n",
      "====> Validation set kl: 25.3133\n",
      "Epoch: 928 [  100/50000 ( 0%)]  \tLoss:   86.433746\trec:   61.411911\tkl:   25.021830\n",
      "Epoch: 928 [10100/50000 (20%)]  \tLoss:   88.373100\trec:   63.352928\tkl:   25.020178\n",
      "Epoch: 928 [20100/50000 (40%)]  \tLoss:   90.065712\trec:   64.401039\tkl:   25.664673\n",
      "Epoch: 928 [30100/50000 (60%)]  \tLoss:   92.255997\trec:   66.285027\tkl:   25.970968\n",
      "Epoch: 928 [40100/50000 (80%)]  \tLoss:   92.537994\trec:   67.218834\tkl:   25.319164\n",
      "====> Epoch: 928 Average train loss: 92.6474\n",
      "====> Validation set loss: 96.9217\n",
      "====> Validation set kl: 25.8307\n",
      "Epoch: 929 [  100/50000 ( 0%)]  \tLoss:   92.119743\trec:   66.802742\tkl:   25.316999\n",
      "Epoch: 929 [10100/50000 (20%)]  \tLoss:   92.583969\trec:   66.297485\tkl:   26.286486\n",
      "Epoch: 929 [20100/50000 (40%)]  \tLoss:   89.121246\trec:   63.846321\tkl:   25.274931\n",
      "Epoch: 929 [30100/50000 (60%)]  \tLoss:   91.908707\trec:   65.437965\tkl:   26.470737\n",
      "Epoch: 929 [40100/50000 (80%)]  \tLoss:   88.149414\trec:   62.215378\tkl:   25.934031\n",
      "====> Epoch: 929 Average train loss: 92.4150\n",
      "====> Validation set loss: 95.7942\n",
      "====> Validation set kl: 25.6323\n",
      "Epoch: 930 [  100/50000 ( 0%)]  \tLoss:   94.085548\trec:   67.821953\tkl:   26.263597\n",
      "Epoch: 930 [10100/50000 (20%)]  \tLoss:   90.302422\trec:   64.491676\tkl:   25.810741\n",
      "Epoch: 930 [20100/50000 (40%)]  \tLoss:   91.248512\trec:   65.809586\tkl:   25.438931\n",
      "Epoch: 930 [30100/50000 (60%)]  \tLoss:   91.519806\trec:   66.468994\tkl:   25.050810\n",
      "Epoch: 930 [40100/50000 (80%)]  \tLoss:   95.231148\trec:   69.013550\tkl:   26.217600\n",
      "====> Epoch: 930 Average train loss: 92.8234\n",
      "====> Validation set loss: 95.8835\n",
      "====> Validation set kl: 26.1106\n",
      "Epoch: 931 [  100/50000 ( 0%)]  \tLoss:   91.257401\trec:   65.323151\tkl:   25.934248\n",
      "Epoch: 931 [10100/50000 (20%)]  \tLoss:   93.356384\trec:   67.415779\tkl:   25.940605\n",
      "Epoch: 931 [20100/50000 (40%)]  \tLoss:   96.682480\trec:   70.178444\tkl:   26.504030\n",
      "Epoch: 931 [30100/50000 (60%)]  \tLoss:   95.138161\trec:   68.873611\tkl:   26.264547\n",
      "Epoch: 931 [40100/50000 (80%)]  \tLoss:   94.892799\trec:   67.974152\tkl:   26.918652\n",
      "====> Epoch: 931 Average train loss: 92.9580\n",
      "====> Validation set loss: 96.9481\n",
      "====> Validation set kl: 25.8100\n",
      "Epoch: 932 [  100/50000 ( 0%)]  \tLoss:   92.855347\trec:   67.094490\tkl:   25.760864\n",
      "Epoch: 932 [10100/50000 (20%)]  \tLoss:   93.606560\trec:   67.861198\tkl:   25.745358\n",
      "Epoch: 932 [20100/50000 (40%)]  \tLoss:   95.768135\trec:   70.235451\tkl:   25.532684\n",
      "Epoch: 932 [30100/50000 (60%)]  \tLoss:   93.516899\trec:   67.299973\tkl:   26.216930\n",
      "Epoch: 932 [40100/50000 (80%)]  \tLoss:   93.784744\trec:   67.209488\tkl:   26.575251\n",
      "====> Epoch: 932 Average train loss: 92.8552\n",
      "====> Validation set loss: 95.9139\n",
      "====> Validation set kl: 26.2030\n",
      "Epoch: 933 [  100/50000 ( 0%)]  \tLoss:   94.973145\trec:   68.685913\tkl:   26.287224\n",
      "Epoch: 933 [10100/50000 (20%)]  \tLoss:   94.894791\trec:   68.660706\tkl:   26.234083\n",
      "Epoch: 933 [20100/50000 (40%)]  \tLoss:   92.554825\trec:   67.087311\tkl:   25.467514\n",
      "Epoch: 933 [30100/50000 (60%)]  \tLoss:   91.968872\trec:   65.673325\tkl:   26.295551\n",
      "Epoch: 933 [40100/50000 (80%)]  \tLoss:   93.515236\trec:   68.056641\tkl:   25.458593\n",
      "====> Epoch: 933 Average train loss: 92.4996\n",
      "====> Validation set loss: 96.5625\n",
      "====> Validation set kl: 25.6148\n",
      "Epoch: 934 [  100/50000 ( 0%)]  \tLoss:   92.019814\trec:   66.496391\tkl:   25.523424\n",
      "Epoch: 934 [10100/50000 (20%)]  \tLoss:   92.719902\trec:   66.910339\tkl:   25.809563\n",
      "Epoch: 934 [20100/50000 (40%)]  \tLoss:   93.360748\trec:   67.052879\tkl:   26.307873\n",
      "Epoch: 934 [30100/50000 (60%)]  \tLoss:   93.167007\trec:   67.078384\tkl:   26.088631\n",
      "Epoch: 934 [40100/50000 (80%)]  \tLoss:   92.637848\trec:   67.250313\tkl:   25.387543\n",
      "====> Epoch: 934 Average train loss: 92.9061\n",
      "====> Validation set loss: 95.9153\n",
      "====> Validation set kl: 25.3438\n",
      "Epoch: 935 [  100/50000 ( 0%)]  \tLoss:   93.918892\trec:   68.880173\tkl:   25.038723\n",
      "Epoch: 935 [10100/50000 (20%)]  \tLoss:   90.242165\trec:   66.044258\tkl:   24.197905\n",
      "Epoch: 935 [20100/50000 (40%)]  \tLoss:   93.306450\trec:   66.349907\tkl:   26.956545\n",
      "Epoch: 935 [30100/50000 (60%)]  \tLoss:   96.229332\trec:   69.291138\tkl:   26.938194\n",
      "Epoch: 935 [40100/50000 (80%)]  \tLoss:   95.251831\trec:   68.797638\tkl:   26.454189\n",
      "====> Epoch: 935 Average train loss: 92.6077\n",
      "====> Validation set loss: 98.6801\n",
      "====> Validation set kl: 28.1755\n",
      "Epoch: 936 [  100/50000 ( 0%)]  \tLoss:   93.210899\trec:   65.094170\tkl:   28.116718\n",
      "Epoch: 936 [10100/50000 (20%)]  \tLoss:   90.122871\trec:   64.867020\tkl:   25.255852\n",
      "Epoch: 936 [20100/50000 (40%)]  \tLoss:   93.627480\trec:   68.345261\tkl:   25.282217\n",
      "Epoch: 936 [30100/50000 (60%)]  \tLoss:   92.784958\trec:   66.208481\tkl:   26.576485\n",
      "Epoch: 936 [40100/50000 (80%)]  \tLoss:   92.592804\trec:   67.063644\tkl:   25.529152\n",
      "====> Epoch: 936 Average train loss: 93.1602\n",
      "====> Validation set loss: 96.7969\n",
      "====> Validation set kl: 25.6995\n",
      "Epoch: 937 [  100/50000 ( 0%)]  \tLoss:   88.240044\trec:   63.439301\tkl:   24.800745\n",
      "Epoch: 937 [10100/50000 (20%)]  \tLoss:   92.031532\trec:   65.343430\tkl:   26.688105\n",
      "Epoch: 937 [20100/50000 (40%)]  \tLoss:   90.828575\trec:   64.308044\tkl:   26.520521\n",
      "Epoch: 937 [30100/50000 (60%)]  \tLoss:   94.020088\trec:   67.519279\tkl:   26.500807\n",
      "Epoch: 937 [40100/50000 (80%)]  \tLoss:   92.821388\trec:   66.797203\tkl:   26.024178\n",
      "====> Epoch: 937 Average train loss: 93.2913\n",
      "====> Validation set loss: 96.7118\n",
      "====> Validation set kl: 26.9495\n",
      "Epoch: 938 [  100/50000 ( 0%)]  \tLoss:   94.191193\trec:   67.326782\tkl:   26.864407\n",
      "Epoch: 938 [10100/50000 (20%)]  \tLoss:   95.579414\trec:   69.636871\tkl:   25.942541\n",
      "Epoch: 938 [20100/50000 (40%)]  \tLoss:   93.081703\trec:   66.602188\tkl:   26.479519\n",
      "Epoch: 938 [30100/50000 (60%)]  \tLoss:   91.654396\trec:   64.750427\tkl:   26.903959\n",
      "Epoch: 938 [40100/50000 (80%)]  \tLoss:   92.973663\trec:   66.719452\tkl:   26.254206\n",
      "====> Epoch: 938 Average train loss: 92.9987\n",
      "====> Validation set loss: 95.9297\n",
      "====> Validation set kl: 25.8750\n",
      "Epoch: 939 [  100/50000 ( 0%)]  \tLoss:   91.285095\trec:   65.593208\tkl:   25.691887\n",
      "Epoch: 939 [10100/50000 (20%)]  \tLoss:   90.717880\trec:   66.247047\tkl:   24.470833\n",
      "Epoch: 939 [20100/50000 (40%)]  \tLoss:   92.647858\trec:   66.649529\tkl:   25.998327\n",
      "Epoch: 939 [30100/50000 (60%)]  \tLoss:   88.804161\trec:   63.942684\tkl:   24.861475\n",
      "Epoch: 939 [40100/50000 (80%)]  \tLoss:   90.269234\trec:   65.453796\tkl:   24.815437\n",
      "====> Epoch: 939 Average train loss: 92.6223\n",
      "====> Validation set loss: 96.2815\n",
      "====> Validation set kl: 25.6321\n",
      "Epoch: 940 [  100/50000 ( 0%)]  \tLoss:   90.508278\trec:   64.952599\tkl:   25.555681\n",
      "Epoch: 940 [10100/50000 (20%)]  \tLoss:   98.784615\trec:   72.355888\tkl:   26.428728\n",
      "Epoch: 940 [20100/50000 (40%)]  \tLoss:   95.314888\trec:   69.152435\tkl:   26.162455\n",
      "Epoch: 940 [30100/50000 (60%)]  \tLoss:   96.457489\trec:   68.594566\tkl:   27.862923\n",
      "Epoch: 940 [40100/50000 (80%)]  \tLoss:   93.630775\trec:   67.858299\tkl:   25.772484\n",
      "====> Epoch: 940 Average train loss: 92.3445\n",
      "====> Validation set loss: 95.4769\n",
      "====> Validation set kl: 25.5942\n",
      "Epoch: 941 [  100/50000 ( 0%)]  \tLoss:   95.828445\trec:   69.731720\tkl:   26.096724\n",
      "Epoch: 941 [10100/50000 (20%)]  \tLoss:   90.903358\trec:   65.731171\tkl:   25.172190\n",
      "Epoch: 941 [20100/50000 (40%)]  \tLoss:   93.605125\trec:   67.031120\tkl:   26.574003\n",
      "Epoch: 941 [30100/50000 (60%)]  \tLoss:   95.850372\trec:   69.446495\tkl:   26.403872\n",
      "Epoch: 941 [40100/50000 (80%)]  \tLoss:   93.416725\trec:   66.920006\tkl:   26.496716\n",
      "====> Epoch: 941 Average train loss: 92.5960\n",
      "====> Validation set loss: 96.0847\n",
      "====> Validation set kl: 25.8670\n",
      "Epoch: 942 [  100/50000 ( 0%)]  \tLoss:   90.672012\trec:   64.691299\tkl:   25.980709\n",
      "Epoch: 942 [10100/50000 (20%)]  \tLoss:   90.250366\trec:   65.166191\tkl:   25.084177\n",
      "Epoch: 942 [20100/50000 (40%)]  \tLoss:   88.983009\trec:   63.867188\tkl:   25.115822\n",
      "Epoch: 942 [30100/50000 (60%)]  \tLoss:   91.295319\trec:   65.333153\tkl:   25.962166\n",
      "Epoch: 942 [40100/50000 (80%)]  \tLoss:   93.621338\trec:   67.588936\tkl:   26.032400\n",
      "====> Epoch: 942 Average train loss: 92.3438\n",
      "====> Validation set loss: 95.6372\n",
      "====> Validation set kl: 25.9602\n",
      "Epoch: 943 [  100/50000 ( 0%)]  \tLoss:   93.834595\trec:   66.657310\tkl:   27.177286\n",
      "Epoch: 943 [10100/50000 (20%)]  \tLoss:   94.986092\trec:   69.798668\tkl:   25.187416\n",
      "Epoch: 943 [20100/50000 (40%)]  \tLoss:   91.551376\trec:   65.039497\tkl:   26.511877\n",
      "Epoch: 943 [30100/50000 (60%)]  \tLoss:   96.148262\trec:   69.262306\tkl:   26.885954\n",
      "Epoch: 943 [40100/50000 (80%)]  \tLoss:   93.840065\trec:   67.437401\tkl:   26.402668\n",
      "====> Epoch: 943 Average train loss: 92.4345\n",
      "====> Validation set loss: 95.8271\n",
      "====> Validation set kl: 25.9495\n",
      "Epoch: 944 [  100/50000 ( 0%)]  \tLoss:   90.011436\trec:   64.190918\tkl:   25.820513\n",
      "Epoch: 944 [10100/50000 (20%)]  \tLoss:   93.106674\trec:   66.673645\tkl:   26.433037\n",
      "Epoch: 944 [20100/50000 (40%)]  \tLoss:   87.853111\trec:   63.803223\tkl:   24.049892\n",
      "Epoch: 944 [30100/50000 (60%)]  \tLoss:   94.464638\trec:   67.821075\tkl:   26.643566\n",
      "Epoch: 944 [40100/50000 (80%)]  \tLoss:   95.659439\trec:   69.023438\tkl:   26.636007\n",
      "====> Epoch: 944 Average train loss: 92.2962\n",
      "====> Validation set loss: 95.9625\n",
      "====> Validation set kl: 26.1852\n",
      "Epoch: 945 [  100/50000 ( 0%)]  \tLoss:   90.175117\trec:   64.465965\tkl:   25.709152\n",
      "Epoch: 945 [10100/50000 (20%)]  \tLoss:   93.023102\trec:   66.560280\tkl:   26.462826\n",
      "Epoch: 945 [20100/50000 (40%)]  \tLoss:   91.364334\trec:   66.342049\tkl:   25.022287\n",
      "Epoch: 945 [30100/50000 (60%)]  \tLoss:   92.396912\trec:   66.222115\tkl:   26.174795\n",
      "Epoch: 945 [40100/50000 (80%)]  \tLoss:   89.275002\trec:   63.959793\tkl:   25.315210\n",
      "====> Epoch: 945 Average train loss: 92.2847\n",
      "====> Validation set loss: 95.5467\n",
      "====> Validation set kl: 25.6719\n",
      "Epoch: 946 [  100/50000 ( 0%)]  \tLoss:   95.232002\trec:   69.006180\tkl:   26.225821\n",
      "Epoch: 946 [10100/50000 (20%)]  \tLoss:   90.732574\trec:   65.712402\tkl:   25.020180\n",
      "Epoch: 946 [20100/50000 (40%)]  \tLoss:   89.338989\trec:   64.360451\tkl:   24.978542\n",
      "Epoch: 946 [30100/50000 (60%)]  \tLoss:   91.414375\trec:   65.890923\tkl:   25.523441\n",
      "Epoch: 946 [40100/50000 (80%)]  \tLoss:   91.291557\trec:   64.480621\tkl:   26.810940\n",
      "====> Epoch: 946 Average train loss: 91.9682\n",
      "====> Validation set loss: 95.2926\n",
      "====> Validation set kl: 25.6590\n",
      "Epoch: 947 [  100/50000 ( 0%)]  \tLoss:   88.041573\trec:   63.722561\tkl:   24.319010\n",
      "Epoch: 947 [10100/50000 (20%)]  \tLoss:   90.409431\trec:   64.739624\tkl:   25.669804\n",
      "Epoch: 947 [20100/50000 (40%)]  \tLoss:   92.517548\trec:   67.740166\tkl:   24.777382\n",
      "Epoch: 947 [30100/50000 (60%)]  \tLoss:   92.570297\trec:   66.936516\tkl:   25.633785\n",
      "Epoch: 947 [40100/50000 (80%)]  \tLoss:   95.412354\trec:   69.198280\tkl:   26.214071\n",
      "====> Epoch: 947 Average train loss: 92.0344\n",
      "====> Validation set loss: 95.6172\n",
      "====> Validation set kl: 25.8010\n",
      "Epoch: 948 [  100/50000 ( 0%)]  \tLoss:   93.804100\trec:   67.298874\tkl:   26.505228\n",
      "Epoch: 948 [10100/50000 (20%)]  \tLoss:   91.195107\trec:   66.092827\tkl:   25.102272\n",
      "Epoch: 948 [20100/50000 (40%)]  \tLoss:   90.813690\trec:   64.607208\tkl:   26.206482\n",
      "Epoch: 948 [30100/50000 (60%)]  \tLoss:   89.629158\trec:   64.691025\tkl:   24.938131\n",
      "Epoch: 948 [40100/50000 (80%)]  \tLoss:   87.347458\trec:   61.739407\tkl:   25.608053\n",
      "====> Epoch: 948 Average train loss: 91.9481\n",
      "====> Validation set loss: 96.5021\n",
      "====> Validation set kl: 25.9981\n",
      "Epoch: 949 [  100/50000 ( 0%)]  \tLoss:   95.505905\trec:   69.202438\tkl:   26.303474\n",
      "Epoch: 949 [10100/50000 (20%)]  \tLoss:   90.606010\trec:   64.401001\tkl:   26.205011\n",
      "Epoch: 949 [20100/50000 (40%)]  \tLoss:   90.893181\trec:   65.815994\tkl:   25.077192\n",
      "Epoch: 949 [30100/50000 (60%)]  \tLoss:   91.332069\trec:   65.211349\tkl:   26.120722\n",
      "Epoch: 949 [40100/50000 (80%)]  \tLoss:   90.390778\trec:   64.802895\tkl:   25.587881\n",
      "====> Epoch: 949 Average train loss: 92.2444\n",
      "====> Validation set loss: 96.2143\n",
      "====> Validation set kl: 25.9299\n",
      "Epoch: 950 [  100/50000 ( 0%)]  \tLoss:   93.337547\trec:   67.556236\tkl:   25.781311\n",
      "Epoch: 950 [10100/50000 (20%)]  \tLoss:   90.000977\trec:   63.521397\tkl:   26.479582\n",
      "Epoch: 950 [20100/50000 (40%)]  \tLoss:   97.630089\trec:   70.126373\tkl:   27.503710\n",
      "Epoch: 950 [30100/50000 (60%)]  \tLoss:   90.013565\trec:   63.828480\tkl:   26.185081\n",
      "Epoch: 950 [40100/50000 (80%)]  \tLoss:   90.819786\trec:   63.928722\tkl:   26.891054\n",
      "====> Epoch: 950 Average train loss: 92.1099\n",
      "====> Validation set loss: 95.5280\n",
      "====> Validation set kl: 25.8091\n",
      "Epoch: 951 [  100/50000 ( 0%)]  \tLoss:   88.539162\trec:   63.320320\tkl:   25.218842\n",
      "Epoch: 951 [10100/50000 (20%)]  \tLoss:   90.242920\trec:   65.077812\tkl:   25.165104\n",
      "Epoch: 951 [20100/50000 (40%)]  \tLoss:   90.139923\trec:   64.489243\tkl:   25.650673\n",
      "Epoch: 951 [30100/50000 (60%)]  \tLoss:   92.094742\trec:   65.800575\tkl:   26.294165\n",
      "Epoch: 951 [40100/50000 (80%)]  \tLoss:   92.616425\trec:   65.475685\tkl:   27.140739\n",
      "====> Epoch: 951 Average train loss: 92.0849\n",
      "====> Validation set loss: 95.5680\n",
      "====> Validation set kl: 25.8858\n",
      "Epoch: 952 [  100/50000 ( 0%)]  \tLoss:   89.097916\trec:   63.287224\tkl:   25.810692\n",
      "Epoch: 952 [10100/50000 (20%)]  \tLoss:   97.144051\trec:   70.476990\tkl:   26.667061\n",
      "Epoch: 952 [20100/50000 (40%)]  \tLoss:   96.387772\trec:   70.344391\tkl:   26.043379\n",
      "Epoch: 952 [30100/50000 (60%)]  \tLoss:   91.862762\trec:   66.788307\tkl:   25.074455\n",
      "Epoch: 952 [40100/50000 (80%)]  \tLoss:   99.963936\trec:   73.387436\tkl:   26.576500\n",
      "====> Epoch: 952 Average train loss: 91.8429\n",
      "====> Validation set loss: 95.5078\n",
      "====> Validation set kl: 25.5655\n",
      "Epoch: 953 [  100/50000 ( 0%)]  \tLoss:   93.717056\trec:   67.040764\tkl:   26.676298\n",
      "Epoch: 953 [10100/50000 (20%)]  \tLoss:   89.957809\trec:   64.149803\tkl:   25.808006\n",
      "Epoch: 953 [20100/50000 (40%)]  \tLoss:   91.312660\trec:   65.284294\tkl:   26.028374\n",
      "Epoch: 953 [30100/50000 (60%)]  \tLoss:   94.592163\trec:   68.573776\tkl:   26.018393\n",
      "Epoch: 953 [40100/50000 (80%)]  \tLoss:   95.364998\trec:   68.678215\tkl:   26.686787\n",
      "====> Epoch: 953 Average train loss: 91.8623\n",
      "====> Validation set loss: 95.2234\n",
      "====> Validation set kl: 25.9740\n",
      "Epoch: 954 [  100/50000 ( 0%)]  \tLoss:   91.195961\trec:   65.285339\tkl:   25.910624\n",
      "Epoch: 954 [10100/50000 (20%)]  \tLoss:   90.709785\trec:   64.826210\tkl:   25.883579\n",
      "Epoch: 954 [20100/50000 (40%)]  \tLoss:   91.729637\trec:   65.961052\tkl:   25.768581\n",
      "Epoch: 954 [30100/50000 (60%)]  \tLoss:   89.549072\trec:   64.209244\tkl:   25.339828\n",
      "Epoch: 954 [40100/50000 (80%)]  \tLoss:   95.042389\trec:   67.330437\tkl:   27.711952\n",
      "====> Epoch: 954 Average train loss: 91.8961\n",
      "====> Validation set loss: 95.5373\n",
      "====> Validation set kl: 26.1712\n",
      "Epoch: 955 [  100/50000 ( 0%)]  \tLoss:   93.221642\trec:   67.578903\tkl:   25.642729\n",
      "Epoch: 955 [10100/50000 (20%)]  \tLoss:   89.415092\trec:   65.000069\tkl:   24.415033\n",
      "Epoch: 955 [20100/50000 (40%)]  \tLoss:   93.342300\trec:   68.225105\tkl:   25.117197\n",
      "Epoch: 955 [30100/50000 (60%)]  \tLoss:   88.278221\trec:   63.517467\tkl:   24.760746\n",
      "Epoch: 955 [40100/50000 (80%)]  \tLoss:   95.246246\trec:   68.238930\tkl:   27.007318\n",
      "====> Epoch: 955 Average train loss: 91.8928\n",
      "====> Validation set loss: 95.4114\n",
      "====> Validation set kl: 25.6657\n",
      "Epoch: 956 [  100/50000 ( 0%)]  \tLoss:   93.190300\trec:   67.436073\tkl:   25.754230\n",
      "Epoch: 956 [10100/50000 (20%)]  \tLoss:   93.261375\trec:   67.970367\tkl:   25.291002\n",
      "Epoch: 956 [20100/50000 (40%)]  \tLoss:   91.142517\trec:   65.739662\tkl:   25.402851\n",
      "Epoch: 956 [30100/50000 (60%)]  \tLoss:   91.678825\trec:   65.677521\tkl:   26.001305\n",
      "Epoch: 956 [40100/50000 (80%)]  \tLoss:   86.609634\trec:   61.107002\tkl:   25.502636\n",
      "====> Epoch: 956 Average train loss: 91.8403\n",
      "====> Validation set loss: 95.3253\n",
      "====> Validation set kl: 25.6616\n",
      "Epoch: 957 [  100/50000 ( 0%)]  \tLoss:   92.008202\trec:   66.682648\tkl:   25.325552\n",
      "Epoch: 957 [10100/50000 (20%)]  \tLoss:   91.640900\trec:   65.643715\tkl:   25.997185\n",
      "Epoch: 957 [20100/50000 (40%)]  \tLoss:   92.306061\trec:   66.277756\tkl:   26.028307\n",
      "Epoch: 957 [30100/50000 (60%)]  \tLoss:   95.090767\trec:   68.888756\tkl:   26.202013\n",
      "Epoch: 957 [40100/50000 (80%)]  \tLoss:   93.107674\trec:   67.613365\tkl:   25.494310\n",
      "====> Epoch: 957 Average train loss: 91.8970\n",
      "====> Validation set loss: 95.7683\n",
      "====> Validation set kl: 25.8374\n",
      "Epoch: 958 [  100/50000 ( 0%)]  \tLoss:   93.042778\trec:   66.292282\tkl:   26.750498\n",
      "Epoch: 958 [10100/50000 (20%)]  \tLoss:   94.204765\trec:   67.444473\tkl:   26.760290\n",
      "Epoch: 958 [20100/50000 (40%)]  \tLoss:   94.494331\trec:   68.682060\tkl:   25.812273\n",
      "Epoch: 958 [30100/50000 (60%)]  \tLoss:   92.358131\trec:   65.386543\tkl:   26.971586\n",
      "Epoch: 958 [40100/50000 (80%)]  \tLoss:   92.414177\trec:   66.681999\tkl:   25.732174\n",
      "====> Epoch: 958 Average train loss: 92.0103\n",
      "====> Validation set loss: 95.5421\n",
      "====> Validation set kl: 25.3370\n",
      "Epoch: 959 [  100/50000 ( 0%)]  \tLoss:   92.152763\trec:   66.793053\tkl:   25.359705\n",
      "Epoch: 959 [10100/50000 (20%)]  \tLoss:   91.232185\trec:   65.866013\tkl:   25.366171\n",
      "Epoch: 959 [20100/50000 (40%)]  \tLoss:   92.346992\trec:   66.682556\tkl:   25.664431\n",
      "Epoch: 959 [30100/50000 (60%)]  \tLoss:   91.811142\trec:   66.668823\tkl:   25.142317\n",
      "Epoch: 959 [40100/50000 (80%)]  \tLoss:   89.676064\trec:   65.049995\tkl:   24.626064\n",
      "====> Epoch: 959 Average train loss: 92.1673\n",
      "====> Validation set loss: 95.7895\n",
      "====> Validation set kl: 26.5554\n",
      "Epoch: 960 [  100/50000 ( 0%)]  \tLoss:   91.546051\trec:   64.716057\tkl:   26.830000\n",
      "Epoch: 960 [10100/50000 (20%)]  \tLoss:   90.958603\trec:   64.531754\tkl:   26.426847\n",
      "Epoch: 960 [20100/50000 (40%)]  \tLoss:   93.894844\trec:   67.895897\tkl:   25.998940\n",
      "Epoch: 960 [30100/50000 (60%)]  \tLoss:   91.289391\trec:   64.982155\tkl:   26.307238\n",
      "Epoch: 960 [40100/50000 (80%)]  \tLoss:   88.986061\trec:   63.539341\tkl:   25.446726\n",
      "====> Epoch: 960 Average train loss: 92.1785\n",
      "====> Validation set loss: 95.3708\n",
      "====> Validation set kl: 25.7527\n",
      "Epoch: 961 [  100/50000 ( 0%)]  \tLoss:   90.147133\trec:   65.141739\tkl:   25.005400\n",
      "Epoch: 961 [10100/50000 (20%)]  \tLoss:   95.706337\trec:   68.546951\tkl:   27.159386\n",
      "Epoch: 961 [20100/50000 (40%)]  \tLoss:   92.977661\trec:   66.664696\tkl:   26.312971\n",
      "Epoch: 961 [30100/50000 (60%)]  \tLoss:   90.853989\trec:   64.426826\tkl:   26.427168\n",
      "Epoch: 961 [40100/50000 (80%)]  \tLoss:   90.243423\trec:   65.168633\tkl:   25.074795\n",
      "====> Epoch: 961 Average train loss: 91.7851\n",
      "====> Validation set loss: 95.5712\n",
      "====> Validation set kl: 25.2464\n",
      "Epoch: 962 [  100/50000 ( 0%)]  \tLoss:   92.680016\trec:   68.725182\tkl:   23.954830\n",
      "Epoch: 962 [10100/50000 (20%)]  \tLoss:   93.773232\trec:   68.067047\tkl:   25.706188\n",
      "Epoch: 962 [20100/50000 (40%)]  \tLoss:   88.504509\trec:   62.836540\tkl:   25.667965\n",
      "Epoch: 962 [30100/50000 (60%)]  \tLoss:   89.510132\trec:   65.112846\tkl:   24.397285\n",
      "Epoch: 962 [40100/50000 (80%)]  \tLoss:   95.143814\trec:   68.227921\tkl:   26.915894\n",
      "====> Epoch: 962 Average train loss: 91.8837\n",
      "====> Validation set loss: 95.6716\n",
      "====> Validation set kl: 25.9245\n",
      "Epoch: 963 [  100/50000 ( 0%)]  \tLoss:   93.412750\trec:   67.228584\tkl:   26.184164\n",
      "Epoch: 963 [10100/50000 (20%)]  \tLoss:   94.835190\trec:   68.669395\tkl:   26.165802\n",
      "Epoch: 963 [20100/50000 (40%)]  \tLoss:   91.210739\trec:   66.189850\tkl:   25.020887\n",
      "Epoch: 963 [30100/50000 (60%)]  \tLoss:   93.161354\trec:   67.104774\tkl:   26.056583\n",
      "Epoch: 963 [40100/50000 (80%)]  \tLoss:   90.669960\trec:   63.965672\tkl:   26.704281\n",
      "====> Epoch: 963 Average train loss: 92.0652\n",
      "====> Validation set loss: 96.4907\n",
      "====> Validation set kl: 25.8031\n",
      "Epoch: 964 [  100/50000 ( 0%)]  \tLoss:   90.332817\trec:   65.292618\tkl:   25.040201\n",
      "Epoch: 964 [10100/50000 (20%)]  \tLoss:   89.472420\trec:   63.719585\tkl:   25.752831\n",
      "Epoch: 964 [20100/50000 (40%)]  \tLoss:   92.698814\trec:   66.382965\tkl:   26.315851\n",
      "Epoch: 964 [30100/50000 (60%)]  \tLoss:   89.987686\trec:   63.620098\tkl:   26.367590\n",
      "Epoch: 964 [40100/50000 (80%)]  \tLoss:   89.270264\trec:   63.025986\tkl:   26.244280\n",
      "====> Epoch: 964 Average train loss: 92.3815\n",
      "====> Validation set loss: 96.1957\n",
      "====> Validation set kl: 25.9047\n",
      "Epoch: 965 [  100/50000 ( 0%)]  \tLoss:   90.627441\trec:   64.688507\tkl:   25.938931\n",
      "Epoch: 965 [10100/50000 (20%)]  \tLoss:   92.688843\trec:   66.521080\tkl:   26.167761\n",
      "Epoch: 965 [20100/50000 (40%)]  \tLoss:   97.995583\trec:   71.026169\tkl:   26.969416\n",
      "Epoch: 965 [30100/50000 (60%)]  \tLoss:   95.836395\trec:   68.229057\tkl:   27.607334\n",
      "Epoch: 965 [40100/50000 (80%)]  \tLoss:   95.023888\trec:   69.336510\tkl:   25.687370\n",
      "====> Epoch: 965 Average train loss: 92.1491\n",
      "====> Validation set loss: 95.5300\n",
      "====> Validation set kl: 25.8967\n",
      "Epoch: 966 [  100/50000 ( 0%)]  \tLoss:   90.497246\trec:   64.699516\tkl:   25.797729\n",
      "Epoch: 966 [10100/50000 (20%)]  \tLoss:   90.673553\trec:   64.819962\tkl:   25.853596\n",
      "Epoch: 966 [20100/50000 (40%)]  \tLoss:   88.265350\trec:   62.376423\tkl:   25.888924\n",
      "Epoch: 966 [30100/50000 (60%)]  \tLoss:   94.206345\trec:   69.183144\tkl:   25.023203\n",
      "Epoch: 966 [40100/50000 (80%)]  \tLoss:   91.417862\trec:   65.393066\tkl:   26.024792\n",
      "====> Epoch: 966 Average train loss: 91.9709\n",
      "====> Validation set loss: 95.5298\n",
      "====> Validation set kl: 25.5007\n",
      "Epoch: 967 [  100/50000 ( 0%)]  \tLoss:   92.087166\trec:   66.724747\tkl:   25.362417\n",
      "Epoch: 967 [10100/50000 (20%)]  \tLoss:   88.700272\trec:   62.973270\tkl:   25.726997\n",
      "Epoch: 967 [20100/50000 (40%)]  \tLoss:   95.168007\trec:   69.462883\tkl:   25.705132\n",
      "Epoch: 967 [30100/50000 (60%)]  \tLoss:   92.738174\trec:   66.373268\tkl:   26.364902\n",
      "Epoch: 967 [40100/50000 (80%)]  \tLoss:   91.130196\trec:   64.757881\tkl:   26.372316\n",
      "====> Epoch: 967 Average train loss: 91.8656\n",
      "====> Validation set loss: 95.6126\n",
      "====> Validation set kl: 25.6175\n",
      "Epoch: 968 [  100/50000 ( 0%)]  \tLoss:   90.885078\trec:   64.684189\tkl:   26.200888\n",
      "Epoch: 968 [10100/50000 (20%)]  \tLoss:   91.920792\trec:   65.340630\tkl:   26.580156\n",
      "Epoch: 968 [20100/50000 (40%)]  \tLoss:   92.018730\trec:   66.290787\tkl:   25.727943\n",
      "Epoch: 968 [30100/50000 (60%)]  \tLoss:   91.086128\trec:   65.461296\tkl:   25.624836\n",
      "Epoch: 968 [40100/50000 (80%)]  \tLoss:   90.279510\trec:   65.322990\tkl:   24.956516\n",
      "====> Epoch: 968 Average train loss: 92.0353\n",
      "====> Validation set loss: 95.5053\n",
      "====> Validation set kl: 25.6323\n",
      "Epoch: 969 [  100/50000 ( 0%)]  \tLoss:   88.524200\trec:   62.730476\tkl:   25.793718\n",
      "Epoch: 969 [10100/50000 (20%)]  \tLoss:   88.335800\trec:   61.887875\tkl:   26.447924\n",
      "Epoch: 969 [20100/50000 (40%)]  \tLoss:   89.042694\trec:   63.938137\tkl:   25.104561\n",
      "Epoch: 969 [30100/50000 (60%)]  \tLoss:   93.899872\trec:   67.571220\tkl:   26.328651\n",
      "Epoch: 969 [40100/50000 (80%)]  \tLoss:   95.349342\trec:   68.780548\tkl:   26.568798\n",
      "====> Epoch: 969 Average train loss: 92.4054\n",
      "====> Validation set loss: 96.6554\n",
      "====> Validation set kl: 26.1194\n",
      "Epoch: 970 [  100/50000 ( 0%)]  \tLoss:   93.157753\trec:   66.223106\tkl:   26.934645\n",
      "Epoch: 970 [10100/50000 (20%)]  \tLoss:   92.910255\trec:   66.587463\tkl:   26.322783\n",
      "Epoch: 970 [20100/50000 (40%)]  \tLoss:   93.392403\trec:   67.656395\tkl:   25.736000\n",
      "Epoch: 970 [30100/50000 (60%)]  \tLoss:   91.826797\trec:   66.753952\tkl:   25.072845\n",
      "Epoch: 970 [40100/50000 (80%)]  \tLoss:   98.351089\trec:   71.522118\tkl:   26.828970\n",
      "====> Epoch: 970 Average train loss: 92.4297\n",
      "====> Validation set loss: 95.7481\n",
      "====> Validation set kl: 25.5828\n",
      "Epoch: 971 [  100/50000 ( 0%)]  \tLoss:   92.876152\trec:   67.062569\tkl:   25.813578\n",
      "Epoch: 971 [10100/50000 (20%)]  \tLoss:   89.747589\trec:   63.801281\tkl:   25.946306\n",
      "Epoch: 971 [20100/50000 (40%)]  \tLoss:   92.966606\trec:   66.932037\tkl:   26.034573\n",
      "Epoch: 971 [30100/50000 (60%)]  \tLoss:   96.132866\trec:   69.911331\tkl:   26.221537\n",
      "Epoch: 971 [40100/50000 (80%)]  \tLoss:   94.935760\trec:   68.203659\tkl:   26.732100\n",
      "====> Epoch: 971 Average train loss: 93.0021\n",
      "====> Validation set loss: 96.0109\n",
      "====> Validation set kl: 25.6104\n",
      "Epoch: 972 [  100/50000 ( 0%)]  \tLoss:   93.762733\trec:   67.517746\tkl:   26.244995\n",
      "Epoch: 972 [10100/50000 (20%)]  \tLoss:   91.014748\trec:   66.141167\tkl:   24.873579\n",
      "Epoch: 972 [20100/50000 (40%)]  \tLoss:   91.537361\trec:   66.626640\tkl:   24.910715\n",
      "Epoch: 972 [30100/50000 (60%)]  \tLoss:   94.578613\trec:   68.383598\tkl:   26.195015\n",
      "Epoch: 972 [40100/50000 (80%)]  \tLoss:   93.570663\trec:   67.265129\tkl:   26.305542\n",
      "====> Epoch: 972 Average train loss: 92.6039\n",
      "====> Validation set loss: 95.7336\n",
      "====> Validation set kl: 25.6673\n",
      "Epoch: 973 [  100/50000 ( 0%)]  \tLoss:   91.812706\trec:   66.725380\tkl:   25.087324\n",
      "Epoch: 973 [10100/50000 (20%)]  \tLoss:   95.862648\trec:   67.831055\tkl:   28.031591\n",
      "Epoch: 973 [20100/50000 (40%)]  \tLoss:   90.534355\trec:   65.072189\tkl:   25.462166\n",
      "Epoch: 973 [30100/50000 (60%)]  \tLoss:   91.558945\trec:   65.253059\tkl:   26.305878\n",
      "Epoch: 973 [40100/50000 (80%)]  \tLoss:   95.225952\trec:   68.401207\tkl:   26.824743\n",
      "====> Epoch: 973 Average train loss: 92.3221\n",
      "====> Validation set loss: 95.7690\n",
      "====> Validation set kl: 25.8815\n",
      "Epoch: 974 [  100/50000 ( 0%)]  \tLoss:   91.953979\trec:   65.758232\tkl:   26.195749\n",
      "Epoch: 974 [10100/50000 (20%)]  \tLoss:   90.051369\trec:   64.372261\tkl:   25.679104\n",
      "Epoch: 974 [20100/50000 (40%)]  \tLoss:   91.426071\trec:   65.585281\tkl:   25.840790\n",
      "Epoch: 974 [30100/50000 (60%)]  \tLoss:   90.282188\trec:   64.103806\tkl:   26.178375\n",
      "Epoch: 974 [40100/50000 (80%)]  \tLoss:   91.311172\trec:   65.394127\tkl:   25.917048\n",
      "====> Epoch: 974 Average train loss: 91.9850\n",
      "====> Validation set loss: 95.9261\n",
      "====> Validation set kl: 25.7635\n",
      "Epoch: 975 [  100/50000 ( 0%)]  \tLoss:   89.854980\trec:   65.236465\tkl:   24.618505\n",
      "Epoch: 975 [10100/50000 (20%)]  \tLoss:   88.417076\trec:   61.893589\tkl:   26.523493\n",
      "Epoch: 975 [20100/50000 (40%)]  \tLoss:   87.342117\trec:   62.156075\tkl:   25.186043\n",
      "Epoch: 975 [30100/50000 (60%)]  \tLoss:   93.356796\trec:   67.058342\tkl:   26.298458\n",
      "Epoch: 975 [40100/50000 (80%)]  \tLoss:   92.939255\trec:   66.957863\tkl:   25.981394\n",
      "====> Epoch: 975 Average train loss: 92.1766\n",
      "====> Validation set loss: 95.8890\n",
      "====> Validation set kl: 25.9583\n",
      "Epoch: 976 [  100/50000 ( 0%)]  \tLoss:   92.125763\trec:   66.233711\tkl:   25.892040\n",
      "Epoch: 976 [10100/50000 (20%)]  \tLoss:   90.637245\trec:   64.758354\tkl:   25.878885\n",
      "Epoch: 976 [20100/50000 (40%)]  \tLoss:   97.511482\trec:   71.351875\tkl:   26.159607\n",
      "Epoch: 976 [30100/50000 (60%)]  \tLoss:   92.251869\trec:   66.182281\tkl:   26.069593\n",
      "Epoch: 976 [40100/50000 (80%)]  \tLoss:   92.285103\trec:   66.734489\tkl:   25.550615\n",
      "====> Epoch: 976 Average train loss: 92.4112\n",
      "====> Validation set loss: 95.5616\n",
      "====> Validation set kl: 25.9134\n",
      "Epoch: 977 [  100/50000 ( 0%)]  \tLoss:   93.545143\trec:   68.578209\tkl:   24.966938\n",
      "Epoch: 977 [10100/50000 (20%)]  \tLoss:   86.032417\trec:   60.655121\tkl:   25.377296\n",
      "Epoch: 977 [20100/50000 (40%)]  \tLoss:   92.442497\trec:   67.471420\tkl:   24.971083\n",
      "Epoch: 977 [30100/50000 (60%)]  \tLoss:   93.809624\trec:   67.936325\tkl:   25.873304\n",
      "Epoch: 977 [40100/50000 (80%)]  \tLoss:   93.131035\trec:   66.675064\tkl:   26.455963\n",
      "====> Epoch: 977 Average train loss: 92.0929\n",
      "====> Validation set loss: 95.4847\n",
      "====> Validation set kl: 26.1837\n",
      "Epoch: 978 [  100/50000 ( 0%)]  \tLoss:   89.735756\trec:   64.007072\tkl:   25.728689\n",
      "Epoch: 978 [10100/50000 (20%)]  \tLoss:   91.781654\trec:   66.137085\tkl:   25.644571\n",
      "Epoch: 978 [20100/50000 (40%)]  \tLoss:   91.303360\trec:   65.265404\tkl:   26.037958\n",
      "Epoch: 978 [30100/50000 (60%)]  \tLoss:   93.521385\trec:   67.263885\tkl:   26.257502\n",
      "Epoch: 978 [40100/50000 (80%)]  \tLoss:   92.457169\trec:   67.160591\tkl:   25.296572\n",
      "====> Epoch: 978 Average train loss: 92.1427\n",
      "====> Validation set loss: 95.4399\n",
      "====> Validation set kl: 25.9280\n",
      "Epoch: 979 [  100/50000 ( 0%)]  \tLoss:   91.935898\trec:   65.968155\tkl:   25.967739\n",
      "Epoch: 979 [10100/50000 (20%)]  \tLoss:   92.108788\trec:   67.022240\tkl:   25.086550\n",
      "Epoch: 979 [20100/50000 (40%)]  \tLoss:   91.145470\trec:   65.561401\tkl:   25.584059\n",
      "Epoch: 979 [30100/50000 (60%)]  \tLoss:   93.299843\trec:   68.011757\tkl:   25.288080\n",
      "Epoch: 979 [40100/50000 (80%)]  \tLoss:   96.526772\trec:   70.260849\tkl:   26.265926\n",
      "====> Epoch: 979 Average train loss: 92.0596\n",
      "====> Validation set loss: 95.3925\n",
      "====> Validation set kl: 25.6772\n",
      "Epoch: 980 [  100/50000 ( 0%)]  \tLoss:   94.400879\trec:   68.533615\tkl:   25.867262\n",
      "Epoch: 980 [10100/50000 (20%)]  \tLoss:   92.272293\trec:   65.920113\tkl:   26.352180\n",
      "Epoch: 980 [20100/50000 (40%)]  \tLoss:   90.921051\trec:   64.941841\tkl:   25.979218\n",
      "Epoch: 980 [30100/50000 (60%)]  \tLoss:   94.890068\trec:   68.218399\tkl:   26.671667\n",
      "Epoch: 980 [40100/50000 (80%)]  \tLoss:   88.526978\trec:   62.745007\tkl:   25.781975\n",
      "====> Epoch: 980 Average train loss: 92.0996\n",
      "====> Validation set loss: 95.9909\n",
      "====> Validation set kl: 26.5742\n",
      "Epoch: 981 [  100/50000 ( 0%)]  \tLoss:   92.213005\trec:   65.589325\tkl:   26.623680\n",
      "Epoch: 981 [10100/50000 (20%)]  \tLoss:   89.865341\trec:   63.925510\tkl:   25.939827\n",
      "Epoch: 981 [20100/50000 (40%)]  \tLoss:   88.872635\trec:   64.024277\tkl:   24.848364\n",
      "Epoch: 981 [30100/50000 (60%)]  \tLoss:   92.838043\trec:   66.610634\tkl:   26.227413\n",
      "Epoch: 981 [40100/50000 (80%)]  \tLoss:   91.363869\trec:   65.355171\tkl:   26.008696\n",
      "====> Epoch: 981 Average train loss: 91.9175\n",
      "====> Validation set loss: 95.5014\n",
      "====> Validation set kl: 26.0035\n",
      "Epoch: 982 [  100/50000 ( 0%)]  \tLoss:   93.707863\trec:   67.364403\tkl:   26.343454\n",
      "Epoch: 982 [10100/50000 (20%)]  \tLoss:   90.734970\trec:   64.649940\tkl:   26.085030\n",
      "Epoch: 982 [20100/50000 (40%)]  \tLoss:   90.999428\trec:   65.617432\tkl:   25.382002\n",
      "Epoch: 982 [30100/50000 (60%)]  \tLoss:   89.095055\trec:   62.736057\tkl:   26.358999\n",
      "Epoch: 982 [40100/50000 (80%)]  \tLoss:   97.936958\trec:   71.720703\tkl:   26.216257\n",
      "====> Epoch: 982 Average train loss: 92.1654\n",
      "====> Validation set loss: 95.7320\n",
      "====> Validation set kl: 26.2201\n",
      "Epoch: 983 [  100/50000 ( 0%)]  \tLoss:   94.079994\trec:   67.348442\tkl:   26.731554\n",
      "Epoch: 983 [10100/50000 (20%)]  \tLoss:   93.120132\trec:   68.320641\tkl:   24.799494\n",
      "Epoch: 983 [20100/50000 (40%)]  \tLoss:   88.082695\trec:   62.025761\tkl:   26.056931\n",
      "Epoch: 983 [30100/50000 (60%)]  \tLoss:   87.180534\trec:   61.675209\tkl:   25.505329\n",
      "Epoch: 983 [40100/50000 (80%)]  \tLoss:   89.825073\trec:   64.337967\tkl:   25.487106\n",
      "====> Epoch: 983 Average train loss: 91.9918\n",
      "====> Validation set loss: 95.5994\n",
      "====> Validation set kl: 25.8279\n",
      "Epoch: 984 [  100/50000 ( 0%)]  \tLoss:   92.228897\trec:   65.789459\tkl:   26.439432\n",
      "Epoch: 984 [10100/50000 (20%)]  \tLoss:   91.925034\trec:   66.651711\tkl:   25.273323\n",
      "Epoch: 984 [20100/50000 (40%)]  \tLoss:   92.000908\trec:   65.928665\tkl:   26.072239\n",
      "Epoch: 984 [30100/50000 (60%)]  \tLoss:   92.976349\trec:   68.041939\tkl:   24.934399\n",
      "Epoch: 984 [40100/50000 (80%)]  \tLoss:   93.680016\trec:   67.438530\tkl:   26.241484\n",
      "====> Epoch: 984 Average train loss: 91.7745\n",
      "====> Validation set loss: 95.6163\n",
      "====> Validation set kl: 25.6039\n",
      "Epoch: 985 [  100/50000 ( 0%)]  \tLoss:   88.648727\trec:   63.460293\tkl:   25.188440\n",
      "Epoch: 985 [10100/50000 (20%)]  \tLoss:   87.472198\trec:   63.313747\tkl:   24.158443\n",
      "Epoch: 985 [20100/50000 (40%)]  \tLoss:   93.038620\trec:   66.204727\tkl:   26.833893\n",
      "Epoch: 985 [30100/50000 (60%)]  \tLoss:   90.790253\trec:   64.214302\tkl:   26.575941\n",
      "Epoch: 985 [40100/50000 (80%)]  \tLoss:   90.716347\trec:   66.394402\tkl:   24.321943\n",
      "====> Epoch: 985 Average train loss: 91.9600\n",
      "====> Validation set loss: 95.7280\n",
      "====> Validation set kl: 25.7155\n",
      "Epoch: 986 [  100/50000 ( 0%)]  \tLoss:   86.800056\trec:   61.991074\tkl:   24.808979\n",
      "Epoch: 986 [10100/50000 (20%)]  \tLoss:   90.339020\trec:   65.026466\tkl:   25.312548\n",
      "Epoch: 986 [20100/50000 (40%)]  \tLoss:   92.681206\trec:   66.621017\tkl:   26.060200\n",
      "Epoch: 986 [30100/50000 (60%)]  \tLoss:   90.829742\trec:   65.411232\tkl:   25.418518\n",
      "Epoch: 986 [40100/50000 (80%)]  \tLoss:   88.323601\trec:   63.057556\tkl:   25.266045\n",
      "====> Epoch: 986 Average train loss: 92.1043\n",
      "====> Validation set loss: 95.3616\n",
      "====> Validation set kl: 25.7983\n",
      "Epoch: 987 [  100/50000 ( 0%)]  \tLoss:   93.513458\trec:   67.451553\tkl:   26.061899\n",
      "Epoch: 987 [10100/50000 (20%)]  \tLoss:   93.363182\trec:   66.704041\tkl:   26.659145\n",
      "Epoch: 987 [20100/50000 (40%)]  \tLoss:   90.671318\trec:   64.569023\tkl:   26.102297\n",
      "Epoch: 987 [30100/50000 (60%)]  \tLoss:   91.380341\trec:   65.590683\tkl:   25.789658\n",
      "Epoch: 987 [40100/50000 (80%)]  \tLoss:   91.152695\trec:   65.650124\tkl:   25.502565\n",
      "====> Epoch: 987 Average train loss: 91.9505\n",
      "====> Validation set loss: 96.4042\n",
      "====> Validation set kl: 26.2313\n",
      "Epoch: 988 [  100/50000 ( 0%)]  \tLoss:   92.261101\trec:   66.150597\tkl:   26.110510\n",
      "Epoch: 988 [10100/50000 (20%)]  \tLoss:   92.000748\trec:   66.235336\tkl:   25.765411\n",
      "Epoch: 988 [20100/50000 (40%)]  \tLoss:   94.380074\trec:   68.043053\tkl:   26.337027\n",
      "Epoch: 988 [30100/50000 (60%)]  \tLoss:   99.931015\trec:   72.918045\tkl:   27.012966\n",
      "Epoch: 988 [40100/50000 (80%)]  \tLoss:   93.593628\trec:   67.170410\tkl:   26.423227\n",
      "====> Epoch: 988 Average train loss: 92.5626\n",
      "====> Validation set loss: 95.8771\n",
      "====> Validation set kl: 25.4081\n",
      "Epoch: 989 [  100/50000 ( 0%)]  \tLoss:   93.898140\trec:   67.860191\tkl:   26.037949\n",
      "Epoch: 989 [10100/50000 (20%)]  \tLoss:   94.074638\trec:   68.288841\tkl:   25.785793\n",
      "Epoch: 989 [20100/50000 (40%)]  \tLoss:   88.985641\trec:   64.148018\tkl:   24.837620\n",
      "Epoch: 989 [30100/50000 (60%)]  \tLoss:   90.961235\trec:   65.390129\tkl:   25.571110\n",
      "Epoch: 989 [40100/50000 (80%)]  \tLoss:   93.572090\trec:   66.950241\tkl:   26.621840\n",
      "====> Epoch: 989 Average train loss: 92.3141\n",
      "====> Validation set loss: 96.6356\n",
      "====> Validation set kl: 26.2120\n",
      "Epoch: 990 [  100/50000 ( 0%)]  \tLoss:   90.699043\trec:   65.223846\tkl:   25.475193\n",
      "Epoch: 990 [10100/50000 (20%)]  \tLoss:   90.713364\trec:   64.454918\tkl:   26.258444\n",
      "Epoch: 990 [20100/50000 (40%)]  \tLoss:   93.275749\trec:   66.343033\tkl:   26.932716\n",
      "Epoch: 990 [30100/50000 (60%)]  \tLoss:   89.676514\trec:   64.016533\tkl:   25.659977\n",
      "Epoch: 990 [40100/50000 (80%)]  \tLoss:   95.178604\trec:   67.986588\tkl:   27.192009\n",
      "====> Epoch: 990 Average train loss: 92.2059\n",
      "====> Validation set loss: 96.2644\n",
      "====> Validation set kl: 25.3054\n",
      "Epoch: 991 [  100/50000 ( 0%)]  \tLoss:   98.683281\trec:   72.227638\tkl:   26.455639\n",
      "Epoch: 991 [10100/50000 (20%)]  \tLoss:   92.416695\trec:   66.646553\tkl:   25.770151\n",
      "Epoch: 991 [20100/50000 (40%)]  \tLoss:   93.840973\trec:   67.006287\tkl:   26.834684\n",
      "Epoch: 991 [30100/50000 (60%)]  \tLoss:   91.569199\trec:   65.660103\tkl:   25.909096\n",
      "Epoch: 991 [40100/50000 (80%)]  \tLoss:   96.912849\trec:   71.174400\tkl:   25.738449\n",
      "====> Epoch: 991 Average train loss: 92.7395\n",
      "====> Validation set loss: 96.1946\n",
      "====> Validation set kl: 25.5411\n",
      "Epoch: 992 [  100/50000 ( 0%)]  \tLoss:   87.882378\trec:   63.725624\tkl:   24.156763\n",
      "Epoch: 992 [10100/50000 (20%)]  \tLoss:   88.559647\trec:   64.205147\tkl:   24.354506\n",
      "Epoch: 992 [20100/50000 (40%)]  \tLoss:   92.072090\trec:   66.416496\tkl:   25.655596\n",
      "Epoch: 992 [30100/50000 (60%)]  \tLoss:   91.553535\trec:   66.254852\tkl:   25.298679\n",
      "Epoch: 992 [40100/50000 (80%)]  \tLoss:   94.130409\trec:   68.027046\tkl:   26.103363\n",
      "====> Epoch: 992 Average train loss: 92.4532\n",
      "====> Validation set loss: 95.8055\n",
      "====> Validation set kl: 25.4305\n",
      "Epoch: 993 [  100/50000 ( 0%)]  \tLoss:   89.711586\trec:   64.977760\tkl:   24.733826\n",
      "Epoch: 993 [10100/50000 (20%)]  \tLoss:   94.759995\trec:   68.671722\tkl:   26.088278\n",
      "Epoch: 993 [20100/50000 (40%)]  \tLoss:   89.724503\trec:   64.258224\tkl:   25.466276\n",
      "Epoch: 993 [30100/50000 (60%)]  \tLoss:   89.868111\trec:   65.151680\tkl:   24.716433\n",
      "Epoch: 993 [40100/50000 (80%)]  \tLoss:   92.155556\trec:   66.401955\tkl:   25.753603\n",
      "====> Epoch: 993 Average train loss: 92.3688\n",
      "====> Validation set loss: 96.6111\n",
      "====> Validation set kl: 26.8690\n",
      "Epoch: 994 [  100/50000 ( 0%)]  \tLoss:   86.308739\trec:   60.268211\tkl:   26.040524\n",
      "Epoch: 994 [10100/50000 (20%)]  \tLoss:   94.704323\trec:   68.506218\tkl:   26.198103\n",
      "Epoch: 994 [20100/50000 (40%)]  \tLoss:   93.827187\trec:   68.134529\tkl:   25.692656\n",
      "Epoch: 994 [30100/50000 (60%)]  \tLoss:   97.801315\trec:   70.167999\tkl:   27.633320\n",
      "Epoch: 994 [40100/50000 (80%)]  \tLoss:   87.412178\trec:   63.658245\tkl:   23.753933\n",
      "====> Epoch: 994 Average train loss: 92.5687\n",
      "====> Validation set loss: 96.2211\n",
      "====> Validation set kl: 25.9210\n",
      "Epoch: 995 [  100/50000 ( 0%)]  \tLoss:   94.376190\trec:   67.656342\tkl:   26.719843\n",
      "Epoch: 995 [10100/50000 (20%)]  \tLoss:   97.417534\trec:   70.474411\tkl:   26.943129\n",
      "Epoch: 995 [20100/50000 (40%)]  \tLoss:   96.296402\trec:   70.300415\tkl:   25.995981\n",
      "Epoch: 995 [30100/50000 (60%)]  \tLoss:   93.118217\trec:   68.625046\tkl:   24.493176\n",
      "Epoch: 995 [40100/50000 (80%)]  \tLoss:   93.043556\trec:   67.233269\tkl:   25.810287\n",
      "====> Epoch: 995 Average train loss: 92.9628\n",
      "====> Validation set loss: 96.8697\n",
      "====> Validation set kl: 25.7902\n",
      "Epoch: 996 [  100/50000 ( 0%)]  \tLoss:   94.548531\trec:   68.520210\tkl:   26.028320\n",
      "Epoch: 996 [10100/50000 (20%)]  \tLoss:   90.669411\trec:   64.467499\tkl:   26.201912\n",
      "Epoch: 996 [20100/50000 (40%)]  \tLoss:   90.173042\trec:   64.551231\tkl:   25.621817\n",
      "Epoch: 996 [30100/50000 (60%)]  \tLoss:   89.701660\trec:   64.658089\tkl:   25.043571\n",
      "Epoch: 996 [40100/50000 (80%)]  \tLoss:   94.686493\trec:   68.293877\tkl:   26.392616\n",
      "====> Epoch: 996 Average train loss: 92.8239\n",
      "====> Validation set loss: 97.1947\n",
      "====> Validation set kl: 25.5057\n",
      "Epoch: 997 [  100/50000 ( 0%)]  \tLoss:   91.693893\trec:   66.134796\tkl:   25.559095\n",
      "Epoch: 997 [10100/50000 (20%)]  \tLoss:   91.629234\trec:   65.888542\tkl:   25.740696\n",
      "Epoch: 997 [20100/50000 (40%)]  \tLoss:   95.103027\trec:   68.724968\tkl:   26.378059\n",
      "Epoch: 997 [30100/50000 (60%)]  \tLoss:   92.234268\trec:   65.933052\tkl:   26.301218\n",
      "Epoch: 997 [40100/50000 (80%)]  \tLoss:   93.153458\trec:   67.277710\tkl:   25.875744\n",
      "====> Epoch: 997 Average train loss: 92.5453\n",
      "====> Validation set loss: 96.0201\n",
      "====> Validation set kl: 25.6612\n",
      "Epoch: 998 [  100/50000 ( 0%)]  \tLoss:   93.237518\trec:   67.323730\tkl:   25.913784\n",
      "Epoch: 998 [10100/50000 (20%)]  \tLoss:   89.750145\trec:   63.825329\tkl:   25.924812\n",
      "Epoch: 998 [20100/50000 (40%)]  \tLoss:   94.662140\trec:   69.530197\tkl:   25.131945\n",
      "Epoch: 998 [30100/50000 (60%)]  \tLoss:   88.038338\trec:   61.954102\tkl:   26.084240\n",
      "Epoch: 998 [40100/50000 (80%)]  \tLoss:   95.696510\trec:   69.989166\tkl:   25.707346\n",
      "====> Epoch: 998 Average train loss: 92.7495\n",
      "====> Validation set loss: 96.0785\n",
      "====> Validation set kl: 25.7101\n",
      "Epoch: 999 [  100/50000 ( 0%)]  \tLoss:   92.646606\trec:   66.842171\tkl:   25.804438\n",
      "Epoch: 999 [10100/50000 (20%)]  \tLoss:   92.818260\trec:   66.700485\tkl:   26.117775\n",
      "Epoch: 999 [20100/50000 (40%)]  \tLoss:   91.348595\trec:   66.652489\tkl:   24.696102\n",
      "Epoch: 999 [30100/50000 (60%)]  \tLoss:   93.007179\trec:   66.641541\tkl:   26.365631\n",
      "Epoch: 999 [40100/50000 (80%)]  \tLoss:   92.084373\trec:   66.000603\tkl:   26.083775\n",
      "====> Epoch: 999 Average train loss: 92.7628\n",
      "====> Validation set loss: 96.0772\n",
      "====> Validation set kl: 26.2732\n",
      "Epoch: 1000 [  100/50000 ( 0%)]  \tLoss:   87.804665\trec:   62.252361\tkl:   25.552303\n",
      "Epoch: 1000 [10100/50000 (20%)]  \tLoss:   91.591072\trec:   65.814789\tkl:   25.776279\n",
      "Epoch: 1000 [20100/50000 (40%)]  \tLoss:   90.639809\trec:   64.504318\tkl:   26.135494\n",
      "Epoch: 1000 [30100/50000 (60%)]  \tLoss:   91.297852\trec:   65.873566\tkl:   25.424290\n",
      "Epoch: 1000 [40100/50000 (80%)]  \tLoss:   98.457695\trec:   72.016289\tkl:   26.441410\n",
      "====> Epoch: 1000 Average train loss: 92.7555\n",
      "====> Validation set loss: 96.7229\n",
      "====> Validation set kl: 25.5522\n",
      "====> Validation set loss: 96.6496\n",
      "====> Validation set kl: 25.5790\n",
      "Computing log-likelihood on test set\n",
      "Progress: 0.00%\n",
      "Progress: 10.00%\n",
      "Progress: 20.00%\n",
      "Progress: 30.00%\n",
      "Progress: 40.00%\n",
      "Progress: 50.00%\n",
      "Progress: 60.00%\n",
      "Progress: 70.00%\n",
      "Progress: 80.00%\n",
      "Progress: 90.00%\n",
      "====> Test set loss: 95.7262\n",
      "====> Test set kl: 25.4610\n",
      "====> Test set log-likelihood: 89.5599\n"
     ]
    }
   ],
   "source": [
    "%run main_experiment_VAE.py -nf 20"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "k20_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
